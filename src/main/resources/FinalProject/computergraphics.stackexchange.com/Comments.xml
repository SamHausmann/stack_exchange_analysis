<?xml version="1.0" encoding="utf-8"?>
<comments>
  <row Id="1" PostId="4" Score="2" Text="You could post the relevant shader code here in case of bitrot, no?" CreationDate="2015-08-04T18:17:21.623" UserId="38" />
  <row Id="2" PostId="3" Score="6" Text="Really, it depends on many factors. NVIDIA provides hardware acceleration for vector graphics. Have you seen it? https://developer.nvidia.com/nv-path-rendering-videos" CreationDate="2015-08-04T18:18:45.080" UserId="36" />
  <row Id="3" PostId="4" Score="1" Text="we should have some prettier code markup for shader code, i'll post on meta if someone hasn't beaten me to it!" CreationDate="2015-08-04T18:21:29.027" UserId="56" />
  <row Id="4" PostId="5" Score="0" Text="A crude solution would be to just paint one contrasting color onto another through the stencil, save that buffer out, and count the number of pixels that were altered." CreationDate="2015-08-04T18:28:55.233" UserId="36" />
  <row Id="5" PostId="5" Score="0" Text="Hmm, the specification says *occulsion queries* count the number of fragments that pass the *depth test*, but off the top of my head I'm not sure how that interacts with the stencil test right now." CreationDate="2015-08-04T18:33:09.697" UserId="6" />
  <row Id="6" PostId="6" Score="6" Text="I'd want more than &quot;A recommendation of a solid book&quot; as this question should be given a direct answer by someone who knows about this. We should try to put information on the site instead of pointing to it off-site." CreationDate="2015-08-04T18:34:42.330" UserId="27" />
  <row Id="7" PostId="6" Score="0" Text="@robobenklein Question edited, thought better be careful ;)" CreationDate="2015-08-04T18:37:46.930" UserId="18" />
  <row Id="11" PostId="6" Score="0" Text="@ChristianRau Removing &quot;Thanks&quot; should discussed in a meta, this is different on every stack exchange site..." CreationDate="2015-08-04T18:41:36.920" UserId="18" />
  <row Id="12" PostId="6" Score="1" Text="@poor No, it isn't really. That's something that nowhere ever changes. And as long as we don't have special rules, we employ the general SE ones anyway. But even then, I can hardly imagine anyone for voting this to be allowed here. I've never seen this being good practice on any other SE site." CreationDate="2015-08-04T18:43:15.267" UserId="6" />
  <row Id="18" PostId="10" Score="1" Text="See also http://www.valvesoftware.com/publications/2007/SIGGRAPH2007_AlphaTestedMagnification.pdf" CreationDate="2015-08-04T19:00:48.043" UserId="34" />
  <row Id="19" PostId="10" Score="0" Text="yeah that's a very cool technique, that's the second technique i mentioned, and i link to that same pdf above as well." CreationDate="2015-08-04T19:02:10.423" UserId="56" />
  <row Id="20" PostId="10" Score="0" Text="Woops, missed it, sorry." CreationDate="2015-08-04T19:03:04.390" UserId="34" />
  <row Id="24" PostId="10" Score="1" Text="An adaptive solution for the edge anti-aliasing of distance fields can be found here: http://www.essentialmath.com/blog/?p=151." CreationDate="2015-08-04T19:35:15.710" UserId="1" />
  <row Id="28" PostId="19" Score="2" Text="I'm voting to close this question as off-topic because it doesn't pertain to computer graphics programming specifically." CreationDate="2015-08-04T19:43:58.817" UserId="71" />
  <row Id="29" PostId="19" Score="0" Text="VTC for more visibility; should be discussed. (Although I would love to know the answer either way)." CreationDate="2015-08-04T19:44:27.143" UserId="71" />
  <row Id="30" PostId="19" Score="3" Text="@Qix [Discussion on Meta.](http://meta.computergraphics.stackexchange.com/q/23/16)" CreationDate="2015-08-04T19:46:55.293" UserId="16" />
  <row Id="32" PostId="21" Score="0" Text="It's one character, but `cleaver` -&gt; `clever`. I can't edit it D: Great answer by the way, this makes the most sense so far." CreationDate="2015-08-04T19:54:51.953" UserId="71" />
  <row Id="33" PostId="21" Score="1" Text="@Qix, haha, good catch, thanks!" CreationDate="2015-08-04T19:57:54.950" UserId="54" />
  <row Id="34" PostId="25" Score="0" Text="I will check those right away ! Thanks !" CreationDate="2015-08-04T20:27:28.023" UserId="116" />
  <row Id="35" PostId="25" Score="0" Text="Additionally I also sometimes have a command that will dump a specified fbo out to an image when I press a key. It's the low-tech way." CreationDate="2015-08-04T20:33:29.797" UserId="125" />
  <row Id="36" PostId="27" Score="0" Text="This article is much more &quot;gamer focused&quot; but it might give you some insights... http://www.pcgamer.com/what-directx-12-means-for-gamers-and-developers/" CreationDate="2015-08-04T22:18:49.450" UserId="54" />
  <row Id="37" PostId="5" Score="0" Text="@ChristianRau It seems that only the fragments that pass the depth tests will be counted, but stencil, discard and alpha tests are ignored." CreationDate="2015-08-04T22:54:39.427" UserId="64" />
  <row Id="38" PostId="31" Score="1" Text="Just curious, what is your reason for wanting jpg, is it the smaller file size? In case it helps your specific situation, do you know about dxt compression or distance field textures?" CreationDate="2015-08-05T03:20:34.207" UserId="56" />
  <row Id="39" PostId="15" Score="0" Text="At SIGGRAPH 2014, in advances in real-time rendering, there was talk of subdivision used in call of duty. I don't remember the specifics but there is probably some good info there for you!" CreationDate="2015-08-05T03:21:46.453" UserId="56" />
  <row Id="40" PostId="40" Score="1" Text="Looking at your other answer, it looks like your math background is better than I was working with, but I hope that it still goes into sufficient detail to be helpful. I wanted it to be useful for people of any background coming into this." CreationDate="2015-08-05T04:34:09.823" UserId="174" />
  <row Id="41" PostId="40" Score="1" Text="If you are talking to me, not at all.  Your answer and Bert's are amazingly enlightening.  Thank you so much!  Gotta digest the info a bit now (:" CreationDate="2015-08-05T05:03:31.397" UserId="56" />
  <row Id="42" PostId="38" Score="3" Text="Really interesting question, and though I suspect most of the nitty-gritty details are secret sauce, this might be a good jumping off point for anyone who wants to do the research and write up a summary: http://blog.imgtec.com/powervr/a-look-at-the-powervr-graphics-architecture-tile-based-rendering" CreationDate="2015-08-05T06:05:53.240" UserId="196" />
  <row Id="43" PostId="11" Score="5" Text="I don't know for certain, but I would definitely suspect that glTexSubImage on a texture that has been rendered in the last frame or two will stall the pipeline, since PC drivers often try to buffer up a frame or two, and are not likely to want to make copies of entire textures because of a tiny update. So I would expect double- or triple-buffering the textures (or pixel buffer objects) to be required for maximum performance." CreationDate="2015-08-05T06:14:42.460" UserId="196" />
  <row Id="44" PostId="5" Score="2" Text="@ChristianRau and Maurice, the original [ARB_occlusion_query](https://www.opengl.org/registry/specs/ARB/occlusion_query.txt) spec explicitly says it counts samples passing both depth *and* stencil tests, though. See also [this StackOverflow question](https://stackoverflow.com/questions/26410637/occlusion-queries-with-stencil-test-only)." CreationDate="2015-08-05T06:19:36.760" UserId="48" />
  <row Id="45" PostId="15" Score="4" Text="This sounds like a question best answered by a survey paper on subdivision surfaces, and indeed, searching Google for &quot;subdivison surfaces survey&quot; brings up a number of relevant publications. For example, &quot;Algorithms for direct evaluation [Sta98, ZK02], editing [BKZ01, BMBZ02, BMZB02, BLZ00], texturing [PB00], and conversion&#xA;to other popular representations [Pet00] have been devised and hardware support for rendering of subdivision surfaces has been proposed [BAD+01, BKS00, PS96]&quot; —[Boier-Martin et al., 2005](http://mrl.nyu.edu/~dzorin/papers/boiermartin2005sbt.pdf)." CreationDate="2015-08-05T07:09:06.853" UserId="106" />
  <row Id="46" PostId="15" Score="1" Text="&quot;We also examine the reason for the low adoption of new schemes with theoretical advantages, [and] explain why Catmull–Clark surfaces have become a de facto standard in geometric modelling&quot; —[Cashman, 2011](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2011.02083.x/abstract)." CreationDate="2015-08-05T07:12:13.487" UserId="106" />
  <row Id="48" PostId="31" Score="1" Text="It's unclear to me what your question is. Do you want to know whether it's good to compress using JPEG? Do you want to know what kinds of images compress well with JPEG? Or, are you already using JPEG and you'd like to know how to author your images in order to minimize the artifacts caused by JPEG?" CreationDate="2015-08-05T07:31:18.370" UserId="197" />
  <row Id="49" PostId="28" Score="0" Text="As an extra note OpenGL generally only works on one thread so a graphics intensive app could max out one core. Something like Vulkan allows multiple threads to dispatch commands to a queue which means many graphics calls can be made from multiple threads." CreationDate="2015-08-05T07:46:37.030" UserId="214" />
  <row Id="50" PostId="51" Score="0" Text="Maybe triplanar projection, [as seen in GPU Gems 3](http://http.developer.nvidia.com/GPUGems3/gpugems3_ch01.html)? The question is whether the noise would look blurry or otherwise undesirable when it's blending between different projections." CreationDate="2015-08-05T08:37:33.923" UserId="48" />
  <row Id="53" PostId="52" Score="1" Text="It's also easy to generate an SDF from a voxel model." CreationDate="2015-08-05T09:10:38.670" UserId="48" />
  <row Id="54" PostId="31" Score="0" Text="@AlanWolfe I have encountered a few occasions that I was only able to use JPEG (mostly in web apps), and that's why I needed it to be in JPEG. Thanks, but I was not familiar with dxt compression nor distance field textures, from what I have seen in [wikipedia](https://en.wikipedia.org/wiki/S3_Texture_Compression), dxt algorithms are different from the ones used in JPEG, do you mean they can be used to create JPEGs?" CreationDate="2015-08-05T10:23:18.237" UserId="157" />
  <row Id="55" PostId="31" Score="0" Text="@Moshoka thanks, it's more related to your last question: how to minimize the &quot;visual impact&quot; brought by artifacts in JPEG images?" CreationDate="2015-08-05T10:24:46.870" UserId="157" />
  <row Id="56" PostId="48" Score="0" Text="Thank you very much for your detailed explanation Nathan +1. I understand the compression algorithms have their limitations for JPEG, but I just wanted to find out if there's a right balance of the amount of compression together with other options (when saving it) that can make the artifacts less noticeable. I edited my question with samples." CreationDate="2015-08-05T10:51:02.013" UserId="157" />
  <row Id="57" PostId="57" Score="0" Text="Are you looking for a more accurate way to sample from a distribution, without montecarlo test i mean? If you don't care much about the computational approach (i.e. you're looking for an accurate approach rather than the computational effort) i could have a solution, but it could be optimized of course." CreationDate="2015-08-05T11:33:16.990" UserId="228" />
  <row Id="58" PostId="57" Score="3" Text="Do you know the analytical function or can you only sample it? Do you know its analytical derivative?" CreationDate="2015-08-05T12:04:10.887" UserId="182" />
  <row Id="60" PostId="5" Score="0" Text="@NathanReed Sounds like you're about to write an answer." CreationDate="2015-08-05T14:00:55.260" UserId="6" />
  <row Id="61" PostId="57" Score="0" Text="@JulienGuertault Does my edit clarify?" CreationDate="2015-08-05T14:07:56.533" UserId="231" />
  <row Id="62" PostId="57" Score="0" Text="@Lukkio I'd like accuracy first, then optimisation can come later once the approach is working." CreationDate="2015-08-05T14:08:44.487" UserId="231" />
  <row Id="63" PostId="29" Score="0" Text="Although all cubic splines are, in a sense, equivalent, it's probably conceptually easier to use Catmull-Rom splines. e.g.  https://www.cs.cmu.edu/~462/projects/assn2/assn2/catmullRom.pdf" CreationDate="2015-08-05T14:51:27.773" UserId="209" />
  <row Id="64" PostId="63" Score="0" Text="So, if `dFdx(p) = p(x1) - p(x)`, then `x1` can be either `(x+1)` or `(x-1)`, depending on the position of pixel `x` in the quad. Either way, `x1` has to be in the same warp/wavefront as `x`. Am I correct?" CreationDate="2015-08-05T15:44:31.643" UserId="88" />
  <row Id="65" PostId="63" Score="3" Text="@ApoorvaJ Essentially the same value for `dFdx` is computed for each of the 2 neighbouring pixels in the 2x2 grid. And this value is just computed using the difference between the two neighbor values, if that is `p(x+1)-p(x)` or `p(x)-p(x-1)` just depends on your notion of what `x` is exactly here. The result is the same however. So yeah, you're correct." CreationDate="2015-08-05T15:46:43.313" UserId="6" />
  <row Id="66" PostId="63" Score="0" Text="@ChristianRau That answers my question. Thanks." CreationDate="2015-08-05T15:47:59.020" UserId="88" />
  <row Id="67" PostId="29" Score="0" Text="Do you think the tau parameter helps or hinders in this case?  I might be wrong but i feel like catmull rom is too &quot;user defined&quot; (and must be tuned), whereas the hermite spline attempts to just use information from the data that's there.  It seems like cubic hermite is closer to a &quot;ground truth&quot;, which i guess would be something like an ideal sinc filter.  What do you think though?" CreationDate="2015-08-05T15:57:32.830" UserId="56" />
  <row Id="68" PostId="53" Score="0" Text="So my question is how do we do this on modern GPUs? I don't think there's any way to query the scanout, and it seems to me you can't really submit per-scanline draw calls. Even if you could -- what guarantees do you have that your draws are going to get there before the scanout?" CreationDate="2015-08-05T16:10:53.227" UserId="197" />
  <row Id="70" PostId="68" Score="0" Text="This doesn't sound like Computer Graphics as our scope intended it to be, this site is specifically oriented towards 3d, rendering, simulation, and the process of viewing them. This sounds like it belongs on graphic design." CreationDate="2015-08-05T17:15:14.783" UserId="27" />
  <row Id="71" PostId="68" Score="0" Text="@robobenklein this would most likely get rejected from graphics design, and its borderline. Besides gamma is about viewing your art" CreationDate="2015-08-05T17:41:02.177" UserId="38" />
  <row Id="72" PostId="72" Score="0" Text="Do we have latex yet?" CreationDate="2015-08-05T17:53:28.320" UserId="38" />
  <row Id="73" PostId="67" Score="1" Text="To come up with this question, you must have had some advantages of hexagonal arrangement of pixels. Can you add those to the question?" CreationDate="2015-08-05T17:54:03.760" UserId="180" />
  <row Id="74" PostId="72" Score="2" Text="No, but [hopefully we will in the future](http://meta.computergraphics.stackexchange.com/q/18/6). Feel free to add you answer as an example for its necessity, though." CreationDate="2015-08-05T18:18:01.940" UserId="6" />
  <row Id="75" PostId="68" Score="5" Text="I'd say this post is on topic. It seems to relate heavily to GPU Gems 3: Chapter 24. The Importance of Being Linear&#xA;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch24.html" CreationDate="2015-08-05T18:30:29.643" UserId="56" />
  <row Id="76" PostId="68" Score="0" Text="Ah actually i lied... This question would be welcome to GD.SE because a similar is there. IF i can get to it first, if im away its most likely gets modded to oblivion." CreationDate="2015-08-05T18:32:39.813" UserId="38" />
  <row Id="77" PostId="68" Score="0" Text="@AlanWolfe i used to own that book, somebody at work stole it from my hand reference." CreationDate="2015-08-05T18:37:02.960" UserId="38" />
  <row Id="78" PostId="74" Score="2" Text="You dont need to store each color plane in each pixel even, or have the planes in same configuration.  BUT you loose separable filtering" CreationDate="2015-08-05T19:07:42.360" UserId="38" />
  <row Id="79" PostId="73" Score="1" Text="No latex support yet." CreationDate="2015-08-05T19:13:27.257" UserId="38" />
  <row Id="80" PostId="74" Score="0" Text="Good points! Separable filtering is pretty big.  I wonder if you could do 3 axis filtering for hexagons?" CreationDate="2015-08-05T19:16:29.310" UserId="56" />
  <row Id="81" PostId="68" Score="5" Text="Whether something is on topic elsewhere is not relevant to this decision. We just need to know whether it's on topic *here*." CreationDate="2015-08-05T19:20:25.787" UserId="231" />
  <row Id="82" PostId="68" Score="1" Text="I'm deliberately posting borderline questions to measure where the border is. Please raise as many uncertainties as possible on Meta." CreationDate="2015-08-05T19:21:00.567" UserId="231" />
  <row Id="83" PostId="72" Score="0" Text="If you want to show how your answer *would* look if we had MathJax, to help make the case, you can use [mathurl.com](http://mathurl.com)." CreationDate="2015-08-05T19:36:50.790" UserId="231" />
  <row Id="84" PostId="68" Score="3" Text="Being welcome elsewhere is *never* a proper reason alone to declare something off-topic *here*." CreationDate="2015-08-05T19:43:17.330" UserId="6" />
  <row Id="85" PostId="67" Score="0" Text="@nitishch it was only the one advantage I had in mind. Does my edit get it across better?" CreationDate="2015-08-05T19:57:54.847" UserId="231" />
  <row Id="86" PostId="68" Score="2" Text="@robobenklein nowhere in this stack's scope states this stack is for 3D graphics specifically." CreationDate="2015-08-05T20:03:42.837" UserId="71" />
  <row Id="87" PostId="68" Score="1" Text="Such discussions are important to have - that's what this private beta is all about. So I've raised it on [meta](http://meta.computergraphics.stackexchange.com/questions/35/are-questions-about-gamma-on-topic)" CreationDate="2015-08-05T20:06:19.210" UserId="231" />
  <row Id="88" PostId="53" Score="1" Text="@Mokosha Correct, there's no way to query the scanout directly AFAIK. At best, you can figure out when vsync is (via some OS signal) and estimate where the scanout is by timing relative to that (knowing details of the video mode). For rendering, you can experiment to find out how long it usually takes between glFlush and when the rendering is done, and make some guesses based on that. Ultimately, you have to build in some slack in your timing in case of error (e.g. stay 2-3 ms ahead of scanout), and accept that there will be probably be occasional artifacts." CreationDate="2015-08-05T20:39:58.377" UserId="48" />
  <row Id="89" PostId="70" Score="3" Text="Matt Pettineo's blog posts [How To Fake Bokeh](https://mynameismjp.wordpress.com/2011/02/28/bokeh/) and [Bokeh II: The Sequel](https://mynameismjp.wordpress.com/2011/04/19/bokeh-ii-the-sequel/) are great introductions to how to practically implement post-process DoF and address the typical artifacts you get from it." CreationDate="2015-08-05T20:55:43.830" UserId="48" />
  <row Id="90" PostId="75" Score="0" Text="I remember that answer last time round :) (I was githubphagocyte back then). Interesting to see it expanded into a blog post." CreationDate="2015-08-05T21:01:28.423" UserId="231" />
  <row Id="91" PostId="62" Score="2" Text="I'd never heard of Fibonacci grids before; that's pretty cool!" CreationDate="2015-08-05T21:09:35.380" UserId="48" />
  <row Id="92" PostId="73" Score="0" Text="I was looking for something that could be used with an implicit surface even if it doesn't have a parameterisation. Is it always possible to parameterise an implicit surface if the derivative is known?" CreationDate="2015-08-05T21:10:49.550" UserId="231" />
  <row Id="93" PostId="73" Score="0" Text="Any questions that would benefit from MathJax for formulae can be added to [this meta answer](http://meta.computergraphics.stackexchange.com/a/34/231) to increase our chances of getting MathJax. (This one has already been added.)" CreationDate="2015-08-05T21:12:23.440" UserId="231" />
  <row Id="94" PostId="73" Score="0" Text="Remember that what you need is the distribution function derived from the curvature, you said you can derive everything (by the way what kind of surface have you got? i.e. the equation). Anyway... what do you mean with &quot;derivative known&quot;? do you know an explicit formula of the derivative? or it is implicit too? (i.e. described by means of differential equation)?" CreationDate="2015-08-05T21:27:45.793" UserId="228" />
  <row Id="95" PostId="73" Score="1" Text="By the way... if the curve/surface is algebric (i mean expressed by polynomial or rational staff) there are computational methods based on bspline/nurbs that explain how to perform parametrization of such curves. I had a glance here http://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=1827&amp;context=cstech, further method (even advanced) could be found in one of my favourite book on Nurbs (The NURBS book by Tiller)." CreationDate="2015-08-05T21:37:28.987" UserId="228" />
  <row Id="96" PostId="4" Score="0" Text="Is that a specific shader language not available in the list of languages covered by our syntax highlighting?" CreationDate="2015-08-05T22:24:19.397" UserId="231" />
  <row Id="97" PostId="4" Score="0" Text="I'm not sure.  It's just GLSL (from webgl to be exact!).  I just did 4 spaces before each line of code, not sure if there's a better way to mark it up..." CreationDate="2015-08-05T22:37:41.687" UserId="56" />
  <row Id="98" PostId="53" Score="0" Text="The effect of increased latency is due to vsync, which causes the front and backbuffer swaps to synchronize with the vblank of the monitor. Double buffering itself doesn't cause this issue by itself and it is useful to minimize flickering because a pixel can only change once in the front buffer." CreationDate="2015-08-06T00:29:21.403" UserId="64" />
  <row Id="99" PostId="57" Score="0" Text="@trichoplax: yes. Now thinking about how it can be used." CreationDate="2015-08-06T05:01:17.340" UserId="182" />
  <row Id="100" PostId="80" Score="0" Text="Wider gamut monitors will also start to be commonplace in near future. For example I do not calibrate to sRGB on my art worstation, but the profile to profile converter makes images still look the same as on my dev machine." CreationDate="2015-08-06T06:41:08.183" UserId="38" />
  <row Id="101" PostId="81" Score="1" Text="It is indeed a complicated paper (still gives me nightmares from time to time!) but I tried to simplify the whole thing a bit. Let me know if you think the below answer should be adjusted in a way or another :)" CreationDate="2015-08-06T08:29:37.570" UserId="100" />
  <row Id="102" PostId="29" Score="0" Text="I don't see how Catmull-Rom is &quot;user defined&quot;. Once you have a sequence of 4 contiguous points, P[i-1], P[i], P[i+1], P[i+2] (4x4 for 2D case) the curve segment is defined between P[i] and P[i+1] and is C1 continuous with the neighbouring segments. &#xA;&#xA;A sinc filter is fine for audio but not video. See Mitchell &amp; Netravali: https://www.cs.utexas.edu/~fussell/courses/cs384g-fall2013/lectures/mitchell/Mitchell.pdf   &#xA;IIRC Catmull-Rom is a special case of the family of filters they propose, but I think that filter is an approximating curve so, unlike C-R, might not go through the original points." CreationDate="2015-08-06T09:24:46.650" UserId="209" />
  <row Id="103" PostId="83" Score="0" Text="Do you mean you have both the pre and post gamma images and you want to find the gamma applied?" CreationDate="2015-08-06T13:55:01.850" UserId="100" />
  <row Id="104" PostId="83" Score="0" Text="@cifz Yes, the original image is from [trichoplax's profile](http://i.stack.imgur.com/iukw5.png?s=328&amp;g=1)." CreationDate="2015-08-06T13:56:52.307" UserId="157" />
  <row Id="105" PostId="29" Score="0" Text="That's how he hermite spline works except that the catmull rom spline has an additional parameter tau (tension) that is user defined.  Also, sinc does apply to video, DSP is DSP :P" CreationDate="2015-08-06T14:04:06.173" UserId="56" />
  <row Id="106" PostId="84" Score="3" Text="Though, I think part of his problem as posed in the question is also that he might not know if it is gamma corrected in any way at all or if the colors were not otherwise (linearly or whatever) modified in contrast to mere gamma correction. But ok, in this case you just take a bigger sample size and try to see if it can be approximated sufficiently well with a gamma transformation." CreationDate="2015-08-06T14:07:57.013" UserId="6" />
  <row Id="107" PostId="67" Score="0" Text="The DSP stack exchange (signal processing) probably would have a more formal answer for you on this BTW." CreationDate="2015-08-06T14:16:31.293" UserId="56" />
  <row Id="108" PostId="84" Score="0" Text="Exactly @ChristianRau, ideally it is to determine the difference even when other color transformations were applied. Thanks cifz, so if you sample several of each image's pixels and the resulting G is approximately 1, then we can conclude that no gamma correction was made?" CreationDate="2015-08-06T14:27:07.320" UserId="157" />
  <row Id="109" PostId="29" Score="0" Text="I must admit, I've never seen a tension parameter associated with Catmull Rom splines before, but then I've really only learned of them via Foley &amp; van Dam (et al) or, say, Watt &amp; Watt which, AFAICR, make no mention of such.&#xA;&#xA;Actually, having said that, given there are four constraints - i.e. the curve has to pass through 2 points and have two defined tangents** at those points and it's a cubic - I'm at a bit of a loss as to how there's any more degrees of freedom to support a tension parameter ....&#xA;&#xA;**Unless you mean the tangents can be scaled?" CreationDate="2015-08-06T14:29:47.557" UserId="209" />
  <row Id="110" PostId="29" Score="0" Text="The tension parameter is shown in that first paper you linked.  And yeah I just mean, the fact that there's a tunable parameter means it must be tuned :p. There is a value of tau you can use such that you end up with a regular cubic hermite spline, and I think it might be &quot;1&quot; but not 100% sure on that." CreationDate="2015-08-06T14:34:42.063" UserId="56" />
  <row Id="111" PostId="84" Score="0" Text="If also the other transformations are unknown, then to my limited knowledge I don't know how and if you can find the gamma. Intuitively I'd say you can't" CreationDate="2015-08-06T14:35:32.827" UserId="100" />
  <row Id="112" PostId="84" Score="1" Text="As @ChristianRau correctly said, you can try and fit the combination of transformations into a gamma function, but that will not tell you what gamma was applied on top of the other unknown transform, but rather an gamma that once applied to the source will give you roughly your destination" CreationDate="2015-08-06T14:45:00.293" UserId="100" />
  <row Id="113" PostId="86" Score="3" Text="There is a authoritatively best filter, its a infinitely wide sinc filter. Its just not possible to use it. untill that time lanczos windowed sinc is a good alternative to michell" CreationDate="2015-08-06T15:38:39.483" UserId="38" />
  <row Id="114" PostId="69" Score="1" Text="This is a really good question that has practical implications as realtime interactive stereo rendering is becoming more prevalent with VR" CreationDate="2015-08-06T16:33:09.960" UserId="56" />
  <row Id="115" PostId="82" Score="0" Text="Thanks! I'm still wondering a couple of things. (1) Even with the diffusion profile approximation, the BSSRDF model still requires integrating over a radius of nearby points on the surface to gather incoming light, correct? How is that accomplished in, say, a path tracer? Do you have to build some data structure so you can sample points on the surface nearby a given point? And (2) Why one positive and one negative light? Is the goal for them to cancel each other in some way?" CreationDate="2015-08-06T18:21:57.887" UserId="48" />
  <row Id="116" PostId="83" Score="1" Text="I don't know for certain that the [CC BY-SA 3.0](http://creativecommons.org/licenses/by-sa/3.0/) licence applies to profile images, but I operate under the assumption that anything that I use as an avatar is automatically licensed that way, and in any case I'm very happy for the image to be reused :)" CreationDate="2015-08-06T18:26:47.613" UserId="231" />
  <row Id="117" PostId="83" Score="1" Text="[Meta Stack Exchange](http://meta.stackexchange.com/questions/176253/profile-images-with-copyright/176256#176256) suggests that profile images are also CC BY-SA 3.0 so as long as you give credit you should be OK using anyone's avatar (provided they complied with the requirement to not post works they don't have the right to...)." CreationDate="2015-08-06T18:42:05.193" UserId="231" />
  <row Id="119" PostId="82" Score="0" Text="1) Indeed, what they propose in the paper with their monte carlo ray tracer is a stochastic sampling with a specific density based on distance and extinction coefficient.  I guess you can dart-throw to find samples and use an appropriate acceptance probability based on the extinction coeff. and distance. (1/2)" CreationDate="2015-08-06T19:37:57.230" UserId="100" />
  <row Id="120" PostId="85" Score="1" Text="These two old articles from *The Inner Product* talk about filters for mipmap generation, which might be relevant to you: [Link1](http://number-none.com/product/Mipmapping,%20Part%201/index.html), [link2](http://number-none.com/product/Mipmapping,%20Part%202/index.html)." CreationDate="2015-08-06T19:38:03.663" UserId="54" />
  <row Id="121" PostId="82" Score="0" Text="I know that Jensen published a hierarchical integration approach in 2002 that unfortunately I just read once and a while ago, so I just remember few bits. The core concept was to decouple the sampling from the diffusion approximation and cluster distant samples. IIRC they used an octree as hierarchical structure. &#xA;&#xA;I never got myself into an offline implementation so I am not of so much help on other details on this I am  afraid.  (2/2)" CreationDate="2015-08-06T19:38:05.870" UserId="100" />
  <row Id="122" PostId="82" Score="0" Text="2) It is this way to satisfy some boundary conditions, you want to let the fluence become zero at a certain extrapolated boundary that has a specific distance from the medium. This distance is computed based on scattering coefficient and scattering albedo." CreationDate="2015-08-06T19:38:19.410" UserId="100" />
  <row Id="123" PostId="86" Score="0" Text="For reference, there's also the [Nvidia image tools](https://code.google.com/p/nvidia-texture-tools/)." CreationDate="2015-08-06T19:48:01.210" UserId="54" />
  <row Id="124" PostId="86" Score="0" Text="If you are using a cubic-esque or lanczos filter, do those guys work equally well for scaling up as they do for scaling down?" CreationDate="2015-08-06T20:14:41.767" UserId="56" />
  <row Id="125" PostId="91" Score="1" Text="A good answer is predicated on knowing how the displays will be connected to the available GPUs (getting 9+ display outputs on a single computer isn't trivial) and the software responsible for your render. Of course it's possible to combine the processing power of multiple GPUs to solve your problem, but the devil is in the details of how the outputs are connected and how many computers are involved." CreationDate="2015-08-06T22:58:52.010" UserId="259" />
  <row Id="126" PostId="90" Score="0" Text="&quot;Laser light is a single frequency. White light however is light made up of the sum of different wavelengths of light.&quot;&#xA;(https://www.physicsforums.com/threads/not-hw-why-are-light-waves-in-the-form-of-the-sine-wave-instead-of-some-other-wave.347805/). One thing you would have to account for is the spectral nature of light." CreationDate="2015-08-07T04:17:34.387" UserId="110" />
  <row Id="127" PostId="82" Score="0" Text="@NathanReed Let me know if this has clarified something, otherwise I can try and expand the thoughts in this comment in the answer itself" CreationDate="2015-08-07T05:38:10.067" UserId="100" />
  <row Id="128" PostId="95" Score="2" Text="You could checkout logarithmic depth buffers. There's an article on [gamasutra](http://www.gamasutra.com/blogs/BranoKemen/20090812/85207/Logarithmic_Depth_Buffer.php)" CreationDate="2015-08-07T08:12:14.850" UserId="214" />
  <row Id="129" PostId="29" Score="0" Text="Re that first paper: That serves me right for pasting in the first link I found that seemed suitable :-)    Again, I think I've only ever seen the tangent at a segment end point set to the (assuming I've done the matrix maths correctly) 1/2 the difference of the two surrounding neighbours." CreationDate="2015-08-07T08:25:01.567" UserId="209" />
  <row Id="132" PostId="97" Score="3" Text="Hm yeah, I'm aware that I can get compiler errors. I was hoping for better runtime debugging. I've also used WebGL inspector in the past, but I believe it only shows you state changes, but you can't look into a shader invocation. I guess this could have been clearer in the question." CreationDate="2015-08-07T09:42:23.417" UserId="16" />
  <row Id="133" PostId="96" Score="0" Text="Have you looked into gDEBugger? Quoting the site: &quot;gDEBugger is an advanced OpenGL and OpenCL Debugger, Profiler and Memory Analyzer. gDEBugger does what no other tool can - lets you trace application activity on top of the OpenGL and OpenCL APIs and see what is happening within the system implementation.&quot; Granted, no VS style debugging/stepping through code, but it might give you some insight in what your shader does (or should do). Crytec released a similar tool for Direct shader &quot;debugging&quot; called RenderDoc (free, but strictly for HLSL shaders, so maybe not relevant for you)." CreationDate="2015-08-07T11:06:15.930" UserId="194" />
  <row Id="134" PostId="96" Score="0" Text="@Bert Hm yeah, I guess gDEBugger is the OpenGL equivalent to WebGL-Inspector? I've used the latter. It's immensely useful, but it's definitely more debugging OpenGL calls and state changes than shader execution." CreationDate="2015-08-07T11:47:56.203" UserId="16" />
  <row Id="135" PostId="96" Score="0" Text="I've never done any WebGL programming and hence I'm not familiar with WebGL- Inspector. With gDEBugger you can at least inspect the entire state of your shader pipeline including texture memory, vertex data, etc. Still, no actual stepping through code afaik." CreationDate="2015-08-07T11:57:21.997" UserId="194" />
  <row Id="136" PostId="96" Score="0" Text="gDEBugger is extremely old and not supported since a while. If you are looking from frame and GPU state analysis you this is other question is strongly related: http://computergraphics.stackexchange.com/questions/23/how-can-i-debug-what-is-being-rendered-to-a-frame-buffer-object-in-opengl/25#25" CreationDate="2015-08-07T12:00:52.567" UserId="100" />
  <row Id="137" PostId="100" Score="4" Text="This question may be a bit controversial, because several other SE sites don't want questions about best practices. This is intentional to see how this community stands regarding such questions." CreationDate="2015-08-07T12:39:15.597" UserId="16" />
  <row Id="138" PostId="100" Score="2" Text="Hmm, looks good to me. I'd say we are to a large degree a bit &quot;broader&quot;/&quot;more general&quot; in our questions than, say, StackOverflow." CreationDate="2015-08-07T12:40:15.333" UserId="6" />
  <row Id="139" PostId="95" Score="1" Text="When you say &quot;co-planar&quot; do you mean &quot;nearly&quot;  or &quot;exactly&quot; co-planar and if the latter, are those surfaces ever identical surfaces/triangles? The rendering hardware should be deterministic (assuming you aren't submitting in random order) for the last case and not have fighting.  &#xA;If it's a case of non-identical surfaces but exactly co-planar, could you update the model to split surfaces into overlapping and non-overlapping portions?" CreationDate="2015-08-07T12:55:57.560" UserId="209" />
  <row Id="140" PostId="29" Score="0" Text="Ah OK! Anyways yeah, catmull rom / hermite is really freaking awesome I totally agree. Really neat way to interpolate between data points and have C1 continuity at the edges. It really simplifies things (:" CreationDate="2015-08-07T13:36:17.957" UserId="56" />
  <row Id="141" PostId="100" Score="2" Text="StackOverflow went from being a 'ask us' to a 'dont ask us unless you **have to** please' board." CreationDate="2015-08-07T14:24:01.983" UserId="296" />
  <row Id="142" PostId="52" Score="0" Text="There's an article on GPU Gems 3 about the construction of signed distance fields for arbitrary meshes using the GPU, which is freely available here: http://http.developer.nvidia.com/GPUGems3/gpugems3_ch34.html" CreationDate="2015-08-07T14:32:46.747" UserId="281" />
  <row Id="143" PostId="100" Score="0" Text="If it's meant to determine on-topicness, then how about an associated Meta question?" CreationDate="2015-08-07T15:43:26.963" UserId="21" />
  <row Id="144" PostId="100" Score="0" Text="@S.L.Barth I figured I could still do that once anyone objects, but sure, why not." CreationDate="2015-08-07T15:51:43.950" UserId="16" />
  <row Id="145" PostId="100" Score="2" Text="[Discussion on meta.](http://meta.computergraphics.stackexchange.com/q/53/16)" CreationDate="2015-08-07T16:06:09.930" UserId="16" />
  <row Id="146" PostId="1" Score="3" Text="Please don't call these optical tricks &quot;holograms&quot;. Refer to the &quot;[Things often confused with holograms](https://en.wikipedia.org/wiki/Holography#Things_often_confused_with_holograms)&quot; for an overview. In this case, you're talking about a classic [Pepper's ghost](https://en.wikipedia.org/wiki/Pepper%27s_ghost) illusion. No 3D at all, apart from having 4 different perspectives. **update:** I've proposed an edit to adjust the question accordingly." CreationDate="2015-08-07T16:13:30.893" UserId="30" />
  <row Id="147" PostId="86" Score="0" Text="I've not tried lanczos so I can't speak to that. We chose catmull-rom for upscaling, which is a cubic, and it worked well." CreationDate="2015-08-07T17:13:17.077" UserId="125" />
  <row Id="148" PostId="103" Score="1" Text="I'm a bit confused about your premise ... how is a lowpass filter qualitatively any different than downsampling? I mean, I get that the algorithms are different and all but they both gather samples from neighboring pixels and suppress high frequencies. The big difference is the resolution of the result image, otherwise the two operations are isomorphic. Seems like applying both is redundant." CreationDate="2015-08-07T17:20:12.627" UserId="125" />
  <row Id="149" PostId="103" Score="0" Text="Well here's what confuses me.  I know that you can't just downsample an image without getting aliasing.  Doing bicubic interpolation of pixels when making an image larger works really well and looks nice.  Doing the same when making an image smaller SEEMS to work decently, but I wasn't sure if the result is likely to have much aliasing as a result.  I was wondering if technically, you'd need to do some kind of low pass filter on the image before the doing bicubic sampling, or if the bicubic sampling was good enough in practice?  I could see it being a low pass filter of sorts on its own maybe." CreationDate="2015-08-07T17:24:12.550" UserId="56" />
  <row Id="150" PostId="103" Score="1" Text="That Mitchell-Netravali paper I mentioned in the other question addresses this idea specifically - he generalized cubics and then found the parameters that alias the least. That doesn't mean they don't alias at all, but perhaps it would direct you towards which cubic to use to minimize aliasing." CreationDate="2015-08-07T18:30:49.443" UserId="125" />
  <row Id="151" PostId="82" Score="0" Text="Thanks, if you wouldn't mind putting the info into the answer as well, that would be great!" CreationDate="2015-08-07T18:37:29.810" UserId="48" />
  <row Id="152" PostId="95" Score="0" Text="@SimonF, by &quot;co-planar&quot; I mean &quot;exactly co-planar&quot;.  Soapy's solution only works in the &quot;nearly co-planar&quot; case." CreationDate="2015-08-07T18:38:11.437" UserId="158" />
  <row Id="153" PostId="95" Score="0" Text="Could you give an example of your surfaces? The only thing I can think of off the top of my head is duplicate triangles as @SimonF mentioned." CreationDate="2015-08-07T20:28:32.447" UserId="310" />
  <row Id="154" PostId="95" Score="0" Text="@RichieSams the most common case that I can think of is decals, where you don't need exactly duplicate triangles." CreationDate="2015-08-07T23:15:29.387" UserId="259" />
  <row Id="155" PostId="25" Score="0" Text="It's also probably a good idea to make sure you have the right depth test setup, for the rendering going into your visualisation FBO." CreationDate="2015-08-07T23:18:31.293" UserId="259" />
  <row Id="156" PostId="91" Score="1" Text="I found a blog post where someone was able to do it: http://www.rchoetzlein.com/website/multi-monitor-rendering-in-opengl/  @rys is right though. In order to get a solid answer we would need to know more details of your setup." CreationDate="2015-08-08T00:39:22.360" UserId="310" />
  <row Id="157" PostId="106" Score="2" Text="GL_ARB_shading_language_include is not a core feature. In fact, only NVIDIA supports it. (http://delphigl.de/glcapsviewer/listreports2.php?listreportsbyextension=GL_ARB_shading_language_include)" CreationDate="2015-08-08T01:08:40.533" UserId="311" />
  <row Id="158" PostId="82" Score="0" Text="@NathanReed Done :) I tried not to dwell on too much to avoid an even longer answer, but let me know if you think something needs to be added or clarified further!" CreationDate="2015-08-08T09:00:20.137" UserId="100" />
  <row Id="159" PostId="82" Score="0" Text="Just to add a new reference, there's a new model per the dipole approximation presented this year at siggraph: http://people.compute.dtu.dk/jerf/code/dirsss/" CreationDate="2015-08-08T09:09:35.210" UserId="281" />
  <row Id="162" PostId="110" Score="0" Text="From my understanding, I think this does not solve the problem of sharing struct definitions." CreationDate="2015-08-08T17:35:24.210" UserId="311" />
  <row Id="163" PostId="110" Score="0" Text="@NicolasLouisGuillemot: yes you are right, only instructions code is shared this way, not declarations." CreationDate="2015-08-08T17:39:53.133" UserId="182" />
  <row Id="164" PostId="107" Score="5" Text="Re-rendering the same geometry with the same transforms does reliably generate the same depth values every time. (I.e. it's not a *might*, it's a *will*). That's why multi-pass forward lighting works, for instance." CreationDate="2015-08-08T17:51:51.780" UserId="48" />
  <row Id="166" PostId="118" Score="0" Text="Yes but how do you handle change of basis to dimensions? If i have is a R^3 -&gt; R^4 or R^5 mapping? Anyway yes this will make it work atleast halfway." CreationDate="2015-08-08T20:58:31.147" UserId="38" />
  <row Id="167" PostId="118" Score="1" Text="I've not had to approach this problem before, but I'm skeptical that this answer will work since color spaces aren't always linear." CreationDate="2015-08-08T22:39:28.103" UserId="125" />
  <row Id="168" PostId="100" Score="0" Text="I wrote [this](http://stackoverflow.com/a/20605122/1888983) a while back. Beyond `#include`, I find injecting `#define`s is most useful. One shader, many permutations." CreationDate="2015-08-09T00:36:10.310" UserId="198" />
  <row Id="169" PostId="118" Score="0" Text="Dot product surely will come into play" CreationDate="2015-08-09T02:22:05.913" UserId="56" />
  <row Id="170" PostId="111" Score="0" Text="Yep, I think this was my conclusion too. Do you know offhand if this is the case with DX12 or (the unreleased) Vulkan? It has pretty large applications for deferred rendering as you've mentioned and none of the existing alternatives right now seem satisfactory." CreationDate="2015-08-09T03:37:53.700" UserId="87" />
  <row Id="171" PostId="111" Score="0" Text="@jeremyong Sorry, I don't think there are any changes to blending operations in DX12. Not sure about Vulkan, but I'd be surprised; the blending hardware hasn't changed. FWIW, in games I've worked on we did a variant of bullet #3 for deferred decals, and preprocessed the geometry to separate it into non-overlapping groups." CreationDate="2015-08-09T03:58:19.847" UserId="48" />
  <row Id="172" PostId="118" Score="0" Text="@jorgeRodriguez you can treat the space linear for max values" CreationDate="2015-08-09T04:50:36.953" UserId="38" />
  <row Id="173" PostId="111" Score="0" Text="Gotcha thanks for the recommendation. Deferred decals is precisely what I'm implementing" CreationDate="2015-08-09T05:30:12.440" UserId="87" />
  <row Id="176" PostId="124" Score="0" Text="Thanks! It looks like I'll need two depth buffers for this. I.e. one to store and filter out the last depths, and one to do the depth testing for the current render. Correct me if I'm wrong but I assume I'll need two depth textures for the FBO which I swap between each peeling pass." CreationDate="2015-08-09T09:07:19.353" UserId="198" />
  <row Id="177" PostId="124" Score="0" Text="@jozxyqk Correct, two depth buffers are necessary for this procedure." CreationDate="2015-08-09T09:09:45.863" UserId="170" />
  <row Id="178" PostId="24" Score="1" Text="For more information, see [Pepper's Ghost](https://en.wikipedia.org/wiki/Pepper's_ghost)" CreationDate="2015-08-09T09:12:03.990" UserId="158" />
  <row Id="179" PostId="133" Score="0" Text="Yes something like this." CreationDate="2015-08-09T11:40:33.210" UserId="38" />
  <row Id="180" PostId="122" Score="2" Text="Yes, using an appropriate color space is what you want." CreationDate="2015-08-09T11:55:11.810" UserId="182" />
  <row Id="182" PostId="140" Score="0" Text="I've actually been thinking about this. Ive also toyed on asking what other approaches modeling pipes than matrix and quat hierarchies one could use." CreationDate="2015-08-09T13:17:36.497" UserId="38" />
  <row Id="183" PostId="120" Score="0" Text="Have you heard of anyone using the depth buffer value as a sanity check?" CreationDate="2015-08-09T14:16:34.443" UserId="56" />
  <row Id="187" PostId="120" Score="0" Text="@AlanWolfe No, and I think that is because the motion vector texture is usually 2-component: you'd need a change-in-Z component to know what the depth buffer value should be, and that's not nicely bounded by the screen size. I suspect you could get better rejection strategies than that by adding more per-pixel information." CreationDate="2015-08-09T16:48:45.367" UserId="196" />
  <row Id="188" PostId="120" Score="0" Text="Ah OK. What kind of information do you think would be helpful to add. Shading parameter type stuff to be able to tell if it's the same material?" CreationDate="2015-08-09T17:26:05.140" UserId="56" />
  <row Id="189" PostId="120" Score="1" Text="@AlanWolfe I don't have a lot of concrete ideas. I'm not an expert on when temporal reprojection with neighborhood clamping breaks down and produces artifacts and what information would be useful in those situations. Perhaps transparents (no motion vector information) combined with high-frequency lighting are producing artifacts, and you need some obscurance information. Perhaps geometric aliasing is your problem and you need some other information." CreationDate="2015-08-09T18:11:28.410" UserId="196" />
  <row Id="190" PostId="145" Score="1" Text="Should this be several questions." CreationDate="2015-08-09T18:33:42.517" UserId="38" />
  <row Id="191" PostId="145" Score="0" Text="@joojaa Potentially. The answers to these questions are interrelated, though. I'm looking for an answer of the form &quot;well, a photon can do X, Y, or Z when it interacts with media; X is described by the phase function, Y is described by the Beer-Lambert law, …&quot;" CreationDate="2015-08-09T18:37:55.153" UserId="196" />
  <row Id="192" PostId="107" Score="0" Text="@NathanReed Corrected. Thank you for the clarification" CreationDate="2015-08-09T18:44:49.937" UserId="310" />
  <row Id="193" PostId="130" Score="1" Text="I haven't looked in too much detail at the code, but the image seems to be alright. The Fresnel effect shows up as a red ring. With the roughness so high (0.9), it makes sense that the rest of the image is mostly yellow (ie. mostly diffuse). If you lower the roughness, you may get a red specular highlight" CreationDate="2015-08-09T18:49:36.253" UserId="310" />
  <row Id="194" PostId="131" Score="1" Text="Here are two source code resources for [Tiled](https://software.intel.com/en-us/articles/deferred-rendering-for-current-and-future-rendering-pipelines) and [Clustered](https://software.intel.com/en-us/articles/forward-clustered-shading)" CreationDate="2015-08-09T18:52:48.500" UserId="310" />
  <row Id="195" PostId="130" Score="0" Text="@RichieSams I added new images for different roughness values but can't see red shiny specular highlight yet." CreationDate="2015-08-09T20:24:29.623" UserId="83" />
  <row Id="196" PostId="15" Score="0" Text="These comments sound like the perfect material for someone to write a summarising answer." CreationDate="2015-08-09T21:22:15.260" UserId="231" />
  <row Id="198" PostId="41" Score="1" Text="Could you add a brief note to explain why the two different 5 by 5 kernels have slightly different numbers (one summing to 273, the other summing to 256)? It seems like a potential confusion for someone new to this." CreationDate="2015-08-09T22:14:56.317" UserId="231" />
  <row Id="199" PostId="41" Score="0" Text="Similarly, could you explain why the kernel is flipped in your second diagram? I don't think it's relevant to the explanation, but the fact that it's an apparent extra step may hinder understanding to someone who doesn't know that it isn't necessary." CreationDate="2015-08-09T22:16:34.077" UserId="231" />
  <row Id="200" PostId="62" Score="0" Text="That's a fascinating paper. It looks like you can adjust the parameters to give nearer to a square grid or a hexagonal grid, which would allow different approaches to noise generation." CreationDate="2015-08-09T22:43:53.933" UserId="231" />
  <row Id="201" PostId="151" Score="1" Text="Does &quot;trackball&quot; mean a camera that orbits around an object, like in 3D modeling apps? If so, I think it's usually done by just keeping track of the 2D mouse coordinates and mapping x=yaw, y=pitch for the camera rotation." CreationDate="2015-08-09T23:15:07.823" UserId="48" />
  <row Id="202" PostId="150" Score="0" Text="Say that we don't care very much about efficiency. How would we go about doing coverage calculations for boolean operations on shapes? Is that possible in general, or only for specific shapes?" CreationDate="2015-08-09T23:18:37.830" UserId="196" />
  <row Id="203" PostId="154" Score="1" Text="Are you intentionally restricting the question to bounding volume hierarchies, or are you open to other forms of spatial partitioning?" CreationDate="2015-08-09T23:19:31.850" UserId="196" />
  <row Id="204" PostId="154" Score="1" Text="@JohnCalsbeek I've edited to clarify - thanks for pointing out my inadvertent restriction." CreationDate="2015-08-09T23:32:37.030" UserId="231" />
  <row Id="205" PostId="153" Score="1" Text="This is super interesting! So is the Disney paper you linked." CreationDate="2015-08-09T23:41:26.250" UserId="196" />
  <row Id="206" PostId="151" Score="1" Text="@NathanReed the other option is [axis-angle based](http://codereview.stackexchange.com/q/51205/32061), You project 2 mouse points onto a (virtual) sphere and then find the rotation from one to the other." CreationDate="2015-08-09T23:42:13.803" UserId="137" />
  <row Id="207" PostId="46" Score="0" Text="I am selecting this answer since it gives orders of magnitude, which is the closest so far to what I asked, even though the mentioned source doesn't give much explanation." CreationDate="2015-08-09T23:42:35.030" UserId="182" />
  <row Id="208" PostId="118" Score="0" Text="@JorgeRodriguez would it work if you first translate to a linear colour space? For more than 3 primary colours there will in general be more than one combination that approximates a given colour, but finding one of those combinations should be possible with this approach, provided that the chosen primary colours form a basis." CreationDate="2015-08-10T01:02:36.503" UserId="231" />
  <row Id="209" PostId="15" Score="0" Text="@Rahul Would you mind turning your comments and any other information you might have into an answer?" CreationDate="2015-08-10T01:04:58.047" UserId="70" />
  <row Id="210" PostId="130" Score="1" Text="Your 2nd and 3rd images do appear to have less red in general (in the yellow diffuse area) than your original image. This isn't very apparent because adding a little red to a yellow area leaves it a similar colour (orange-yellow rather than yellow). Do you see any more detail of the red distribution if you reduce the yellow significantly? Omitting the yellow altogether may help in identifying what is going wrong." CreationDate="2015-08-10T01:26:42.160" UserId="231" />
  <row Id="212" PostId="15" Score="0" Text="Sorry, I'm at SIGGRAPH and have absolutely no free time (except to write this comment). Someone else should feel free to write an answer using the links in my comments." CreationDate="2015-08-10T04:17:38.400" UserId="106" />
  <row Id="213" PostId="21" Score="2" Text="If you have an expensive per-pixel shader, the stencil buffer can also be used to batch similar threads, reducing divergence and increasing occupancy as in [this OIT paper, sec 2.6](http://goanna.cs.rmit.edu.au/~pknowles/rbs-preprint.pdf) (disclaimer: I'm an author)." CreationDate="2015-08-10T05:45:33.600" UserId="198" />
  <row Id="214" PostId="162" Score="0" Text="Ha, there you go, taking images from others is more efficient than drawing them yourself." CreationDate="2015-08-10T08:05:00.250" UserId="38" />
  <row Id="215" PostId="15" Score="2" Text="Apologies to @NoviceInDisguise for also not having time, but WRT to Catmull-Clark, perhaps one of the reasons for it still being very much in use was DeRose et al's extensions to it to include, e.g. sharpness factors in the tessellation to allow creases etc. http://www.cs.rutgers.edu/~decarlo/readings/derose98.pdf  &#xA;IIRC those extensions weren't initially free to use (but some commercial tools licensed it from Pixar) however, unless I'm mistaken, it now seems to be free e.g. http://graphics.pixar.com/opensubdiv/docs/subdivision_surfaces.html#arbitrary-topology" CreationDate="2015-08-10T08:12:12.877" UserId="209" />
  <row Id="216" PostId="130" Score="0" Text="@trichoplax i reduced the yellow but again no way to see red specular. I just see red ring(fresnel) effect. Doesnt matter what i set value for roughness I cant see specular effect that focused a point." CreationDate="2015-08-10T08:32:09.737" UserId="83" />
  <row Id="217" PostId="162" Score="0" Text="@joojaa Yes, faster if you remember where they were, but without that gratifying feeling of doing it yourself :P. Also I have this stupid [sub-pixel rendering bug](http://superuser.com/q/930036/264276) in chrome so the text is all colourful." CreationDate="2015-08-10T08:44:38.337" UserId="198" />
  <row Id="218" PostId="162" Score="1" Text="Well that subpixel rendering is something that has not been asked. Yet." CreationDate="2015-08-10T08:59:04.347" UserId="38" />
  <row Id="219" PostId="162" Score="1" Text="[done](http://computergraphics.stackexchange.com/q/165/198)." CreationDate="2015-08-10T09:41:23.373" UserId="198" />
  <row Id="220" PostId="165" Score="0" Text="I believe that some screens have a different layout of the primary colours. Have you viewed your results on different types of screen?" CreationDate="2015-08-10T09:58:22.040" UserId="231" />
  <row Id="221" PostId="165" Score="0" Text="@trichoplax no, but I'm confident both my monitors are RGB. Also here I'm more interested in how subpixel antialiasing techniques are meant to work than a fix for my issue." CreationDate="2015-08-10T10:02:22.253" UserId="198" />
  <row Id="222" PostId="165" Score="0" Text="I didn't mean different primary colours, I just meant that the red, green and blue are arranged in different geometric patterns, so your algorithm would need to know which pattern is being used in order to give good results." CreationDate="2015-08-10T10:06:17.063" UserId="231" />
  <row Id="223" PostId="165" Score="0" Text="[This image](https://en.wikipedia.org/wiki/Subpixel_rendering#/media/File:Pixel_geometry_01_Pengo.jpg) shows how varied the arrangement of subpixels can be." CreationDate="2015-08-10T10:06:48.327" UserId="231" />
  <row Id="224" PostId="165" Score="0" Text="Incidentally, I like the question and I'm interested to see the answers, but I can't upvote until midnight..." CreationDate="2015-08-10T10:09:22.740" UserId="231" />
  <row Id="225" PostId="165" Score="1" Text="@trichoplax yes, sorry I should have clarified, both monitors have pixels split into thirds in R-G-B order from left to right as in [this photo](http://i.stack.imgur.com/SZl3W.png)." CreationDate="2015-08-10T10:09:51.153" UserId="198" />
  <row Id="226" PostId="165" Score="1" Text="To a certain extent, italic text will have less noticeable colour fringes since the sloping lines don't allow the same colour to be present for more than a few consecutive pixels vertically." CreationDate="2015-08-10T10:14:26.257" UserId="231" />
  <row Id="227" PostId="165" Score="0" Text="Hopefully someone with knowledge of this can confirm, but my guess is that the first image has been antialiased at the pixel level first, and then that antialiased image has been antialiased again at the subpixel level. You can see areas where the original antialiasing aligns with whole pixels and has no colour fringes, despite clearly having variations in brightness due to the initial antialiasing, and areas where it does not align resulting in flat colour fringes, rather than the graduated colour fringes in the last image. It appears to be a scaled rasterised font, rather than a vector font." CreationDate="2015-08-10T10:30:46.243" UserId="231" />
  <row Id="228" PostId="107" Score="1" Text="To get that functionality you need to use the invariant qualifier in glsl: https://www.opengl.org/wiki/Type_Qualifier_%28GLSL%29#Invariance_qualifiers" CreationDate="2015-08-10T11:11:30.830" UserId="135" />
  <row Id="229" PostId="107" Score="0" Text="Note that identical shader expressions (and inputs, obviously) when evaluating vertex positions may not be enough to get identical results as some optimisations may depend on the rest of the shader. &#xA;&#xA;GLSL has the &quot;invariant&quot; keyword to declare shader outputs that must be evaluated identically in different shaders." CreationDate="2015-08-10T11:13:07.507" UserId="344" />
  <row Id="231" PostId="130" Score="2" Text="First normalise the Normal vector before using it and second the viewDirection is the outgoing vector from the Position to the camera: uCameraPosition - Position." CreationDate="2015-08-10T11:35:59.267" UserId="290" />
  <row Id="232" PostId="167" Score="0" Text="I would expect problems with pixel geometry, gamma or colour space conversion to show up as colour distortion at arbitrary points in the image, rather than the regular cycle seen in the question's image. The fact that it cycles horizontally between exaggerated colour antialiasing and pure greyscale antialiasing hints that the first application of antialiasing was performed at a different scale." CreationDate="2015-08-10T12:01:41.727" UserId="231" />
  <row Id="233" PostId="167" Score="0" Text="I don't have the full explanation, as the distortion does not seem to be aligned between the different rows of text, but it does seem the problem is related to sub-pixel rendering of already rasterised text rather than vector text." CreationDate="2015-08-10T12:16:59.983" UserId="231" />
  <row Id="234" PostId="167" Score="0" Text="@trichoplax What I try to say is that I doubt there an issue with Anti-Grain's sub-pixel rendering. Instead I would guess the input gets mangled earlier than it enters the rasterizer. Or later, but not in the rasterizer itself." CreationDate="2015-08-10T12:39:48.860" UserId="141" />
  <row Id="235" PostId="167" Score="0" Text="Yes I think the sub-pixel rendering is being applied correctly, but when applied to pre-rasterised text it is not possible to give a good result. I don't think that the renderer is broken, I just think it is being fed the wrong kind of text." CreationDate="2015-08-10T12:46:31.897" UserId="231" />
  <row Id="236" PostId="169" Score="0" Text="You won't find anything relevant just looking at OpenGL, maybe a JavaScript library can do that" CreationDate="2015-08-10T13:18:39.680" UserId="217" />
  <row Id="238" PostId="169" Score="0" Text="I dont think you can, that's why Microsoft asks the user." CreationDate="2015-08-10T13:50:34.907" UserId="38" />
  <row Id="239" PostId="169" Score="0" Text="I wonder if that means there is never a way to determine this, or just that some monitors do not provide that information." CreationDate="2015-08-10T13:58:40.427" UserId="231" />
  <row Id="240" PostId="151" Score="0" Text="@NathanReed yes that is what I meant by trackball, I thought it was a common name in the CG community." CreationDate="2015-08-10T14:11:54.123" UserId="116" />
  <row Id="242" PostId="169" Score="3" Text="well, i think you can get the model and make of your monitor maybe there is some repository that lists this? If there is not what stops anybody of us form building it?" CreationDate="2015-08-10T14:15:14.630" UserId="38" />
  <row Id="243" PostId="151" Score="0" Text="@ratchetfreak yes my approach considers an axis-angle based rotation. My doubt is that if it is needed to map the 2D mouse coords to world-coord or not. I know I can use the (x,y) to compute the `z` value of a sphere of radius `r`, however I'm not sure if that sphere lives in world-space or image-space and what the implications are. Perhaps I'm overthinking the problem." CreationDate="2015-08-10T14:17:15.137" UserId="116" />
  <row Id="245" PostId="24" Score="0" Text="Its an interesting comment about Pepper's ghost. Notice that Pepper's ghost was originally a reflection of a 3D object so the reflection itself was 3D. In the case of these popular device based projections, the projections are only 2D so the effect is only a 2D reflection suspended in space. The most popular Youtube videos meant for doing this don't even have 4 different perspectives, just 1 repeated 4 times." CreationDate="2015-08-10T15:19:01.417" UserId="20" />
  <row Id="246" PostId="164" Score="1" Text="Depth of field may help too." CreationDate="2015-08-10T15:31:29.733" UserId="196" />
  <row Id="247" PostId="175" Score="3" Text="I would take http://www.littlecms.com/ for a spin" CreationDate="2015-08-10T15:44:00.240" UserId="38" />
  <row Id="248" PostId="175" Score="0" Text="@joojaa This is great! They actually say: &quot;lcms is a CMM engine; it implements fast transforms between ICC profiles&quot;. And by their trial of the professional solution, they provide a lot of options (&quot;input for RGB, Gray, and CMYK spaces&quot;, &quot;intents&quot;, &quot;destination color space&quot;, etc.)... I never imagined that it could be so vast." CreationDate="2015-08-10T16:07:35.907" UserId="157" />
  <row Id="249" PostId="175" Score="0" Text="The [international color consortium](http://www.color.org/index.xalter) defines 4 transformation intents for the colors that can not fit the gamut, read the tutorial." CreationDate="2015-08-10T16:08:51.743" UserId="38" />
  <row Id="250" PostId="175" Score="0" Text="@joojaa actually, in their own program they have a small description associated to all of the 7 types of &quot;rendering intents&quot; we can choose from when converting." CreationDate="2015-08-10T16:13:02.087" UserId="157" />
  <row Id="251" PostId="118" Score="0" Text="I expanded my answer to be explicit about how to deal with bases of differing dimensions. Is there any way to use LaTeX for math here? This will probably be a very common feature request." CreationDate="2015-08-10T16:18:55.310" UserId="327" />
  <row Id="252" PostId="118" Score="0" Text="And regarding linearity: afaik most if not all color spaces are linear or have a 1:1 mapping that makes them linear. The CIE XYZ and xyY color spaces (which are supposed to represent all colors visible to the human eye) fit that definition, and are probably what you'd want to use as the &quot;conversion&quot; color space rather than RGB." CreationDate="2015-08-10T16:26:33.650" UserId="327" />
  <row Id="253" PostId="175" Score="0" Text="hey allow for more intents and different white point/ blackpoint combinations. (adobe thus has 3*4 intents)" CreationDate="2015-08-10T16:33:24.357" UserId="38" />
  <row Id="255" PostId="118" Score="2" Text="MathJax request is in works you should add your post as a reference to this [meta post](http://meta.computergraphics.stackexchange.com/a/34/38)" CreationDate="2015-08-10T16:36:56.713" UserId="38" />
  <row Id="256" PostId="164" Score="2" Text="As an addendum: Sebastien Lagarde did a [series of  blog posts](https://seblagarde.wordpress.com/2012/12/10/observe-rainy-world/) that documented how they implemented dynamic rain in the game &quot;Remember Me&quot;" CreationDate="2015-08-10T16:39:34.113" UserId="310" />
  <row Id="257" PostId="175" Score="0" Text="@joojaa I am impressed with the quantity of destination spaces they provide, for the conversions I tested there are shocking differences when ProPhotoRGB is used, [it's very hard to tell the differences when different intents are applied though], EDIT: in some situations intents are also very noticeable." CreationDate="2015-08-10T16:44:22.730" UserId="157" />
  <row Id="260" PostId="170" Score="0" Text="Just noting that I've definitely used extensions via GLES 2.0 on android (and in native code) so you shouldn't have any issues on that part of your solution." CreationDate="2015-08-10T18:37:46.093" UserId="329" />
  <row Id="261" PostId="21" Score="0" Text="@jozxyqk, that's some outside the box thinking right there! Very nice." CreationDate="2015-08-10T19:05:39.823" UserId="54" />
  <row Id="262" PostId="176" Score="0" Text="I'm not sure I'm understanding you. Slightly transparent == translucent. So you're already using translucency. Could you clarify what you're trying to do? Fully transparent background? (Aka, clear/invisible) Or translucent background? (Aka, you can see some stuff behind it, but covered with the background color)" CreationDate="2015-08-10T19:44:53.450" UserId="310" />
  <row Id="263" PostId="130" Score="0" Text="Thanks all of you(RichieSams, trichoplax and xpicox ) for the answers. I lower the roughness, change the color of material and reversed the ViewDirection then i start to see shiny red specular :). Actually i was using right ViewDirection but i could not see the specular because of the color and roughness value. All answers help to fix my problem." CreationDate="2015-08-10T20:05:33.543" UserId="83" />
  <row Id="264" PostId="176" Score="0" Text="@RichieSams Currently I use a simple black background with an applied alpha transparency. So the scene can be seen, but is still sharp, which distracts the readability of the text. Thus I would like to change the simple alpha transparency to translucency. A real-world example maybe is the difference between looking through colored respectively iced glass." CreationDate="2015-08-10T20:07:21.307" UserId="127" />
  <row Id="265" PostId="176" Score="0" Text="@Nero In graphics, &quot;translucent&quot; doesn't usually imply &quot;frosted&quot; or &quot;iced&quot;, so I suggest making what your looking for clearer in your question, since I was also quite confused by it." CreationDate="2015-08-10T20:22:07.777" UserId="327" />
  <row Id="266" PostId="181" Score="0" Text="I reposted this question [from SO where it went unanswered](https://stackoverflow.com/questions/30687682/how-to-render-an-object-that-recieves-shadows-but-does-not-cast-them-in-a-varian). I don't know if it's a good thing to do but it seemed like a good idea since I think it's a good fit here." CreationDate="2015-08-10T20:41:02.740" UserId="349" />
  <row Id="267" PostId="166" Score="2" Text="This GPU Gems article rasterizes quadratic curves by identifying parts of the hull that are curved and analytically computing the coverage in the pixel shader, might be worth a look: https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch25.html" CreationDate="2015-08-10T20:41:47.357" UserId="327" />
  <row Id="268" PostId="183" Score="1" Text="While certainly interesting, I don't think we want to be flooded with plain &quot;How do I achieve effect X?&quot; type questions later on. So: What have you tried?" CreationDate="2015-08-10T22:00:55.653" UserId="16" />
  <row Id="269" PostId="166" Score="2" Text="@yuriks Yeah, Loop &amp; Blinn, totally forgot about it. But isn't it patented?" CreationDate="2015-08-10T22:12:03.490" UserId="141" />
  <row Id="270" PostId="177" Score="0" Text="I think you would also be interested in techniques to simulate Rough Refraction such as those described in http://www-sop.inria.fr/reves/Basilic/2011/DBSHR11/RoughRefr_I3D_Final.pdf. Look at the screenshots and see how it could benefit ice cube rendering !" CreationDate="2015-08-10T22:21:21.550" UserId="110" />
  <row Id="271" PostId="177" Score="1" Text="@wil While those results for rough surfaces are very impressive (especially for a real-time algorithm), ice tends to be very smooth on the surface, and rough inside - almost the opposite effect." CreationDate="2015-08-10T22:31:17.223" UserId="231" />
  <row Id="273" PostId="183" Score="3" Text="I think the question is fine up to the title, but the body is kind of vague. There should be more context like if it's meant for real-time of offline use, level of detail and scale required, etc.  The current answer for example assumes the scale to be able to give a more specific answer taking into account possible tradeoffs, which I think is an important part of a good answerable question." CreationDate="2015-08-11T00:06:56.847" UserId="327" />
  <row Id="274" PostId="170" Score="0" Text="Are we considering very specific programming questions like these? This is a very API-specific programming question and doesn't even involve anything related to rendering techniques or usage of an API to implement them." CreationDate="2015-08-11T00:46:25.163" UserId="327" />
  <row Id="275" PostId="130" Score="0" Text="If the various different suggestions came together into a solution, you could write them up and post it as a self-answer. That's generally encouraged. Then you can include the working final image too..." CreationDate="2015-08-11T01:28:24.907" UserId="231" />
  <row Id="276" PostId="118" Score="0" Text="@trichoplax yuriks and joojaa seem to be confident in this answer so I'll defer to them." CreationDate="2015-08-11T02:47:12.823" UserId="125" />
  <row Id="277" PostId="177" Score="3" Text="Maybe questionslike this would require pictures" CreationDate="2015-08-11T04:44:58.767" UserId="38" />
  <row Id="278" PostId="193" Score="1" Text="I know about the Notification Center and Control Center. But I doubt that these blurs are calculated in real time as the active app is frozen while one of them is shown (or at least it does not update the UI anymore). But the links look promising. Thanks!" CreationDate="2015-08-11T06:40:26.967" UserId="127" />
  <row Id="279" PostId="183" Score="1" Text="@yuriks Yes, the question could use more detail as well, but I think the &quot;What have you tried?&quot; part is important. We should set ourselves similar quality standards as other SE sites like Stack Overflow and expect question authors to show *some* effort on their part. Otherwise, we'll just become a site where people go to have others do their work for them." CreationDate="2015-08-11T08:22:19.557" UserId="16" />
  <row Id="280" PostId="170" Score="0" Text="@yuriks Yes, I think we should definitely except if they are very specific programming questions related to computer graphics. In fact, I think that specific programming questions are what we are lacking the most so far (probably because these are harder to come up with if you don't encounter them right then and there)." CreationDate="2015-08-11T08:30:34.893" UserId="16" />
  <row Id="281" PostId="183" Score="1" Text="[Relevant meta discussion.](http://meta.computergraphics.stackexchange.com/a/69/16)" CreationDate="2015-08-11T08:57:32.987" UserId="16" />
  <row Id="282" PostId="177" Score="2" Text="Do you want to render stills or animation? If the cube is to be animated are you looking for real-time effects? And yes, if this wasn't the private beta, an image of your current results would be nice." CreationDate="2015-08-11T09:00:35.547" UserId="16" />
  <row Id="283" PostId="187" Score="1" Text="&quot;Normally, this kind of data will undergo a lot of processing after being loaded from a file until it is, for instance, displayed on the screen.&quot; like an image gets transformed to a pixel array before getting pushed on screen *regardless* of what the original format was. (hardware compression notwithstanding)" CreationDate="2015-08-11T09:04:51.697" UserId="137" />
  <row Id="284" PostId="199" Score="1" Text="It would be great if you could contain some information about this method within the answer to make it more self-contained." CreationDate="2015-08-11T09:39:32.047" UserId="16" />
  <row Id="285" PostId="196" Score="2" Text="This seems close to being a link-only answer. Could you include an explanation so that this can be understood even without following the link?" CreationDate="2015-08-11T11:21:46.560" UserId="231" />
  <row Id="286" PostId="184" Score="0" Text="Nice explanation and great links... Your video should be visible in this answer :)" CreationDate="2015-08-11T14:04:11.550" UserId="157" />
  <row Id="287" PostId="193" Score="1" Text="@Nero I suspect that's mostly for power reasons. When you swipe down from the Springboard to get to Spotlight, you get a variable-width blur depending on how far you've pulled down, and that's certainly real-time." CreationDate="2015-08-11T14:30:37.093" UserId="196" />
  <row Id="288" PostId="51" Score="0" Text="I added another possible method to my post. It may be worth clarifying your question with more detailed requirements. i.e. why does a 2D slice of 3D noise not satisfy your visual requirements? What are your performance requirements?" CreationDate="2015-08-11T15:02:26.827" UserId="196" />
  <row Id="289" PostId="201" Score="0" Text="It's not your question, but you see commonly on TV that 4:3 images are extended to widescreen by using a blurred and stretched copy in the background. That may work for you." CreationDate="2015-08-11T15:50:17.723" UserId="125" />
  <row Id="290" PostId="196" Score="0" Text="Apologies, you are correct.  I should have given screenshots at least, and it turns out i even linked to the wrong link!" CreationDate="2015-08-11T15:55:09.820" UserId="56" />
  <row Id="291" PostId="121" Score="0" Text="I'm not sure if this should go as an answer or not, but the errors in your image are caused by rendering without depth testing on *all* primitives. You should render the scene in 2 passes: First render normally all solid geometry. Afterwards, disable depth *writes* (not GL_DEPTH_TEST) and render translucent geometry in roughly back-to-front order. This will ensure that transparent geometry will not be drawn in front of solid geometry that's in front of it." CreationDate="2015-08-11T17:45:01.473" UserId="327" />
  <row Id="292" PostId="196" Score="1" Text="Thanks for the images, but it would also be interesting to see an explanation of how this works and why it improves the appearance. A good answer should provide understanding without the need to leave the site - then the links are there for further reading in more detail." CreationDate="2015-08-11T20:43:49.680" UserId="231" />
  <row Id="293" PostId="196" Score="0" Text="I mentioned you apply bump mapping to lighting and refraction calculations. Would you expect my answer to explain bump mapping?" CreationDate="2015-08-11T20:48:29.437" UserId="56" />
  <row Id="294" PostId="196" Score="0" Text="Explain whatever you feel would be helpful. If you are presenting something which answers the question, then you can explain why it helps." CreationDate="2015-08-11T21:41:51.340" UserId="231" />
  <row Id="295" PostId="209" Score="1" Text="A couple questions... are you doing the drawing logic on the CPU or GPU?  Also, are you looking for integer based algorithms or floating point?" CreationDate="2015-08-11T23:54:29.043" UserId="56" />
  <row Id="296" PostId="209" Score="5" Text="@AlanWolfe, integer algorithms on the CPU -- the same environment that Bresenham's algorithm was designed for." CreationDate="2015-08-11T23:56:30.247" UserId="158" />
  <row Id="297" PostId="209" Score="3" Text="https://en.wikipedia.org/wiki/Xiaolin_Wu%27s_line_algorithm is the classic one, though the wikipedia page is pretty half-baked and I don't have access to the paper. This feels like a lazy question though, since it's pretty easy to find this by doing some basic googling." CreationDate="2015-08-12T00:33:52.797" UserId="327" />
  <row Id="298" PostId="209" Score="0" Text="@yuriks, mind turning that into a full-fledged answer?" CreationDate="2015-08-12T00:34:21.103" UserId="158" />
  <row Id="299" PostId="209" Score="0" Text="I fat-fingered the enter key, so I just edited my comment. :)" CreationDate="2015-08-12T00:35:01.630" UserId="327" />
  <row Id="300" PostId="196" Score="1" Text="I had an awesome chance to see ice cubes close up today during dinner, and they seemed to actually be pretty smooth and without bumps. I think the vital part of this is giving them that &quot;wet&quot; look." CreationDate="2015-08-12T00:36:31.713" UserId="327" />
  <row Id="301" PostId="168" Score="1" Text="I guess the division by 300 is just a parameter for the sensitivity of the rotation given the displacement of the mouse?" CreationDate="2015-08-12T00:43:43.130" UserId="116" />
  <row Id="302" PostId="210" Score="2" Text="Are you using path tracing or physically-accurate ray tracing? If so, then the ambient occlusion is already a built-in consequence of the algorithm and does not need to be specifically modelled. Intuitively, to me it seems correct that the shadow is faint: your spheres are mirrors and so diffuse rays shot from the ground near the sphere will tend to be reflected away and towards the skybox instead of back towards the ground as would happen with diffuse spheres." CreationDate="2015-08-12T00:48:52.457" UserId="327" />
  <row Id="303" PostId="211" Score="0" Text="Wouldn't albedo values of 1 produce an image that never converges? Afaik, albedo should always be &lt; 1 for convergence to happen. An albedo of 1 would specifically not cause a contact shadow because rays would be perfectly reflected until they reach a light source such as the sky." CreationDate="2015-08-12T00:52:09.333" UserId="327" />
  <row Id="304" PostId="168" Score="0" Text="Correct. It's what happened to work well with my resolution at the time." CreationDate="2015-08-12T01:02:50.050" UserId="310" />
  <row Id="305" PostId="196" Score="0" Text="Interesting! I wonder what would make it look wet?" CreationDate="2015-08-12T01:35:33.710" UserId="56" />
  <row Id="306" PostId="213" Score="1" Text="It can be more accurate too.  The farther you get away from zero, the less resolution there is between integers, when using floating point numbers." CreationDate="2015-08-12T01:39:21.023" UserId="56" />
  <row Id="307" PostId="203" Score="2" Text="I remembered this paper I read some time ago. The authors compare spectral and RGB-rendered results with different illuminants. Unfortunately the comparison is done on a color chart, so I'm not sure how much the differences affect real life scenes. http://cg.cs.uni-bonn.de/en/publications/paper-details/lyssi-2009-btfspectral/" CreationDate="2015-08-12T02:12:42.263" UserId="327" />
  <row Id="308" PostId="214" Score="1" Text="Your ideas sound a bit like cone tracing and importance sampling, you might give them a read (:" CreationDate="2015-08-12T02:14:08.993" UserId="56" />
  <row Id="309" PostId="151" Score="2" Text="On your edit: Yes. You would need to transform your (x, y, z) values to world space using the View matrix." CreationDate="2015-08-12T02:19:09.637" UserId="310" />
  <row Id="310" PostId="215" Score="1" Text="Do you get a similarly smooth image from only the second light? i.e. it's definitely an artifact of multiple lights, not just the geometry of the second light?" CreationDate="2015-08-12T03:07:56.277" UserId="196" />
  <row Id="311" PostId="214" Score="2" Text="This sounds a bit like this thesis: http://www.graphics.cornell.edu/pubs/2004/Don04.pdf It's effectively importance sampling with an adaptive probability density function. My instinct is that it could work but you'd need to take care to avoid missing and therefore ignoring small features (small distant lights, say)." CreationDate="2015-08-12T03:27:44.193" UserId="196" />
  <row Id="312" PostId="214" Score="2" Text="Here's yet another paper on adaptive importance sampling: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.2520" CreationDate="2015-08-12T03:31:37.200" UserId="196" />
  <row Id="313" PostId="212" Score="0" Text="What do you mean by &quot;only compute the transform once&quot;? Meaning you don't have to compute the inverse transform, or you only ever need to transform an object once? Computing the inverse transform is cheap compared to a geometry intersection, and you definitely would need to transform an object multiple times (into the space of each different ray)." CreationDate="2015-08-12T03:35:20.343" UserId="196" />
  <row Id="314" PostId="215" Score="0" Text="Yes, using any kind of single light gives a smooth image. The noise only comes back when there are multiple lights to choose from. (And I suppose that the 3rd example image is somewhat misleading, but when there are 2 lights, the image looks almost as bad as the first one which uses the naive algorithm if I use only 1024 spp.)" CreationDate="2015-08-12T03:36:00.983" UserId="327" />
  <row Id="315" PostId="215" Score="0" Text="To me, that suggests a bug. But it's rather difficult to debug code just by reading it. I'd try disabling pieces of code—for example, in this scene the importance weight of the BRDF sample should be extremely small since light sampling is far more likely to hit the light, so you should be able to set brdf_pdf to 0 and only use light sampling and see if the noise goes away." CreationDate="2015-08-12T03:39:30.193" UserId="196" />
  <row Id="316" PostId="209" Score="2" Text="Just thinking out loud, I figure it should be easy to adapt Bresenham for drawing multi-pixel-thick lines. Then you can do antialiasing by calculating the distance of each pixel center from the mathematical ideal line, and applying some falloff function." CreationDate="2015-08-12T03:57:05.190" UserId="48" />
  <row Id="317" PostId="209" Score="1" Text="Regarding Bresenham being adapted to do AA, this page shows that with some simple code.  From the description of the xaolin wu algorithm it may be similar.  http://members.chello.at/easyfilter/bresenham.html" CreationDate="2015-08-12T04:06:42.050" UserId="56" />
  <row Id="318" PostId="212" Score="3" Text="I think the assumption is that all primitives would be transformed to a flat world-space position during scene preparation. (Which does dramatically increase memory usage.) Then rays would be directly intersected against the primitives without a preceding transformation between spaces, since they're now both in world-space." CreationDate="2015-08-12T04:19:11.740" UserId="327" />
  <row Id="319" PostId="213" Score="2" Text="@AlanWolfe, wouldn't the same resolution issues show up when computing the inverse transformation and applying it to the ray, though?" CreationDate="2015-08-12T04:27:13.073" UserId="158" />
  <row Id="321" PostId="215" Score="0" Text="@JohnCalsbeek Looks like you gave me a good lead! This is how it looks like if I ignore BRDF sampling: i.imgur.com/4c7bRCo.png I'll try to further track down the issue later when I have more time. It seems like the issue is that the impact of the BRDF is overestimated, causing it to overwhelm the light contribution in some cases." CreationDate="2015-08-12T05:54:57.620" UserId="327" />
  <row Id="322" PostId="121" Score="0" Text="@yuriks In this case it's probably a poor example on my part, but everything is meant to be transparent. I wanted something to show how wrong transparency might look when done badly. Also an example where sorting geometry would be amazingly difficult (for example here the floor is one giant polygon and covers the entire depth range)." CreationDate="2015-08-12T06:00:44.490" UserId="198" />
  <row Id="323" PostId="214" Score="1" Text="It's not quite the same thing, but the focus on areas of high gradient reminds me of [gradient-domain path tracing](https://mediatech.aalto.fi/publications/graphics/GPT/). However, before attacking a complex technique like that, I'd start with more basic things like stratified sampling and importance sampling to get the variance down." CreationDate="2015-08-12T06:24:04.383" UserId="48" />
  <row Id="324" PostId="214" Score="1" Text="Here's [a reference on stratified and low-discrepancy sampling](http://cg.informatik.uni-freiburg.de/course_notes/graphics2_04_sampling.pdf) btw." CreationDate="2015-08-12T06:30:08.633" UserId="48" />
  <row Id="325" PostId="211" Score="0" Text="In global illumination, there would be no contact shadow, yes. However, the question is about ambient occlusion, which only performs one bounce. Additionally, as long as the scene is not closed (i.e. the sky is visible), the scene would converge if global illumination was used even with albedo of 1." CreationDate="2015-08-12T08:20:01.360" UserId="79" />
  <row Id="326" PostId="215" Score="2" Text="You should actually add the significant parts of your code into the question instead of linking them elsewhere." CreationDate="2015-08-12T09:26:27.740" UserId="6" />
  <row Id="327" PostId="216" Score="1" Text="This seems to be more of a comment than an answer. Maybe you could elaborate on why fluorescent materials depend on an ultraviolet channel and provide some references?" CreationDate="2015-08-12T09:35:18.963" UserId="16" />
  <row Id="328" PostId="208" Score="3" Text="Ideally, answers should be self-contained and depend vitally on external links. Having links is nice for supplementary material, but an answer shouldn't just consist of a keyword. If you could include some details on what an A-buffer is and how it works that would greatly improve your answer." CreationDate="2015-08-12T09:38:58.897" UserId="16" />
  <row Id="329" PostId="218" Score="1" Text="Have you tried a closed surface with control points, i.e. nurbs or bspline? or free form deformation stuff? both of these are described by using point, but moving a point you deforms the surface described. (In the mean while i try to gather more info on the problem). I was even thinking to convex hull, but i'm not sure of the result, since a liquid deformation could be not convex at all." CreationDate="2015-08-12T10:08:43.980" UserId="228" />
  <row Id="330" PostId="51" Score="0" Text="@JohnCalsbeek Honestly, I don't have any hard requirements at hand. This was just a question I was curious about which I thought I'd give a go for the private beta. Of course, cutting a 2D slice out of 3D noise will be sufficient for many applications, but I'm sure it will have some performance impact and anisotropies (which may or may not be noticeable). &quot;Cutting the sphere out of 3D noise is your best option, because...&quot; is definitely a valid answer." CreationDate="2015-08-12T11:44:37.037" UserId="16" />
  <row Id="331" PostId="217" Score="2" Text="I'm not sure that's the correct takeaway. I would first double-check all the computations in the BRDF sampling branch. I think ShapeSphere::areaPdf looks suspect. It should depend on the distance squared and both the surface normal and the light normal." CreationDate="2015-08-12T13:37:15.447" UserId="196" />
  <row Id="332" PostId="203" Score="0" Text="Beer's law (absorption of color through a transparent object over distance) is hard to model with rgb." CreationDate="2015-08-12T13:50:13.877" UserId="56" />
  <row Id="333" PostId="219" Score="1" Text="Could you perhaps expand on your answer so it's self-contained. Reading it without the link and its contents, the answer isn't all that clear." CreationDate="2015-08-12T15:36:25.090" UserId="4" />
  <row Id="334" PostId="219" Score="0" Text="Actually i think the &quot;idea&quot; (not the implementation) is quite clear. @Cristian said he implemented the simulation on a set of points, basically he should consider the set of points as they belong to a deformable surface (i.e. a continuous surface described by a set of discrete points), then when the points position are updated the surface surface shape too is updated. The effectiveness depends on what he wants specifically to achieve, but i've assumed he wanted to use a kind of &quot;continuous&quot; instead of a discrete point set." CreationDate="2015-08-12T15:48:24.733" UserId="228" />
  <row Id="335" PostId="219" Score="0" Text="Also... i don't know why i can't use formulas here... but of course they would be useful for a more detailed explanation. However physically speaking good reference is any book of computational fluidodynamics, for rendering game physics of course, simplified methods could be found in literature too. For &quot;code&quot; itself depends on the environment he's using he can even find a complete implementation on the CUDA toolkit, i'm not sure about openCL, even the directX SDK presents some example of fluid simulation (espacially the last version, but probably it simulates using tons of points too)." CreationDate="2015-08-12T15:53:13.983" UserId="228" />
  <row Id="336" PostId="219" Score="3" Text="My understanding of the question is that the author has the simulation part covered, but is only asking about visualization. I think it'll prove to be quite difficult to create sets of spline patches that satisfactorily represent the &quot;shape&quot; of the water, and there is little detail on this answer about how you'd do that." CreationDate="2015-08-12T16:39:23.130" UserId="327" />
  <row Id="337" PostId="151" Score="0" Text="Actually, after reading your answer again and thinking about it. If I have the current position of the camera I can transform that into spherical coords and then add to the angles the delta produced by the mouse movement, then go back to cartesian coords and update my view matrix, would that make sense?" CreationDate="2015-08-12T16:47:01.913" UserId="116" />
  <row Id="338" PostId="150" Score="0" Text="@JohnCalsbeek ok im starting to build the analytic answer, its going to take a while" CreationDate="2015-08-12T17:50:18.053" UserId="38" />
  <row Id="339" PostId="225" Score="0" Text="Some others: photon mapping, sphere tracing, cone tracing" CreationDate="2015-08-12T20:11:02.493" UserId="56" />
  <row Id="340" PostId="210" Score="1" Text="do you know about srgb correction? If not that could be a factor.  http://http.developer.nvidia.com/GPUGems3/gpugems3_ch24.html" CreationDate="2015-08-12T21:57:00.563" UserId="56" />
  <row Id="341" PostId="62" Score="0" Text="I had some not-bad results interpolating near the edges of the tile (edge-wrapped), but it depends on what effect you're trying to achieve and the exact noise parameters. Works great for somewhat blurry noise, not so good with spikey/fine-grained ones." CreationDate="2015-08-13T03:17:25.727" UserDisplayName="user379" />
  <row Id="345" PostId="231" Score="2" Text="Do you happen to have links to any more info about tiled raytracing?  I'm guessing there are multiple ways that tiles can help, and probably different ways that people have exploited those benefits.  Trying to get a toe hold on some info to look a bit deeper at existing techniques." CreationDate="2015-08-13T03:39:39.467" UserId="56" />
  <row Id="346" PostId="231" Score="0" Text="@AlanWolfe, nothing really useful.  The 2x2 tiling was a paper I read several years ago and can't find now, while the larger tiles were just a passing mention on a webpage." CreationDate="2015-08-13T05:20:40.270" UserId="158" />
  <row Id="347" PostId="216" Score="1" Text="I mentioned this in my post just not using the word flourescent. Anyway this can be accomplised at shader level." CreationDate="2015-08-13T05:35:47.267" UserId="38" />
  <row Id="348" PostId="159" Score="3" Text="Subsurface scattering is for sure going to be important in such a close-up. You might look at Jorge Jimenez's work for some inspiration; see his talks at [SIGGRAPH 2012](http://advances.realtimerendering.com/s2012/index.html) and [GDC 2013](http://www.iryoku.com/stare-into-the-future). His work is for real-time but I'm sure some of the ideas can be adapted." CreationDate="2015-08-13T06:13:57.473" UserId="48" />
  <row Id="349" PostId="233" Score="3" Text="Short answer: &quot;light probes&quot; usually means a compact spherical harmonic representation of the environment. They're blurry both because they're very compact (only a few tens of bytes storage) and because they're prefiltered for use as diffuse lighting. I'll expand into a longer answer when I have a chance. :)" CreationDate="2015-08-13T06:18:42.793" UserId="48" />
  <row Id="350" PostId="226" Score="0" Text="related: http://computergraphics.stackexchange.com/questions/161/what-is-ray-marching-is-sphere-tracing-the-same-thing" CreationDate="2015-08-13T07:36:10.913" UserId="198" />
  <row Id="352" PostId="176" Score="0" Text="@yuriks `translucent: (of a substance) allowing light, but not detailed shapes, to pass through`. Translucent generally has the meaning used in the question, so this seems clear." CreationDate="2015-08-13T10:58:40.523" UserId="231" />
  <row Id="354" PostId="181" Score="0" Text="There is discussion and guidance about SE cross posting [on Meta Stack Exchange](http://meta.stackexchange.com/questions/64068/is-cross-posting-a-question-on-multiple-stack-exchange-sites-permitted-if-the-qu). Despite it saying not to cross post, I think it is a good thing that you have posted here and the question clearly fits here. The meta post mentions migrating rather than cross posting, and alternatively deleting the original post (since it is too late to migrate in this case, now that you have posted)." CreationDate="2015-08-13T11:21:38.833" UserId="231" />
  <row Id="355" PostId="233" Score="0" Text="Is there any context for your question? The term 'light probe' is ambigious and used differently in different scenarios, algorithms and engines." CreationDate="2015-08-13T12:07:22.630" UserId="385" />
  <row Id="356" PostId="233" Score="0" Text="Specifically, if you have been to shadertoy.com you'll see two sets of cube maps available for use.  One set is environment maps, the other set looks the same but blurry that are labeled light probes.  Just curious about that and light probes in general." CreationDate="2015-08-13T14:49:46.413" UserId="56" />
  <row Id="357" PostId="234" Score="0" Text="Just want to note that you can do actual integration of a ray path analytically as well, you don't have to use Ray marching if it's undesirable." CreationDate="2015-08-13T15:29:46.127" UserId="56" />
  <row Id="358" PostId="234" Score="0" Text="@AlanWolfe thats what you do in the uniform case, however if the medium participates with geometry then you need to do something more nifty. Anyway i didnt claim this is all methods." CreationDate="2015-08-13T15:53:14.523" UserId="38" />
  <row Id="359" PostId="234" Score="0" Text="For sure, just adding to your answer.  When you say uniform case not sure what you mean exactly but for the case of fog, it doesn't have to be uniform density, just some density function that you can integrate. Is that what you meant by uniform case?" CreationDate="2015-08-13T16:05:33.680" UserId="56" />
  <row Id="360" PostId="219" Score="0" Text="Many of us would like to see the ability to use MathJax for formulas, and plenty of other SE sites already have it. All we need to do is demonstrate a need for it. Any questions that would benefit from MathJax, just add a link to them to [this meta answer](http://meta.computergraphics.stackexchange.com/a/34/231)." CreationDate="2015-08-13T17:23:20.537" UserId="231" />
  <row Id="361" PostId="219" Score="0" Text="The additional information in your comments would make good additions to the answer." CreationDate="2015-08-13T17:25:43.667" UserId="231" />
  <row Id="363" PostId="241" Score="1" Text="Perhaps because CMYK has 4 components? RGB is a triangle, where each corner represents one of the components. So, it would makes sense that CMYK would be a 4 sided shape, since there are 4 components (Cyan, Magenta, Yellow, Black)" CreationDate="2015-08-13T19:54:53.377" UserId="310" />
  <row Id="366" PostId="241" Score="0" Text="@RichieSams yes, thats what i used to tgink. but black is absense of luminance the luminance is on a axis thats perpendicular on your screen. So black should be away from your viewing direction." CreationDate="2015-08-13T20:37:18.053" UserId="38" />
  <row Id="367" PostId="238" Score="0" Text="I've edited to add syntax highlighting - feel free to change the language if there's a more suitable one." CreationDate="2015-08-13T20:51:25.383" UserId="231" />
  <row Id="368" PostId="241" Score="0" Text="Ok, i think i know why... i need to check this from the media department. But ill leave this question for a while so others have time to answer the question. Its definitely not the K component." CreationDate="2015-08-13T21:06:05.027" UserId="38" />
  <row Id="369" PostId="242" Score="0" Text="Actually i think its clipped that way for the same reason that the cromaticity graphs outer edge abruptly changes direction. The space model is not entirely trivial but yes your right the space shape is complex. And its volume cant be drawn because my fogra measurements in the icc profile dont work all the way.+1" CreationDate="2015-08-13T21:23:10.087" UserId="38" />
  <row Id="370" PostId="241" Score="0" Text="Yea, that makes sense, now that I think about it. Black isn't a color. It's the absence of color." CreationDate="2015-08-13T21:23:33.710" UserId="310" />
  <row Id="371" PostId="243" Score="0" Text="iir is likely &quot;infinite impulse response&quot; which is a type of filtering (the other being finite impulse response). Not sure what rle could stand for. It makes me think of &quot;run length encoding&quot; but I don't think that's what it stands for." CreationDate="2015-08-14T04:03:51.747" UserId="56" />
  <row Id="372" PostId="243" Score="0" Text="I guess I was right about rle but don't understand how it applies.  http://docs.gimp.org/en/plug-in-gauss.html" CreationDate="2015-08-14T04:08:03.363" UserId="56" />
  <row Id="373" PostId="244" Score="0" Text="Great explanation! Just to confirm about the part about using trilinear filtering on the mips... Mips are preferred over volume textures because they use less resolution, and higher Blur levels need less resolution?" CreationDate="2015-08-14T05:37:33.373" UserId="56" />
  <row Id="374" PostId="244" Score="1" Text="I'd say that both these &quot;kinds&quot; of light probes are the same. Light probes are a measure of the radiance (usually pre-convoluted in some way) received at a point in the scene. Cubemaps and SH are just different ways to store/compute a light probe, making different storage/perf/quality trade-offs. (EDIT: Just to make it clear, I agree with the answer, I just think it's counter-productive to think of them as separate things.)" CreationDate="2015-08-14T05:39:21.823" UserId="327" />
  <row Id="375" PostId="216" Score="0" Text="@joojaa: Sorry.. missed that. I'd delete my post if there was an obvious button to do so. Though, having said that, I would say that you'd still need extra channels elsewhere (and not just shaders) to handle it, e.g. on-the-fly generation of environment maps." CreationDate="2015-08-14T08:50:14.530" UserId="209" />
  <row Id="376" PostId="216" Score="2" Text="Delete or don't delete it, same for me. I would rather see you  expand it., there's nothing wrong with supporting evidence and things said differently as long a you contribute with better clarity or new info." CreationDate="2015-08-14T09:19:49.443" UserId="38" />
  <row Id="377" PostId="240" Score="1" Text="Well, there are *quite* a bit more and *quite* a bit more advanced algorithms for mesh parameterization than what you linked to, so I wouldn't really subscribe to the idea that it *&quot;won't be pretty&quot;*, let alone such a short answer that barely touches the tip of the iceberg." CreationDate="2015-08-14T11:02:31.647" UserId="6" />
  <row Id="378" PostId="244" Score="0" Text="@AlanWolfe There is a use and that is to have maps for diffuse ligtning taken from a real environment (called gray ball/white ball), not just specular. Heres a disucssion of both [Mirror/Gray Ball Shaders](http://docs.autodesk.com/MENTALRAY/2015/ENU/mental-ray-help/files/shaders/production/prod_mirrorball.html)" CreationDate="2015-08-14T11:04:40.630" UserId="38" />
  <row Id="379" PostId="240" Score="0" Text="Could you elaborate/link any of those algorithms? I will be happy to add more to the answer." CreationDate="2015-08-14T11:40:49.107" UserId="310" />
  <row Id="380" PostId="240" Score="0" Text="Not at the moment really." CreationDate="2015-08-14T13:37:17.943" UserId="6" />
  <row Id="381" PostId="244" Score="1" Text="@AlanWolfe Correct&amp;mdash;you need less resolution on the more blurred levels. Also, cubemap volume textures aren't a thing that exists in hardware. :)" CreationDate="2015-08-14T18:12:25.503" UserId="48" />
  <row Id="382" PostId="243" Score="0" Text="&quot;RLE Gaussian Blur is best used on computer-generated images or those with large areas of constant intensity.&quot; Makes me think it's some kind of optimization of the blur process, that's somehow data-dependent so it works better on certain kinds of images? Is there any visible difference between the results of the two?" CreationDate="2015-08-14T21:32:57.017" UserId="48" />
  <row Id="383" PostId="243" Score="1" Text="This question seems very poorly formulated... You could have taken the time to do a quick search to at least get the definitions for the acronyms, since the previous commenters don't seem to know what they mean exactly." CreationDate="2015-08-15T04:18:03.440" UserId="54" />
  <row Id="384" PostId="252" Score="2" Text="Side by side images *really* make a difference for getting an intuitive feel for the explanation. I'm going to bear this in mind for my own answers." CreationDate="2015-08-15T09:49:46.923" UserId="231" />
  <row Id="385" PostId="240" Score="2" Text="@RichieSams: A very popular algorithm that quickly gained popularity is the least sqares conformal maps algorithm: https://www.cs.jhu.edu/~misha/Fall09/Levy02.pdf" CreationDate="2015-08-15T10:53:50.973" UserId="29" />
  <row Id="387" PostId="258" Score="0" Text="Does this mean that applying a Gaussian blur 4 times would give twice the size, as required?" CreationDate="2015-08-15T20:08:25.447" UserId="231" />
  <row Id="388" PostId="258" Score="0" Text="Yes, absolutely!" CreationDate="2015-08-15T20:12:56.647" UserId="79" />
  <row Id="389" PostId="257" Score="0" Text="Is the directional blurring you are referring to motion blur?" CreationDate="2015-08-15T20:14:08.747" UserId="231" />
  <row Id="390" PostId="257" Score="0" Text="An example before and after image would help confirm what type of blur is being asked about." CreationDate="2015-08-15T20:14:44.000" UserId="231" />
  <row Id="391" PostId="258" Score="0" Text="That covers the question entirely then - would it be worth mentioning how to get twice the size in the question so we can tidy away the comments?" CreationDate="2015-08-15T20:19:31.133" UserId="231" />
  <row Id="392" PostId="260" Score="5" Text="Rendering to a smaller texture and upsampling is a good way to do it. But if for some reason you really need to write to every 16th pixel of the large texture, using a compute shader with one invocation for every 16th pixel plus image load/store to scatter the writes into the render target could be a good option." CreationDate="2015-08-16T00:28:39.233" UserId="48" />
  <row Id="393" PostId="258" Score="0" Text="If it takes 4 to double the size, what size would 3  represent?" CreationDate="2015-08-16T01:01:10.063" UserId="56" />
  <row Id="394" PostId="258" Score="0" Text="I found the answer is N*sqrt(3). It turns out that the total Blur amount of multiple blurs is equal to the square root of the sum of the sizes squared. Wikipedia says that: https://en.wikipedia.org/wiki/Gaussian_blur" CreationDate="2015-08-16T01:08:55.603" UserId="56" />
  <row Id="395" PostId="15" Score="0" Text="Since [the wikipedia page](https://en.wikipedia.org/wiki/Subdivision_surface) lists a number of different improvements over the intervening time (almost 4 decades), and branching into several different types (approximating/interpolating, quads/triangles), I suspect this question may be too broad to be a good fit for this site." CreationDate="2015-08-16T11:58:54.183" UserId="231" />
  <row Id="396" PostId="15" Score="1" Text="I've [raised this on meta](http://meta.computergraphics.stackexchange.com/questions/82/is-this-question-too-broad) to see what people think." CreationDate="2015-08-16T12:06:31.653" UserId="231" />
  <row Id="397" PostId="263" Score="0" Text="I'll give this a shot, thanks" CreationDate="2015-08-16T14:05:42.110" UserId="56" />
  <row Id="398" PostId="266" Score="1" Text="My guess would be that if the UV coords are calculated in VS, the texture unit can start to prefetch them while the PS is starting. If they're calculated in the PS, the texture unit has to wait first." CreationDate="2015-08-16T14:41:33.030" UserId="310" />
  <row Id="399" PostId="266" Score="2" Text="Fwiw this is called a &quot;dependant texture read&quot;, in case it helps your search." CreationDate="2015-08-16T14:51:05.583" UserId="56" />
  <row Id="400" PostId="263" Score="0" Text="BTW just to help future folk. Bicubic interpolation will give higher quality results than bilinear when taking samples, at a higher computational cost." CreationDate="2015-08-16T15:21:07.823" UserId="56" />
  <row Id="401" PostId="271" Score="1" Text="I know there is Instancing in OpenGL, that's why I also posted an answer here. But maybe there is also some other way to achieve the same result." CreationDate="2015-08-16T16:28:04.357" UserId="127" />
  <row Id="402" PostId="266" Score="0" Text="Do you have some measurements showing the perf difference? I actually wouldn't expect there to be much difference at all; texture fetch latency should swamp a few ALU ops. BTW, dependent texture reads are where there are two (or more) texture reads, with the coordinates for the second dependent on the output of the first. Those are slower because of the strict ordering required between the two texture reads." CreationDate="2015-08-16T17:58:36.403" UserId="48" />
  <row Id="403" PostId="271" Score="1" Text="Seems you were right... :)" CreationDate="2015-08-16T21:17:37.130" UserId="231" />
  <row Id="404" PostId="15" Score="0" Text="@trichoplax I'll keep an eye on it, thanks" CreationDate="2015-08-16T21:29:33.483" UserId="70" />
  <row Id="405" PostId="266" Score="0" Text="Well, any operation done in the fragment shader will be more expensive then in the vertex shader. Each triangle takes 3 invocations of a vertex shader, but it might take orders of magnitude more invocations of the fragment shader, depending on its screen size." CreationDate="2015-08-17T01:13:54.647" UserId="54" />
  <row Id="406" PostId="51" Score="0" Text="You might check out this shadertoy which does noise on a sphere:&#xA;https://www.shadertoy.com/view/4sfGzS" CreationDate="2015-08-17T14:58:17.797" UserId="56" />
  <row Id="407" PostId="278" Score="1" Text="Have you tried the normal distribution (Gaussian function)? It seems that it would help here since it is used to figure out where things will be on average with certain characteristics of probabilities.  Dust settling randomly but less often where there's more airflow and more often in crevices seems right in its wheelhouse." CreationDate="2015-08-17T17:23:19.647" UserId="56" />
  <row Id="408" PostId="277" Score="2" Text="Are you sure they were talking about multiple gaussian blurs? Doing several Box blurs is a common way to approximate a gaussian blur." CreationDate="2015-08-17T17:26:18.647" UserId="327" />
  <row Id="409" PostId="277" Score="0" Text="Interesting info.  I believe so, yes, but could be mistaken!" CreationDate="2015-08-17T17:32:44.280" UserId="56" />
  <row Id="410" PostId="278" Score="0" Text="@AlanWolfe thanks for the suggestion - I've added in some more images based on that." CreationDate="2015-08-17T18:22:25.437" UserId="231" />
  <row Id="411" PostId="278" Score="0" Text="exponential looks better to me than linear or the normal distribution based one, but i don't have any non opinion answers to back anything up about correctness :P" CreationDate="2015-08-17T18:38:04.013" UserId="56" />
  <row Id="412" PostId="277" Score="1" Text="It may be simpler just to sample neighboring pixels, its also much more intuitive as a physical model of diffusion, see [12 steps to Navier-Stokes, step 7](http://nbviewer.ipython.org/github/barbagroup/CFDPython/blob/master/lessons/09_Step_7.ipynb)" CreationDate="2015-08-17T18:41:06.873" UserId="38" />
  <row Id="413" PostId="278" Score="0" Text="How about a cellular automata of some kind? Diffusion step and then erode diffuse then erode..." CreationDate="2015-08-17T18:51:53.140" UserId="38" />
  <row Id="414" PostId="280" Score="1" Text="possible duplicate of [Why is this conditional in my fragment shader so slow?](http://computergraphics.stackexchange.com/questions/259/why-is-this-conditional-in-my-fragment-shader-so-slow)" CreationDate="2015-08-18T08:46:09.777" UserId="16" />
  <row Id="415" PostId="280" Score="0" Text="As the answer explains on my question, the fragments get grouped into &quot;warps&quot; or &quot;wavefronts&quot; and if all fragments in such a group use the same branch, only that branch is executed." CreationDate="2015-08-18T08:47:44.803" UserId="16" />
  <row Id="416" PostId="280" Score="0" Text="But what about shaders different from fragment?" CreationDate="2015-08-18T08:58:20.317" UserId="386" />
  <row Id="417" PostId="280" Score="2" Text="I believe [vertices get assembled into warps or wavefronts just the same](https://fgiesen.wordpress.com/2011/10/09/a-trip-through-the-graphics-pipeline-2011-part-13/)." CreationDate="2015-08-18T09:01:05.100" UserId="16" />
  <row Id="418" PostId="280" Score="0" Text="The result being that if all threads follow the same path there will have less &quot;divergence&quot; and the shader will run faster. E.g. I don't think putting `if (true) { ... }` around the entire program would measurably alter the performance (assuming it's not optimized it out, which it would be)." CreationDate="2015-08-18T09:02:36.593" UserId="198" />
  <row Id="419" PostId="280" Score="0" Text="@jozxyqk I think the OP is more interested in wrapping the entire program in `if (false) { ... }` but yes. :)" CreationDate="2015-08-18T09:04:51.867" UserId="16" />
  <row Id="420" PostId="266" Score="0" Text="@NathanReed I don't think you have to limit &quot;dependent texture reads&quot; to just those that come from a previous texture access. I'd probably also include any coordinates computed in the frag shader, as opposed to those that can be determined merely from the linear (well, hyperbolic with perspective) interpolation of vertex attributes." CreationDate="2015-08-18T10:15:51.753" UserId="209" />
  <row Id="422" PostId="280" Score="1" Text="I suspect this is not a duplicate, but it needs to be edited to make it clear what is being asked before that can be determined. Some example code or an explanation of the two options being compared would help a lot." CreationDate="2015-08-18T13:29:55.827" UserId="231" />
  <row Id="423" PostId="280" Score="0" Text="`Is condition anyway will be executed every time?` Are you asking whether the condition is calculated for each pixel/vertex/fragment, or whether the condition is recalculated each time the shader is applied?" CreationDate="2015-08-18T13:31:58.820" UserId="231" />
  <row Id="424" PostId="280" Score="0" Text="Even though there is an accepted answer, the question will still benefit from being edited so that it is clear to future readers." CreationDate="2015-08-18T13:34:47.717" UserId="231" />
  <row Id="430" PostId="275" Score="0" Text="By the way: I experienced that on an iPad 3. So maybe this is actually hardware specific." CreationDate="2015-08-18T17:18:20.950" UserId="127" />
  <row Id="436" PostId="283" Score="0" Text="I think you might get a good answer from the signal processing or math site." CreationDate="2015-08-19T01:01:03.970" UserId="56" />
  <row Id="437" PostId="283" Score="1" Text="I'm hoping that asking on computergraphics.SE will lead to answers that don't just give me signal processing theory or mathematical proofs, but the perspective of people who work with and research computer graphics. There may be something I haven't thought of that makes the question irrelevant, or it may only matter in certain circumstances, and if so I want the computer graphics angle on that." CreationDate="2015-08-19T10:02:58.427" UserId="231" />
  <row Id="438" PostId="283" Score="0" Text="I've no idea how you would efficiently achieve random access to the final constructed data, nor how to extend it to 3D, but could you use something based on aperiodic tiling, e.g. https://en.wikipedia.org/wiki/Penrose_tiling ? i.e. have a random value at the centre of each tile?" CreationDate="2015-08-19T11:47:19.700" UserId="209" />
  <row Id="439" PostId="283" Score="0" Text="(caught by the edit period timeout) ...this was presuming you are concerned with more global alignment of the grid points and not just local effects." CreationDate="2015-08-19T11:53:46.887" UserId="209" />
  <row Id="440" PostId="283" Score="0" Text="@SimonF I'm happy to use any kind of aligned grid (just a square grid would be fine) if a random offset can be applied to each vertex using a probability distribution that results in no grid aligned average frequency variations. I'd be interested to hear about another grid type if that makes the frequency isotropic - or any approaches that haven't occurred to me. My suspicion is that an aperiodic tiling, although globally isotropic, will be locally biased for any practical length scales." CreationDate="2015-08-19T12:08:53.813" UserId="231" />
  <row Id="441" PostId="283" Score="0" Text="@trichoplax Another thought that occurred to me is that the displacements you're suggesting sound like the schemes used to approximate minimum distance Poisson disc distributions using a jittered grid, e.g as used for antialiasing.  I believe some care is needed when choosing how to generate those jittered offsets. I tried quick search in my papers collection and one that sprang up is &quot;Filtered Jitter&quot;, by V. Klassen, (http://onlinelibrary.wiley.com/doi/10.1111/1467-8659.00459/abstract). It's from 2000 so there may be better approaches, but it's surely worth a try." CreationDate="2015-08-19T13:05:35.270" UserId="209" />
  <row Id="443" PostId="284" Score="1" Text="What do you mean by &quot;texture baking&quot;? I'm not familiar with this use of the term." CreationDate="2015-08-19T14:23:54.310" UserId="196" />
  <row Id="444" PostId="283" Score="2" Text="Here's an interesting paper: http://www.cs.utah.edu/~aek/research/noise.pdf (useful keywords: &quot;Fourier spectrum&quot;)" CreationDate="2015-08-19T14:39:14.103" UserId="196" />
  <row Id="445" PostId="284" Score="0" Text="@Calsbeek, its turning 3d calculation on the surface back to a texture in 2d for re-using. Pixar has a paper or technical report where they coin the name. Id search for you but its a bit painfull to do these things on the phone while in transit." CreationDate="2015-08-19T14:39:17.913" UserId="38" />
  <row Id="446" PostId="166" Score="0" Text="See also [Massively-Parallel Vector Graphics](http://w3.impa.br/~diego/projects/GanEtAl14/), published in SIGGRAPH Asia 2014." CreationDate="2015-08-20T01:24:20.347" UserId="192" />
  <row Id="447" PostId="166" Score="0" Text="Between the options you've listed in your question and the Loop and Blinn paper, I think you've pretty much exhausted all the possibilities." CreationDate="2015-08-20T05:41:34.620" UserId="48" />
  <row Id="448" PostId="57" Score="1" Text="You might want to take a look into [finite-element methods](https://en.wikipedia.org/wiki/Finite_element_method), which also use triangulation (or more generally: simplices) and often face the problem of needing a higher sampling density in selected regions. They are bound to have developed algorithms for this." CreationDate="2015-08-20T06:56:31.913" UserId="394" />
  <row Id="449" PostId="293" Score="0" Text="You were like a few seconds faster than I and even have information about Spir-V, which I did not even know." CreationDate="2015-08-20T11:07:42.403" UserId="127" />
  <row Id="450" PostId="166" Score="0" Text="You can tessellate a line, like described [here](http://www.gamedev.net/page/resources/_/technical/directx-and-xna/d3d11-tessellation-in-depth-r3059). Or you can triangulate in a compute shader." CreationDate="2015-08-20T13:28:20.537" UserId="386" />
  <row Id="451" PostId="297" Score="2" Text="You're comparing a cube with a more complex mesh however. Why not replicate the same scenario? The Susan model is easy to get." CreationDate="2015-08-20T15:15:03.147" UserId="4" />
  <row Id="452" PostId="297" Score="0" Text="It's not so easy in a shadertoy implementation! (:" CreationDate="2015-08-20T15:16:15.037" UserId="56" />
  <row Id="453" PostId="297" Score="2" Text="You cube looks correct to me: Get more transparent as it approaches the edges. If you can do full blown Suzanne the a sphere should at least give a better approximation of the look in the other picture." CreationDate="2015-08-20T15:51:06.460" UserId="327" />
  <row Id="454" PostId="297" Score="0" Text="ok fair enough.  i'll try it with better geometry, but i think you are right.  I'll accept it if you make an aswer!" CreationDate="2015-08-20T16:37:42.147" UserId="56" />
  <row Id="455" PostId="299" Score="1" Text="GLSL functions cannot return a value? Hum? From where did you get this idea?" CreationDate="2015-08-20T17:41:34.143" UserId="54" />
  <row Id="456" PostId="280" Score="0" Text="If you add some example code to your question, we can more easily tell you if it is a compile time constant or not (:" CreationDate="2015-08-20T17:59:09.763" UserId="56" />
  <row Id="457" PostId="299" Score="0" Text="I always used to the value-return calling convention as described here https://www.opengl.org/wiki/Core_Language_%28GLSL%29#Functions. If it is possible to define the return type and use &quot;return variable;&quot; syntax I will change it. EDIT: nvm, you're right. The OpenGL 4.5 specification says it should be possible (but I don't know when it was introduced)" CreationDate="2015-08-20T18:14:37.627" UserId="64" />
  <row Id="458" PostId="299" Score="1" Text="Return values have always been supported, AFAIK. If you look at the [1.2 specification, section 6.1](https://www.opengl.org/registry/doc/GLSLangSpec.Full.1.20.8.pdf), you can see the function declaration syntax: `returnType functionName (type0 arg0, type1 arg1, ..., typen argn);`. Plus, a lot of builtin functions return a value... Perhaps the existence of `in/out` parameters has tricked you into thinking it didn't support return values, but both are orthogonal concepts. Apart from that, your answer is pretty good, btw ;)" CreationDate="2015-08-20T18:26:30.610" UserId="54" />
  <row Id="459" PostId="299" Score="1" Text="Yeah, I probably just looked at the wiki instead of the specification itself. Never occurred to me that both methods are possible, although indeed the buildin functions do use it." CreationDate="2015-08-20T18:42:21.180" UserId="64" />
  <row Id="460" PostId="300" Score="0" Text="This question would benefit from a few (artificial) sample pictures" CreationDate="2015-08-20T18:59:56.403" UserId="38" />
  <row Id="462" PostId="304" Score="0" Text="Indeed, the idea was to use complex 3D models, but surround them with rough brushes for portaling. I had brifely considered using voxelization, but dismissed it due to thinking the cost of traversing it would be prohibitive, but I didn't even think of using a tree structure! (Which looks obvious in restrospect...)" CreationDate="2015-08-20T21:26:14.777" UserId="327" />
  <row Id="463" PostId="303" Score="0" Text="Now that you mentioned culling this makes a lot of sense. I just checked and there's a culling mask property in light objects to control which meshes (in a layers) are affected by it." CreationDate="2015-08-20T21:39:29.170" UserId="250" />
  <row Id="465" PostId="281" Score="4" Text="This is true, but it is *not* the only thing you need to consider for performance. GPUs still statically schedule resources per shader, so this may still resources as though you were executing both branches, which may hurt occupancy." CreationDate="2015-08-21T06:16:53.847" UserId="196" />
  <row Id="467" PostId="306" Score="0" Text="As a matter of fact Adam already posted source code on shadertoy. Here's the link: https://www.shadertoy.com/view/ltXSDB" CreationDate="2015-08-21T13:02:37.213" UserId="250" />
  <row Id="468" PostId="306" Score="0" Text="You had me excited.  He did post the bezier stuff on shadertoy but not the texture distance field stuff!" CreationDate="2015-08-21T14:33:14.070" UserId="56" />
  <row Id="469" PostId="313" Score="1" Text="Thanks for answering. What does BSDF stand for?" CreationDate="2015-08-21T15:41:28.387" UserId="433" />
  <row Id="470" PostId="298" Score="6" Text="The new explicit graphics APIs aren't really designed for consumption by the average graphics programmer, they're more for the people doing infrastructure work in engines, or for those who *really* need the performance. They give you a much bigger gun and point it right at your foot." CreationDate="2015-08-21T16:07:06.500" UserId="174" />
  <row Id="471" PostId="313" Score="3" Text="BSDF = Biderectional Scattering Distribution Function" CreationDate="2015-08-21T16:41:42.903" UserId="100" />
  <row Id="472" PostId="315" Score="4" Text="There can definitely be differences between IHV implementations. It could be due to a driver bug, or different interpretations of ambiguities in the standard, or even possibly differences in how the compilers treat floating-point arithmetic, etc. Would need some more detailed debugging to understand what's going on." CreationDate="2015-08-21T20:20:52.653" UserId="48" />
  <row Id="474" PostId="306" Score="0" Text="@AlanWolfe I think he has done only for procedurally set bezier curves. I'm not sure the effort required to integrate this into a ttf render lib. When I have some time I'll take a look at it." CreationDate="2015-08-21T22:07:14.407" UserId="250" />
  <row Id="475" PostId="306" Score="0" Text="it looks he has some magic sauce on the side of actually storing and retrieving the distances from a texture.  Without a texture in play, the shadertoy examples are missing that part of the equation." CreationDate="2015-08-21T22:11:30.443" UserId="56" />
  <row Id="476" PostId="315" Score="1" Text="@NathanReed ,what kind of information could be helpful?" CreationDate="2015-08-22T00:20:14.553" UserId="437" />
  <row Id="477" PostId="315" Score="7" Text="Start by debugging it like any other graphics/shader problem. Isolate each pass and see in which pass the error is being introduced, then isolate where in that shader is something going wrong." CreationDate="2015-08-22T00:47:23.243" UserId="48" />
  <row Id="478" PostId="320" Score="0" Text="I dont know but it certainly does make sense not to gamma correct." CreationDate="2015-08-22T12:27:02.963" UserId="38" />
  <row Id="479" PostId="319" Score="1" Text="This is actually a pretty good method. But is it less expensive?" CreationDate="2015-08-22T12:28:39.017" UserId="38" />
  <row Id="480" PostId="311" Score="0" Text="It would be great if you had reference footage for the effect you want. Say, something like this? https://www.youtube.com/watch?v=XH-groCeKbE" CreationDate="2015-08-22T15:20:44.137" UserId="196" />
  <row Id="481" PostId="311" Score="0" Text="@JohnCalsbeek yes that would make it easier to get across what I want. In the video you linked to the individual birds are discernable (just). I'm looking to render a flock a little more distant so that individuals are not visible, but the variations in density are still consistent and realistic." CreationDate="2015-08-22T15:24:42.263" UserId="231" />
  <row Id="482" PostId="26" Score="2" Text="I believe Simplex noise is only patented for 3D and above." CreationDate="2015-08-22T15:49:00.753" UserId="231" />
  <row Id="483" PostId="319" Score="0" Text="It's as expensive as you want your simulation to be. If it's *too* expensive, use fewer samples." CreationDate="2015-08-22T16:02:29.190" UserId="197" />
  <row Id="484" PostId="320" Score="0" Text="I'm not posting this as an answer since I'm not confident in it, but human vision's perception of brightness is *not* linear. In fact, sRGB does a quite good job of compensating for that and giving the most precision in the areas that matter. So you might find that gamma correcting before compressing luma may actually yield worse results." CreationDate="2015-08-22T19:55:49.240" UserId="327" />
  <row Id="489" PostId="324" Score="0" Text="If you are worried about the square root, you can probably square both sides of the equation." CreationDate="2015-08-24T00:26:41.520" UserId="56" />
  <row Id="490" PostId="324" Score="0" Text="@AlanWolfe I can precompute the square root - it's only ever root 2. My main reason for changing the side length is the significant reduction in area. I just wonder if it broke anything..." CreationDate="2015-08-24T00:35:46.710" UserId="231" />
  <row Id="491" PostId="320" Score="0" Text="AFAIK, video standards assume R'G'B', ie. a non-linear colour space, when applying the 3x3 colour transforms to/from YCbCr.  In an application such as video where one wants to maximise quality per bit, it doesn't make sense to use linear.&#xA; I think sections 27 and 29 of Charles Poyton's Color FAQ express it more clearly: http://poynton.com/notes/colour_and_gamma/ColorFAQ.html#RTFToC27" CreationDate="2015-08-24T07:54:04.493" UserId="209" />
  <row Id="492" PostId="320" Score="0" Text="&quot;Video demystified&quot; also says: _&quot;YCbCr is the color space originally defined by BT.601, and now used for all digital component video formats. .... The technically correct notation is&#xA;Y'Cb'Cr' since all three components are derived from R'G'B'.&quot;_" CreationDate="2015-08-24T08:07:05.627" UserId="209" />
  <row Id="493" PostId="247" Score="0" Text="Subdivision surfaces are used a lot more than constructive solid geometry. They still involve triangles (or alternatively splines)." CreationDate="2015-08-24T18:06:28.447" UserDisplayName="user458" />
  <row Id="494" PostId="326" Score="1" Text="Isn't the point of stereoscopic rendering to render the occluded parts of both views for each eye?" CreationDate="2015-08-24T20:55:31.253" UserId="197" />
  <row Id="495" PostId="326" Score="0" Text="IMO, occlusion isn't really the point of it.&#xA;&#xA;The point is the illusion of depth that happens when giving different levels of parallax to each eye based on an object's distance." CreationDate="2015-08-24T20:58:08.737" UserId="56" />
  <row Id="496" PostId="325" Score="0" Text="This is going to be a zero net comment, but... just wanted to mention, if you are unsure of performance, you could profile and see.  But, of course, there might be different characteristics on different hardware that you might not have access to, and you may not be aware of the ways it might be faster or slower.  Like, texture reads are really cheap til you are texture read bound :P" CreationDate="2015-08-24T23:36:28.950" UserId="56" />
  <row Id="497" PostId="328" Score="0" Text="I've watched that presentation before, it is very good. I did not know about what you mention in the first paragraph though, so thanks for that info!" CreationDate="2015-08-25T01:39:07.027" UserId="54" />
  <row Id="498" PostId="318" Score="0" Text="I hadn't heard of bilateral up sampling. Do you have any links by chance? It's different than bicubic or bilinear sampling right?" CreationDate="2015-08-25T01:42:47.320" UserId="56" />
  <row Id="499" PostId="329" Score="0" Text="The question lacks some basic explanation about what is being asked. What exactly are you trying to do? From your video, what seems to be happening is that when the rotation crosses a certain point, the rotation instantly snaps back/forward by 180 degrees. (That is, the colors aren't being swapped, you're seeing the back of the cube instead.)" CreationDate="2015-08-25T04:39:10.677" UserId="327" />
  <row Id="500" PostId="329" Score="0" Text="I assume that the `m_setPan` path isn't used here? Also, why does your rotation code modify position? (And I don't see it setting a rotation anywhere.)" CreationDate="2015-08-25T04:49:56.480" UserId="327" />
  <row Id="501" PostId="329" Score="0" Text="You assume correct. What I'm doing is to orbit the camera over a sphere centered at the origin, therefore given the displacement in (x,y) of the mouse, I compute the spherical coords of the position of the camera on that sphere, once I have placed the camera I update the view-matrix (done inside `setPosition()`)" CreationDate="2015-08-25T04:53:57.070" UserId="116" />
  <row Id="502" PostId="329" Score="1" Text="Just as a note, this is the kind of thing that can be most easily debugged by printing the `m_position`, `theta` and `phi` values to the screen on every update and observing their values as you drag the cube around. This should make it easy to observe any singularities or abnormalities that could be causing the problem." CreationDate="2015-08-25T05:06:13.067" UserId="327" />
  <row Id="503" PostId="330" Score="2" Text="This is usually called &quot;stereo reprojection&quot;, since one way of implementing it is to take the matrix that transforms from the new perspective to the old perspective." CreationDate="2015-08-25T05:15:09.187" UserId="327" />
  <row Id="504" PostId="330" Score="0" Text="@yuriks that makes sense." CreationDate="2015-08-25T05:16:25.527" UserId="38" />
  <row Id="505" PostId="333" Score="0" Text="You could render every pass to a texture and read back the buffers to obtain the output." CreationDate="2015-08-25T06:08:38.767" UserId="127" />
  <row Id="507" PostId="331" Score="1" Text="or use atan2 which is there to deal with all quadrants correctly" CreationDate="2015-08-25T07:53:23.183" UserId="137" />
  <row Id="508" PostId="302" Score="1" Text="FWIW, some monitors come with a built-in motorised calibration unit. It's amusing to watch the &quot;arm&quot; rise out of the bezel so it can sample the screen output during the calibration process." CreationDate="2015-08-25T09:20:40.457" UserId="209" />
  <row Id="509" PostId="331" Score="2" Text="@ratchetfreak Yes, but in general, solution that avoid trigonometry or angles tend to be more robust and elegant." CreationDate="2015-08-25T10:06:14.517" UserId="327" />
  <row Id="510" PostId="335" Score="0" Text="This is a bit too much stuff for one question. I can essentially boil down my introductory lecture on the subject in a few hours. Bit going into detail of use of De casteljanu and the de boor's algorithm would take me too much time." CreationDate="2015-08-25T13:42:10.423" UserId="38" />
  <row Id="511" PostId="335" Score="1" Text="So i would like to see questions 3, 5 and possibly 6 split off as separate questions to make answering and understanding more meal sized." CreationDate="2015-08-25T13:51:16.213" UserId="38" />
  <row Id="512" PostId="335" Score="0" Text="@joojaa Sure, I can split the question, just a sec..." CreationDate="2015-08-25T14:03:19.830" UserId="141" />
  <row Id="513" PostId="335" Score="1" Text="So the question [3](http://computergraphics.stackexchange.com/questions/338), [5](http://computergraphics.stackexchange.com/questions/339/nurbs-curve-drawing) and [6](http://computergraphics.stackexchange.com/questions/340/splitting-of-nurbs-curves) were split off to separate questions." CreationDate="2015-08-25T14:11:09.837" UserId="141" />
  <row Id="514" PostId="340" Score="0" Text="I don't know if it will do the same, but De Boor's algorithm is the equivalent of De Casteljeau.  Interestingly, I know you can use De Boor's algorithm to split a NURBS or b-spline into a piecewise Bezier curve." CreationDate="2015-08-25T14:44:00.587" UserId="56" />
  <row Id="515" PostId="338" Score="0" Text="What do you mean exactly by offset curves?" CreationDate="2015-08-25T14:44:27.810" UserId="56" />
  <row Id="516" PostId="338" Score="0" Text="An &quot;offset&quot; curve has constant distance to the given curve, aka [parallel curve](http://mathworld.wolfram.com/ParallelCurves.html)." CreationDate="2015-08-25T14:47:56.580" UserId="141" />
  <row Id="525" PostId="342" Score="2" Text="I don't have a citation, but if I recall correctly Monster's University was the first Pixar movie to move towards a near-full radiosity solution instead of doing lots of artist controlled &quot;False Radiosity,&quot; so it would certainly seem like it's still used." CreationDate="2015-08-25T18:38:31.753" UserId="174" />
  <row Id="528" PostId="341" Score="0" Text="My understanding is that if your not using the peculiarities of nonuniformness/rationality  then they are using  possibly the same berentein basis formulations. In fact i seem to remember somebody telling me that in fact B-splines started that way. Its only later that we have realized they are the same. Some examples: https://reference.wolfram.com/language/ref/BSplineBasis.html" CreationDate="2015-08-25T19:33:50.240" UserId="38" />
  <row Id="529" PostId="225" Score="4" Text="I vote to close this question, because it is too broad. There are simply too many variants, especially when including *possibly others* like the ones [Alan Wolfe named in his comment](http://computergraphics.stackexchange.com/questions/225/ray-based-rendering-terms#comment339_225). More specific/narrow questions that compare a few techniques e.g. to achieve a specific goal could very well be suited for this format." CreationDate="2015-08-25T20:29:11.020" UserId="127" />
  <row Id="530" PostId="341" Score="0" Text="Right. A bezier curve is a special case of a bspline and a bspline is a special case of a NURBS." CreationDate="2015-08-25T21:47:18.070" UserId="56" />
  <row Id="531" PostId="344" Score="0" Text="The notation for path tracing suggests that it can't handle paths like `ES*L` but of course it can if they are area lights (not punctual lights). Plus, I think that statement in your reference [2] is just plain wrong. Path tracing doesn't ignore caustics; it's just not very efficient at them (photon mapping, Metropolis, VCM etc. are better)." CreationDate="2015-08-25T22:20:52.263" UserId="48" />
  <row Id="532" PostId="344" Score="0" Text="Thanks Ecir for the explanation (specially the regex... I wonder if they ever considered E{2} for both eyes ;). When I mentioned &quot;ray tracing&quot; I was kind of quoting the tutorial of Cornell University, they didn't mention any specific technique, that's why I was doubting if radiosity was a type or partly belonged to ray tracing. So if you were to create a diffuse reflection, would you choose path-tracing over radiosity? Why (which one would be more efficient)?" CreationDate="2015-08-26T01:17:20.417" UserId="157" />
  <row Id="533" PostId="225" Score="1" Text="I would split this question to a few more questions.. We already have a pretty good answer for [ray-marching](http://computergraphics.stackexchange.com/questions/161/what-is-ray-marching-is-sphere-tracing-the-same-thing)." CreationDate="2015-08-26T10:27:57.190" UserId="38" />
  <row Id="534" PostId="343" Score="0" Text="regarding (*), Bezier curves have the same problem.  The issue there is that the curves (in both cases) are defined as x = f(t), y = f(t). However, you can also define a univariate / explicit / 1 dimensional curve (again, in both cases) as y = f(x), using x in place of t.  In the case of rational curves, instead of being able to represent conic sections, you can represent sine and cosine (and more of course).  NURBS / b-splines aren't special in that regard." CreationDate="2015-08-26T14:04:49.080" UserId="56" />
  <row Id="536" PostId="343" Score="0" Text="I think your statement about length is wrong (only works with linear functions?), and not sure how length calculations is supposed to fit in your explanation (good info you gave, just sayin'!)" CreationDate="2015-08-26T14:16:36.920" UserId="56" />
  <row Id="538" PostId="343" Score="0" Text="@AlanWolfe deleted anyway" CreationDate="2015-08-26T14:21:08.100" UserId="38" />
  <row Id="539" PostId="287" Score="1" Text="Just for future people's curiosity, you might want to make &quot;another question&quot; be a link to that question." CreationDate="2015-08-26T15:22:03.767" UserId="174" />
  <row Id="540" PostId="287" Score="1" Text="@porglezomp that's a good point - done." CreationDate="2015-08-26T15:26:13.020" UserId="231" />
  <row Id="541" PostId="342" Score="2" Text="Games use fake fill lights all the time. One good example I know of is Tomb Raider (2013). There was an awesome presentation about it at GDC 2013. http://www.gdcvault.com/play/1017934/Casting-a-New-Light-on" CreationDate="2015-08-26T15:41:45.873" UserId="310" />
  <row Id="542" PostId="351" Score="4" Text="Also, I've gotten the impression that &quot;albedo&quot; in astronomy is averaged over the whole visible spectrum (and sometimes a wider spectrum, including UV and infrared), while diffuse/specular coefficients are RGB or ideally spectral quantities." CreationDate="2015-08-26T21:00:52.863" UserId="48" />
  <row Id="544" PostId="344" Score="1" Text="@NathanReed I asked about it at [ompf2](http://ompf2.com/viewtopic.php?f=6&amp;t=2057) and ingenious says: &quot;The only type of light paths that a forward path tracer cannot sample is E(D|G)*S+L, where L is a light source whose definition involves a delta distribution, either in the directional emission or the positional. Examples are point lights and directional lights. Such paths can be described using Veach's extended notation for luminaires and sensors, see section 8.3.2 in his thesis.&quot;" CreationDate="2015-08-26T21:10:36.907" UserId="141" />
  <row Id="545" PostId="344" Score="0" Text="@Armfoot I would definitely go with path tracing. Lots of research, books, code to learn from. I don't know which would be faster, though, too many variables (acceleration structure, shading system, ...). Radiosity apparently simulates the heat propagation after splitting the scene into many tiny triangles ([FEM](https://en.wikipedia.org/wiki/Finite_element_method)), I never tried it and the only product to used it I know of was Autodesk Lightscape. Last but not least, are you really sure you will ever need only diffuse reflections?" CreationDate="2015-08-26T21:19:59.523" UserId="141" />
  <row Id="546" PostId="343" Score="0" Text="Most awesome! Thanks a lot, very good explanation!" CreationDate="2015-08-26T21:28:37.290" UserId="141" />
  <row Id="547" PostId="343" Score="0" Text="Typo perhaps? &quot;Instead the underlying surface has a customizable parameter range. The parameter is stored in something called a knot, and each knot can have a arbitrary value that is bigger than the next.&quot; -&gt; &quot;Instead the underlying **curve** has a customizable parameter range. The parameter is stored in something called a knot, and each knot can have a arbitrary value that is bigger than the **previous**.&quot; Btw., could you please clarify what you mean by &quot;UV range&quot;? &quot;UV&quot; implies 2D..?" CreationDate="2015-08-26T21:31:50.683" UserId="141" />
  <row Id="548" PostId="344" Score="0" Text="@Armfoot The notation doesn't use E{2} for the same reason that it doesn't use L{n} for multiple lights. This describes a single path, or a single sample. The way that we normally formalise Monte Carlo rendering is to take the Kajiya rendering equation, and then turn it into a random variable, the expected value of which is the solution to the equation. You can then calculate the value of a pixel by taking lots of samples and estimating the mean. Light paths more or less correspond to Feynman diagrams." CreationDate="2015-08-26T21:34:20.893" UserId="159" />
  <row Id="549" PostId="352" Score="0" Text="I'm sorry I'm very new to NURBS: what do you mean by &quot;maximum multiplicity&quot;? I mean, when I do it in the former way, do I end up with multiple overlapping control points?" CreationDate="2015-08-26T22:03:14.283" UserId="141" />
  <row Id="550" PostId="351" Score="3" Text="Interesting. In that case, it doesn't make sense to call something Albedo Map does it?" CreationDate="2015-08-26T22:31:51.383" UserId="250" />
  <row Id="551" PostId="354" Score="0" Text="This seems like several interesting questions in one. At least (4), (5) and (6) sound like independent questions, and maybe (2) and (3) could be a single question. I'd upvote any of those as separate questions, but all in one it seems too broad." CreationDate="2015-08-26T22:31:57.127" UserId="231" />
  <row Id="552" PostId="351" Score="0" Text="@PhilLira that does seem like an unhelpful use of the term... Hopefully it won't catch on and cause more confusion..." CreationDate="2015-08-26T22:34:28.340" UserId="231" />
  <row Id="553" PostId="354" Score="0" Text="Ok! Will do separates and then delete this. thx!" CreationDate="2015-08-26T22:34:30.550" UserId="250" />
  <row Id="554" PostId="354" Score="0" Text="Actually, I just edited this and kept link to others." CreationDate="2015-08-26T22:54:09.513" UserId="250" />
  <row Id="555" PostId="352" Score="0" Text="Let me try to explain in the answer." CreationDate="2015-08-27T01:20:13.860" UserId="159" />
  <row Id="556" PostId="353" Score="0" Text="Hey mjp how are you. I remember you from gamedev.net. I asked a question about actual usages of curves in games and I remember you gave some good info.  Howdy!" CreationDate="2015-08-27T02:17:52.843" UserId="56" />
  <row Id="557" PostId="166" Score="0" Text="You should also look into signed distance fields and signed distance textures." CreationDate="2015-08-27T02:19:42.313" UserId="56" />
  <row Id="558" PostId="352" Score="1" Text="Pseudonym err no not a good knot vector to demonstrate this. I See thet i might need to expand the other post. Altough @EcirHana it might be a good idea to ask what a multilicity is." CreationDate="2015-08-27T09:03:53.187" UserId="38" />
  <row Id="559" PostId="344" Score="0" Text="Thanks again Ecir, I get your point: if we can have a lot more with one method, it's kind of pointless to pursue another if we don't know exactly how better it will perform." CreationDate="2015-08-27T10:14:45.790" UserId="157" />
  <row Id="560" PostId="344" Score="0" Text="I was trying to make a little joke when I mentioned E{2} @Pseudonym but thanks for further explaining why it's a single path and the reason behind it :)" CreationDate="2015-08-27T10:22:20.827" UserId="157" />
  <row Id="561" PostId="343" Score="0" Text="@EcirHana Done do you need the multiplicity explanation here" CreationDate="2015-08-27T11:05:22.887" UserId="38" />
  <row Id="562" PostId="343" Score="0" Text="@joojaa Thanks! Yes, I've just posted another [question](http://computergraphics.stackexchange.com/questions/358/)" CreationDate="2015-08-27T11:20:51.860" UserId="141" />
  <row Id="563" PostId="359" Score="0" Text="You might want to add meta on wether or not this is a relevant qestion?" CreationDate="2015-08-27T12:05:32.300" UserId="38" />
  <row Id="564" PostId="359" Score="0" Text="@joojaa no one has complained about it yet, but feel free to raise it on meta if you like." CreationDate="2015-08-27T12:14:03.480" UserId="231" />
  <row Id="565" PostId="360" Score="0" Text="When you say &quot;knots lie on top of each other&quot;, you mean that their values don't change in the knot vector? So in your example [0, 0, 0, 0], [1, 1 ,1], [2, 2, 2] all &quot;lie on top of each other&quot;?" CreationDate="2015-08-27T12:33:48.817" UserId="141" />
  <row Id="566" PostId="360" Score="0" Text="@EcirHana yes that is what i mean a more normal parametrisation would be [0, 0, 0, 0, 0.5, 1, 1.5, 2, 2, 2, 2] Ill add a animation" CreationDate="2015-08-27T12:45:26.903" UserId="38" />
  <row Id="567" PostId="361" Score="0" Text="The cross sectional area visible dimishes as your incidence angle grows." CreationDate="2015-08-27T13:01:45.767" UserId="38" />
  <row Id="568" PostId="352" Score="0" Text="You're probably right about that @joojaa." CreationDate="2015-08-27T13:03:38.617" UserId="159" />
  <row Id="569" PostId="361" Score="1" Text="@joojaa I follow that bit, but the bit in bold seems to be talking about tilting the surface away from its initial normal vector, which would only make sense for the specific case that the incident light is perpendicular to the surface, or I'm missing something." CreationDate="2015-08-27T13:10:07.617" UserId="231" />
  <row Id="570" PostId="360" Score="0" Text="Just to check: so to split a NURBS curve the main task is to insert knots so that the overall shape remains unchanged, correct?" CreationDate="2015-08-27T13:28:23.723" UserId="141" />
  <row Id="571" PostId="360" Score="0" Text="Yes, once you have enough knots you can delete whats on the other side." CreationDate="2015-08-27T13:31:22.490" UserId="38" />
  <row Id="572" PostId="360" Score="0" Text="There is a different strategy also you can insert one knot and delete the points that nolonger affect the knot you get a slightly different parametrisation though. I prefer this method for most of my own modeling. But nobody else seems to have heard of this." CreationDate="2015-08-27T13:34:43.487" UserId="38" />
  <row Id="573" PostId="360" Score="0" Text="Out of curiosity, what kind of modeling software do you use? Or you mean in your own code?" CreationDate="2015-08-27T13:54:31.220" UserId="141" />
  <row Id="574" PostId="360" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/27460/discussion-between-joojaa-and-ecir-hana)." CreationDate="2015-08-27T13:56:28.700" UserId="38" />
  <row Id="575" PostId="166" Score="0" Text="Something to keep an eye on: https://twitter.com/sheredom/status/636572086211903488" CreationDate="2015-08-27T16:20:06.833" UserId="141" />
  <row Id="577" PostId="356" Score="1" Text="What do you mean by vertical vs horizontal layout of triangles?" CreationDate="2015-08-28T04:04:39.290" UserId="197" />
  <row Id="578" PostId="338" Score="0" Text="I wonder what property of NURBS make them able to do this.  I would think it would be that they are rational, but then rational bezier curves would also have this property." CreationDate="2015-08-28T04:07:40.140" UserId="56" />
  <row Id="579" PostId="315" Score="0" Text="This might be a vsync issue if you're using a variable timestep per-frame." CreationDate="2015-08-28T04:09:40.887" UserId="197" />
  <row Id="580" PostId="323" Score="2" Text="This might be a better question for [Physics.SE](https://physics.stackexchange.com/) or [Astronomy.SE](https://astronomy.stackexchange.com/). I know a point mass does produce lensing effects (see e.g. [this](http://spiro.fisica.unipd.it/~antonell/schwarzschild/)) but no idea if a galaxy can be well-approximated by a point mass for something like this." CreationDate="2015-08-28T05:40:39.697" UserId="48" />
  <row Id="581" PostId="364" Score="2" Text="Great answer! Thank you!" CreationDate="2015-08-28T05:51:19.167" UserId="197" />
  <row Id="582" PostId="333" Score="1" Text="As an improvement to depth peeling, you might look into [Multi-Layer Depth Peeling Via Fragment Sort](http://research-srv.microsoft.com/pubs/70307/tr-2006-81.pdf) which allows you to extract multiple layers in each pass, reducing the total number of passes needed." CreationDate="2015-08-28T06:10:02.113" UserId="48" />
  <row Id="584" PostId="366" Score="3" Text="Besides a source of advancements its also a great way to learn and an even better place to find examples. I know myself have learned shaders over the past year mostly through Shadertoy. I've found it to be such an open source community it's awesome how everyone shares their techniques." CreationDate="2015-08-28T15:32:31.113" UserId="376" />
  <row Id="585" PostId="366" Score="4" Text="Good point! I also have to say, that as a professional game programmer, when I see a demo scene person make something I didn't even think was possible, it makes me want to learn about it and try to bring those techniques into the games I'm working on." CreationDate="2015-08-28T15:46:22.373" UserId="56" />
  <row Id="586" PostId="359" Score="0" Text="It really does seem like a &quot;conversation&quot; and not a question that can be answered.  I know on other stack exchange sites that they prefer questions that can be answered, but not sure what the policy here is." CreationDate="2015-08-28T16:17:22.667" UserId="56" />
  <row Id="587" PostId="365" Score="1" Text="Thanks! That was really helpful. Regading the constants/uniform cache. Are they any tips besides precision (mediump, lowp) I could use to improve uniforms cache hit ratio? Does the order in which I declare uniforms make any difference (as for packing more tightly)?" CreationDate="2015-08-28T18:32:24.263" UserId="250" />
  <row Id="588" PostId="359" Score="0" Text="@AlanWolfe we're in the progress of deciding collectively what our policy will be, so go ahead and mention anything you find relevant on Meta. That way we can have clear guidelines before opening up to a wider community in public beta. I do like to ask questions on the borderline to try and kick start that discussion about policy..." CreationDate="2015-08-28T18:33:44.007" UserId="231" />
  <row Id="589" PostId="365" Score="2" Text="@PhilLira Packing can make a difference, yeah. The compiler will insert padding to prevent vectors from being split across 16-byte boundaries, so try to avoid that. I don't think mediump/lowp actually does anything on uniforms, at least on desktop GPUs (maybe it does on mobile). I wouldn't worry too much about uniform cache hit ratio though. That's extremely rarely, if ever, a bottleneck." CreationDate="2015-08-28T20:23:42.223" UserId="48" />
  <row Id="590" PostId="367" Score="2" Text="GPUs work in a different way. (Some architectures) don't have the concept of a global &quot;program stack&quot;, so recursive function calls are not possible in those. OpenCL probably adopts the lowest common denominator, thus disallowing it completely to remain portable across GPUs. Newer CUDA hardware seems to have introduced support for recursion at some point: http://stackoverflow.com/q/3644809/1198654" CreationDate="2015-08-29T02:04:27.370" UserId="54" />
  <row Id="591" PostId="368" Score="5" Text="I'm reluctant to share this secret sauce, but I've had pretty good luck having a fixed maximum bounce count and having a stack of a fixed size (and a loop with a fixed number of iterations) to handle this. Also (and this is the real secret sauce imo!) I have my materials be either reflective or refractive but never both, which makes it so rays don't split when they bounce. The end result of all this is recursive type raytraced rendering, but through fixed size iteration, not recursion." CreationDate="2015-08-29T03:03:43.217" UserId="56" />
  <row Id="592" PostId="366" Score="0" Text="...and on the other hand, Wolfenstein 3D used ray marching (in 2D)." CreationDate="2015-08-29T11:05:01.710" UserId="159" />
  <row Id="593" PostId="366" Score="0" Text="Oh right totally! John Carmack did some amazing things with ray casting" CreationDate="2015-08-29T14:25:16.227" UserId="56" />
  <row Id="594" PostId="370" Score="1" Text="Have you seen this? Not a whole lot of info but some.  http://www.gamedev.net/topic/573051-lighting-in-object-space/" CreationDate="2015-08-29T18:53:47.497" UserId="56" />
  <row Id="595" PostId="372" Score="1" Text="Can you confirm whether the code shown (with blue removed) also causes the artefacts? If you have narrowed down the code then showing the image from the narrowed down code will help exclude any irrelevant details and highlight the problem." CreationDate="2015-08-30T17:23:05.893" UserId="231" />
  <row Id="596" PostId="369" Score="0" Text="Welcome to Compiter Graphics.SE,  glad to see a familiar face im sure you will be a great addition to our small community. Sorry I cant answer your question though." CreationDate="2015-08-30T18:51:35.957" UserId="38" />
  <row Id="601" PostId="203" Score="0" Text="@trichoplax Sorry for the noise!" CreationDate="2015-08-30T20:41:41.770" UserId="482" />
  <row Id="602" PostId="203" Score="0" Text="@luserdroog thanks for the interest :) Even though this question is only about materials, we could do with new questions related to colour spaces..." CreationDate="2015-08-30T20:43:36.530" UserId="231" />
  <row Id="603" PostId="374" Score="0" Text="Are you looking for a general method that you can apply to an arbitrary Bezier surface, or a way of preparing a fast method for a specific surface? Will your surface shape be fixed before runtime?" CreationDate="2015-08-30T22:05:22.257" UserId="231" />
  <row Id="606" PostId="209" Score="0" Text="@Mark-Both the answers suggested by Alan Wolfe and yuriks are correct" CreationDate="2015-08-31T02:27:59.957" UserDisplayName="user489" />
  <row Id="608" PostId="374" Score="1" Text="Note that you can raymarch bezier surfaces a lot easier than raytracing it. You can also raytrace or raymarch univariate surfaces a lot easier than other kinds!  http://blog.demofox.org/2015/07/28/rectangular-bezier-patches/" CreationDate="2015-08-31T04:16:10.380" UserId="56" />
  <row Id="609" PostId="372" Score="0" Text="Yes it does, I figured I'd take a screenshot with colour in there otherwise it looks even uglier :)" CreationDate="2015-08-31T04:54:39.483" UserId="193" />
  <row Id="610" PostId="209" Score="2" Text="I can't mark a comment as correct, though." CreationDate="2015-08-31T04:59:46.233" UserId="158" />
  <row Id="611" PostId="379" Score="0" Text="Technically light from the sun is white." CreationDate="2015-08-31T08:09:59.047" UserId="137" />
  <row Id="612" PostId="379" Score="0" Text="@ratchetfreak: True; there's something to say about temperature, but the topic is unrelated to the original question." CreationDate="2015-08-31T09:17:15.757" UserId="182" />
  <row Id="613" PostId="372" Score="0" Text="I can understand that, but since it's the ugliness that you're asking for help with, you might have a better chance of someone seeing the problem if you include the screenshot that matches the code too." CreationDate="2015-08-31T11:04:49.763" UserId="231" />
  <row Id="614" PostId="372" Score="1" Text="Fair enough. I've added an extra screenshot." CreationDate="2015-08-31T13:35:00.863" UserId="193" />
  <row Id="615" PostId="376" Score="0" Text="think about it this way: a deep crease will have shadows in it." CreationDate="2015-08-31T14:21:11.977" UserId="56" />
  <row Id="616" PostId="372" Score="0" Text="Have you tried playing around a bit more with the bias?" CreationDate="2015-08-31T19:36:47.713" UserId="100" />
  <row Id="617" PostId="372" Score="0" Text="This looks like shadow acne https://msdn.microsoft.com/en-us/library/windows/desktop/ee416324(v=vs.85).aspx - this is what the bias factor is supposed to alleviate, as cifz suggests try adjusting the value." CreationDate="2015-08-31T19:49:59.960" UserId="495" />
  <row Id="618" PostId="376" Score="1" Text="The key thing to understand here is that we are trying to calculate occlusion of the ambient light, not occlusion from view." CreationDate="2015-08-31T20:15:57.220" UserId="231" />
  <row Id="619" PostId="368" Score="0" Text="Like tail recursion?" CreationDate="2015-08-31T20:34:32.567" UserId="240" />
  <row Id="621" PostId="325" Score="0" Text="Desktop or mobile? Uniforms can be surprisingly costly on some mobile GPUs." CreationDate="2015-08-31T21:50:42.060" UserId="506" />
  <row Id="622" PostId="306" Score="0" Text="Bit late to the party but this older thread from reddit has a ton of info on various methods of improving the sharpness of SDF based rendering: https://www.reddit.com/r/gamedev/comments/2879jd/just_found_out_about_signed_distance_field_text/" CreationDate="2015-08-31T21:56:04.753" UserId="102" />
  <row Id="623" PostId="388" Score="2" Text="You are probably thinking of occlusion of one point from another. If you think of bodies or light sources with non-zero spatial extent, then the object can be partially occluded (like how the sun may be more or less occluded during a solar eclipse)." CreationDate="2015-08-31T23:26:19.390" UserId="16" />
  <row Id="628" PostId="390" Score="0" Text="&quot;the surface is lit based on how many of those directions are not occluded by objects in the scene. &quot; I unterstand this. But the question is from where comes the light? Where is the origin?&#xA;&#xA;Because if I have two balls, and the ambient lighting has it's origin between those balls, then there won't be nothing occluded, because there is nothing between objects and light source. But if the origin is behind one of the balls, then one ball is occluded by the other. -&gt; http://i.imgur.com/IRDvCzF.png&#xA;So the position of the source is important to determine what will be occluded. But Amb.L. has no pos." CreationDate="2015-09-01T00:28:55.947" UserId="480" />
  <row Id="629" PostId="392" Score="5" Text="Great answer. You might want to add that one way to think about affine transforms is that they keep parallel lines parallel. Hence, scaling, rotation, translation, shear and combinations, count as affine. Perspective projection is an example of a non-affine transformation." CreationDate="2015-09-01T06:08:21.133" UserId="14" />
  <row Id="630" PostId="393" Score="0" Text="I did post an answer, but i don't think looking up Wiktionary/wikipedia for you is a good use of this sites resources. Anyway you may wish to refine your question or we can wait for community opinion. Setting boundaries for what is and what is not in scope, is still valuable for the community." CreationDate="2015-09-01T07:51:17.370" UserId="38" />
  <row Id="631" PostId="392" Score="2" Text="You could add some pictures. If you wont I will :P Also might be good to mention order in matrix and row/column orientation is arbitrary. And that rotations in 3d are not comutative." CreationDate="2015-09-01T07:54:03.350" UserId="38" />
  <row Id="632" PostId="390" Score="2" Text="@Joey With one point light source every point on a surface will be either lit or not. With two point light sources a point on a surface may be lit by zero, one or two lights, giving three different light levels. With many point lights, there is gradual variation in the lighting. Ambient occlusion pretends that there are an infinite number of point light sources in the distance in all directions. This is not physically realistic, but it gives an approximation to the lighting in  a real scene, where light is reflected from the objects in the scene so that they are all lit by second hand light." CreationDate="2015-09-01T09:05:56.367" UserId="231" />
  <row Id="633" PostId="393" Score="0" Text="@joojaa I've added a [meta post](http://meta.computergraphics.stackexchange.com/questions/133/should-we-allow-word-definition-questions) so people can discuss whether word definitions should be on topic." CreationDate="2015-09-01T09:24:47.337" UserId="231" />
  <row Id="634" PostId="393" Score="1" Text="I'm voting to close this question as off-topic because its something you should be able to google in 60 seconds." CreationDate="2015-09-01T11:30:01.913" UserId="38" />
  <row Id="635" PostId="396" Score="0" Text="Oops. the shared models of the red, metal and green balls don't react to ambient light but they do react to my ambient occlusion light. Hope you get the point nonetheless." CreationDate="2015-09-01T11:40:49.123" UserId="38" />
  <row Id="636" PostId="393" Score="0" Text="Thought I marked answer as accepted I'm not 100% satisfied (but that's maybe because I asked wrongly). What is *temporal* in some algorithm? The answers says *temporal* means *depends on time*. But which time? How can I measure something, say 10 seconds ago? And *stochastic*. Of course I googled it before asking and far more than 60 second. But how is it differ from *random*?" CreationDate="2015-09-01T12:19:49.823" UserId="386" />
  <row Id="637" PostId="393" Score="0" Text="The answers answer to what you asked. The time is one :) How to get info from previous frames is a different question (you can reconstruct infos, store them in a buffer etc. etc.). Sthocastic and random are basically synonyms, often you use sthocastic to describe a process and random to describe a variable." CreationDate="2015-09-01T13:13:59.727" UserId="100" />
  <row Id="638" PostId="390" Score="2" Text="It might help a bit for understanding to specify that ambient occlusion is usually simulating light coming from the &quot;sky,&quot; which would clear up questions about what the source of the light is." CreationDate="2015-09-01T13:23:28.933" UserId="174" />
  <row Id="639" PostId="390" Score="2" Text="@porglezomp that's a useful way of thinking about it to gain understanding, but it can also be used in a closed room with no sky, where the ambient occlusion is the occlusion of the light reflected multiple times from the walls." CreationDate="2015-09-01T13:44:08.270" UserId="231" />
  <row Id="640" PostId="390" Score="0" Text="@trichoplax Yeah, that's why I put it in quotes." CreationDate="2015-09-01T13:48:59.960" UserId="174" />
  <row Id="641" PostId="397" Score="0" Text="I feel like temporal coherence could be used to denoise video as well." CreationDate="2015-09-01T16:17:03.930" UserId="56" />
  <row Id="642" PostId="297" Score="1" Text="I can't separate the refraction from the attenuation. Can you render the cube with IOR=1.0 please?" CreationDate="2015-09-01T16:19:11.247" UserId="523" />
  <row Id="643" PostId="297" Score="0" Text="added imallett, also linked to the shadertoy" CreationDate="2015-09-01T16:26:15.773" UserId="56" />
  <row Id="644" PostId="393" Score="0" Text="I think you should ask the question that you want to know. Like i sad its very clear what the words mean but the implementation of those things is a different story." CreationDate="2015-09-01T16:52:55.770" UserId="38" />
  <row Id="648" PostId="386" Score="0" Text="I updated the second item as the edit wasn't what I meant. Otherwise, it looks great!" CreationDate="2015-09-01T23:15:18.120" UserId="511" />
  <row Id="649" PostId="297" Score="1" Text="@AlanWolfe Your IOR=1 render looks exactly as I would expect it, and I skimmed the shadertoy impl and it looks good." CreationDate="2015-09-02T01:26:01.410" UserId="523" />
  <row Id="651" PostId="382" Score="0" Text="On modern hardware, I'm unaware of hardware-based Phong modes. Also, incidentally, for programmable shaders, which by now are ubiquitous, people almost always use microfacet-based BRDFs." CreationDate="2015-09-02T02:14:37.727" UserId="523" />
  <row Id="652" PostId="359" Score="0" Text="At SIGGRAPH this year, there was a demoscener who showed an old demo. They did texture mapping with two instructions per pixel, by using self-rewriting code. Not exactly a discovery, but pretty neat." CreationDate="2015-09-02T02:21:52.700" UserId="523" />
  <row Id="653" PostId="382" Score="0" Text="My point is that the speed increase of Gourard over Phong isn't enough to justify the loss of quality -- modern computers can (or at least should) be able to do both in realtime." CreationDate="2015-09-02T02:32:31.847" UserId="158" />
  <row Id="654" PostId="382" Score="0" Text="The way you phrased it, it sounded like there is fixed-function Phong functionality, while I don't believe there is. Separately, you say you should use Phong &quot;if quality is important&quot;, but Phong is actually a poor quality BRDF model. By some measures, worse even than Blinn-Phong, which is what hardware Gouraud shading uses to shade vertices." CreationDate="2015-09-02T02:37:40.240" UserId="523" />
  <row Id="655" PostId="392" Score="2" Text="@joojaa I made pictures! [postscript sources](https://groups.google.com/d/topic/comp.lang.postscript/m2QqV4QFFaM/discussion)" CreationDate="2015-09-02T02:45:00.567" UserId="482" />
  <row Id="656" PostId="403" Score="1" Text="I aim to leave regional spelling differences as the author intended, and only standardise in tags (where it matters). However, for my own posts I am interested in the accepted local spelling, so I'll be using matte from now on - thank you :)" CreationDate="2015-09-02T06:17:13.163" UserId="231" />
  <row Id="657" PostId="405" Score="2" Text="I personally see this question more on PostScript than it is on computer graphics." CreationDate="2015-09-02T07:45:08.153" UserId="100" />
  <row Id="658" PostId="387" Score="2" Text="I answered a similar question http://gamedev.stackexchange.com/questions/23/what-is-ambient-occlusion/66638#66638" CreationDate="2015-09-02T09:10:06.683" UserId="8" />
  <row Id="661" PostId="406" Score="0" Text="I dont really see the point of generating the encapsulation with ps2eps cant you just type the eps header in your template. It makes the instruction cleaner (and user needs less dependenies). Instead of converting each file separately do `mogrify -format png *.eps`" CreationDate="2015-09-02T09:43:01.093" UserId="38" />
  <row Id="663" PostId="386" Score="0" Text="Hi Christophe, did you mean &quot;equilateral&quot; triangles rather than &quot;isosceles&quot;?&#xA;&#xA;Rather than &quot;Hilbert&quot; I'd have said &quot;Morton&quot; order as the addressing is *much* easier in hardware." CreationDate="2015-09-02T11:04:42.923" UserId="209" />
  <row Id="664" PostId="407" Score="0" Text="When you say &quot;more colorful&quot; do you mean specifically as if looking through tinted glass?" CreationDate="2015-09-02T11:48:29.010" UserId="231" />
  <row Id="665" PostId="407" Score="0" Text="Yes, that's what i mean" CreationDate="2015-09-02T11:50:53.190" UserId="205" />
  <row Id="666" PostId="407" Score="0" Text="Do you also want to take the tint colour as a separate input, or do you just want an arbitrary tint colour that happens to be cheap to implement?" CreationDate="2015-09-02T11:51:10.013" UserId="231" />
  <row Id="667" PostId="407" Score="0" Text="Sorry, I think I don't understand your question. Could you explain that more?" CreationDate="2015-09-02T11:53:58.373" UserId="205" />
  <row Id="668" PostId="408" Score="0" Text="What do you mean by &quot;composite&quot;? Is there some mathematical operation  that lets me to composite two colours?" CreationDate="2015-09-02T12:00:33.587" UserId="205" />
  <row Id="669" PostId="408" Score="0" Text="That's what the example equations in his post do." CreationDate="2015-09-02T12:51:13.703" UserId="327" />
  <row Id="670" PostId="386" Score="0" Text="@Christophe thanks! This is really helpfull. So, for the border pixels, doesn't texture cache matter? That's was kind of what I was wondering. So, If I have a triangle that intersects tiles (x, y) and (x+1, y) and GPU just rasterized tile (x, y). Assuming tile (x+1, y) will be next, even if a different Execution Units processes it,  won't I benefit from texture cache when sampling texels for this triangle?" CreationDate="2015-09-02T12:57:25.303" UserId="250" />
  <row Id="671" PostId="386" Score="0" Text="Also, I got curious about the Hilbert pattern. I always assumed this was true for block compressed textures. Is this true for all textures? &#xA;&#xA;PS: I also didn't follow the last paragraph." CreationDate="2015-09-02T13:00:59.193" UserId="250" />
  <row Id="672" PostId="356" Score="0" Text="@Mokosha sorry, this somehow got unnoticed to me. I just saw it now. This is more a theoretical than pratical question and I don't even know if this makes sense now. Anyway, what I meant was, say a triangle intersect tiles (x, y) and (x+1, y) and that the these two tiles get processed one after another. Would that be a better for texture cache than If I had a triangle intersecting (x, y) and (x, y+1)? (Because of the border pixels and the layout of triangles not being in the same direction the tiles processing is)" CreationDate="2015-09-02T13:06:14.887" UserId="250" />
  <row Id="673" PostId="408" Score="2" Text="I don't think this is what he wants. If you wear red-tinted glasses, you see red wavelengths at basically full strength, and very little from other wavelengths. That's not a great match for alpha compositing." CreationDate="2015-09-02T14:58:30.927" UserId="196" />
  <row Id="674" PostId="407" Score="0" Text="If [Kostas Anagnostou's answer](http://computergraphics.stackexchange.com/a/410/231) is what you want then ignore my comments. I was just asking whether you wanted that or a hardcoded colour." CreationDate="2015-09-02T15:32:20.770" UserId="231" />
  <row Id="675" PostId="411" Score="0" Text="Pretty much any 3d animation software should be able to do this, so in alphabetic order 3ds Max, Blender, Maya..." CreationDate="2015-09-02T15:33:33.073" UserId="38" />
  <row Id="676" PostId="405" Score="1" Text="@cifz right this would fit [GD.SE](http://graphicdesign.stackexchange.com/) better. But thats not a reason why it can not be here." CreationDate="2015-09-02T19:12:16.817" UserId="38" />
  <row Id="677" PostId="410" Score="0" Text="Also you might want to tint something other than 100% pure one channel color." CreationDate="2015-09-02T19:14:16.117" UserId="38" />
  <row Id="679" PostId="411" Score="0" Text="@trichoplax, the models are things like an upper arm, or a hand." CreationDate="2015-09-02T20:35:11.567" UserId="540" />
  <row Id="682" PostId="406" Score="0" Text="Cool. I didn't know about `mogrify`. For the `ps2eps` part, it automatically calculates (and adds) the bounding-box info. Omitting that part, `convert` will produce full-page-sized images" CreationDate="2015-09-02T20:48:28.243" UserId="482" />
  <row Id="683" PostId="405" Score="0" Text="@citz I think I see what you mean. What if I take it out of the question, and open it up for tikz and asymptote and whatnot." CreationDate="2015-09-02T21:20:08.617" UserId="482" />
  <row Id="684" PostId="412" Score="5" Text="[Vote for MathJax formatting on this site.](http://meta.computergraphics.stackexchange.com/questions/18/should-we-have-mathjax-support)" CreationDate="2015-09-02T22:28:52.713" UserId="482" />
  <row Id="685" PostId="412" Score="2" Text="I've added this question to that meta post as an example of further need for MathJax" CreationDate="2015-09-02T23:14:53.637" UserId="231" />
  <row Id="686" PostId="414" Score="1" Text="`Here is what a cube looks like (...) using my own path tracer.`  &#xA;Do you happen to have open-sourced it by any chance ?" CreationDate="2015-09-03T02:53:30.507" UserId="110" />
  <row Id="687" PostId="414" Score="2" Text="No, not yet. I was planning on finishing and releasing this particular variant along with a blog post about glass rendering, but it's been on my backlog for a while." CreationDate="2015-09-03T04:27:27.400" UserId="207" />
  <row Id="689" PostId="410" Score="3" Text="I went ahead and added some example images to the post - hope you don't mind!" CreationDate="2015-09-03T05:29:52.243" UserId="48" />
  <row Id="690" PostId="410" Score="0" Text="Not at all Nathan, thanks!" CreationDate="2015-09-03T07:33:27.863" UserId="270" />
  <row Id="691" PostId="407" Score="0" Text="For more realistic results, you might find this answer interesting: http://photo.stackexchange.com/a/49267/23075" CreationDate="2015-09-03T09:06:49.237" UserId="110" />
  <row Id="692" PostId="349" Score="0" Text="I depends on the amount of calculations.GPU is really fast about math operations.So it is a matter of try and benchmarking." CreationDate="2015-09-03T09:30:42.943" UserId="213" />
  <row Id="693" PostId="405" Score="0" Text="I am answering here to show that this is a bit opinionated as a question, and answers definitely will be. Maybe this entire thread should live in our Meta, like this does on [GD.SE](http://meta.graphicdesign.stackexchange.com/questions/790/how-to-embed-screen-capture-videos-as-animated-gifs-in-answers). Any thoughts, @cifz" CreationDate="2015-09-03T10:15:48.103" UserId="38" />
  <row Id="697" PostId="410" Score="0" Text="That is exactly what I meant. Thanks for effort and examples :)" CreationDate="2015-09-03T16:17:26.723" UserId="205" />
  <row Id="699" PostId="326" Score="1" Text="It is not blue you are thinking of; it is cyan. There are two options: Red/Cyan, and Magenta/Green. In both cases, all three human cone types (color channels) are covered, not just two. I think Magenta/Green is universally recognized as being superior but I could be misremembering; I prefer it anyway." CreationDate="2015-09-03T16:26:25.673" UserId="504" />
  <row Id="700" PostId="369" Score="0" Text="Sorry, but I'm a bit lost as to what you are aiming to do. Are you trying to write an &quot;arbitrary polygon&quot; filling routine that also, say, clips to the sides of the viewing rectangle?" CreationDate="2015-09-03T16:32:18.670" UserId="209" />
  <row Id="703" PostId="424" Score="0" Text="Will your subpixel-rendered images still look okay if viewed on screens in a portrait orientation?" CreationDate="2015-09-03T18:28:39.267" UserId="551" />
  <row Id="704" PostId="428" Score="3" Text="Playing devil's advocate, if possible to do color subpixel rendering (i'm a skeptic!), it seems like it could be useful in VR situations, where they struggle to have sufficient resolution.  Perhaps on (non retina?) phones or tables as well?" CreationDate="2015-09-03T18:30:18.223" UserId="56" />
  <row Id="705" PostId="428" Score="1" Text="@AlanWolfe yes but its also 3 times as expensive to render" CreationDate="2015-09-03T18:36:03.860" UserId="38" />
  <row Id="706" PostId="428" Score="0" Text="yeah for sure, that's true. There are some situations where that isn't a problem though.  For instance, I know of a couple algorithms where you don't need to shoot rays for every pixel every frame.  It seems like this could also have play in rasterized graphics btw.  Again, i am not convinced color subpixel rendering is a real thing, but ya know, IF! :P" CreationDate="2015-09-03T18:39:14.930" UserId="56" />
  <row Id="707" PostId="427" Score="0" Text="With that explanation it totally makes sense, thank you! In case you remember, how was your experience with cone tracing in comparison to normal ray tracing? Of course it's an approximation, but does it achieve a considerable speedup at acceptable quality?" CreationDate="2015-09-03T19:07:57.057" UserId="385" />
  <row Id="708" PostId="428" Score="0" Text="@AlanWolfe I don't understand your doubts. All images are already rendered with sub pixels, but the colours are misaligned by between a third and a half of a pixel. I can't see how correcting that misalignment would fail to produce a higher quality image. Do you have any specific concerns (which might make a good question...)?" CreationDate="2015-09-03T19:10:56.147" UserId="231" />
  <row Id="709" PostId="428" Score="0" Text="@trichoplax Quality would riase at the cost of portability and simplicity. Not a good trade if the quality increase is minuscule. Its more important to know the color profile of the device than this." CreationDate="2015-09-03T20:02:04.447" UserId="38" />
  <row Id="711" PostId="215" Score="0" Text="Try randomly picking a light source *per sample* instead of per pixel." CreationDate="2015-09-03T21:01:10.693" UserId="553" />
  <row Id="714" PostId="428" Score="0" Text="@joojaa I agree that the effort is unlikely to be worth it. I'm just asserting that it is possible. My final section is meant as advice to not work on this unless there is a purely theoretical interest." CreationDate="2015-09-03T21:57:48.737" UserId="231" />
  <row Id="715" PostId="405" Score="1" Text="[Meta question concerning this question.](http://meta.computergraphics.stackexchange.com/questions/147/simple-2d-illustrations-question-main-or-meta)" CreationDate="2015-09-03T22:09:35.070" UserId="482" />
  <row Id="716" PostId="428" Score="3" Text="@joojaa There's no reason it would be 3x as expensive to render. You wouldn't need to shoot 3x the number of rays; you'd just apply 3 different weights when accumulating the rays into the framebuffer. Effectively, you're using a different antialiasing kernel for each color channel." CreationDate="2015-09-03T23:02:28.283" UserId="48" />
  <row Id="717" PostId="429" Score="0" Text="Nathan you are the one that wrote that about depth precision? Wow cool... i read that thing and it helped a lot! small world :P" CreationDate="2015-09-03T23:03:24.317" UserId="56" />
  <row Id="718" PostId="405" Score="0" Text="I wish we could use latex figures like we can latex math markup :P" CreationDate="2015-09-03T23:04:05.170" UserId="56" />
  <row Id="720" PostId="1431" Score="0" Text="By displaying the images magnified, you entirely negate the benefit of using subpixels, so the comparison images aren't representative of the actual results. You should try posting the original-size images if possible." CreationDate="2015-09-04T05:48:17.307" UserId="327" />
  <row Id="721" PostId="1431" Score="1" Text="@yuriks There's another smaller resolution that I can find (http://journals.cambridge.org/fulltext_content/SIP/SIP1/S2048770312000030_fig11p.jpeg) but it's not immediately apparent to me that that version is not minified or magnified. I do think it's educational to see the magnified version, since it lets you see the fringing and doesn't actually get rid of the sharpness relative to the regular downsampling." CreationDate="2015-09-04T05:57:12.960" UserId="196" />
  <row Id="722" PostId="1431" Score="1" Text="@yuriks, problem with these kinds of images are that not all monitors have same subpixel sequence. And on mobile devices the orientation needs to change when the user turns the device." CreationDate="2015-09-04T06:21:43.853" UserId="38" />
  <row Id="723" PostId="405" Score="0" Text="@AlanWolfe that sounds like a Meta Question tagged `[feature-request]` to me, if you want to post one..." CreationDate="2015-09-04T08:49:10.267" UserId="231" />
  <row Id="724" PostId="1436" Score="5" Text="RIP maps also probably aren't used because they don't help on the, rather common, diagonal case.&#xA;&#xA;FWIW, if you can find the code for the Microsoft Refrast, the anistropic filter implementation in that is probably a good reference for how today's HW does it." CreationDate="2015-09-04T09:43:19.890" UserId="209" />
  <row Id="725" PostId="427" Score="0" Text="Oh, gosh, it was a long time ago.  Actually, I only implemented the cone tracing. Whether I actually tried turning off the radius part I simply can't recall but, if I get time, I'll try to remember the pros and cons of going down the cone-tracing route." CreationDate="2015-09-04T09:55:20.113" UserId="209" />
  <row Id="726" PostId="1436" Score="1" Text="&quot;This can be verified by noting that texture usage requirements don't increase when using AF, rather, only bandwidth does.&quot; Killer argument. Good answer!" CreationDate="2015-09-04T10:22:07.030" UserId="385" />
  <row Id="727" PostId="432" Score="1" Text="Illumination does not actually fall of the surface area towards the light is just smaller" CreationDate="2015-09-04T17:11:50.277" UserId="38" />
  <row Id="728" PostId="430" Score="0" Text="Tervetuloa! Yes your point is valid, id say that the system or hardware has to do this as only the system can in future realisticallt be aware of the orientation of screen and the organisation of colors on screen. Preferably the screen itself would do this." CreationDate="2015-09-04T17:17:42.007" UserId="38" />
  <row Id="729" PostId="1434" Score="2" Text="I generally agree with your argumentation. But I think what the author means is, that the approach needs to perform its calculations for all existing patches, not only the visible ones. One could argue that path-tracing in contrast computes radiance only for visible patches/samples. While rays may still go everywhere, there might be parts of the scene that never receive any view-rays/paths; therefore there are no computations at all. Comparing with local GI the &quot;problem of viewpoint independence&quot; its even more apparent. Though, I still agree with you that this should be rephrased." CreationDate="2015-09-04T17:25:00.083" UserId="528" />
  <row Id="733" PostId="1432" Score="3" Text="The spec for [`GL_EXT_texture_filter_anisotropic`](https://www.opengl.org/registry/specs/EXT/texture_filter_anisotropic.txt) is very detailed. Maybe it might help you better understand the process." CreationDate="2015-09-04T20:39:03.017" UserId="54" />
  <row Id="734" PostId="1434" Score="1" Text="I agree with Wumpf, the view-independent technique does not take visibility into account which results in extra calculations because it needs to compute lighting for *entire scene* no matter where the camera is looking. Furthermore, you cannot reduce resolution of your computation in areas that are far. I think @Wumpf should paraphrase his comment as an answer." CreationDate="2015-09-04T23:02:46.410" UserId="14" />
  <row Id="735" PostId="1438" Score="1" Text="SIGGRAPH 2014 advances in real time rendering has a really interesting talk on call of duty's subdivision surfaces.  You should check it out.  Instead of having a high poly mesh that is made lower poly, they defined shapes analytically and added more triangles as needed" CreationDate="2015-09-05T03:09:18.943" UserId="56" />
  <row Id="736" PostId="1438" Score="0" Text="We can talk about the state of the art in LOD algorithms and data structures here, but if the question is about how modern games do it specifically, you might have more luck asking in gamedev.se: http://gamedev.stackexchange.com" CreationDate="2015-09-05T06:02:37.420" UserId="159" />
  <row Id="737" PostId="1434" Score="0" Text="@ap_ Done. Feel free to edit :)" CreationDate="2015-09-05T12:13:20.047" UserId="528" />
  <row Id="740" PostId="283" Score="0" Text="@JohnCalsbeek I think that paper contains the makings of a great answer if you or anyone else wants to post it." CreationDate="2015-09-05T15:24:12.010" UserId="231" />
  <row Id="741" PostId="432" Score="0" Text="You're right I think, &quot;fall-off&quot; to me is anything that makes the surface area smaller from the lights perspective, so distance from and rotation away from the light have the same effect to me, but my definition of &quot;fall-off&quot; is probably not mathematically correct :P" CreationDate="2015-09-05T15:27:52.980" UserId="554" />
  <row Id="742" PostId="1438" Score="3" Text="I doubt that. Game dev is very graphics light. It's mostly unity and java questions with some path finding and fixed frame rate questions thrown in :p" CreationDate="2015-09-05T16:52:26.470" UserId="56" />
  <row Id="743" PostId="432" Score="0" Text="well yes but that would be hard for a layman to understand. lots of things can fall off." CreationDate="2015-09-05T17:38:40.670" UserId="38" />
  <row Id="744" PostId="283" Score="0" Text="@trichoplax One of the reasons that I haven't tried to write it up yet is that I don't see the anisotropy that you are talking about in any of the frequency domain images in that paper, so I feel like I'm missing something." CreationDate="2015-09-05T17:52:22.860" UserId="196" />
  <row Id="745" PostId="283" Score="0" Text="@JohnCalsbeek The first image shatters my intuition that Perlin noise would be heavily anisotropic, and shows that the problems are due to poor implementation and not really related to being grid based. I still don't have an understanding of why my initial intuition was so wrong." CreationDate="2015-09-05T18:27:23.203" UserId="231" />
  <row Id="746" PostId="283" Score="0" Text="@trichoplax I can see directional artifacts in image 1b in that paper, even though the Fourier transform doesn't seem to show it. Unless that's what the extremely thin lines at right angles are?" CreationDate="2015-09-05T21:45:58.577" UserId="196" />
  <row Id="747" PostId="1438" Score="1" Text="@Alan, Activision used a lot of state of the arts algorithm to create a real 3D strategic game not sn isometric Sprite base one which I am ok with that, but they did a great job in COD however it is still a bit sluggish and lazy even at the early levels with small number of assets (I am talking about their mobile game on an Iphone 5s). I think you need to learn OpenGLES expert features and underlying layers to succeed writing such a game." CreationDate="2015-09-06T06:58:27.263" UserId="537" />
  <row Id="755" PostId="1446" Score="0" Text="Down voter, please consider you vote again, if it is still a -1, Consider leaving a comment for me.Thanks" CreationDate="2015-09-06T13:00:25.020" UserId="537" />
  <row Id="756" PostId="1438" Score="0" Text="I'm talking about their console version of call of duty in case that clears it up." CreationDate="2015-09-06T13:53:12.393" UserId="56" />
  <row Id="758" PostId="405" Score="0" Text="[Another Meta question about this question.](http://meta.computergraphics.stackexchange.com/questions/150/2d-illustrations-question-help-crafting-the-question)" CreationDate="2015-09-06T15:58:21.757" UserId="482" />
  <row Id="759" PostId="1446" Score="0" Text="I didnt downvote but part of me wonders what is the sate of the art here? So i cant up vote either." CreationDate="2015-09-06T15:59:03.380" UserId="38" />
  <row Id="761" PostId="1446" Score="1" Text="I've downvoted because it was hard to read and IMHO it's not relevant to LODs in games. After reconsidering I decided to cancel my donwvote by upwoting and simply add my own answer." CreationDate="2015-09-06T18:02:06.800" UserId="93" />
  <row Id="762" PostId="1446" Score="0" Text="State of the art suppose to mean a very outstanding job but different methods result differently on various cases, for example Call of duty has Layer management and mipmaping, however Dear haunting(DH 2014) uses background with parallax and a mipmaping which has pre rendered generalized textures. Subway surfer is completely a diffrent story, and I say state of the art to all of them, even though Subway Surfer discretely draws buildings and other urban objects or Call of duty is a bit sluggish while zooming or panning. I think they are all best in their case." CreationDate="2015-09-06T18:04:05.847" UserId="537" />
  <row Id="764" PostId="1451" Score="1" Text="game engines usually have a set of pre defined methods and you can not get in to the core to actually changing the LOD algorithm. am I right? I was talking about the case you are writing a game yourself with OpenGL or SpriteKit framework, I dont know if one is able to customize LOD's algorithm in Unity or Unreal, is it possible?" CreationDate="2015-09-06T18:19:19.567" UserId="537" />
  <row Id="765" PostId="1450" Score="1" Text="I'm fairly certain that these values are hardware/version specific, but there are minimum values that an implementation must support. You can query them with [`glGet`](https://www.opengl.org/sdk/docs/man/html/glGet.xhtml)." CreationDate="2015-09-06T19:12:20.497" UserId="54" />
  <row Id="766" PostId="1452" Score="0" Text="Also as glampert hinted, you want to find the minimum that must be supported, because that is the amount you can actually rely on, on all hardware." CreationDate="2015-09-06T20:43:18.473" UserId="56" />
  <row Id="767" PostId="1452" Score="0" Text="Well, OpenGL3 enforces 48 as I have mentioned. There might be a higher minimum in practice of course. Or did I misunderstand you?" CreationDate="2015-09-06T20:45:16.870" UserId="528" />
  <row Id="769" PostId="1450" Score="2" Text="Note that the textures you've described are 64 MB each, so you may get limited by available VRAM before you hit API limits on the number of textures. 8 textures = 512 MB, so should be fairly safe, but many older cards or mobile cards only have 1 or 2 GB of VRAM, so you won't be able to go much more than 8 of these textures and still have VRAM left over for anything else." CreationDate="2015-09-07T00:01:07.253" UserId="48" />
  <row Id="771" PostId="1453" Score="0" Text="thanks, they even have binary gltf (`.bgltf`) versions available! +1" CreationDate="2015-09-07T08:48:35.527" UserId="361" />
  <row Id="772" PostId="1450" Score="1" Text="What do you mean by 256*256*256? what is the third number?" CreationDate="2015-09-07T09:44:37.250" UserId="537" />
  <row Id="773" PostId="1440" Score="1" Text="Ok, seems the main problem was that I read &quot;viewpoint independent&quot; as &quot;BRDF is isotropic&quot;, not &quot;lighting is calculated, whether you look at the surface or not&quot;. I'll wait another day for another answer and then probably accept this, thanks :)" CreationDate="2015-09-07T10:20:12.323" UserId="385" />
  <row Id="774" PostId="1455" Score="0" Text="I feel this is a very broad question. What exactly do you want to learn that you can't when using one of said low-level graphic APIs?" CreationDate="2015-09-07T11:56:37.447" UserId="385" />
  <row Id="775" PostId="1455" Score="0" Text="@DavidKuri, drawing a 2D geometric primitive (circle or line) in 3D is easy (just need to convert/translate 3D coordinates to 2D coordinates or vice versa). suppose I want to draw a sphere without OpenGL. Where to start from? Just give me a guideline to study." CreationDate="2015-09-07T11:59:45.210" UserId="464" />
  <row Id="776" PostId="1455" Score="0" Text="You mean software rendering?" CreationDate="2015-09-07T12:02:32.213" UserId="137" />
  <row Id="777" PostId="1455" Score="0" Text="@ratchetfreak, yes. I need to draw a sphere/ellipsoid/or whatever it is on the screen. Rasterization in 3D." CreationDate="2015-09-07T12:03:56.820" UserId="464" />
  <row Id="778" PostId="1455" Score="0" Text="@ratchetfreak, I need to demonstrate 3D algorithms like Z-buffer algorithm. So, first, I need to know how to draw a 3D object in 3D." CreationDate="2015-09-07T12:05:46.417" UserId="464" />
  <row Id="780" PostId="1455" Score="0" Text="@trichoplax, I need to learn both." CreationDate="2015-09-07T12:12:21.513" UserId="464" />
  <row Id="782" PostId="1455" Score="0" Text="@trichoplax, line drawing. I will learn shadows and shading later." CreationDate="2015-09-07T12:16:41.050" UserId="464" />
  <row Id="784" PostId="1454" Score="1" Text="It does extend to 3D. For example,  you could consider that, when texturing (a portion of) a triangle, you are, in effect, evaluating a surface cut through the 3D texture.  That shape doesn't have to be isotropic.&#xA;&#xA;Alternatively, just like a 2D anisotropic filtering may approximate with an elliptical footprint, the 3D version could use an ellipsoid." CreationDate="2015-09-07T15:09:05.573" UserId="209" />
  <row Id="785" PostId="1454" Score="0" Text="@SimonF Hmmm, you're right! That is actually I think perhaps a better generalization than the one I gave, and it seems better behaved (as in, it's more obvious what to do)." CreationDate="2015-09-07T16:51:22.520" UserId="523" />
  <row Id="786" PostId="386" Score="0" Text="PVRTC encodes texture blocks in a [morton order](https://en.wikipedia.org/wiki/PVRTC#Data_structure)" CreationDate="2015-09-07T19:09:25.310" UserId="135" />
  <row Id="789" PostId="369" Score="0" Text="@SimonF Sorry I didn't notice your comment till now. I'm trying to implement a fully-generalized clipping routine for complex self-intersecting shapes AND complex self-intersecting clipping-regions and parameterized with a winding-number rule. But if I could visualize the data structure in concrete terms, that usually works to get me moving." CreationDate="2015-09-07T23:21:32.447" UserId="482" />
  <row Id="790" PostId="412" Score="0" Text="@trichoplax, you used mathurl for pasting images? I'm reading from Android app and i can't check." CreationDate="2015-09-08T01:56:03.440" UserId="316" />
  <row Id="792" PostId="1458" Score="1" Text="I dont really understand why this shopping list question is within scope while asking what graphics api questions are available fo ubuntu is not." CreationDate="2015-09-08T04:18:42.570" UserId="38" />
  <row Id="794" PostId="420" Score="0" Text="I tend to think it does.He can learn from the code inside how the trackball works." CreationDate="2015-09-08T07:59:31.830" UserId="213" />
  <row Id="796" PostId="369" Score="0" Text="*If* you have solved the fill problem for an &quot;unclipped&quot; arbitrary polygon, then you 'only' need to do CSG with an intersection (i.e. AND) operator to then clip it against another arb. poly.  Does that make sense?  In screen/scanline space the CSG is relatively easy. OTOH if you need a vector model, it seems to me that  a scanline model could be extended so that you construct sets of trapezia  (http://mathworld.wolfram.com/Trapezium.html - British  definition (i.e. correct def)) for each that describe the interiors. It should be relatively easy to intersect those to generate the final model." CreationDate="2015-09-08T09:26:11.867" UserId="209" />
  <row Id="798" PostId="386" Score="0" Text="@yuumei No. Although early MBX PVRTC files *were* in morton(ish) order, for later systems (e.g. SGX and Rogue), the data is in raster(ish) order and the driver/GPU loads and rearranges them into whatever is the preferred order for that particular GPU." CreationDate="2015-09-08T12:23:01.563" UserId="209" />
  <row Id="800" PostId="1459" Score="0" Text="`3D computer graphics and geometric modeling` includes a ton of things, ranging from lighting algorithms over mesh processing algorithms to the 'creative' task of creating a 3-dimensional virtual object, complete with textures and possibly animations. Except for being in the same space, these things don't have too much in common. Could you go into detail on what exactly you want to learn?" CreationDate="2015-09-08T12:36:58.927" UserId="385" />
  <row Id="802" PostId="1459" Score="0" Text="@DavidKuri,  http://computergraphics.stackexchange.com/questions/1455/how-to-get-started-with-drawing-3d-primitives-without-using-opengl-or-directx" CreationDate="2015-09-08T13:49:42.797" UserId="464" />
  <row Id="803" PostId="1459" Score="0" Text="The way I read your other questions, you want to learn everything from the ground up. So after software rasterization (in which you are essentially replicating the job of the dedicated GPU), plain renderings APIs would be next." CreationDate="2015-09-08T13:54:27.313" UserId="385" />
  <row Id="804" PostId="1459" Score="0" Text="@DavidKuri, I didn't get you." CreationDate="2015-09-08T13:56:01.723" UserId="464" />
  <row Id="805" PostId="1459" Score="0" Text="You seem to be interested in learning, more so than creating an application. You will probably not learn how, for example, shadow techniques work if you start using a game engine, because they are already there and ready to use. Start with a low-level rendering API and implement a shadowing technique yourself to learn it." CreationDate="2015-09-08T13:58:50.823" UserId="385" />
  <row Id="806" PostId="412" Score="0" Text="@psicomante yes I used mathurl.com and included the links inline - are they readable on Android? It's the next best thing until we get MathJax activated." CreationDate="2015-09-08T14:05:50.627" UserId="231" />
  <row Id="807" PostId="412" Score="0" Text="@psicomante press edit under the question if you want to see the markup (you need to add `.png` to the end of the mathurl.com link to make it show as an image here). Any questions just @mention me in [chat]." CreationDate="2015-09-08T14:07:56.430" UserId="231" />
  <row Id="808" PostId="1460" Score="4" Text="Nitpicking: Blender has also an [integrated Game Engine](https://www.blender.org/manual/game_engine/index.html)." CreationDate="2015-09-08T14:09:02.743" UserId="528" />
  <row Id="810" PostId="412" Score="0" Text="@trichoplax yep, they are almost perfectly readable on Android SE app. It's a good tool until MathJax activated" CreationDate="2015-09-08T15:38:13.600" UserId="316" />
  <row Id="811" PostId="413" Score="0" Text="I agree---my answer is likely not valid any more because I haven't used diagrams in any answers so far. But I expect that to happen soon. So if it's OK for this answer to hang around for a few days, I am sure I will be able to find an answer." CreationDate="2015-09-08T19:04:01.790" UserId="14" />
  <row Id="813" PostId="1455" Score="0" Text="Since this is more than one question, it may help to ask them separately rather than trying to fit them all into one post, which is making this too broad" CreationDate="2015-09-08T20:23:05.353" UserId="231" />
  <row Id="814" PostId="1450" Score="0" Text="@Iman it's a 3d texture rather than a 2d texture - if you want more info you could ask a separate question if you like." CreationDate="2015-09-08T20:44:07.357" UserId="231" />
  <row Id="815" PostId="1447" Score="0" Text="Note that the question is specifically asking about the notation Im() and Re()." CreationDate="2015-09-08T20:46:33.267" UserId="231" />
  <row Id="817" PostId="1444" Score="1" Text="Is there disagreement between this answer and John Calsbeek's answer? Does the implementation match both descriptions? If not then it would be useful to have a reference for one or other (or both if they are two different techniques both in use)." CreationDate="2015-09-08T20:54:34.700" UserId="231" />
  <row Id="818" PostId="1442" Score="0" Text="Is there disagreement between this answer and Nathan Reed's answer? Does the implementation match both descriptions? If not then it would be useful to have a reference for one or other (or both if they are two different techniques both in use)." CreationDate="2015-09-08T20:55:10.693" UserId="231" />
  <row Id="819" PostId="1442" Score="1" Text="@trichoplax I think Nathan's assertion that &quot;generating good quality mips for a non-power-of-two texture is a little trickier&quot; makes our answers disagree at least slightly. That alone probably merits more elaboration." CreationDate="2015-09-08T21:09:24.700" UserId="196" />
  <row Id="822" PostId="315" Score="0" Text="Are you using any vendor-dependent GLSL functions like noise*(which as far as I know, most vendors don't implement anyway)?" CreationDate="2015-09-08T22:42:38.267" UserId="1578" />
  <row Id="824" PostId="1461" Score="0" Text="See http://stackoverflow.com/questions/21593786/in-gouraud-shading-what-is-the-t-junction-issure-and-how-to-demonstrate-it-with." CreationDate="2015-09-09T01:35:03.463" UserId="192" />
  <row Id="825" PostId="1462" Score="0" Text="See also http://stackoverflow.com/questions/23530807/glsl-tessellated-environment-gaps-between-patches." CreationDate="2015-09-09T01:48:28.167" UserId="192" />
  <row Id="827" PostId="315" Score="0" Text="@Sam no. I am only using texture(). That shouldn't cause a problem, right?" CreationDate="2015-09-09T10:00:19.927" UserId="437" />
  <row Id="828" PostId="1450" Score="0" Text="I never heard such a thing, I thought textures are 2D (u,v) raster map that draped on 3d objects, however I take a look at tutorials and I become familiar with the concept. Thanks for your tip Terry" CreationDate="2015-09-09T13:19:38.813" UserId="537" />
  <row Id="832" PostId="1467" Score="1" Text="Original question: http://math.stackexchange.com/questions/1428101/an-introduction-to-lane-riesenfeld-algorithms" CreationDate="2015-09-09T15:59:06.333" UserId="192" />
  <row Id="833" PostId="1442" Score="1" Text="I think the problem here is that we're confusing the logical position of texels with their &quot;physical&quot; layout in memory. 1) Pixels are discrete items, i.e. you always need an integer dimension, and so down scaling an odd dimension means that we have to either round up or round down. Since we have to round up once we get to an Nx1 or 1xN texture, it makes sense to always round up.&#xA;2) When laid out in physical addresses, it is not uncommon to pad the texture out to some &quot;convenient&quot;  &quot;stride&quot; size. This may be done for 2 reasons: a) It may make HW cheaper &amp; b) if a P.of.2, Morton order is easy." CreationDate="2015-09-09T16:23:49.693" UserId="209" />
  <row Id="834" PostId="1468" Score="3" Text="As hinted by @BenediktBitterli &quot;Physically Based Rendering&quot; isn't really a yes or a no. In rendering, we always have to balance realism with computational cost. Some renderers will have just a few 'Physically Based' features, for example, Microfacet BRDFs and HDR render targets. Whereas others may have many, for example, full BSDFs, full spectrum render target, light tracing, area lights, etc." CreationDate="2015-09-09T18:19:46.420" UserId="310" />
  <row Id="835" PostId="1436" Score="0" Text="The &quot;High-Performance Software Rasterization on GPUs&quot; link only mentions anisotropic filtering in passing once, and gives no mention of any details. So I'm going to edit it out of the answer because I don't think it's relevant in a helpful way." CreationDate="2015-09-09T18:30:27.923" UserId="327" />
  <row Id="836" PostId="1464" Score="2" Text="When you say &quot;vertex (X,Y) values are nearly always represented by fixed-point numbers&quot;, I'm guessing you mean the screen-space vertex coordinates in the rasterizer, right? Not the original model-space vertices." CreationDate="2015-09-09T20:17:47.533" UserId="48" />
  <row Id="837" PostId="386" Score="1" Text="Updated, thanks @Simon!" CreationDate="2015-09-09T21:14:52.317" UserId="511" />
  <row Id="838" PostId="1456" Score="0" Text="Scratchapixel is definitely the best place to learn about that stuff (2D &amp; 3D).  They explain rasterisation and also ray-tracing and everything there is to know around 3D techniques (texturing, how to store polygonal objects in memory, etc.). Really cool website and it's free content." CreationDate="2015-09-09T21:49:30.003" UserId="1608" />
  <row Id="839" PostId="315" Score="0" Text="@nilspin I don't think so. They're both running on the same OpenGL version, right?" CreationDate="2015-09-09T23:25:30.043" UserId="1578" />
  <row Id="840" PostId="315" Score="0" Text="@Sam yes they are." CreationDate="2015-09-10T04:36:55.543" UserId="437" />
  <row Id="841" PostId="1464" Score="2" Text="@NathanReed Yes. Just the screen-space X&amp;Y (and, perhaps on some GPUs the Z).  I'll edit it to make that clearer." CreationDate="2015-09-10T05:41:56.587" UserId="209" />
  <row Id="842" PostId="1460" Score="1" Text="@Wumpf there is also nothing that states maya can not be used for interactive game like elements. It can, this is how mayas motion capture works, its just not very conductive as a game engine given the software price. Anyway the terms are decidedly diffuse." CreationDate="2015-09-10T06:20:41.840" UserId="38" />
  <row Id="843" PostId="26" Score="1" Text="@trichoplax IANAPL but, as all the claims in the link provided by Benedikt , either explicitly mention either 3 dimensions (i,j,k  or x y z) or a hypercube, it seems you are correct." CreationDate="2015-09-10T08:40:51.893" UserId="209" />
  <row Id="844" PostId="26" Score="0" Text="@SimonF I wasn't as diligent as you - I was basing my opinion on  [this statement on Wikipedia](https://en.wikipedia.org/wiki/Simplex_noise#Legal_status)." CreationDate="2015-09-10T08:44:55.913" UserId="231" />
  <row Id="845" PostId="1460" Score="0" Text="I edited the answer to include your remarks. Thanks." CreationDate="2015-09-10T09:26:04.657" UserId="385" />
  <row Id="846" PostId="1470" Score="1" Text="I don't see yet why the dirac deltas are a problem. While it is impossible to compute that with a computer using sampling (hence the `if`s), the mathematics are clearly defined, right? Looking forward for somebody who can clarify that. Besides, in nature there are no real dirac deltas / infinity values since there are no perfect mirrors; but I guess that is another topic." CreationDate="2015-09-10T10:08:48.700" UserId="528" />
  <row Id="848" PostId="1470" Score="0" Text="As long as BSDF and radiance can be dirac deltas at the same time than the rendering equation(as it is) is not mathematically well defined. Even if only BSDF would be allowed to be Dirac delta and we would formally treat BSDF as distribution than radiance needs to be smooth function in order to be mathematically 100% correct. But radiance under no way can be smooth function e.g.  sharp shadows form discontinuities in radiance." CreationDate="2015-09-10T10:25:34.243" UserId="1613" />
  <row Id="849" PostId="1470" Score="0" Text="Yes in reality you cannot have perfect mirrors, point and directional light sources or pin hole cameras. But we write programs where these things are and we need a theory which underpins them." CreationDate="2015-09-10T10:31:31.473" UserId="1613" />
  <row Id="850" PostId="1471" Score="0" Text="&quot;I would like to point out that correlation is &quot;always&quot; bad. If you can afford to make brand new sample than do it.&quot; Could you elaborate? To me this sounds like any kind of heuristic for sample distribution is bad, which is probably not what you wanted to say." CreationDate="2015-09-10T11:37:55.023" UserId="385" />
  <row Id="851" PostId="1471" Score="0" Text="I edited the answer, I hope that cleared a thing or two." CreationDate="2015-09-10T12:28:23.113" UserId="1613" />
  <row Id="852" PostId="1447" Score="0" Text="Well, Trichoplax, I thing the whole concept is being asked while two parameters are considered unknown,however I thought that it is obvious to everyone that these are complex number and complex values are projected on both real and imaginary axis using Sin and Cos, I have tried to explain what they are for! why down votes? look like the down voter didn't get the theorem fully. By the way, Terry You are everywhere! I love You" CreationDate="2015-09-10T13:43:40.147" UserId="537" />
  <row Id="853" PostId="1472" Score="6" Text="I think you need anisotropic texture sampling" CreationDate="2015-09-10T14:01:59.483" UserId="56" />
  <row Id="854" PostId="1472" Score="1" Text="Please attach your shader completely, it's variable definition I mean, may be you need to define a High precision or mid precision variable instead of lowp" CreationDate="2015-09-10T14:29:46.250" UserId="537" />
  <row Id="855" PostId="309" Score="0" Text="Would you like to specify which texture compression format you prefer? I am guessing but your answer will likely involve a compute-mode texture compression routine." CreationDate="2015-09-10T17:13:32.990" UserId="14" />
  <row Id="856" PostId="1472" Score="3" Text="It migh be an issue with the texture filtering you're using. Which filter is it? Point, bilininear, trilinear? Also, make sure you did compute the correct mipmaps for the texture." CreationDate="2015-09-10T18:22:26.520" UserId="54" />
  <row Id="857" PostId="1470" Score="1" Text="@tom The rigorous mathematics that underlies delta distributions is [measure theory](https://en.wikipedia.org/wiki/Measure_%28mathematics%29). See the [definition of the Dirac delta as a measure](https://en.wikipedia.org/wiki/Dirac_delta_function#As_a_measure). I don't know off the top of my head of a work specifically treating the rendering equation in the context of measure theory, but pretty sure all this stuff is well-founded at the level of mathematical physics." CreationDate="2015-09-10T18:28:11.070" UserId="48" />
  <row Id="859" PostId="1447" Score="0" Text="I'm not sure who Terry is. I understand that you already knew what Im() and Re() mean, but not everyone does. I interpreted the question as asking what the notation means, but it's not my question so I can't be sure of the intention. You could post a comment on the question to ask the question poster to clarify if you like." CreationDate="2015-09-10T23:04:06.557" UserId="231" />
  <row Id="861" PostId="1447" Score="0" Text="Yes you are right I had to do this first, next time I will, and terry is just an abbreviation for your account name which is really hard to write completely." CreationDate="2015-09-10T23:21:27.697" UserId="537" />
  <row Id="862" PostId="1447" Score="0" Text="Oh I see, that bit was to me as well - then thank you :)" CreationDate="2015-09-10T23:28:15.503" UserId="231" />
  <row Id="863" PostId="1447" Score="0" Text="If you use @ before someone's username, then they will get a notification. It also has an autocomplete so you don't need to type out the whole name, if that helps. You should be able to just type `@t` and it will show you `@trichoplax` as an option." CreationDate="2015-09-10T23:31:14.560" UserId="231" />
  <row Id="864" PostId="1473" Score="1" Text="I don't think that there's any meaningful way to make that kind of judgement call at the moment, aside from checking who makes the GPU. Ultimately there's more factors than just &quot;can the hardware execute commands from multiple queues simultaneously&quot;, and D3D12 abstracts away those details. In fact D3D12 doesn't even distinguish between hardware that might execute queues concurrently and those that might do it sequentially, the docs just say that their abstraction *allows* for concurrent execution." CreationDate="2015-09-10T23:46:30.187" UserId="207" />
  <row Id="865" PostId="1473" Score="1" Text="good question ! i also feel it would be special to gain perf to exec compute and shading concurrently. maybe gains can happens thanks to the same facts that makes hyperthreading somehow faster. interleaving operations when some units are busy for the other queue. like shaders clogging the texture units, which are not used by the compute stage, which itself clogs the FPU or DPU." CreationDate="2015-09-11T01:26:37.987" UserId="1614" />
  <row Id="866" PostId="1470" Score="0" Text="I don't understand why the BSDF can have a dirac. BSDF can represents 100% reflectivity using a value of 1, not infinite ? you don't need to make so that the integral is 1 over the hemisphere, you need to make so that it is less than one, strictly. or there is something i don't get. more than 1, for some angle, would mean light from other directions than the perfect reflection also contributes to this particular angle. which would not be a mirror, but some kind of lens !" CreationDate="2015-09-11T01:48:58.433" UserId="1614" />
  <row Id="867" PostId="424" Score="0" Text="as an engineer I would never agree to implement such a... broken idea. EXCEPT if I'm sure the display is totally fixed. like an appli for iPhone 5S. Using this technique generates broken images from screens using reversed patterns, or different arrangements." CreationDate="2015-09-11T02:00:10.627" UserId="1614" />
  <row Id="868" PostId="1436" Score="0" Text="@SimonF also we can add that the additional bandwidth requirement is pretty scary." CreationDate="2015-09-11T02:06:37.223" UserId="1614" />
  <row Id="869" PostId="1455" Score="0" Text="your second image will be much harder to generate than the first." CreationDate="2015-09-11T02:08:03.693" UserId="1614" />
  <row Id="871" PostId="1472" Score="0" Text="@AlanWolfe You were right! I will add some proper answer - hope you don' t mind :)  Lman I am using floats everywhere, but suggestion above solved the problem anyway. Glampert As I sad I think I don't compute mipmaps at all (I think because it might be done by default somewhere, but I don' t know about it :) I used &quot;LinearWrap&quot; sampler state if that's what you mean" CreationDate="2015-09-11T06:11:48.617" UserId="205" />
  <row Id="872" PostId="1470" Score="1" Text="(Disclaimer: I Am Not A Rendering Person.) At any surface point $x$, the role of the BSDF is to act as a linear operator mapping the incident light $L_i$ to the exitant light $L_o$. Now there is no problem with $L_i$ and $L_o$ both being distributions, i.e. linear functions $D(\mathbb S^2)\to\mathbb R$, because addition and scalar multiplication of distributions is well-defined so they form a vector space. When $L_i$ and $L_o$ are functions we can represent the BSDF $\rho$ as a distribution, but if they're not we can still speak of linear transformations." CreationDate="2015-09-11T06:24:04.620" UserId="106" />
  <row Id="875" PostId="1473" Score="0" Text="Hm too bad. Maybe then &quot;aside from checking who makes the GPU, no&quot;  counts already as answer if there is not more to it. After reading all those AMD marketing stuff I'm glad to hear that I'm not alone with my confusion." CreationDate="2015-09-11T07:52:22.560" UserId="528" />
  <row Id="876" PostId="336" Score="1" Text="_&quot;I never really understood FFT, but I saw it being used for JPEG&quot;_.  I'm not sure exactly what you meant by this, but FWIW, JPEG doesn't use FFT. Instead it uses a different transform, the Discrete Cosine Transform (DCT).  The DCT has some advantages over the FFT in that, although they both repeat ad infinitum, the DCT reflects at each repetition which thus implies better continuity." CreationDate="2015-09-11T08:33:51.733" UserId="209" />
  <row Id="877" PostId="336" Score="0" Text="@SimonF You see, I even thought DCT is a special case of FFT. Thanks for clarification!" CreationDate="2015-09-11T09:19:07.820" UserId="141" />
  <row Id="878" PostId="1455" Score="1" Text="Stack Exchange works best when you ask about very specific problems you might encounter in your day to day work/studies. It doesn't work as well for book-length studies. If an answer cannot comfortably fit in the space of a post, it is probably too soon for a Q&amp;A site like this. That is why we close these questions as *too broad.*" CreationDate="2015-09-11T12:10:52.413" UserId="53" />
  <row Id="880" PostId="1447" Score="0" Text="@trichoplax thanks. I am new to SE, i saw its autocomplete but I didn't know that it will notify the guy. but thanks , looks like only one person could be notified and the post owner will always be notified  :) still love you" CreationDate="2015-09-11T14:04:25.563" UserId="537" />
  <row Id="881" PostId="1470" Score="0" Text="@Rahul Good idea to think about BSDF as linear operator taking $L_i$ to $L_o$, but I still wonder if it is somehow advantageous to define radiance as distribution, because it seams to me that measure is sufficient, in which case you can too think about BSDF as linear mapping from one measure $L_i$ to another $L_o$ and in addition it can be represented it as integral of some measure valued function over measure $L_i$. And decomposition of that measure valued function into abs. continuous and singular part with the respect to the solid angle gives you diffusive and specular part of BSDF." CreationDate="2015-09-11T14:22:31.350" UserId="1613" />
  <row Id="882" PostId="1476" Score="1" Text="Glad to help! Anisotropic filtering is more expensive than bilinear. If that becomes a problem for you, you might try a  higher resolution texture, or distance field textures since it looks to be just 2 colors.  http://blog.demofox.org/2014/06/30/distance-field-textures/" CreationDate="2015-09-11T14:33:35.453" UserId="56" />
  <row Id="883" PostId="1478" Score="1" Text="It's probably feasible, but will have to be done on a game-to-game basis. More recent consoles, like the PS3/XB360 also use shaders, so assuming you can reverse engineer the assets, you could modify the shaders to apply additional effects. But having the protected disc complicates things, so you would probably also need a jailbroken device to run the modified software as if it was homebrew." CreationDate="2015-09-11T17:46:16.693" UserId="54" />
  <row Id="885" PostId="1479" Score="1" Text="Try the book [Space-Filling Curves - An Introduction with Applications in Scientific Computing](http://www.space-filling-curves.org/)." CreationDate="2015-09-11T19:14:30.400" UserId="192" />
  <row Id="886" PostId="1479" Score="0" Text="See also section 2.1.1.2 of Samet's *Foundations of Multidimensional and Metric Data Structures*." CreationDate="2015-09-12T01:02:50.127" UserId="192" />
  <row Id="887" PostId="342" Score="1" Text="@porglezomp Mostly marketing speak, but http://www.theverge.com/2013/6/21/4446606/how-pixar-changed-the-way-light-works-for-monsters-university" CreationDate="2015-09-13T07:13:11.773" UserId="457" />
  <row Id="888" PostId="1484" Score="0" Text="What kind of properties are you after for that surface?" CreationDate="2015-09-13T12:35:40.917" UserId="38" />
  <row Id="889" PostId="1486" Score="3" Text="What, [no MathJax](http://meta.computergraphics.stackexchange.com/questions/18/should-we-have-mathjax-support) ? :-(" CreationDate="2015-09-13T14:24:25.790" UserId="192" />
  <row Id="891" PostId="1485" Score="0" Text="It seems that you think &quot;project this pyramid into a 2D star-shaped object&quot; is a defined operation. It is not, until you do so." CreationDate="2015-09-13T16:47:28.220" UserId="504" />
  <row Id="892" PostId="1488" Score="4" Text="I don't think questions asking for tool recommendations are on topic here. Computer Graphics SE is a Q&amp;A site for computer graphics researchers and programmers." CreationDate="2015-09-13T18:05:31.800" UserId="54" />
  <row Id="893" PostId="1486" Score="0" Text="Yep, looks like it's not working yet :(. Would you mind just using plain code formatting for time being, so all those symbols won't distract from the formulas?" CreationDate="2015-09-13T18:09:00.987" UserId="54" />
  <row Id="894" PostId="413" Score="0" Text="Done! My answer should be valid now." CreationDate="2015-09-13T18:24:50.363" UserId="14" />
  <row Id="895" PostId="413" Score="0" Text="Excellent. Deleting my earlier comments." CreationDate="2015-09-13T19:44:01.363" UserId="482" />
  <row Id="896" PostId="1489" Score="0" Text="Please do not spread the idea of using the .obj format. Please choose a more capable format to promote, whether exporting manually or using the .blend directly, which will probably be best for a beginner's purposes." CreationDate="2015-09-13T22:52:40.237" UserId="504" />
  <row Id="897" PostId="1489" Score="0" Text="Although I do not agree with your comment, this is not the right place for that debate. I have edited my answer to remove the OBJ reference." CreationDate="2015-09-13T23:40:54.133" UserId="14" />
  <row Id="898" PostId="1485" Score="2" Text="In order to UV map in that way, you must think of (5) as four different vertices that happen to have the same XYZ coordinates." CreationDate="2015-09-13T23:43:35.430" UserId="1634" />
  <row Id="899" PostId="1489" Score="0" Text="I'd love to hear your thoughts. Give us a link if you ever post them." CreationDate="2015-09-14T01:54:02.063" UserId="504" />
  <row Id="900" PostId="1473" Score="1" Text="You know just to lift a bit of weight into the importance (actually UNimportance) of this matter. The PS4 SDK has a bug that doesnt allow emitting to any other queue than queue 0. I think if it was so crucial it would have been fixed faster." CreationDate="2015-09-14T02:03:50.897" UserId="1614" />
  <row Id="901" PostId="1489" Score="0" Text="Post a question and watch everyone fire shots :-)" CreationDate="2015-09-14T05:45:09.800" UserId="14" />
  <row Id="902" PostId="1490" Score="0" Text="_&quot;I have also another doubt: are scan converting and rastering the same thing ?&quot;_   &lt;sarcasm&gt;That might depend on who's paying the patent lawyer you meet &lt;/sarcasm&gt;.  I, however, would tend to say that scan converting is probably a subset of the rasterisation process, i.e. Scan conversion being the process that determines which pixels are inside a (or all) each primitive(s). Whether you should also include the shading/texturing in the &quot;scan conversion&quot; is a bit uncertain. I tend to think of those as a 'separate' step." CreationDate="2015-09-14T08:00:20.543" UserId="209" />
  <row Id="903" PostId="1488" Score="3" Text="[Relevant meta post.](http://meta.computergraphics.stackexchange.com/a/143/16)" CreationDate="2015-09-14T09:20:30.070" UserId="16" />
  <row Id="904" PostId="1490" Score="0" Text="See https://en.wikipedia.org/wiki/Bézier_curve#Computer_graphics." CreationDate="2015-09-14T11:03:25.847" UserId="192" />
  <row Id="905" PostId="1486" Score="0" Text="I've added this question to the [list of examples on meta](http://meta.computergraphics.stackexchange.com/questions/18/should-we-have-mathjax-support) to add to the case for adding MathJax to our site." CreationDate="2015-09-14T13:37:44.347" UserId="231" />
  <row Id="906" PostId="1486" Score="0" Text="&quot;which works for _every quadrilateral_.&quot; Unless I've messed up my &quot;back of the envelope sketch&quot;, I think you need to amend that to &quot;every _convex_ quadrilateral&quot;." CreationDate="2015-09-14T13:45:54.443" UserId="209" />
  <row Id="907" PostId="1486" Score="0" Text="@SimonF, you're right. Fixed. Thanks." CreationDate="2015-09-14T13:56:49.990" UserId="192" />
  <row Id="908" PostId="1477" Score="0" Text="From these details, as I understand it, JPEG expects the input RGB values to be encoded in a way that the display will apply a power function upon display. In order to recreate those specific RGB values, they should *not* be corrected prior to encoding." CreationDate="2015-09-14T23:06:07.037" UserId="197" />
  <row Id="909" PostId="320" Score="0" Text="The gamma exponent is stored in JPEG exif data. most software totally ignore it. but you can assume than after decoding a jpeg its already in gamma space so there is no conversion to do before sending the rgb value on the display buffer." CreationDate="2015-09-15T02:23:33.447" UserId="1614" />
  <row Id="910" PostId="185" Score="1" Text="There has been enough psychology of perception studies that told that we cannot tell what image looks more real. using eyeballing would be a terrible measurement method." CreationDate="2015-09-15T02:26:55.233" UserId="1614" />
  <row Id="911" PostId="1471" Score="0" Text="it feels indeed contradictory, but I would not say stratified sampling reduces the error, it reduces the noise only." CreationDate="2015-09-15T02:30:16.700" UserId="1614" />
  <row Id="912" PostId="1477" Score="1" Text="The trouble with stating it like that is that it's a bit ambiguous. We should probably state that, if your &quot;RGB&quot; data is, in fact, R'G'B' (and let's assume sRGB falls into that category) then you shouldn't modify the values before applying the R'G'B'=&gt;YCbCr matrix.  If, however, the data has, say, been computed with a renderer (so possibly linear), been processed using downscaling (which should be done in linear space) or, say, captured (and cleaned up) with a CCD (which I think is linear), then it has to be remapped prior to JPEG compression." CreationDate="2015-09-15T08:24:24.187" UserId="209" />
  <row Id="913" PostId="1493" Score="2" Text="Assuming a perspective projection, AFAICS the 'boundary' formed by the view-points horizon will be a (truncated) cone and thus most of the projection will be a conic section: https://en.wikipedia.org/wiki/Conic_section.  An ellipse is thus a possibility, but not the only one." CreationDate="2015-09-15T12:27:54.890" UserId="209" />
  <row Id="914" PostId="1493" Score="0" Text="Pardon my naivety, isn't an ellipse a conic section? Could a projected sphere ever result in a parabola or hyperbola?" CreationDate="2015-09-15T12:31:23.007" UserId="1647" />
  <row Id="915" PostId="1493" Score="0" Text="If you look at the wikipedia diagram, https://en.wikipedia.org/wiki/Conic_section#/media/File:Conic_Sections.svg, and consider the plane onto which you are projecting, you can get anything from an ellipse/circle, through to unbounded parabolas or hyperbolas (and I guess if the plane passes through the eye, even degenerate cases)" CreationDate="2015-09-15T12:35:38.403" UserId="209" />
  <row Id="916" PostId="1493" Score="0" Text="Apologies! I omitted a key element of my question, that I was only concerned with *perspective projection*. I'm very rusty in this field and its terminology after many years away from it, yet I remain interested. By the way a [tag:perspective] would be a worthwhile addition to the site for questions such as this." CreationDate="2015-09-15T14:55:59.870" UserId="1647" />
  <row Id="917" PostId="1493" Score="1" Text="In that case I will promote my comments to an answer..." CreationDate="2015-09-15T15:06:46.273" UserId="209" />
  <row Id="918" PostId="1498" Score="0" Text="I'm unable to imagine how the result could be a parabola or hyperbola despite the absolute logic of your argument. Some words clarifying what kind of layout would lead to these would be great. The best I can get my brain around is &quot;something to do with infinities somehow&quot; ..." CreationDate="2015-09-15T15:27:42.237" UserId="1647" />
  <row Id="919" PostId="1497" Score="2" Text="Thanks for your answer. Please see my addenda about perspective projection. Apologies for this oversight in my original wording." CreationDate="2015-09-15T15:28:33.860" UserId="1647" />
  <row Id="921" PostId="1498" Score="3" Text="Maybe something equivalent might help. Imagine you are holding a torch (flashlight for those in North America), which makes a conic beam, and you are in in a dark empty (infinite) warehouse.  Shining the torch at the floor you see an ellipse. Now _gradually_ tilt the axis of the torch back towards the horizontal. The ellipse will get longer and longer until the point when the topmost 'edge' of the beam itself is horizontal, i.e. parallel to the floor. Now the projection is a parabola and it stretches on forever. Tilting it further will form a hyperbola." CreationDate="2015-09-15T15:42:19.340" UserId="209" />
  <row Id="923" PostId="1497" Score="0" Text="Circles are special kind of ellipsis," CreationDate="2015-09-15T16:41:54.680" UserId="537" />
  <row Id="924" PostId="1497" Score="2" Text="Yes I tried to cover that in my original question. Points and line segments are other degenerate ellipses too I believe." CreationDate="2015-09-15T17:27:34.050" UserId="1647" />
  <row Id="925" PostId="1501" Score="1" Text="Very prettily illustrative! What do you think about tackling the parabola and hyperbola cases?" CreationDate="2015-09-15T20:47:34.230" UserId="1647" />
  <row Id="927" PostId="1501" Score="2" Text="@hippietrail Unfortunately, vector art programs don't have parabola and hyperbola tools the way they have ellipse tools, so it would be a bit harder... :)" CreationDate="2015-09-15T22:00:33.020" UserId="48" />
  <row Id="928" PostId="1498" Score="1" Text="@hippietrail: It's perhaps worth noting that, with a view plane in front of the camera, the only way you can end up with a parabola or a hyperbola is if at least part of the sphere is *between* the focal point and the view plane." CreationDate="2015-09-15T22:43:58.657" UserId="525" />
  <row Id="929" PostId="1497" Score="3" Text="@hippietrail: The Earth is actually an excellent example also for perspective projections. If you take an ordinary photograph outdoors, pointing the camera towards the horizon, then (assuming that your lens has no distortion, and that the Earth is approximately a perfect sphere) the image of the Earth in the picture will be (a section of) a very broad hyperbola." CreationDate="2015-09-15T22:53:04.833" UserId="525" />
  <row Id="930" PostId="1493" Score="1" Text="you need to add a constraint. fisheye is also a perspective projection, and you won't get ellipses. the constraint you need is linearity." CreationDate="2015-09-16T01:16:17.283" UserId="1614" />
  <row Id="931" PostId="1506" Score="1" Text="awesome explanation. however a bit fast on 2 points, a bit more details would be loved : 1. how do you jump from dot products to matrix products ? 2. between line 2 and 3 of last quoted section, what happens (n is moved from left to right a bit magically to me)" CreationDate="2015-09-16T01:24:32.717" UserId="1614" />
  <row Id="932" PostId="1506" Score="3" Text="1. (a^T)b is the same as dot(a, b) if a and b are column matrices of the same dimension. Try out the math for yourself!&#xA;2. (AB)^T = (B^T)(A^T), and (A^T)^T = A&#xA;For more matrix identities, check out [The Matrix Cookbook](http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)" CreationDate="2015-09-16T01:30:29.597" UserId="197" />
  <row Id="933" PostId="1493" Score="0" Text="@v.oddou: Thanks for your help with the terminology. Would that result in something like &quot;projected into linear perspective&quot; or something else?" CreationDate="2015-09-16T05:18:15.043" UserId="1647" />
  <row Id="934" PostId="1497" Score="1" Text="@IlmariKaronen: Wow that makes it super clear and is worthy of an answer of its own! Would there be a version of this that would result in a parabola?" CreationDate="2015-09-16T05:20:12.070" UserId="1647" />
  <row Id="935" PostId="1506" Score="2" Text="@v.oddou Yep, Mokosha is right. Dot product can be expressed as multiplying a 1×n matrix (row vector) with a n​×1 matrix (column vector); the result is a 1×1 matrix whose single component is the dot product. The transpose of a column vector is a row vector, so we can write a·b as a^T b. For the second question, transposing a product of matrices is equivalent to transposing the individual factors and reversing their order." CreationDate="2015-09-16T05:42:33.543" UserId="48" />
  <row Id="936" PostId="1506" Score="0" Text="perfect, its all clear without issue now. thanks both." CreationDate="2015-09-16T06:09:21.337" UserId="1614" />
  <row Id="937" PostId="1493" Score="1" Text="I would rather say something like &quot;where the projection is a linear application&quot;. There might be some shortcut term for this, like &quot;linear epimorphism&quot; or something, but I long forgot that." CreationDate="2015-09-16T06:14:06.193" UserId="1614" />
  <row Id="938" PostId="338" Score="0" Text="Where did you see this claim? It is rather surprising to me (but I don't immediately see that it's impossible)." CreationDate="2015-09-16T06:19:04.597" UserId="1657" />
  <row Id="939" PostId="1493" Score="0" Text="@v.oddou: I've tweaked the wording of the question based on your advice." CreationDate="2015-09-16T06:21:29.973" UserId="1647" />
  <row Id="940" PostId="1509" Score="0" Text="surely you mean illustrator and inkscape (instead of illustrator and illustrator)" CreationDate="2015-09-16T08:10:30.780" UserId="38" />
  <row Id="942" PostId="1509" Score="0" Text="I sure do! Tnx!" CreationDate="2015-09-16T08:30:45.907" UserId="1659" />
  <row Id="943" PostId="1506" Score="0" Text="@NathanReed (Gosh this takes me back to the early PowerVR days where we modelled most things with planes). It might also be worth mentioning that, for optimisation purposes, if you have a matrix _Mr_ that only contain rotations, (i.e. is orthogonal) then Inverse(_Mr_) = Transpose(_Mr_), and so Trans(Inverse(_Mr_)=_Mr_.   You can also take shortcuts with the translation part and if you know the scaling is uniform. FWIW in the SGL PowerVR graphics library, we used to keep booleans to track whether a transformation matrix had these properties to save costs with the normal transformations." CreationDate="2015-09-16T08:32:11.153" UserId="209" />
  <row Id="944" PostId="1500" Score="1" Text="What might be really nice is if your animation could change the shading for the various outcomes: Say white for ellipse, green (for the 'one frame' of parabola), and red for hyperbola. :-)" CreationDate="2015-09-16T08:42:32.223" UserId="209" />
  <row Id="945" PostId="1500" Score="2" Text="@SimonF i thought about this, i was planning something like nathan reed. But i was in a bit of hurry, i was lucky to get this render done. Initially i was a bit sceptical whether hyperbola could exist at all, but yes now it seems obvious." CreationDate="2015-09-16T09:08:58.803" UserId="38" />
  <row Id="946" PostId="1501" Score="0" Text="@NathanReed sure but they do have general graphing tools, (if not you can get one from me) graph a generic parabola and scale/rotate to fit." CreationDate="2015-09-16T09:11:46.980" UserId="38" />
  <row Id="947" PostId="1497" Score="1" Text="@hippietrail I add some explanation at the end of my answer, hope it could answer new aspects of edited question. and thanks for your complement." CreationDate="2015-09-16T10:41:15.820" UserId="537" />
  <row Id="954" PostId="1510" Score="0" Text="nice pic! thanks" CreationDate="2015-09-17T08:50:08.860" UserId="316" />
  <row Id="955" PostId="1510" Score="0" Text="Yeah, complex surfaces leave something to be desired if the textures need to be appraised, as you can not distinguish easily what is texture and what is not. In this case Suzanne is also badly oriented as its predominantly showing the flat angle towards the camera." CreationDate="2015-09-17T09:34:35.957" UserId="38" />
  <row Id="956" PostId="1510" Score="0" Text="@joojaa You're right, the main reason why Suzanne loses here is probably because of the angle. I'll mention that in the answer, or better, render another pic with better orientation." CreationDate="2015-09-17T10:31:02.377" UserId="385" />
  <row Id="957" PostId="1510" Score="0" Text="Looks better, still complex shapes are harder to appraise now i dont know if certain features are because of the normal map or are they there because of Suzannes shape. PS: maybe the literal answer to the question should be &quot;because they are trying to solve same problem&quot;" CreationDate="2015-09-17T11:35:36.283" UserId="38" />
  <row Id="959" PostId="1508" Score="0" Text="What is $G^n$ ? Didn't you mean $C^n$ i.e. $n$-times continuously differentiable? Actually I would be interested if there is subdivision algorithm which gives higher smoothness than Catmul-Clark. Catmul-Clark gives you $C^1$ at extraordinary vertices and $C^2$ everywhere else. People making 3d models for living are actually quite concerned about minimizing number of those extraordinary vertices in their meshes." CreationDate="2015-09-17T21:21:30.903" UserId="1613" />
  <row Id="960" PostId="1515" Score="1" Text="How about rendering a series of comparison swatches and try to fit a conversion curve?" CreationDate="2015-09-18T06:36:53.597" UserId="38" />
  <row Id="961" PostId="1514" Score="1" Text="Why does the incident ray is split after reflection/refraction? If the light is a particle does that means that that this particle recursively split? And if the light is a wave does that means that is splits by frequency (but in this case why it splits after second/third/etc hit)?" CreationDate="2015-09-18T06:59:34.400" UserId="386" />
  <row Id="962" PostId="1514" Score="5" Text="The particle does not split. Rather, the images show the potential paths it could take." CreationDate="2015-09-18T07:03:06.197" UserId="310" />
  <row Id="963" PostId="1514" Score="2" Text="Many particles will hit the (nearly) same spot from the (nearly) same angle. For every particle going out there is (usually) a particle that went in. That means that averaged out the *beam* of particles from a certain angle on a certain spot gets split up in several (a lot) reflections." CreationDate="2015-09-18T09:09:51.863" UserId="137" />
  <row Id="964" PostId="1497" Score="0" Text="I've made some edits which hopefully don't change the intended meaning of your answer - feel free to roll back the changes if you wish." CreationDate="2015-09-18T09:33:41.090" UserId="231" />
  <row Id="965" PostId="1514" Score="1" Text="Great answer shedding light on most of my questions. Why is the specular part of non-metals colorless and not affected by the albedo? How and where does [polarization](http://filmicgames.com/archives/547) come into play?" CreationDate="2015-09-18T10:58:27.123" UserId="385" />
  <row Id="966" PostId="1488" Score="1" Text="A quick search over at [softwarerecs.stackexchange.com](http://softwarerecs.stackexchange.com/) should help you." CreationDate="2015-09-18T11:01:02.690" UserId="385" />
  <row Id="967" PostId="1514" Score="1" Text="_&quot;A material's observed color is the light that is not absorbed.&quot;_ At this point it might be worth referencing the [Are there common materials that aren't represented well by RGB?](https://computergraphics.stackexchange.com/questions/203/are-there-common-materials-that-arent-represented-well-by-rgb) discussion, as fluorescent materials spring to mind." CreationDate="2015-09-18T11:01:51.843" UserId="209" />
  <row Id="968" PostId="1497" Score="0" Text="I changed quote blocks to headings since they weren't quoting anything. Hopefully this is clearer now - I've opened a [meta discussion](http://meta.computergraphics.stackexchange.com/questions/162/should-use-of-quote-blocks-be-restricted-to-quotes) about whether to edit to convert to headings like this." CreationDate="2015-09-18T11:16:52.733" UserId="231" />
  <row Id="969" PostId="1514" Score="0" Text="@DavidKuri I don't know the specific physical reason to *why* metals absorb all diffracted light and non-metals highly attenuate reflected light (IE. black diffuse for metals, and very small, monochrome specular for non-metals). Rather, it's just an observed phenomenon in nature. If anyone has a link or explanation, I would love to see it." CreationDate="2015-09-18T14:55:06.097" UserId="310" />
  <row Id="970" PostId="1514" Score="1" Text="@DavidKuri For polarization, you have to look into the other way of representing light, ie. waves. Polarization filters, for example in cameras, use long polymer strands  to block out certain wave orientations. A similar, but much less pronounced, process happens when the light wave interacts with all materials. This is one of the ways that light 'loses' energy." CreationDate="2015-09-18T15:01:41.967" UserId="310" />
  <row Id="971" PostId="1517" Score="0" Text="I wonder if it will be helpful to plot the various methods on a single plot. For example - I plotted two of your links here: http://i.imgur.com/YJjIMOQ.png" CreationDate="2015-09-18T17:22:46.927" UserId="14" />
  <row Id="972" PostId="1518" Score="0" Text="&quot;you may be able to use a 16-bit RGBA texture and a&quot; AND A WHAT??? :)" CreationDate="2015-09-19T01:12:24.700" UserId="48" />
  <row Id="973" PostId="1518" Score="2" Text="Haha, this is what happens when you leave the tab open to finish later. Edited." CreationDate="2015-09-19T01:13:15.507" UserId="197" />
  <row Id="974" PostId="1498" Score="0" Text="@IlmariKaronen: What would &quot;focal point&quot; mean in this context? The point the eye is focussing on? The vanishing point? (I taught myself 3D perspective rotation and projection as a 12 or 13 year old and never gained fluency in the math and terminology.)" CreationDate="2015-09-19T04:47:42.520" UserId="1647" />
  <row Id="975" PostId="1519" Score="3" Text="Thank you! I only knew the simplifications. These extra details are awesome" CreationDate="2015-09-21T06:07:42.403" UserId="310" />
  <row Id="976" PostId="1498" Score="0" Text="@hippietrail Focal point, in this context, would be the apex of the cone. Effectively the &quot;pinhole&quot; of the perspective, pinhole camera model. (PS Does the name imply meeting &quot;a strange lady. She made me nervous..&quot;?)" CreationDate="2015-09-21T07:49:41.237" UserId="209" />
  <row Id="977" PostId="1521" Score="0" Text="owell it is possible to do this analytically for polygons. Turn each poly into polar coordinates. You can then either analytically render these or sample like any other polygon, except they are now curved. Samplig may be faster though." CreationDate="2015-09-21T19:25:14.143" UserId="38" />
  <row Id="979" PostId="1519" Score="1" Text="This is a fascinating answer. Could you clarify/link the acronym SSS please?" CreationDate="2015-09-21T20:28:47.780" UserId="231" />
  <row Id="982" PostId="1523" Score="0" Text="What's &quot;normal compression&quot; - things like JPEG and PNG? Are you asking about the differences between those and hardware-supported formats like DXT and ASTC?" CreationDate="2015-09-21T21:14:09.627" UserId="48" />
  <row Id="984" PostId="1519" Score="0" Text="@trichoplax Thanks! SSS == sub-surface scattering." CreationDate="2015-09-21T22:06:34.003" UserId="523" />
  <row Id="985" PostId="1523" Score="6" Text="(At last, a subject I know a bit about!)&#xA;&#xA;What makes it different to PNG/JPEG is random access.  Given you want to access Texel(X.Y) you can quickly determine the small footprint of data needed to produce that texel. JPG or PNG might require decompression of up to all of the data!   Sections 1 and 2 of the [Wikipedia article](https://en.wikipedia.org/wiki/Texture_compression) are a good summary." CreationDate="2015-09-21T22:36:22.550" UserId="209" />
  <row Id="987" PostId="1521" Score="0" Text="Thanks for your comment @joojaa, but I cann't get your point. I intend to get the viewing field for each vertex (why do you mention &quot;polygons&quot;?). Is this what you mean: make each vertex as coordinate origin; then express all other vertices in polar coordiates (theta, phi, r) according to this origin; then do some analytical analysis using these coordinates or do sampling?  Could you please make it more clear?" CreationDate="2015-09-22T01:45:42.093" UserId="1692" />
  <row Id="988" PostId="1521" Score="1" Text="Possibly relevant reading: [Interactive Horizon Mapping](http://research.microsoft.com/en-us/um/people/cohen/bs.pdf). The visibility maps you describe are similar to what are called horizon maps in the literature, which store the elevation angle of the &quot;horizon&quot; for a predefined set of directions around each point on a surface. That paper is mainly about using them at runtime, though, with not much detail on generating them." CreationDate="2015-09-22T06:18:14.137" UserId="48" />
  <row Id="990" PostId="1519" Score="0" Text="Thanks :) If you clarify it in the question, it will survive deletion of the comments (which are not guaranteed to be long lived). I've edited in a link and hover text which hopefully leaves your intended presentation intact." CreationDate="2015-09-22T09:48:26.563" UserId="231" />
  <row Id="991" PostId="1525" Score="0" Text="Instead of software to test against yest against known real world measurements and fluid dynamics bechmarks. Otherwise your error is tainted. I saw the same question posted elsewhere on the stackexhange network btw" CreationDate="2015-09-22T13:11:06.700" UserId="38" />
  <row Id="993" PostId="1525" Score="1" Text="I think that testing against real world measurement is good for testing if you have the physics right. If you only want to debug you program, than testing against others code is better idea. Plus in computer simulation you can measure anything without affecting the experiment. For example measuring fluid speed at any point is just impossible in real world experiment, but trivial in computer simulation." CreationDate="2015-09-22T13:32:19.240" UserId="1613" />
  <row Id="994" PostId="1519" Score="2" Text="While i do appreciate the pedantry of this answer. Sub surface scattering is considered a mm scale effect while its true that at molecular ranges everything passes the surface to some degree. But the base constraint is that we are generally counting mm scale effects and trying to abstract lower levels as statistical models. Hence micrometer is equal to immediately as most pixels see much greater area than this. Same applies to color which does not meaningfully exist in physics the same way as our eyes and brain precieve it" CreationDate="2015-09-22T13:38:11.347" UserId="38" />
  <row Id="995" PostId="1525" Score="0" Text="Yes but you also inherit problems of their solvers. I admit i did do this a few times developing a multibody simulator and checking against results form MSC Adams but in hindsight that wasn't really stellarly useful" CreationDate="2015-09-22T14:30:26.947" UserId="38" />
  <row Id="997" PostId="1526" Score="3" Text="Would be interesting to see some results of your detailed recipe if there are any you can share." CreationDate="2015-09-22T14:52:22.670" UserId="457" />
  <row Id="998" PostId="1525" Score="0" Text="Checking against real world experiment was any better? I doubt it, but I might be wrong. The situation with multibody physics is quite different to fluid physics. Even something as simple as billiard has chaotic behavior. Moreover rigid body dynamics with contacts is not even well posed mathematical problem, do you know Painlevé paradox? So doing numerical simulation of multibody physics is doomed to fail in general. Some references: https://plus.maths.org/content/chaos-billiard-table https://en.wikipedia.org/wiki/Painlev%C3%A9_paradox" CreationDate="2015-09-22T15:12:51.850" UserId="1613" />
  <row Id="999" PostId="1525" Score="0" Text="The situation with fluids is quite different. You know that numerical solution converges to weak solution of Navier-Stokes eq. So every solver, if it is consistent, has to converge to the same answer. But you have to do it for small Reynolds numbers, so no turbulence occur." CreationDate="2015-09-22T15:16:46.017" UserId="1613" />
  <row Id="1000" PostId="1525" Score="1" Text="Yes i am aware of how multi body dynamics work, i kind of teach it (and briefly researched it for a year or two). But no checking against known analytical solutions was easier. But a real fluid is similarly chaotic as a multi body dynamic. So one should be able to check against laminar flow situations etc. Friction is a bitch though." CreationDate="2015-09-22T15:18:55.793" UserId="38" />
  <row Id="1001" PostId="1523" Score="0" Text="As SimonF wrote. This is an extremely broad question, and the answer depends on which type you're interested in. Did you look at the specification for e.g. DXT?" CreationDate="2015-09-22T16:08:01.373" UserId="523" />
  <row Id="1002" PostId="1507" Score="0" Text="Also, [LuxRender](http://www.luxrender.net/wiki/images/7/79/Luxball.png)." CreationDate="2015-09-22T16:12:16.720" UserId="523" />
  <row Id="1003" PostId="1524" Score="0" Text="Maybe you can recompute the terms of the N-S equation with numerical differentiation and check how they cancel out." CreationDate="2015-09-23T06:19:43.570" UserId="1703" />
  <row Id="1004" PostId="1527" Score="0" Text="Thanks @joojaa. I think I understand your point now. So, basically, your method is to project the whole polygon sets to a unit sphere that is located at a observer point. Then, the spherical field that is covered by the projected polygons is not visible, right?&#xA;&#xA;This is an analytic method that gives the precise result. I think when apply this to all vertices, at each vertex I need to project to whole polygon set. So, this maybe computational heavy." CreationDate="2015-09-23T06:23:54.490" UserId="1692" />
  <row Id="1005" PostId="1527" Score="0" Text="Depends on how many polygons you have. You can do all the tricks of scanline rendering. And backface cull etc. In fact you can do this with normal hardware all you need to do is duplicate the triangles on the wrapping seams. So all things that apply for normal perspective projection apply here. This may be faster than raytracing extremely many rays though." CreationDate="2015-09-23T06:28:53.833" UserId="38" />
  <row Id="1006" PostId="1491" Score="0" Text="For cubic curves, the scanline approach can also be looked at as a 1D problem, only involving the Y equation, which is a cubic. You can find the two extrema analytically (it's a quadratic equation) to achieve reliable separation of the roots. Full analytical computation of the roots is also possible, but incremental resolution (Newton or other) is less expensive." CreationDate="2015-09-23T06:33:21.050" UserId="1703" />
  <row Id="1007" PostId="1527" Score="0" Text="Strictly from a CS perspective both raytracing and polygon projecting has a O(N) complexity its just that N is dramatically smaller for the polygon projection method. So your raytracer shoots 1-8 rays at the cost of one analytic polygon." CreationDate="2015-09-23T06:37:35.083" UserId="38" />
  <row Id="1009" PostId="1529" Score="1" Text="There are many scanning techniques. Becuase they have a camera, and a accurate position sensor, that is basically a video you could use video tracking techniques to produce point clouds and build polygonmodels out of that.  (for some ideas see https://www.ssontech.com/learning.html great resource even if you dont use their apps)" CreationDate="2015-09-23T08:43:29.903" UserId="38" />
  <row Id="1012" PostId="1526" Score="0" Text="Not at the moment, sorry :/" CreationDate="2015-09-23T13:40:05.343" UserId="1699" />
  <row Id="1013" PostId="1531" Score="0" Text="[Meta discussion](http://meta.computergraphics.stackexchange.com/questions/167/how-should-we-respond-to-homework-questions) about homework questions." CreationDate="2015-09-23T17:23:14.007" UserId="231" />
  <row Id="1014" PostId="1531" Score="4" Text="Could you refine your question? You have a nice introduction, but it's difficult to know what you're really asking. How to use GL_POINTS? How to use points to draw a 2D shape? etc.." CreationDate="2015-09-23T18:34:42.750" UserId="310" />
  <row Id="1017" PostId="1533" Score="0" Text="Bit of nitpicking its not impossible to do exactly. Its just not possible to do exactly for all possible cases. There are lots of cases where this can in fact be done exactly its just a bit hard to generalize this." CreationDate="2015-09-23T23:40:56.507" UserId="38" />
  <row Id="1018" PostId="1533" Score="0" Text="@joojaa IMO possible cases are exceptions rather than the rule. Can you link to a reference ?" CreationDate="2015-09-23T23:50:12.467" UserId="1703" />
  <row Id="1024" PostId="1533" Score="0" Text="Yes but thats enough to refute impossible. Not in front of a desktop right now bit consider this: Since rational splines can do exact circular arcs that means you can do exact offsets of said arc. Since linear splines are possible and their offsets contains lines and circular arcs that too is possible. A big set of realworld uses there." CreationDate="2015-09-23T23:55:40.947" UserId="38" />
  <row Id="1029" PostId="1533" Score="0" Text="So if you say impossible to do exactly in the general case then its ok." CreationDate="2015-09-24T00:04:09.457" UserId="38" />
  <row Id="1032" PostId="1533" Score="0" Text="Yes; I meant impossible in the general case (at least for Bézier). If you actually read the answers I was linking to when I said this, you'll find it says exactly that." CreationDate="2015-09-24T02:16:05.580" UserId="523" />
  <row Id="1034" PostId="1533" Score="1" Text="Point being, what you say should be self contained. You should update your post so that others who dont have the time dont make wrong conclusions. Remember discussions under posts are not permanent. What you dont say on the otherhand you can leave to the link." CreationDate="2015-09-24T06:58:26.377" UserId="38" />
  <row Id="1035" PostId="1533" Score="0" Text="@joojaa: are there any other cases than circles that can be exploited ?" CreationDate="2015-09-24T08:21:08.310" UserId="1703" />
  <row Id="1036" PostId="1514" Score="1" Text="@RichieSams : the explanation is held in Maxwell 4 equations of electromagnetism. Because of conduction there can be no wave intensity at the surface of a metal. This is the reason why grids protects micro-wave doors." CreationDate="2015-09-25T03:02:51.627" UserId="1614" />
  <row Id="1039" PostId="1539" Score="0" Text="&quot;Calculations in affine coordinates often require divisions&quot;: I don't see why. In fact you compute exactly the same expressions." CreationDate="2015-09-25T07:07:58.123" UserId="1703" />
  <row Id="1040" PostId="1495" Score="0" Text="This is an image-processing related question, you are not really on the right site. (Maybe Signal Processing.)" CreationDate="2015-09-25T07:14:34.163" UserId="1703" />
  <row Id="1041" PostId="1539" Score="0" Text="@Yves: I'm responding to the more general &quot;use in computer graphics&quot; topic, not the specific &quot;computing matrix transformations&quot; question." CreationDate="2015-09-25T07:54:39.000" UserId="1713" />
  <row Id="1042" PostId="1539" Score="0" Text="@Hurkyl: so do I. When rendering a scene, you compute exactly the same expressions, with the same amount of divisions (the difference lies in dummy terms with a 0 factor)." CreationDate="2015-09-25T08:02:52.633" UserId="1703" />
  <row Id="1043" PostId="1539" Score="0" Text="@Yves: Hrm. I'm used to doing calculations where the conversion back to affine can be deferred to some extent; I'll cede to your expertise if you say that doesn't come up often." CreationDate="2015-09-25T08:05:49.240" UserId="1713" />
  <row Id="1044" PostId="1540" Score="1" Text="In principle, data types can be implemented that don't actually store those entries even though they act like they do." CreationDate="2015-09-25T08:06:53.710" UserId="1713" />
  <row Id="1045" PostId="1540" Score="1" Text="@Hurkyl Obviously. This is rarely done, as general-purpose matrix toolboxes are on hand." CreationDate="2015-09-25T08:09:54.507" UserId="1703" />
  <row Id="1046" PostId="1538" Score="3" Text="I think this question is too broad. In the end your formula will boil down to a sum of *some terms* that depend on *some input* (e.g. normals, light data etc.). The procedure is basically rewriting the calculation code as formulas. There is no special rendering magic there. If you can get any more specific, please add details to the question." CreationDate="2015-09-25T08:51:57.443" UserId="385" />
  <row Id="1049" PostId="1537" Score="0" Text="What kind of examples are you looking for? Translation matrices and anything related to perspective projections should be easy enough to look up?" CreationDate="2015-09-25T15:07:33.867" UserId="4" />
  <row Id="1050" PostId="1537" Score="0" Text="@Bart, Analogy needed." CreationDate="2015-09-25T15:20:27.290" UserId="464" />
  <row Id="1051" PostId="1537" Score="2" Text="I'm sorry @anonymous, but that doesn't really tell me anything. You're going to have to use more words to explain what exactly you are looking for." CreationDate="2015-09-25T15:21:27.963" UserId="4" />
  <row Id="1055" PostId="1543" Score="0" Text="This will certainly help. Are you aware of any simpler hextocolor conversions, much narrower? Was thinking more like 8 or 16 different color names" CreationDate="2015-09-26T14:23:44.540" UserId="1723" />
  <row Id="1056" PostId="1543" Score="0" Text="This would work with any number of colors." CreationDate="2015-09-26T14:33:20.363" UserId="38" />
  <row Id="1057" PostId="1543" Score="0" Text="No, no, I understand that. But I want to limit the results to an extremely limited palette consisting of red, blue, green, white, orange, purple, brown and grey, for example" CreationDate="2015-09-26T16:11:13.527" UserId="1723" />
  <row Id="1058" PostId="1543" Score="0" Text="Web or CSS colors have a very high number of possible matches" CreationDate="2015-09-26T16:11:49.663" UserId="1723" />
  <row Id="1059" PostId="1543" Score="0" Text="Yes, whats the problem? Instead of taking the color names form web colors supply your own" CreationDate="2015-09-26T16:52:15.577" UserId="38" />
  <row Id="1060" PostId="1543" Score="0" Text="The problem? Brain not working properly, I think.   Makes perfect sense,  thanks!" CreationDate="2015-09-26T16:55:00.963" UserId="1723" />
  <row Id="1061" PostId="1542" Score="2" Text="Not a full answer, but [color surveys like this one from xkcd](http://blog.xkcd.com/2010/05/03/color-survey-results/) may give some approximation if you're prepared to do a fair bit of preprocessing to exclude meaningless results and narrow down to the number of colors you require." CreationDate="2015-09-26T21:19:58.773" UserId="231" />
  <row Id="1062" PostId="1536" Score="1" Text="There's a very similar question on Gamedev.SE: [What does the graphics card do with the fourth element of a vector as the final position?](https://gamedev.stackexchange.com/questions/17987/what-does-the-graphics-card-do-with-the-fourth-element-of-a-vector-as-the-final)" CreationDate="2015-09-26T22:32:25.247" UserId="48" />
  <row Id="1064" PostId="1535" Score="0" Text="You write &quot;arbitrary kernel with circular symmetry&quot;: Doesn't that mean that you actually only need the convolution with the (Hemispheric) Zonal Harmonics part? If your symmetry axis is different you can still use it by adding rotations before and after the Zonal convolution. How to perform rotations is described in the paper. Integration with the Zonal part (m=0) should be comparatively easy. However, as with Spherical Harmonics, it won't be analytically solvable for arbitrary functions. Simple things like cosine lobes should work fine (haven't tried yet though)." CreationDate="2015-09-27T12:46:29.590" UserId="528" />
  <row Id="1065" PostId="1546" Score="3" Text="The [Graphics Programming Black Book](http://www.jagregory.com/abrash-black-book/) is certainly a classic worth reading. A lot of oldschool black-magic in it ;)" CreationDate="2015-09-27T19:08:12.660" UserId="54" />
  <row Id="1066" PostId="1545" Score="0" Text="excellent answer. also you can add that there are some papers mentioning that in some situations you can do compression yourself and decode it with client code in a pixel shader, rather than depend on dedicated hardware. i know of no real world use of that, that may be only worth for research, but it exists." CreationDate="2015-09-28T03:25:52.790" UserId="1614" />
  <row Id="1067" PostId="1535" Score="0" Text="@Wumpf You're right, that's pretty much what it boils down to. For SH, I'd just scale &quot;each band of f by the corresponding m=0 term from [kernel function] h&quot; (quoting Sloan's Stupid SH Tricks). Question is, can I do the same for HSH?" CreationDate="2015-09-28T07:42:25.580" UserId="385" />
  <row Id="1069" PostId="1541" Score="0" Text="&quot;Sony's PS4 can perform massive matrix multiplications.&quot; You mean the Cell processor of the PS3, right? The PS4 has a rather ordinary x86 processor." CreationDate="2015-09-28T20:40:51.503" UserId="528" />
  <row Id="1070" PostId="1545" Score="1" Text="@Nathan-Reed re transform-based compression, actually, Microsofts's Talisman project used a compression scheme called TREC which (as one of the modes) used DCT but unlike JPEG, allowed random access to blocks (I suspect there must have been a table containing addresses).  This'd then allow variable length data for various blocks, but the indirection is unpleasant for HW - a reason VQ TC fell out of fashion. FWIW I experimented with about a dozen TC ideas B4 PVRTC; some were fixed-rate, transform-based, but &quot;missing&quot; coeffs still use bits. BTC-like fixed coeff locations implies &quot;free&quot; info." CreationDate="2015-09-29T06:08:22.323" UserId="209" />
  <row Id="1073" PostId="1550" Score="1" Text="It might be that he does not need to draw a solid object! Or just draw in a 2d projection screen." CreationDate="2015-09-29T16:34:04.137" UserId="38" />
  <row Id="1074" PostId="1552" Score="0" Text="Welcome to Computer Graphics StackExchange. Your question is very broad and also primarily opinion based. That's why I flagged it for closure. For more information about [How to ask?](http://computergraphics.stackexchange.com/help/how-to-ask), visit our HelpCenter." CreationDate="2015-09-30T07:16:13.547" UserId="127" />
  <row Id="1075" PostId="1552" Score="0" Text="Do you mean 2D or 3D graphics ? They are completely different worlds." CreationDate="2015-09-30T08:08:00.657" UserId="1703" />
  <row Id="1076" PostId="1553" Score="0" Text="IMO, advising Direct3D to a beginner is pure cruelty." CreationDate="2015-09-30T08:09:53.530" UserId="1703" />
  <row Id="1077" PostId="1546" Score="0" Text="Yes, I second the book by Michael Abrash. It is a GREAT read. There are a lot more trick in the sleeve to what is written in this book but the philosophy behind it is important (even to this day !)" CreationDate="2015-09-30T10:06:23.587" UserId="105" />
  <row Id="1079" PostId="1556" Score="4" Text="Not really, it can also mean just that it is redder than what  your monitor can make." CreationDate="2015-09-30T11:42:36.957" UserId="38" />
  <row Id="1080" PostId="1545" Score="2" Text="@Nathan-Reed. From what I have seen, all HW decoders can be implemented with pure logic path (bit decode, some lookup, some math in the data path) but no loop / register necessity. Are you aware of any scheme that add cycle latency to the texture lookup ? (I have for fun implemented a VHDL ETC1 decoder) I was under the impression that each texture unit (TU) had decoders embedded." CreationDate="2015-09-30T14:06:51.400" UserId="105" />
  <row Id="1081" PostId="1553" Score="0" Text="@YvesDaoust, not necessarily true. For a programmer with familiarity with OOP languages, D3D will probably feel much more natural than the procedural ways of OpenGL. At least that's how I felt when I started with D3D9 and C++." CreationDate="2015-09-30T18:54:23.403" UserId="54" />
  <row Id="1082" PostId="1553" Score="0" Text="@glampert: it seems that you never were a beginner." CreationDate="2015-09-30T18:59:29.393" UserId="1703" />
  <row Id="1083" PostId="1553" Score="0" Text="@YvesDaoust, haha, in graphics yes, not in programming, true. My reading is that the OP has some knowledge of programming, but in any case, this is not a point worth discussing, I was just raising another perspective ;)" CreationDate="2015-09-30T19:05:56.603" UserId="54" />
  <row Id="1084" PostId="1557" Score="0" Text="i heard once that we do get the same noise than cameras because noise is actually physical and not only electrical. (i.e there are not so many photons after all). But the brain erases it, using temporal antialiasing I reckon. (i.e we see with lots of motion blur at night)." CreationDate="2015-10-01T01:27:05.613" UserId="1614" />
  <row Id="1085" PostId="1556" Score="1" Text="I don't know about rgb but about energies, it is very possible to get &gt;1 reflectance thanks to fluorescence. Some materials can gather energy from other wavelengths." CreationDate="2015-10-01T01:36:30.790" UserId="1614" />
  <row Id="1086" PostId="1555" Score="0" Text="I'm not too familiar with NVENC, but it looks as if the `dstPitch` for the Y array is being supplied by the NVENC API, so it might conceivably differ from `srcPitch`, thus requiring the 2D copy instead of a straight memory copy. Not sure why the Y channel would require this where Uand V don't, though." CreationDate="2015-10-01T04:45:16.193" UserId="48" />
  <row Id="1087" PostId="1561" Score="0" Text="What a coincidence that we would post at the same time. You save me from having to post an image which is hard to draw when im on mobile." CreationDate="2015-10-01T05:39:51.630" UserId="38" />
  <row Id="1088" PostId="1561" Score="4" Text="@joojaa “You wrote all that on mobile? …You’re braver than I thought.”" CreationDate="2015-10-01T05:51:24.907" UserId="196" />
  <row Id="1089" PostId="1557" Score="0" Text="I don't quite get the idea. If you render an image in low light and simulate a Purkinje effect, it won't look realistic as the human eye will add its own effect, won't it ?" CreationDate="2015-10-01T06:47:31.977" UserId="1703" />
  <row Id="1090" PostId="1557" Score="1" Text="@YvesDaoust Since the image is shown on a LDR monitor under unknown lighting conditions, probably not. Simply put, the image you see on the screen will be brighter so it's easier to perceive. If we were using a HDR monitor and could reproduce the luminance values of a nighttime scene exactly (and have an otherwise dark room), you're right." CreationDate="2015-10-01T06:50:51.667" UserId="385" />
  <row Id="1091" PostId="1561" Score="1" Text="Yes I did. I was thinking of drawing a graph with Mathematica on my mobile over ssh, but that would have been a bit too much... Anyway the spellchecker of my mobile sucks so if you see a lot of misspellings feel free to fix." CreationDate="2015-10-01T07:00:43.170" UserId="38" />
  <row Id="1092" PostId="1557" Score="1" Text="There's nothing wrong with what your striving for, but im afraid that this seems a bit too broad to me as there are so many effects that we need to consider. I could not write this in SE format, because it would indeed be wrong. However if you adjust your scope a bit like &quot;Can you suggest **some** of the effects that i would need to consider&quot; than it would be easier to begin." CreationDate="2015-10-01T07:10:35.407" UserId="38" />
  <row Id="1093" PostId="1557" Score="0" Text="@joojaa I changed the questions as you suggested, thanks." CreationDate="2015-10-01T08:36:03.313" UserId="385" />
  <row Id="1094" PostId="1552" Score="1" Text="Turtle graphics! turtles all the way down" CreationDate="2015-10-01T12:42:44.113" UserId="38" />
  <row Id="1095" PostId="1562" Score="0" Text="Need more context. Are you trying to calculate analytic normals for a parametric surface? An implicit surface? Or do you want to calculate the normals from a generic triangle mesh? Or something else?" CreationDate="2015-10-02T01:23:41.690" UserId="48" />
  <row Id="1096" PostId="1562" Score="0" Text="Thanks, I added more detail. To answer your question I need to calculate normals from a generic triangle mesh. Though to be clear that mesh is different depending on the inputs. My shape is a 3D arrow, as an example here is a screenshot of it 2 different forms (i.e. radial, and linear). The class changes the width, depth, length, arc, and radius of the mesh as requested. http://cl.ly/image/3O0P3X3N3d1d You can see the odd lighting I am getting with my poor attempts at solving this." CreationDate="2015-10-02T01:42:12.217" UserId="1774" />
  <row Id="1097" PostId="1562" Score="3" Text="The short version is: calculate each vertex normal as the normalized sum of the normals of all the triangles that touch it. However, this will make everything smooth, which may not be what you want for this shape. I'll try to expand into a full answer later." CreationDate="2015-10-02T01:58:22.140" UserId="48" />
  <row Id="1098" PostId="1562" Score="0" Text="Smooth is what I am going for!" CreationDate="2015-10-02T02:03:42.117" UserId="1774" />
  <row Id="1099" PostId="1562" Score="0" Text="I implemented my understanding of your short answer and the result is better, but still a bit off. http://cl.ly/image/2T3k2R1Z0V25/o I am going for the smooth look of the cube in that screenshot. On the right is all the shape points along with the surface normals as my code as calculated them." CreationDate="2015-10-02T02:42:44.147" UserId="1774" />
  <row Id="1100" PostId="1562" Score="4" Text="In most cases, if you calculate the vertex positions analytically, you can also calculate the normals analytically. For a parametric surface, the normals are the cross product of the two gradient vectors. Calculating the average of triangle normals is just an approximation, and often results in visually much poorer quality. I would post an answer, but I already posted a detailed example on SO (http://stackoverflow.com/questions/27233820/providing-normals-with-triangle-strips-and-fans-for-opengl-gouraud-shading/27244693#27244693), and I'm not sure if we want replicated content here." CreationDate="2015-10-02T07:03:06.370" UserId="331" />
  <row Id="1101" PostId="1564" Score="0" Text="Related: [Do I need to rebind uniforms or attributes when changing shader programs?](http://computergraphics.stackexchange.com/q/305/127)" CreationDate="2015-10-02T08:36:21.993" UserId="127" />
  <row Id="1102" PostId="1569" Score="0" Text="I understand what PBR is and that it's a vague definition, as you pointed out I should have said BRDF. I get how basic Blinn-Phong shading works, but I'm wondering if (in probably naive terms) going for a (even simplistic) BRDF model is just a rewrite of shader + a slight change in the rendering pipeline, or something more complicated. As for my last question, I mean low end hardware with limited capabilities (think embedded devices)." CreationDate="2015-10-03T19:27:14.637" UserId="34" />
  <row Id="1103" PostId="1569" Score="0" Text="Yes, implementing a BRDF is just a case of re-writing a shader and swapping out which textures are used as imputs. However, as mentioned above, making those textures is the hard part." CreationDate="2015-10-03T21:43:28.917" UserId="310" />
  <row Id="1104" PostId="1569" Score="3" Text="Beyond just rewriting the BRDF, it's also important for the rendering pipeline to be gamma-correct and support HDR to some level. Those are definitely more involved changes if the engine doesn't have them already. Also, PBR tends to require more attention to indirect lighting (particularly specular, to look good with glossy materials), and adding various forms of real-time or semi-real-time indirect lighting to your engine can also be a huge task." CreationDate="2015-10-03T22:25:01.223" UserId="48" />
  <row Id="1108" PostId="343" Score="0" Text="@joojaa Hi, May I ask you a question about `interpolate` a set of points with `closed B-spline curve`. Please see [here](http://scicomp.stackexchange.com/questions/20921/how-to-interpolate-a-set-of-points-with-a-continuous-closed-b-spline-curve)" CreationDate="2015-10-05T07:49:30.763" UserId="1796" />
  <row Id="1109" PostId="1571" Score="0" Text="Thanks for your answer. I consider it to be incomplete though. Your statement from Interpretation 1 &quot;You perceive the spectrum as you would have perceived the rendered spectrum&quot; is arguably wrong. When perceiving the real spectrum, effects kick in that don't when using the conversion you described (e.g. you'd have to use a *scotopic standard observer* in low lighting conditions, as mentioned in Jameson, Hurvich: Visual Psychophysics). What you described is the idea of spectral rendering. Interpretation 2 is what I want to learn more about. The paper will be a good start, thanks for that." CreationDate="2015-10-05T08:30:23.777" UserId="385" />
  <row Id="1110" PostId="1528" Score="1" Text="&quot;Note that lines, ... can be described by an implicit equation with integer coefficients.&quot; Do you mean that it is possible to write Bresenham's algorithm as a Diophantine equation?" CreationDate="2015-10-05T10:14:22.390" UserId="1786" />
  <row Id="1111" PostId="1528" Score="1" Text="@AlexeyPopkov: in a way. In the first quadrant, the line equation is `Y-Y_0=(X-X0)(Y1-Y0)/(X1-X0)`, which gives rational `Y`'s. Setting `Z=Y(X1-X0)`, the equation becomes `Z-Z0=(X-X0)(Y1-Y0)`, of the form `Z=aX+b`." CreationDate="2015-10-05T10:23:54.207" UserId="1703" />
  <row Id="1112" PostId="343" Score="0" Text="@ShutaoTANG [image 1-2 postscript file](http://pastebin.com/Ncz4KxAC) as per request." CreationDate="2015-10-05T17:35:38.827" UserId="38" />
  <row Id="1113" PostId="1575" Score="0" Text="Or you can just download the image files..." CreationDate="2015-10-05T17:46:18.773" UserId="38" />
  <row Id="1114" PostId="1575" Score="0" Text="Thanks. I you hit the money when you said 'render each frame'. That is what I want to capture... the frames output by application. I'd like to 'grab' each rendered frame and pipe them to a client. I'll update my question." CreationDate="2015-10-05T18:14:36.430" UserId="1800" />
  <row Id="1115" PostId="1575" Score="0" Text="Another question: Are you wanting it to be dynamic, or just a static animation? If it's static, @joojaa is right. Just render out all the frames, convert them to a movie file, then just serve the movie file from the server." CreationDate="2015-10-05T18:25:40.463" UserId="310" />
  <row Id="1117" PostId="1569" Score="0" Text="Honestly, this is a bit off topic but to me it seems that focusing on crazy microscopic adjustments on the 5D space curves of BRDF by developping fitting models of microfacets integration, is just nit pick. It is much more important to focus on getting correct global illumination. it doesnt even have to be real time." CreationDate="2015-10-06T00:29:10.157" UserId="1614" />
  <row Id="1118" PostId="1576" Score="0" Text="There is a fourth way, artist supplied normals ;)" CreationDate="2015-10-06T04:19:45.657" UserId="38" />
  <row Id="1119" PostId="1576" Score="0" Text="@joojaa: I assume you are referring to normal maps? I've never heard of manually authored normals otherwise." CreationDate="2015-10-06T05:09:13.187" UserId="182" />
  <row Id="1120" PostId="1576" Score="1" Text="No, manually authored normals. It sometimes happens that your artist knows more about how the normals should behave than the programmers models do. It is sometimes a bit problematic to the calculation engines if they assume normals come from underlying calculations. But certainly it happens and you save lot of time in mathematical modeling." CreationDate="2015-10-06T07:30:19.340" UserId="38" />
  <row Id="1126" PostId="1579" Score="5" Text="Raytracers which render reflections and refractions have been ubiquitous for many many years." CreationDate="2015-10-06T13:59:46.793" UserId="457" />
  <row Id="1128" PostId="1579" Score="0" Text="Path tracing does just that: bounces rays around the scene using the properties of the surfaces it hits to determine bounces.  http://www.thepolygoners.com/tutorials/GIIntro/GIIntro.htm http://www.iquilezles.org/www/articles/simplepathtracing/simplepathtracing.htm" CreationDate="2015-10-06T14:39:52.620" UserId="310" />
  <row Id="1130" PostId="1582" Score="2" Text="The definition order did affect the layout. The relevant part here is the `s_buffer_load_dword` instructions - those are reading the input uniforms, and the last number in hex is the offset to read from. It shows in the first case `xy` is at offset 0 and `zw` at offset 16. In the second case you have `xy` at offset 0, `z` at offset 16, and `zw` at offset 32. It appears all the uniforms are individually 16-byte-aligned, and not packed together or reordered." CreationDate="2015-10-06T17:27:52.623" UserId="48" />
  <row Id="1132" PostId="1585" Score="0" Text="This is the correct beginning for an answer to this question. But it lacks continuation, you should talk of modern unbiased rendering as well to conclude about the recent convergence of techniques toward Kajiya's equation." CreationDate="2015-10-07T01:23:15.187" UserId="1614" />
  <row Id="1133" PostId="1587" Score="4" Text="They're probably referring to the sRGB color space. https://en.m.wikipedia.org/wiki/SRGB" CreationDate="2015-10-07T01:29:55.510" UserId="310" />
  <row Id="1135" PostId="1585" Score="1" Text="@v.oddou: feel free to enter your own. My point is about early techniques to show that this is a pretty old idea." CreationDate="2015-10-07T06:10:41.580" UserId="1703" />
  <row Id="1136" PostId="1575" Score="0" Text="@RichieSams it would be dynamic. I would like to allow for interaction." CreationDate="2015-10-07T08:35:04.333" UserId="1800" />
  <row Id="1137" PostId="1590" Score="0" Text="How would I go about accessing the key frames? I'm not really sure where to start. The game would be streamed from the cloud to a client. Thanks" CreationDate="2015-10-07T11:29:39.523" UserId="1800" />
  <row Id="1138" PostId="1592" Score="0" Text="Have you verified that mipmaps are actually used? The screenshot looks like bilinear filtering, instead of trilinear," CreationDate="2015-10-07T13:47:17.887" UserId="182" />
  <row Id="1140" PostId="1592" Score="0" Text="@JulienGuertault It may just be my poor eyesight but I can't see any distinct discontinuities that'd be indicative of just bilinear + nearest MIP map." CreationDate="2015-10-07T13:57:03.960" UserId="209" />
  <row Id="1141" PostId="1592" Score="0" Text="Actually my sampler state is set to AnisotropicWrap, which, I believe, is the best. Also I am 100% sure, that I use mipmaps." CreationDate="2015-10-07T14:04:13.337" UserId="205" />
  <row Id="1142" PostId="1592" Score="0" Text="@SimonF: sorry for the confusion, I meant linear. But anyway, apparently the problem would be elsewhere." CreationDate="2015-10-07T14:06:34.733" UserId="182" />
  <row Id="1143" PostId="1593" Score="0" Text="Sorry for maybe stupid question, but what is '2x2 box filter'? Does it have any other name?" CreationDate="2015-10-07T14:18:29.363" UserId="205" />
  <row Id="1144" PostId="1593" Score="0" Text="I just meant the naive method of generating the next lower, X*Y MIP map level from the 2X *  2Y level above it. I.e.  that of averaging 2x2 texels in the upper level to produce the corresponding texel in the lower one.  It's cheap, but it's not great.  Even a simple 4x4 tent filter should produce a far better result." CreationDate="2015-10-07T14:25:08.200" UserId="209" />
  <row Id="1145" PostId="1593" Score="0" Text="Just to make sure I fully understand you - now I am creating my mip maps like this: http://pastebin.com/nX1MVvEp  . Instead of that i should generate mipmaps from my original texture with original size yes?" CreationDate="2015-10-07T14:34:54.963" UserId="205" />
  <row Id="1146" PostId="1593" Score="0" Text="Well, I don't know what is inside the &quot;resize&quot; function so it's a little difficult to answer. The chaining, ie. 1024 generates 512, 512 generates 256, etc is not going to be a huge problem, as long as what's going on inside &quot;resize&quot; is ok.&#xA;As an experiment, could you create a 1024^2 texture that's all 0xFFFFFF _except_ for the top left pixel which you set to pure red, (0x0000FF).  What do you get in the 512^2  result?" CreationDate="2015-10-07T14:47:27.457" UserId="209" />
  <row Id="1147" PostId="1593" Score="0" Text="My result: http://postimg.org/image/atd8nc4j3/3636ad65/" CreationDate="2015-10-07T14:55:04.413" UserId="205" />
  <row Id="1149" PostId="1593" Score="0" Text="Sorry, I wasn't very clear. What I meant was &quot;what is in the top left, say, 10x10 texels of 512x512 image&quot;." CreationDate="2015-10-07T15:40:53.390" UserId="209" />
  <row Id="1150" PostId="1584" Score="0" Text="i wouldnt say we can render all effects. We can render most effects we know of. But rarely do we have the time to program in all effects." CreationDate="2015-10-07T16:34:09.477" UserId="38" />
  <row Id="1151" PostId="1591" Score="0" Text="I think a image would do wonders." CreationDate="2015-10-07T16:40:41.067" UserId="38" />
  <row Id="1152" PostId="1595" Score="1" Text="Do you mean that you want to detect areas of an image that are sharp and in-focus, versus blurred and out-of-focus?" CreationDate="2015-10-07T19:46:31.267" UserId="48" />
  <row Id="1156" PostId="1595" Score="1" Text="How did you used edge detection algorithms? Not sure it works and can't test now (so no answer for now :P), but just off the top of my head, you can divide your image in subregions and measure the value of gradients. Based on a threshold you can then decide whether that subregion is &quot;in-focus&quot; or not. Then you can re-use the edge info to refine your first approximation I described before. It makes sense to me know, but it may very well be a brain-fart being it late here :) I'll give it a try tomorrow if I have a minute to." CreationDate="2015-10-07T21:13:59.867" UserId="100" />
  <row Id="1159" PostId="1434" Score="1" Text="Are you going to improve article on Wikipedia? I see question like yours pretty often in Comptuter Graphics SE. Maybe it's worth considering? :)" CreationDate="2015-10-08T05:24:39.500" UserId="205" />
  <row Id="1162" PostId="1595" Score="0" Text="@cifz - Thanks! Great idea to measure the gradiants, sounds like a simple and fast approach. But this will only work for a rough detection, right?" CreationDate="2015-10-08T10:24:07.547" UserId="18" />
  <row Id="1163" PostId="1595" Score="0" Text="@NathanReed Right. Sorry, not a native :) Do you think I should rephrase the question?" CreationDate="2015-10-08T10:28:27.593" UserId="18" />
  <row Id="1164" PostId="1599" Score="0" Text="Many thanks Fabrice! If possible, can you elaborate *local max frequency*?" CreationDate="2015-10-08T10:44:00.727" UserId="18" />
  <row Id="1165" PostId="1593" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/30017/discussion-between-simon-f-and-bartosz-baczek)." CreationDate="2015-10-08T12:05:38.610" UserId="209" />
  <row Id="1166" PostId="1599" Score="0" Text="I don't remember how these papers were estimating the native focus before refocusing. But I could think of various solutions: poor-man wavelets by calculating the FFT in a grid of subimages (i.e. a grid of windowed FFT), estimating local autocorrelation, poor-man FFT by convolving with a set of Differential of Gaussians (to detect ranges of focus), etc. But this is kind-of instant-hacking, the best is to read what the experts did for real :-)" CreationDate="2015-10-08T12:58:38.223" UserId="1810" />
  <row Id="1167" PostId="1579" Score="0" Text="Youd likely be interested in reading about how the used raytracing techniques to render the black hole in the movie interstellar. They used realistic physics equations and the result was so interesting that they were able to publish a scientific research paper with the results.  http://www.wired.com/2014/10/astrophysics-interstellar-black-hole/" CreationDate="2015-10-08T14:26:19.093" UserId="56" />
  <row Id="1168" PostId="1595" Score="0" Text="@poor Yes, I think it would be helpful to rephrase and edit the title to make it more clear." CreationDate="2015-10-08T17:10:30.513" UserId="48" />
  <row Id="1169" PostId="1551" Score="1" Text="Wow, this should be a blog post somewhere! Great answer!" CreationDate="2015-10-08T18:39:32.033" UserId="54" />
  <row Id="1171" PostId="210" Score="0" Text="I'm with Alan too, gamma can cause all sorts of contrast issues. But a uniformly emissive sphere will tend to produce very faint occlusions. You should try to use a falloff factor in the emission, such that vertical emissions are stronger than grazing emissions. Also your spheres are reflective, make then absorbing. but i guess your tracer does not take that into account otherwise we'd see more energy under your spheres." CreationDate="2015-10-09T01:24:09.873" UserId="1614" />
  <row Id="1172" PostId="1604" Score="0" Text="Interesting - thank you. For the additional light reaching the floor from the spheres, I am expecting this to be less than the light that would reach the floor directly from the sky if the spheres were not there, since they are convex. That is, I can't picture caustics resulting from only convex surfaces when the light is coming evenly from the sky. Even in the extreme case of all light being reflected (no absorption), I would expect simply no shadow, rather than a caustic." CreationDate="2015-10-09T02:47:27.653" UserId="231" />
  <row Id="1173" PostId="361" Score="0" Text="Yes, the wording is quite strange (what is a rotating point for instance ? :-) ). It's not a mistake, it's poor wording. I'm afraid that once a guy quicky made the whole &quot;basic computer graphics&quot; content of wikipedia for some reason, letting a lot of polishing (or more) to be done. Seems like hot topics are well edited and completed (by academics and master/PhD students ?), but not basic topics (I did, for a very few)." CreationDate="2015-10-09T07:44:52.237" UserId="1810" />
  <row Id="1175" PostId="1551" Score="2" Text="Glad it's of help. As for a blog, I did write [this a decade ago](http://web.onetel.net.uk/~simonnihal/texcom/texcompcomp.html) but I really don't have time to do them." CreationDate="2015-10-09T08:01:52.133" UserId="209" />
  <row Id="1177" PostId="1592" Score="0" Text="@bartosz.baczek It's great that you found a solution to your problem. But instead of editing the solution into your question, please post it as an answer to your question, as [answering your own question is perfectly fine](http://computergraphics.stackexchange.com/help/self-answer)." CreationDate="2015-10-09T15:41:33.163" UserId="127" />
  <row Id="1178" PostId="1592" Score="1" Text="@Nero I did so :)" CreationDate="2015-10-09T15:56:13.237" UserId="205" />
  <row Id="1179" PostId="1605" Score="1" Text="Mirror is likely to define what happens when your texture repeats. In the normal mode (just considering horizontal only) if you had a texture that looked like &quot;&lt;&quot;  you'd get &quot;&lt;&lt;&lt;&lt;&lt;&quot;.  With mirroring you get &quot;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&quot;.  Obviously can also apply to vertical direction as well." CreationDate="2015-10-09T16:15:52.590" UserId="209" />
  <row Id="1180" PostId="1605" Score="0" Text="It looks like increasing the resolution is the main helper. Moires patterns are just barely starting to form in the back corner of the rectangle." CreationDate="2015-10-09T16:32:09.603" UserId="310" />
  <row Id="1181" PostId="1605" Score="0" Text="Actually no, mirroring does most of the job :) I tested it with mirroring only and with bigger texture only as well, and turned out that mirror provides great effect. Fortunatelly tile texture is simetric so it' s not a big deal." CreationDate="2015-10-09T16:38:44.877" UserId="205" />
  <row Id="1184" PostId="1607" Score="2" Text="While this answers the question, instead of simply providing the solution to the specific problem it would be much better to describe how the OP could come up with this solution on his own. The question clearly is a homework related question. As of now, this answer does not help to solve the problem with different numbers." CreationDate="2015-10-09T18:51:18.327" UserId="127" />
  <row Id="1185" PostId="1606" Score="1" Text="You can read off the matrix elements from the coefficients of x, y, z in the result. For instance if the result had 2x + 3y in one component, then the corresponding column of the matrix would have values (2, 3, 0). The problem in this case is a bit more complicated as it appears to be relying on homogeneous coordinates (since there's division by x and y in the result), but still you can basically read off the matrix elements from the form of the result." CreationDate="2015-10-09T18:51:22.410" UserId="48" />
  <row Id="1186" PostId="1607" Score="0" Text="Thank you. I just investigated what &quot;perspective divide&quot; is and I found it has to do with finding a w for (x, y, z) in order to get (x, y, z, w). After finding such a value then everything is straight forward. Is this correct? Also, the issue here is to get rid of the variables in denominator." CreationDate="2015-10-09T19:02:02.020" UserId="1836" />
  <row Id="1188" PostId="1607" Score="0" Text="I wonder if I can ask another question related to this same problem. &quot;Assume you have performed the projection on the canonical view volume.  Give the matrix you would use to make the final image of size 5 units wide and 3 units tall.&quot; I think the answer is [5, 0, 0, 0][0, 3, 0, 0][0, 0, 1, 0][0, 0, 0, 1] Am I correct?" CreationDate="2015-10-09T19:23:59.467" UserId="1836" />
  <row Id="1189" PostId="1609" Score="0" Text="I am sorry but I do not get the idea. What do you exactly mean by &quot;reverse order&quot;?" CreationDate="2015-10-09T20:21:31.200" UserId="1836" />
  <row Id="1190" PostId="1609" Score="1" Text="Multiply x by z instead of z by x;" CreationDate="2015-10-09T20:27:31.040" UserId="1826" />
  <row Id="1191" PostId="1609" Score="1" Text="actually order is arbitrary one can model using row vectors and one can model column vectors. The computation yelds same result in both but the multiplication order changes. But yes this is sortof the right answer." CreationDate="2015-10-10T05:11:52.983" UserId="38" />
  <row Id="1192" PostId="1607" Score="0" Text="@JORGE i guess you should update the original question. Please explain this topic or link useful resources. I am interested" CreationDate="2015-10-10T09:05:55.533" UserId="316" />
  <row Id="1194" PostId="1607" Score="0" Text="@JORGE if it's a different question but relating to this one then you can post it separately as a new question but include a link to this question." CreationDate="2015-10-10T15:52:28.570" UserId="231" />
  <row Id="1195" PostId="1609" Score="0" Text="Joojaa, thanks for making that clear! Row matrix means reversed order of multiplication, is that correct?" CreationDate="2015-10-10T19:55:37.973" UserId="1826" />
  <row Id="1197" PostId="211" Score="0" Text="The trick of putting the sphere on the ground and looking at the contact point is a great one. Notice it works for debugging your (spherical) lights too, but in the opposite direction!" CreationDate="2015-10-11T05:04:43.907" UserId="523" />
  <row Id="1200" PostId="210" Score="0" Text="Your sky may be casting a little penumbra instead of a shadow. Reduce it to a small sphere for a check. Also, the spheres are indirectly illuminating the background." CreationDate="2015-10-11T18:45:36.700" UserId="1703" />
  <row Id="1201" PostId="1606" Score="0" Text="[Relevant meta discussion on whether multi part questions should be in one post or separate questions](http://meta.computergraphics.stackexchange.com/questions/181/editing-a-question-to-add-a-second-part)" CreationDate="2015-10-11T20:02:02.933" UserId="231" />
  <row Id="1202" PostId="1606" Score="0" Text="I'd recommend asking the second part as a separate question, since there is already an answer that was based on there only being one part. I've rolled back to the previous edit with only one question, but I've also raised this on Meta to see how others see it." CreationDate="2015-10-11T20:08:12.620" UserId="231" />
  <row Id="1203" PostId="1606" Score="0" Text="I don't know what will be decided on Meta but I've rolled back the edit quickly rather than waiting for the outcome of the Meta discussion, in order to prevent new answers coming in for the second part that would then prevent the edit being rolled back. I don't mean this to seem pushy. If there's anything you'd like to discuss please feel free to join us in [chat]." CreationDate="2015-10-11T20:14:33.037" UserId="231" />
  <row Id="1205" PostId="1590" Score="0" Text="maybe try to record it and than search them by hand. I know this is far away from any kind of automated test or even realtime feedback, but it seemed to be the rather uncomplicated" CreationDate="2015-10-12T07:28:21.890" UserId="1818" />
  <row Id="1210" PostId="1614" Score="1" Text="Can you atomic-add 3 to the counter instead of incrementing it?" CreationDate="2015-10-12T22:38:52.533" UserId="48" />
  <row Id="1211" PostId="1614" Score="1" Text="No, atomic counters can only be queried, incremented (by 1) or decremented (by 1). See [the Article](https://www.opengl.org/wiki/Atomic_Counter#Operations) in the opengl wiki. They are not the same as atomic Add etc on Images. You only have 8 (or so) of them and they are supposedly much faster when accessed extremely often (like when generating (and thus counting) thousands of ... things.)" CreationDate="2015-10-12T23:21:37.290" UserId="1699" />
  <row Id="1212" PostId="1614" Score="0" Text="Yeah, I guess you would have to turn it from an &quot;atomic counter&quot; to just a variable in an SSBO. Would be interesting to see if that's actually any slower (it might not be, depending on HW). Other than that, the only thing I can think of is to do like you said and run a compute shader to multiply the value by 3." CreationDate="2015-10-13T00:35:03.507" UserId="48" />
  <row Id="1213" PostId="1613" Score="1" Text="I would be very carrfull with that associativity comment its easy to misunderstand" CreationDate="2015-10-13T03:55:31.857" UserId="38" />
  <row Id="1214" PostId="1613" Score="0" Text="@joojaa I don't know what exactly you mean, but I've tried to clarify that bit." CreationDate="2015-10-13T07:36:30.703" UserId="16" />
  <row Id="1215" PostId="1613" Score="0" Text="Its hard for a layman to separate betveen order you multiply things and the order in which the elements are in multiplication." CreationDate="2015-10-13T11:25:49.913" UserId="38" />
  <row Id="1216" PostId="1613" Score="0" Text="so they do not understand the difference between assiocative and commutative. so if you talk of the order of multiplication many may think of commutativity" CreationDate="2015-10-13T11:37:31.017" UserId="38" />
  <row Id="1217" PostId="1615" Score="0" Text="I believe the question is seeking embedding of code in the image file, rather than in the image itself. If you're uncertain what a question is looking for you can also comment on the question once you have sufficient reputation." CreationDate="2015-10-13T13:58:22.043" UserId="231" />
  <row Id="1218" PostId="1616" Score="0" Text="I like the second idea because it makes clear that I need two different things: A way to atomically add triangles and a way to get the final index count. Doesn't have to be the same mechanism. And yes, I'll try atomicAdd as well. As Nathan suggested in a comment above, it would be interesting to see if it really is slower. I'll report back with the findings." CreationDate="2015-10-13T14:33:49.207" UserId="1699" />
  <row Id="1219" PostId="1615" Score="1" Text="thanks user1846, I was looking to embed the code inside the image file, as trichoplax mentions." CreationDate="2015-10-13T18:13:03.397" UserId="1806" />
  <row Id="1220" PostId="1616" Score="1" Text="Can confirm that atomicAdd is indeed not slower for this use case on nvidia kepler ( ~1.5k indices: 0.1ms - probably needs larger size for meaningful benchmark). Binding the indirect command buffer as an SSBO and writing to the uint at position 0 works fine. Sometimes one should just try the easy solutions first..." CreationDate="2015-10-13T22:15:00.713" UserId="1699" />
  <row Id="1223" PostId="1581" Score="1" Text="Something slightly unrelated, what about images that are code? Look at the esolang [piet](https://esolangs.org/wiki/Piet) which lets you program with images. And [this video](https://www.youtube.com/watch?v=FvS_DG8yIqQ) where an image is created and then built as an executable." CreationDate="2015-10-14T15:40:37.467" UserId="214" />
  <row Id="1224" PostId="1619" Score="0" Text="Are you sure lerping the forward transformed object is the same as lerping the backward transformed ray? For example, I can renormalize the ray after the lerp (and scale the hit distance accordingly). This doesn't change the result." CreationDate="2015-10-14T19:41:56.740" UserId="523" />
  <row Id="1225" PostId="1620" Score="0" Text="I break it down into a linear approximation, which is quite typical. One can handle more complex transforms with multiple such steps.¶ My problem is not in making the transform nonlinear, but in making the linear approximation to it correct." CreationDate="2015-10-14T19:45:49.290" UserId="523" />
  <row Id="1226" PostId="1619" Score="0" Text="@imallett Lerping the ray should be equivalent to lerping the inverse matrices, but not necessarily to lerping the forward matrices or lerping the object (as inversion isn't a linear operation). And I don't think renormalizing the ray after the lerp fixes things entirely - you can still be in a sheared, nonuniformly-scaled coordinate system that can screw up the math in your intersection routines and suchlike." CreationDate="2015-10-14T20:46:39.937" UserId="48" />
  <row Id="1227" PostId="1619" Score="0" Text="[See edit; better picture] At least, I do think renormalizing should rule out problems with the intersection--but that is what I thought; lerping the ray isn't lerping the object. In your answer you suggested lerping [inverse?] TRS and then recombining. Is this the way production renderers do it?" CreationDate="2015-10-14T21:01:21.383" UserId="523" />
  <row Id="1228" PostId="1622" Score="0" Text="your second `glVertexAttribPointer` doesn't look right, it's missing a *5 in the stride param" CreationDate="2015-10-15T10:31:15.700" UserId="137" />
  <row Id="1229" PostId="1622" Score="0" Text="@ratchetfreak Yes the *5 was missing, But the problem still exists." CreationDate="2015-10-15T12:53:33.407" UserId="1861" />
  <row Id="1230" PostId="1623" Score="1" Text="What is missing for you from the SH wikipedia page ? https://en.wikipedia.org/wiki/Spherical_harmonics" CreationDate="2015-10-15T13:20:06.867" UserId="1810" />
  <row Id="1231" PostId="1623" Score="1" Text="Fourier basis functions are simply sines (or sines.x * sines.y). Like sound can decompose in spectrums (i.e. set of sines), so is it for a 1D drawing. For 2D data, you simply do that in x then y (but I find quite intuitive to extrapolate the notion of wavelength to 2D).  Maybe you should tell what is your background in maths ? (which school level ?)" CreationDate="2015-10-15T13:23:38.570" UserId="1810" />
  <row Id="1233" PostId="1604" Score="1" Text="I understand your intuition. I wanted to dare make an analytical computation for what energy we should expect on a pixel laying just under the vertical tangent of the sphere but I'm so slow at math I gave up lol. Indeed convex surfaces will reflect through only one path to any given singular emission source-point, so caustics seems implausible." CreationDate="2015-10-16T01:19:25.370" UserId="1614" />
  <row Id="1234" PostId="1627" Score="4" Text="I think you're going to have to ask this on a site about electrical engineering..." CreationDate="2015-10-16T01:48:37.990" UserId="54" />
  <row Id="1235" PostId="1623" Score="0" Text="@FabriceNEYRET Thank you for the link. While I don't think this information will benefit other users, my background in maths is prépa(MP*) + engineering school (Master Degree) ; the program did not have in-depth covering of SH. I am indeed looking for material as rigorous as what I was used to study, although I now have less time to do so, so brevity is also important." CreationDate="2015-10-16T03:02:28.307" UserId="110" />
  <row Id="1236" PostId="1623" Score="1" Text="I read the paper you linked, it seems pretty solid." CreationDate="2015-10-16T05:10:24.150" UserId="38" />
  <row Id="1237" PostId="1627" Score="0" Text="Agree with glampert but, assuming all are monochromatic, it's probably just the electronics which control the voltage patterns that, in turn drive the electromagnets (or charged plates??) that deflect the electron beam in order to sweep it across the display phosphor." CreationDate="2015-10-16T06:40:22.217" UserId="209" />
  <row Id="1238" PostId="1605" Score="0" Text="It's hard to judge how much of an improvement this has made since the plane does not extend as far into the distance as in the example in the question." CreationDate="2015-10-16T07:55:13.733" UserId="231" />
  <row Id="1239" PostId="1629" Score="1" Text="Indeed the way Fourier is treated at university or enginneering school can be totally different from place to place, from a pure fancy pure math toy in strange mathematical spaces (but here it's even worse for Finite Element methods) to something really connected to signal, theory and applications. Similarily SH are sometime presented as cool math object dedicated to the physics of electronic orbitals. In all these case it's not easy for (ex)students to transpose to CG or signal. -&gt; that's why it's important to tell where he starts from and where he seeks to." CreationDate="2015-10-16T08:06:35.957" UserId="1810" />
  <row Id="1240" PostId="1627" Score="0" Text="Please not that cathode ray tubes are at the brink of being discontinued as technology. Only very very special uses even can get hold of these things. Even oscilloscopes are using LCD monitors by now. Ancient tech." CreationDate="2015-10-16T16:03:11.870" UserId="38" />
  <row Id="1241" PostId="1635" Score="0" Text="That is indeed a much better test. Actually, the Phong normalization I implemented is indeed based on Global Illumination Compendium 31a. It works out to be a 1D table which you precompute to any desired resolution for each BRDF.&#xA;&#xA;The other normalization term produces much weaker results, as might be expected in this glancing reflection case. I'm also trying to solve it myself--a similar 1D table approach should be possible." CreationDate="2015-10-17T16:53:08.640" UserId="523" />
  <row Id="1242" PostId="1633" Score="0" Text="Is this per-frame? As in a prepass to fill the depth buffer?" CreationDate="2015-10-18T05:49:56.820" UserId="523" />
  <row Id="1244" PostId="1585" Score="0" Text="If you don't count ray casting (which you shouldn't), this was the first ray tracing paper. So reflections/refractions/shadows have all been here since literally the very beginning." CreationDate="2015-10-18T06:20:46.497" UserId="523" />
  <row Id="1245" PostId="1579" Score="0" Text="Because we currently render movies using . . . magic?" CreationDate="2015-10-18T06:21:44.070" UserId="523" />
  <row Id="1246" PostId="1633" Score="0" Text="Nope, done in the loading screen. Not talking about a depth only render!" CreationDate="2015-10-18T14:16:14.810" UserId="56" />
  <row Id="1247" PostId="1637" Score="0" Text="In my understanding, raycasting isnt very good for boundary information. Depthmaps also dont dont benefit from raycasting. Deepshadow maps might benefit from raycasting. Otherwise you would just be better of baking lightning to model uv maps or some kind of voxel tree. Possibly per vertex/face. The question is a bit hard to swallow, could you specify it a bit. Best is not defined, efficient in this case is not terribly defined either" CreationDate="2015-10-19T06:15:54.390" UserId="38" />
  <row Id="1248" PostId="1637" Score="0" Text="Are you asking for something geometric such as Franklin Crow's [Projected Shadow Volumes](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.424.6834)?" CreationDate="2015-10-19T12:19:21.930" UserId="209" />
  <row Id="1249" PostId="1599" Score="0" Text="Any work done on single image, not with Coded Aperture? Thank You." CreationDate="2015-10-19T15:08:50.363" UserId="234" />
  <row Id="1251" PostId="1637" Score="0" Text="Please fill in the &quot;etc&quot; and tell us the purpose of storing the shadow data." CreationDate="2015-10-19T21:07:33.990" UserId="1703" />
  <row Id="1252" PostId="1637" Score="0" Text="I apologize for being so vague.. Its basically for an application where I want to be able to move the shadow around manually to see the change in the 3d objects position. Say I apply a force to the shadow of a ball, I can actually see the ball move and also the change in the shadow. So I need to be able to set up colliders for the shadow and hence need a way to be able to store the boundary of the shadow and information about the object that cast it to carry out the actions." CreationDate="2015-10-20T01:03:27.893" UserId="1879" />
  <row Id="1253" PostId="1637" Score="0" Text="Can I infer that &quot;etc&quot; is void ? What other information is relevant ?" CreationDate="2015-10-20T08:49:06.520" UserId="1703" />
  <row Id="1255" PostId="1639" Score="0" Text="Why not trace on click? There sa really neat trick if there is less than 32-96 objects." CreationDate="2015-10-20T14:48:37.183" UserId="38" />
  <row Id="1256" PostId="375" Score="0" Text="In a word: evil." CreationDate="2015-10-20T15:21:08.137" UserId="523" />
  <row Id="1257" PostId="379" Score="4" Text="@ratchetfreak only if you're in space, which you're typically not. Our glorious atmosphere makes it appear yellowish (by scattering the blue)." CreationDate="2015-10-20T15:23:56.237" UserId="523" />
  <row Id="1259" PostId="1637" Score="0" Text="@ichigo1191 your comments are giving a lot more detail than the question, which makes a big difference. I recommend you [edit] the question to include the extra detail all in one place. Comments are not guaranteed to last forever." CreationDate="2015-10-20T15:40:47.627" UserId="231" />
  <row Id="1260" PostId="1639" Score="0" Text="@joojaa: no idea what you mean." CreationDate="2015-10-20T15:55:13.470" UserId="1703" />
  <row Id="1261" PostId="1641" Score="0" Text="Thank you for the reply. I am going to replicate what I did and post a screen as suggested. Anyway the final intent of this question isn't to debug my code, but to understand if what I am doing is right or there are others steps that I am missing in between!" CreationDate="2015-10-20T15:55:23.550" UserId="1895" />
  <row Id="1262" PostId="1639" Score="0" Text="Why make a buffer of rays? If your only interested in one ray, shoot the ray when your needing it. Its fast enough. Why shoot a shadow buffer with rays just use a separate pass and use depth maps + index color. Again you can do this on demand. If you have 3 color channels you can use a bit mask for surfaces hit tahtway you can store ALL objects in pass of ray if you must etc. Too many open variables in question to nake any meaningful impact." CreationDate="2015-10-20T16:21:27.457" UserId="38" />
  <row Id="1264" PostId="1639" Score="0" Text="@joojaa: I guess this comment belongs to the OP. I am answering the question &quot;how to store ?&quot;." CreationDate="2015-10-20T17:05:27.403" UserId="1703" />
  <row Id="1265" PostId="1642" Score="0" Text="Wow, i want to die now :(&#xA;I even said i wanted gl_GlobalInvocationID to behave like gl_FragCoord, which uses uses window coordinates.&#xA;This was one of the most stupid error ever.&#xA;&#xA;You saved my day and my sanity." CreationDate="2015-10-20T21:56:09.980" UserId="281" />
  <row Id="1266" PostId="1641" Score="0" Text="added examples as suggested!" CreationDate="2015-10-21T11:38:50.710" UserId="1895" />
  <row Id="1267" PostId="1644" Score="0" Text="and what's wrong with a 1024x512 texture that gets blitted to the display buffer by the hardware?" CreationDate="2015-10-21T16:03:15.850" UserId="137" />
  <row Id="1268" PostId="1644" Score="0" Text="Nothing is inherently wrong with that, but what about smaller operations like outputting a single character? Also some smaller systems might not have a GPU, etc." CreationDate="2015-10-21T16:16:38.943" UserId="1902" />
  <row Id="1269" PostId="1644" Score="4" Text="This used to be a thing in VGA days. 0xA0000 anyone?" CreationDate="2015-10-21T16:59:42.720" UserId="48" />
  <row Id="1271" PostId="1641" Score="0" Text="The points from the poisson sampling are right. The algorithm that generates them is fully unit tested and the ones you see in the screens are spheres with the center in the sampled point which i programmatically created before calling Voronoi(points)! I am worried that I am not following the proper path or I am handling the Voronoi result in a wrong way" CreationDate="2015-10-21T20:10:46.213" UserId="1895" />
  <row Id="1272" PostId="1641" Score="0" Text="The images you show have done the voronoi on the 2d function." CreationDate="2015-10-22T03:57:12.260" UserId="38" />
  <row Id="1273" PostId="1641" Score="0" Text="@joojaa From the example images I expected that the Voronoi cell edges on the 2D surface were what was required (to give a collection of line segments connecting points on the sphere surface, rather than the collection of plane sections that would be given in 3D). However, [scipy.spatial.Voronoi](http://scipy.github.io/devdocs/generated/scipy.spatial.Voronoi.html) seems to be designed for N dimensional spaces rather than surfaces embedded in them. I can't immediately see how it would be used for 3D points constrained to a 2D surface." CreationDate="2015-10-22T10:07:57.903" UserId="231" />
  <row Id="1274" PostId="1641" Score="0" Text="Is not a problem to me if I have to change library or to write my own implementation.  If you can suggest any source I need to look at I will be happy to do it!" CreationDate="2015-10-22T10:20:03.673" UserId="1895" />
  <row Id="1275" PostId="1641" Score="0" Text="I'm guessing you would need to implement a distance function for two points on a surface. On the sphere you can get away with just using the 3D Euclidean distance but for arbitrary shapes you'll need something more specific otherwise points that are near to each other in 3D due to folds in the surface will appear nearer to each other than to points between them." CreationDate="2015-10-22T14:29:47.847" UserId="231" />
  <row Id="1276" PostId="1641" Score="0" Text="Do you mean to sample points? I am using isotopic distance to accomplish this. Anyway as you see, my problem is with the Voronoi computation, not the initial sampling procedure, but maybe I didn't understand what you were saying!" CreationDate="2015-10-22T14:36:31.333" UserId="1895" />
  <row Id="1281" PostId="1622" Score="0" Text="What if you remove the `layout` stuff from the shaders?" CreationDate="2015-10-22T18:47:39.383" UserId="1881" />
  <row Id="1282" PostId="1641" Score="0" Text="No I meant the Voronoi step - converting cell centres (the Poisson sampled points) into cell edges. This also requires a length calculation in order to determine which line is equidistant from a given two points." CreationDate="2015-10-22T20:37:01.267" UserId="231" />
  <row Id="1284" PostId="153" Score="0" Text="+1 So many answers to things I've always wondered about!" CreationDate="2015-10-22T20:53:48.117" UserId="457" />
  <row Id="1287" PostId="1644" Score="0" Text="1024 x 512 x 4 bytes = 2MB. Peanuts in a 2GB address space." CreationDate="2015-10-23T13:37:03.110" UserId="1703" />
  <row Id="1294" PostId="1651" Score="0" Text="Now this appears to be Mac only, which may not work for you with Direct3D, but [Syphon Inject](http://syphon.v002.info) which OBS uses on Mac can capture game windows that use OpenGL _much_ faster than standard video capture.  Syphon &quot;allows applications to share frames - full frame rate video or stills - with one another in realtime&quot; to quote the website." CreationDate="2015-10-24T22:45:32.967" UserId="1922" />
  <row Id="1296" PostId="1652" Score="0" Text="Thanks for the answer. I think that ultimately, I will instead of saving images, stream them to a remote machine, so copying the framebuffer may be the way to go. You seem to have a fair bit of experience with this stuff. Are you able to expand on any of the questions that I posed? Thanks for the help!" CreationDate="2015-10-25T10:10:03.303" UserId="1800" />
  <row Id="1297" PostId="1651" Score="0" Text="@Gliderman Thanks, I will check that out!" CreationDate="2015-10-25T10:10:32.563" UserId="1800" />
  <row Id="1298" PostId="1652" Score="1" Text="@pookie Hey! Sure, I can expand this. Is there anything specifically you'd like me to comment on? For your questions, I think 1&amp;2 can be done with the DLL hooks, so be sure to read the links I've included (specially the one about starcraft). 3&amp;4 will be API specific, but on OpenGL, you can start with glReadPixels and work from there if you find it to be too slow. My experience is from recording frames from my own games plus some DLL hacking done in the past, but I've never combined both in the attempt of recording frames from a third party app ;)" CreationDate="2015-10-25T17:13:52.243" UserId="54" />
  <row Id="1299" PostId="1652" Score="0" Text="Thank you! I've checked the links - windows DLL injection and starcraft link (pretty awesome) seem to be good starting points. I will take a deapre look into glReadPixels, too. In your opinion, which did you find easier to work with and which did you find faster: DirectX or OpenGL?" CreationDate="2015-10-25T17:23:39.877" UserId="1800" />
  <row Id="1300" PostId="1652" Score="0" Text="@pookie, if you have some programming experience, I think Direct3D will be much easier to use, the libraries are better designed. OpenGL has too may &quot;rough edges&quot; in my opinion (this coming from someone that uses OpenGL more than D3D). D3D is Windows only, so that might be a negative for you, don't know. Performance-wise, I don't think there's going to make much of a difference which one you use. The best advice I can probably give you is: first figure out how to get it working, then worry about optimizations." CreationDate="2015-10-25T18:08:27.037" UserId="54" />
  <row Id="1301" PostId="1652" Score="0" Text="Thanks, I've a fair bit of c# knowledge - not sure how I will take to D3D, but I will give it a stab." CreationDate="2015-10-25T18:11:45.210" UserId="1800" />
  <row Id="1302" PostId="1652" Score="1" Text="Ah, ok, if you're familiar with OOP programming, then D3D will definitely be easier. OpenGL is all procedural and uses a bunch of global state, that's mostly what I meant about rough edges. Best of luck to you!" CreationDate="2015-10-25T18:30:15.723" UserId="54" />
  <row Id="1306" PostId="1656" Score="2" Text="Code looks right on a first glance. Are you checking for GL errors? Do you have [KHR_debug](http://renderingpipeline.com/2013/09/opengl-debugging-with-khr_debug/) output set up?" CreationDate="2015-10-26T02:41:25.983" UserId="48" />
  <row Id="1308" PostId="1641" Score="0" Text="These articles look relevant to what you are trying to achieve:&#xA;http://meshlabstuff.blogspot.com/2009/03/creating-voronoi-sphere.html&#xA;http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-2.html" CreationDate="2015-10-26T03:19:08.577" UserId="182" />
  <row Id="1311" PostId="1657" Score="5" Text="I'm voting to close this question as off-topic because [current consensus](http://meta.computergraphics.stackexchange.com/q/146/16) seems to be that asking for literature (or similar) recommendations is not a good fit for the site." CreationDate="2015-10-26T08:51:24.840" UserId="16" />
  <row Id="1312" PostId="1656" Score="0" Text="Updated my question." CreationDate="2015-10-26T09:11:02.463" UserId="214" />
  <row Id="1314" PostId="1658" Score="0" Text="It's the book. :-)" CreationDate="2015-10-27T00:30:18.227" UserId="1886" />
  <row Id="1316" PostId="1662" Score="0" Text="If it's a read-only lookup table, you can just use a buffer/texture. You could either pack it into one of the normal texture formats, or you can use some of the newer features of DX11 / OpenGL to have a custom format. UAV in DX11 land, or a texture / shader_image_load_store in OpenGL land." CreationDate="2015-10-27T17:33:49.673" UserId="310" />
  <row Id="1318" PostId="1662" Score="0" Text="In addition, give this presentation a look: https://www.cvg.ethz.ch/teaching/2011spring/gpgpu/cuda_memory.pdf It's for CUDA, but it should give you a better idea of what is happening on the underlying hardware" CreationDate="2015-10-27T17:41:58.977" UserId="310" />
  <row Id="1319" PostId="1663" Score="0" Text="In your code sample the instance buffer is empty, is this intended? Also there is no vertex buffer binding (either glBindBufferRange​ or the newer glBindVertexBuffer)." CreationDate="2015-10-27T22:21:27.677" UserId="528" />
  <row Id="1320" PostId="1663" Score="0" Text="I'm filling the data into the second paragraph, also, why would I need to call glBindBufferRange?&#xA;&#xA;Isn't that just needed for UBOs / SSBOs?" CreationDate="2015-10-28T11:32:11.837" UserId="1938" />
  <row Id="1321" PostId="1665" Score="0" Text="Amazing, just finished implementing it and Element Buffers are the way to go." CreationDate="2015-10-28T16:46:49.410" UserId="116" />
  <row Id="1322" PostId="1663" Score="0" Text="Oh sorry, overlooked the buffer filling. And I also remembered the glBindBufferRange wrong since I was by now too much used to the newer glBindVertexBuffer semantics. But there should be a glBindBuffer for the per-vertex data somewhere between your VertexAttrib calls, right?" CreationDate="2015-10-28T19:43:50.567" UserId="528" />
  <row Id="1323" PostId="1663" Score="0" Text="Yes. As you can see, my per-instance's data's vertex attrib locations start at 3, that's because, previously, from 0 to 2, I've defined my per-vertex data. I just posted the instancing related parts of the code." CreationDate="2015-10-29T04:38:49.910" UserId="1938" />
  <row Id="1324" PostId="1663" Score="0" Text="Sorry for double posting, however, strangely enough, [this](http://imgur.com/KgVN4L1) is what happens when I turn on the &quot;wireframe mode&quot;. The middle quad gets cut." CreationDate="2015-10-29T05:01:15.347" UserId="1938" />
  <row Id="1326" PostId="1669" Score="3" Text="Any context? Or reference where you have read that? Because it does not make sense to me. Plus, if I'm not mistaken, Gn continuity is defined only for piece-wise polynomial surfaces, there is no reason for a surface to be polynomial and in practice most of the surfaces are piece-wise linear." CreationDate="2015-10-30T14:29:20.230" UserId="1613" />
  <row Id="1327" PostId="1669" Score="2" Text="G2 just mentions the geometric n-derivability, independently of any parameterisation." CreationDate="2015-10-30T20:06:05.620" UserId="1810" />
  <row Id="1329" PostId="1671" Score="0" Text="I would be more modest on general claims about &quot;textures can be modeled as&quot;. I would rather say &quot;some (restricted) famillies of textures can&quot;." CreationDate="2015-10-31T09:20:29.293" UserId="1810" />
  <row Id="1330" PostId="1672" Score="0" Text="[Skeletal Animation](https://en.wikipedia.org/wiki/Skeletal_animation)" CreationDate="2015-10-31T17:48:22.650" UserId="54" />
  <row Id="1331" PostId="1671" Score="0" Text="It would help if you could add some links to explain the concepts you are referring to, like Markov Random Fields and texture neighborhoods. Otherwise, we're just guessing what you're talking about." CreationDate="2015-10-31T21:08:47.123" UserId="48" />
  <row Id="1332" PostId="1669" Score="0" Text="@tom He is talking of general surfece design like in CAD. No they dont need to be polynomials, but in practice they often are (except for circular arcs and conics)" CreationDate="2015-11-01T08:00:08.843" UserId="38" />
  <row Id="1333" PostId="1672" Score="0" Text="I assume the question is really how are transformation matrices interpolated. http://stackoverflow.com/questions/3093455/3d-geometry-how-to-interpolate-a-matrix" CreationDate="2015-11-01T08:50:02.107" UserId="457" />
  <row Id="1334" PostId="1673" Score="1" Text="I edited the question to include the image, code and link to the other question. However, in my opinion, it would have been better to edit the other question and include the new things you found out there. It is also encouraged to update the question to include what you have tried to solve the problem even after you posted it. [See our Help Center](http://computergraphics.stackexchange.com/help/no-one-answers)." CreationDate="2015-11-01T11:38:06.240" UserId="127" />
  <row Id="1336" PostId="1673" Score="0" Text="Oh, I did not know about this. Thank you for both editing my questions and providing advice." CreationDate="2015-11-01T14:26:15.407" UserId="1938" />
  <row Id="1337" PostId="1669" Score="0" Text="@joojaa Than I'm still puzzled why the use of special notation Gn. In mathematics there is standard notion of Cn differentiable manifold. So is Gn and Cn the same? I thought that Gn manifold is piece-wise polynomial, so it is C-infty manifold except at the patch seams." CreationDate="2015-11-01T16:48:00.427" UserId="1613" />
  <row Id="1339" PostId="1672" Score="0" Text="Or have a look at screw theory." CreationDate="2015-11-01T22:06:49.970" UserId="1613" />
  <row Id="1340" PostId="1675" Score="4" Text="your question is too vague. &quot;design&quot;, &quot;implement&quot; = the rendering only ? now there are several aspects in rendering: intersecting the shape, surface materials, volume materials, light transport... the strange thing here is that the caustics on the floor seems very accurate, but the glass aspect seem very non-physical. Anyway, yes, ray-based algorithms are involved at many place here.  Anyway people would more consider design = shape and motion. (+ textures, when present)." CreationDate="2015-11-02T13:11:45.747" UserId="1810" />
  <row Id="1341" PostId="1671" Score="0" Text="Do you mean &quot;Texture Synthesis&quot; in which a large texture can be generated from a small exemplar?" CreationDate="2015-11-02T15:14:52.437" UserId="209" />
  <row Id="1342" PostId="1675" Score="0" Text="Do you mean the mathematical shape?" CreationDate="2015-11-02T18:36:39.023" UserId="38" />
  <row Id="1344" PostId="1529" Score="0" Text="I have actually designed and implemented (with help of my colleague) an algorithm to create 3D mesh of the city from the data that is collected (for a competitor company). However, what you're asking would require breaking NDA and I don't believe any sane person would be willing to do that, so your question can only be truly answered by developers who have worked on something like this in their spare time and thus are not bound by legal paperwork (regardless of whether they still work for that company or not)." CreationDate="2015-10-07T18:35:40.373" UserId="1820" />
  <row Id="1345" PostId="1562" Score="0" Text="Since you are using SceneKit, would `SCNShape` be useful to you? It can create a 3D mesh with normals and texture coordinates by extruding a Bézier path." CreationDate="2015-10-13T22:17:45.723" UserId="1852" />
  <row Id="1347" PostId="1669" Score="0" Text="@tom C continuity is the parametric continuity and G is the geonetric continuity and in this case the continuity over 2 separate geometries." CreationDate="2015-11-03T05:19:53.993" UserId="38" />
  <row Id="1348" PostId="420" Score="1" Text="@MichaelIV Nevertheless, Alan Wolfe has a point. You could greatly improve your answer by including relevant code in the answer itself to make it self-contained and future-proof against the link going dead some day." CreationDate="2015-11-03T08:45:30.887" UserId="16" />
  <row Id="1349" PostId="1589" Score="0" Text="This seems to have a couple of problems. a) It's not entirely clear whether you're the developer or just a player of the game (in the latter case, this is clearly off-topic, as this site is about computer graphics *programming* and *research*). b) If you are the developer this doesn't seem to be actually about graphics but more about the network code of your game. In that case, you might want to try [GameDev.SE](http://gamedev.stackexchange.com/) or [so] but be sure to read their respective help centres to make sure your question is of high-quality and on-topic." CreationDate="2015-11-03T08:50:00.717" UserId="16" />
  <row Id="1350" PostId="1624" Score="0" Text="(A belated) Welcome to Computer Graphics SE! I'm afraid questions about 3D printing are not on topic here, as this site is about computer graphics programming and research. Unfortunately, I can't think of any better Stack Exchange to redirect you to either. There *is* [a proposal for a 3D Printing SE](http://area51.stackexchange.com/proposals/82438/3d-printing-and-rapid-prototyping) in the commitment phase though, so you might want to keep an eye on that." CreationDate="2015-11-03T08:54:32.170" UserId="16" />
  <row Id="1351" PostId="1672" Score="0" Text="Welcome to Computer Graphics SE! I think there's a good and interesting question there, but currently it's not clear what exactly is being asked. Please consider editing some more detail into your question and also show what you've tried/found out yourself." CreationDate="2015-11-03T08:56:22.190" UserId="16" />
  <row Id="1352" PostId="1675" Score="0" Text="Welcome to Computer Graphics SE! As Fabrice's and joojaa's comments show it's not entirely clear what you're asking. Please include some more detail whether your question is about the rendering (refraction and caustics), or the geometry or something else. It might also help to include the source of the image." CreationDate="2015-11-03T08:59:23.493" UserId="16" />
  <row Id="1353" PostId="1621" Score="1" Text="The original vector is  $(2/(x+y),(5y+z)/(2x+2y),3)$ while your expansion assumes  $(2/(x+y),5(y+z)/(2x+2y),3)$" CreationDate="2015-11-03T13:47:17.367" UserId="137" />
  <row Id="1354" PostId="1486" Score="0" Text="@ratchet, MathJax, yeah!" CreationDate="2015-11-03T15:13:15.963" UserId="192" />
  <row Id="1355" PostId="1621" Score="0" Text="@ratchetfreak Thanks for your comment. I corrected my mistake." CreationDate="2015-11-03T15:36:09.700" UserId="127" />
  <row Id="1356" PostId="258" Score="0" Text="N.B. this fact is also used when deriving theoretical bounds for [anisotropic diffusion](https://en.wikipedia.org/wiki/Anisotropic_diffusion)." CreationDate="2015-11-03T17:12:34.340" UserId="523" />
  <row Id="1358" PostId="317" Score="0" Text="Direct3D is also what you would use if you're programming for any of the Xbox consoles." CreationDate="2015-11-03T23:37:08.393" UserId="1989" />
  <row Id="1359" PostId="295" Score="2" Text="ratchet freak's answer is a good one.  If you want to learn Direct3D you can read my free book [&quot;The Direct3D Graphics Pipeline&quot;](https://legalizeadulthood.wordpress.com/the-direct3d-graphics-pipeline/).  The API details are mostly Direct3D9, but the concepts are the same.  The thing is, the concepts for programming 3D graphics haven't really changed at all since the late 1960s.  The APIs for expressing those concepts have gotten much smarter." CreationDate="2015-11-03T23:38:58.073" UserId="1989" />
  <row Id="1360" PostId="1687" Score="0" Text="obj is simple enough to write a parser for, collada also has free libs to parse it." CreationDate="2015-11-04T22:16:51.503" UserId="137" />
  <row Id="1362" PostId="1691" Score="1" Text="Yes, it makes a difference. As an extreme example, suppose the incident light spectrum is a delta function at 600 nm and the reflectance of the surface is a delta function at 610 nm. Then the exitant light is zero, but if you convert both spectra to XYZ or RGB that's not what you will get." CreationDate="2015-11-05T11:01:38.407" UserId="106" />
  <row Id="1363" PostId="1691" Score="0" Text="@Rahul True. I changed the question to reflect that I'm less interested in these kind of contrived cases and more in the usual, real-world (somewhat) continuous spectra." CreationDate="2015-11-05T11:37:42.180" UserId="385" />
  <row Id="1364" PostId="1691" Score="0" Text="I think this previous question covers a lot of similar ground: [Are there common materials that aren't represented well by RGB?](http://computergraphics.stackexchange.com/q/203/106) Most of the answers apply equally well to any three-component colour space, like XYZ." CreationDate="2015-11-05T12:22:43.103" UserId="106" />
  <row Id="1365" PostId="1691" Score="1" Text="I understand that there are &quot;Effects for which the path of a ray is dependent on its wavelength&quot;, and I've explicitly excluded those. I also understand that there are &quot;Colours that the human eye can detect that cannot be displayed in RGB&quot;. Since I will end up in sRGB anyway, I can't change that. The question holds: do the intermediate calculations yield the same results _in common cases_ (no sodium lamps, no delta functions, just wide-range-of-frequencies stuff)?" CreationDate="2015-11-05T12:43:04.497" UserId="385" />
  <row Id="1366" PostId="1691" Score="1" Text="There will almost always be a difference in intermediate results when rendering using a three-component color space rather than the full spectrum. The pathological cases presented here just take that difference to an extreme. You say you don't care about sodium lamps, but you do care about wide-range-of-frequencies stuff - this makes this question very vague. Unless we know where your threshold is, we won't be able to answer this question." CreationDate="2015-11-05T13:08:26.880" UserId="79" />
  <row Id="1367" PostId="1691" Score="0" Text="@BenediktBitterli I'd be happy about some mathematical formulation of how to quantify the error, or a detailed description of the sources of error. Is early integration the only source, or does the inclusion of a reference illuminant pose a problem itself?" CreationDate="2015-11-05T13:18:27.120" UserId="385" />
  <row Id="1370" PostId="1693" Score="0" Text="It's entirely possible to run the pixel shader for overlapping triangles at the same time. All that matters is that the *blending* happens sequentially. Pixel shaders can kick off a lot of work to the blending unit in any order, which can then reorder all those blend operations depending on the triangle that it came from." CreationDate="2015-11-05T14:44:47.070" UserId="196" />
  <row Id="1371" PostId="1693" Score="0" Text="I've seen situations where flickering does occur due to multiple triangles lying in the same plane, but I don't remember if that happened while the camera/world/etc. transforms were static or if it required movement and recalculation. That's an edge case, though." CreationDate="2015-11-05T16:07:18.187" UserId="2000" />
  <row Id="1372" PostId="1693" Score="0" Text="@JAB It sounds like you're referring to [z-fighting](https://en.wikipedia.org/wiki/Z-fighting).  This usually occurs due to the fact that even if two primitives are analytically co-planar, quantization in the interpolator produces a pattern where some pixels from both primitives are visible.  The pattern will shift, causing flickering, only if the vertices (or camera) are moved, otherwise the interference pattern should be consistent between frames." CreationDate="2015-11-05T17:46:51.760" UserId="1992" />
  <row Id="1373" PostId="1693" Score="0" Text="@MooseBoys That's exactly it, yeah. It's been a few years since I did much with 3D graphics." CreationDate="2015-11-05T18:43:21.977" UserId="2000" />
  <row Id="1374" PostId="1682" Score="0" Text="Thanks for the advice, I chose the lazy solution for now. I use a function in the vertex shader that determines the wave attenuation from the distance from tha camera." CreationDate="2015-11-05T18:52:22.637" UserId="1987" />
  <row Id="1375" PostId="1693" Score="0" Text="Besides just running multiple vertices and pixels in parallel, it's also possible for multiple draw calls to be in flight at the same time, if enough GPU resources are available and there is no dependency between the draw calls." CreationDate="2015-11-05T21:15:30.600" UserId="48" />
  <row Id="1376" PostId="1694" Score="0" Text="There are many possibilities, so you should describe more:&#xA;&#xA;- What kind of material you aim at (basic Lambertian, or mirror specular  + glossy specular + multi-transparent layers + diffuse + subsurface scattering ?)&#xA;&#xA;- What paradigm of &quot;light particule&quot; you want to stick with (monochromatic photons, polychromatic photons, chuck of light ray) &#xA;&#xA;- What paradigm of &quot;light transport algorithm&quot; you want to stick with (from pure stochastic with each basic behavior in 0/1, to full valuated)." CreationDate="2015-11-06T15:45:31.220" UserId="1810" />
  <row Id="1377" PostId="1694" Score="0" Text="@FabriceNEYRET I updated the post. But what do you mean under monochromatic photons, polychromatic photons, full valuated light transport algorithm?" CreationDate="2015-11-06T16:29:05.437" UserId="386" />
  <row Id="1378" PostId="1694" Score="4" Text="a monochromatic photon (i.e. a &quot;real photon&quot;) die or not, but cannot fade. a polychromatic photon is a bag of synchronized photon, each can have his own fate.  &quot;full valued&quot; means that you transport the faded amount of energy (i.e. the optical geometry flux) rather than throwing coins to either absorb or reflect unfaded." CreationDate="2015-11-06T16:37:54.630" UserId="1810" />
  <row Id="1392" PostId="1701" Score="0" Text="Can you give an example with a 3d modeling software ?" CreationDate="2015-11-09T00:41:21.270" UserId="1636" />
  <row Id="1393" PostId="1700" Score="3" Text="Following from your comment on the answer, could you clarify whether you want to know about a theoretical approach, how to achieve this programmatically or how to achieve this in modelling software? This Stack Exchange is specifically about computer graphics programming and theory. If your question is specifically about how to achieve the effect in modelling software, I'm afraid your question is off topic. It makes a good question as a general CG problem though." CreationDate="2015-11-09T09:14:42.337" UserId="16" />
  <row Id="1397" PostId="1701" Score="4" Text="@Valerio this is not a forum for how to use modeling software." CreationDate="2015-11-09T14:03:21.607" UserId="38" />
  <row Id="1399" PostId="138" Score="0" Text="[Also see this related question on GameDev SE.](http://gamedev.stackexchange.com/q/111051/19876)" CreationDate="2015-11-10T09:05:28.537" UserId="16" />
  <row Id="1403" PostId="1709" Score="0" Text="Thanks, the docs recommended a fuzz factor. Combined with shaving border artifacts it worked perfectly. `convert original.png -shave 5x5 -fuzz 1% -trim +repage trimmed.png`" CreationDate="2015-11-12T19:13:30.373" UserId="2040" />
  <row Id="1407" PostId="1711" Score="3" Text="You could improve this answer by elaborating a bit, or even just linking to something that explains further." CreationDate="2015-11-12T20:41:33.173" UserId="48" />
  <row Id="1409" PostId="1711" Score="1" Text="One way of expanding this answer would be to address the drawbacks the author mentions, and explain how your approach helps with those." CreationDate="2015-11-12T20:46:54.930" UserId="231" />
  <row Id="1410" PostId="1714" Score="1" Text="Note that I might have answered a question that is slightly different from yours, but I did that on purpose as yours is clearly an homework question and I preferred to give you the tools to come up with an answer by yourself rather than giving you one directly :)" CreationDate="2015-11-13T09:35:22.210" UserId="100" />
  <row Id="1412" PostId="1716" Score="0" Text="Yes, algorithms are on topic, software programs aren't, and so I guess libraries are in-between? I was hoping for an answer along the lines of &quot;Oh, you need the &lt;maximal euclidian distance in XYZ colorspace&gt; algorithm. There's an implementation in xyz.js; go an study it there.&quot;" CreationDate="2015-11-13T12:50:10.797" UserId="2040" />
  <row Id="1414" PostId="1716" Score="0" Text="For your particular question, I suspect your main task will be choosing which [colour space](https://en.wikipedia.org/wiki/Color_space) best suits your purpose, which should simplify the remainder of the task." CreationDate="2015-11-13T14:18:27.197" UserId="231" />
  <row Id="1415" PostId="1720" Score="0" Text="Not super experienced here either but I suppose you could be using a different shader in one. Lambert's would look like the one on the right. But gourands would average them and make it appear smoother. http://prosjekt.ffi.no/unik-4660/lectures04/chapters/jpgfiles/3-shadings.jpg" CreationDate="2015-11-14T04:07:38.430" UserId="113" />
  <row Id="1416" PostId="1720" Score="1" Text="Thanks for your comment! Quite frankly, the only thing I did was to apply the Smoother modifier. There was no added shader before and I did not add any shader after." CreationDate="2015-11-14T04:09:48.693" UserId="2061" />
  <row Id="1417" PostId="1722" Score="0" Text="Great answer! That's exactly what I was looking for: a conceptual explanation that didn't leave crucial details out but was still clear and direct to the point. Many thanks" CreationDate="2015-11-16T00:47:13.523" UserId="2061" />
  <row Id="1418" PostId="1708" Score="0" Text="I'm closing this question as off-topic because it appears to be about using image processing software, not about computer graphics programming and research. [This might be on topic on Super User](http://superuser.com/questions/tagged/imagemagick)." CreationDate="2015-11-16T09:49:11.447" UserId="16" />
  <row Id="1419" PostId="1722" Score="2" Text="_&quot; The eye however is extremely sensitive to abrupt changes in color, and interprets that as a hard crease.&quot;_   Actually the human visual system is very good at detecting changes in the _derivative_ of the shading. The shading can be continuous but if there are discontinuities in the rate of change [as in this image](http://www.cs.ru.ac.za/research/Groups/vrsig/pastprojects/041machbands/image03.png) these can be surprisingly noticeable. Search for Mach band effect." CreationDate="2015-11-16T10:57:28.713" UserId="209" />
  <row Id="1420" PostId="1722" Score="1" Text="@SimonF thats why they see a crease, because they derive it edge detection and all that. But the human brain does not really eveluate the gradient flow 2 smooths are allmost equal to most humans (theres no second derviate sensing for example). So having  smooth sphere is smooth, even if the normals do not behave entirely spherically just as long as they are smooth. Thets why we get away with the trick. Very few surfaces actually behave this way." CreationDate="2015-11-16T11:59:40.677" UserId="38" />
  <row Id="1421" PostId="1722" Score="0" Text="_&quot;(theres no second derviate sensing for example)&quot;_ I just checked in  Glassner's &quot;Principles of Digital Image Synthesis&quot; (Volume 1 page 29)  ... and now I am more confused than ever." CreationDate="2015-11-16T12:41:54.293" UserId="209" />
  <row Id="1422" PostId="1722" Score="0" Text="@SimonF You may be confused about the reflections that are naturally one derivative lower than what the surface is. Thus a human can under certain conditions sense the second derivative. But the point is rather that humans can see creases, but they don't make meaningful difference between all different changes, the fact that a reflection is slightly off or in wrong direction isnt automatically apparent to a human. Without deeper analysis. Just as long as there's no abrupt change is for the most part good enough in many circumstances. We should continue this in the chat room though" CreationDate="2015-11-16T12:49:24.150" UserId="38" />
  <row Id="1424" PostId="1728" Score="1" Text="The algorithm in your link is very simple. It looks less sophisticated than some of the papers I found from the nineties. It looks like a good starting point, but I'm hoping for the highest-performance solution for a production system, not just a &quot;my first raytracer&quot;." CreationDate="2015-11-17T08:35:18.037" UserId="2041" />
  <row Id="1425" PostId="1725" Score="0" Text="Screen-space reflections: create a height-field using the depth and frame buffer, ray trace it to get crude reflections. I don't know about the details, but I'd imagine Crysis, Killzone, lately Frostbite etc. will have used some sophisticated technique to get it fast. Have you looked into this?" CreationDate="2015-11-17T10:15:27.807" UserId="385" />
  <row Id="1426" PostId="13" Score="2" Text="&quot;Whereas, in Monte Carlo ray tracing or simply path tracing, you sample only one ray in a direction preferred by the BRDF.&quot; Per se, you don't know how the ray is selected. Naive approaches use random rays. Taking the BRDF into account is importance sampling and not inherent to Monte Carlo ray tracing or path tracing." CreationDate="2015-11-17T10:20:32.100" UserId="385" />
  <row Id="1427" PostId="1729" Score="0" Text="dynamic indexing can cause issues in some drivers" CreationDate="2015-11-17T10:21:21.460" UserId="137" />
  <row Id="1428" PostId="1725" Score="1" Text="@DavidKuri Thanks, that's a good pointer for how to get the core ray-marching fast. There should be a lot of optimizations possible for a more static height-field that don't work so well on screen-space tracing, such as pre-computing mipmaps or a min-max quadtree, so I'm still hoping for an answer that covers that." CreationDate="2015-11-17T10:38:27.833" UserId="2041" />
  <row Id="1429" PostId="1728" Score="0" Text="This stuff is used in demoscene code and screenspace reflections in the most advanced modern games. The fastest code is sometimes the simplest. I wouldn't dismiss it due to its simplicity. It'll be interesting to see if you get any other responses though." CreationDate="2015-11-17T14:18:41.427" UserId="56" />
  <row Id="1430" PostId="1725" Score="0" Text="Hey Dan BTW are you looking for CPU or GPU solutions? And real time or non real time rendering?" CreationDate="2015-11-17T14:27:43.217" UserId="56" />
  <row Id="1431" PostId="1725" Score="0" Text="@AlanWolfe My use is GPU and non-real-time (i.e. max throughput rather than best image quality you can manage in 16 ms), but I'll still upvote interesting answers that are fast on the CPU or primarily for interactive renderers." CreationDate="2015-11-17T14:46:44.920" UserId="2041" />
  <row Id="1432" PostId="1725" Score="0" Text="You could try to create a signed distance field from the height map. Thats basically a 3d texture that stores the distance to the next surface. This allows to &quot;travel the ray faster&quot;. Unreal Engine 4 uses this for mid-range ambient occlusion, soft shadows and terrain shadows in general" CreationDate="2015-11-17T15:34:12.997" UserId="1888" />
  <row Id="1433" PostId="1730" Score="0" Text="is it possible for bode the modes to be set to 1? and if it shouldn't be are you guarding against it?" CreationDate="2015-11-17T16:25:10.333" UserId="137" />
  <row Id="1434" PostId="1730" Score="0" Text="for now I am assuming that the user is either in editMode or deleteMode. he doesn't press 'c' and 'd' together. if 'c' is pressed, it is made sure he exits from editMode before pressing ' d'." CreationDate="2015-11-17T16:27:28.313" UserId="1588" />
  <row Id="1435" PostId="1730" Score="0" Text="is that pseudo code or the actual code in your app (if so what language)?" CreationDate="2015-11-17T16:30:30.950" UserId="137" />
  <row Id="1436" PostId="1730" Score="0" Text="its c++. and this is the pseudo code giving the general idea of what I want to achieve. The actual code has some other flags for calling drawBeizer fn etc." CreationDate="2015-11-17T16:51:53.837" UserId="1588" />
  <row Id="1437" PostId="1721" Score="0" Text="Both answers are really good, it was hard for me to pick. Since I had asked about the simplest way, I think Nathan takes the cake." CreationDate="2015-11-17T17:28:29.103" UserId="14" />
  <row Id="1438" PostId="1730" Score="1" Text="It may help to try to remove everything from your code that you can, without removing the problem. This may highlight the cause for you, but even if it doesn't you can then edit the minimal code into your question, which will increase the chance of someone being able to spot the underlying cause." CreationDate="2015-11-17T17:44:41.283" UserId="231" />
  <row Id="1439" PostId="1730" Score="0" Text="Relevant discussion on Meta about [whether we should require a minimal working example](http://meta.computergraphics.stackexchange.com/questions/126/should-we-require-minimal-working-examples-mwes). This needs more attention to make clear the community consensus." CreationDate="2015-11-17T17:47:03.237" UserId="231" />
  <row Id="1440" PostId="1728" Score="2" Text="What's missing in your response is that IQ uses a standard heightfield mesh as an initial guess to kickstart raymarching the actual terrain. He first renders a low-poly version of the terrain using standard rasterization, and then runs a pixel shader over the image that starts raymarching at the rasterized depth minus some conservative threshold. This is the only way to actually make this realtime." CreationDate="2015-11-17T19:23:46.443" UserId="79" />
  <row Id="1441" PostId="1728" Score="0" Text="I believe that only part of what you are saying is true.  he does use heuristics based on terrain height (along with distance from camera) to come up with how far the ray can march, but as far as i have heard, he doesn't use rasterization.  Here is an example of his work, which does not use rasterization, but that isn't to say that there aren't implementations that DO use rasterization:  https://www.shadertoy.com/view/MdX3Rr" CreationDate="2015-11-17T19:27:12.690" UserId="56" />
  <row Id="1442" PostId="41" Score="0" Text="don't forget to work in linear color space for correct results." CreationDate="2015-11-18T00:57:19.487" UserId="1614" />
  <row Id="1443" PostId="1731" Score="1" Text="I was considering a similar solution to Nero's answer, but you've tagged this [tag:bezier-curve]. Are you limited to using Bezier curves with thickness, or is drawing circular arcs actually an option?" CreationDate="2015-11-18T09:05:37.100" UserId="16" />
  <row Id="1444" PostId="1732" Score="2" Text="+1 for the not at all intended pun" CreationDate="2015-11-18T09:44:50.190" UserId="2041" />
  <row Id="1445" PostId="1581" Score="0" Text="Besides vulnerabilities in image decoders (e.g. GDI+) it is possible to embed code e.g. AFTER the EOF marker on a JPEG. In this case the custom code is in same image file but technically not part of the image. See example in this document re: &quot;hammertoss&quot; malware: https://www2.fireeye.com/rs/848-DID-242/images/rpt-apt29-hammertoss.pdf" CreationDate="2015-11-18T17:12:26.290" UserId="2081" />
  <row Id="1446" PostId="1581" Score="0" Text="Related: http://security.stackexchange.com/questions/55061/can-malware-be-attached-to-an-image" CreationDate="2015-11-18T17:12:55.610" UserId="2081" />
  <row Id="1447" PostId="1732" Score="0" Text="Yes, that's what I'm after, but there's still an overlap. If it helps, I'm using Adobe After Effects’ shape layers and expressions to create this. Rectangle 1 is driving Rectangle 2's round corner value.&#xA;&#xA;e.g.&#xA;&#xA;Rectangle 1&#xA;Round Corners = 30&#xA;&#xA;Rectangle 2&#xA;Stroke width = 40&#xA;Round Corners = 60 (30*2)&#xA;&#xA;produces this:&#xA;![rectangles2](https://dl.dropboxusercontent.com/u/1414976/rectangles2.png)&#xA;&#xA;I've set the opacity of Rectangle 1 to 50% so you can see the overlap. The darker purple is the edge of Rectangle 2's stroke.&#xA;&#xA;I can supply by After Effects file if it will help." CreationDate="2015-11-18T20:37:37.240" UserId="2076" />
  <row Id="1448" PostId="1732" Score="0" Text="@GregGunn If your inner/blue box has a border radius of $30px$ and the red stroke's width is $40px$, then the radius in the middle of the red stroke needs to be $30px + (40px/2) = 50px$." CreationDate="2015-11-18T20:46:16.047" UserId="127" />
  <row Id="1449" PostId="1732" Score="0" Text="@Nero Nailed it. That equation works perfectly—thank you." CreationDate="2015-11-18T20:58:43.250" UserId="2076" />
  <row Id="1454" PostId="1737" Score="1" Text="Thanks Nathan that's exactly what I was after, where did you get this diagram may I ask, is it in any public documentation from oculus ?" CreationDate="2015-11-19T05:34:35.090" UserId="288" />
  <row Id="1455" PostId="1737" Score="1" Text="@GarryWallis Cass [tweeted it](https://twitter.com/casseveritt/status/608677561674149889) a few months ago." CreationDate="2015-11-19T05:37:24.127" UserId="48" />
  <row Id="1457" PostId="1734" Score="4" Text="Pro tip: `x * x` is much faster than `pow(x, 2.0f)`." CreationDate="2015-11-19T09:49:11.077" UserId="2041" />
  <row Id="1459" PostId="1728" Score="0" Text="I'm a little confused that the question is about ray tracing, and this answer is about ray marching. There is a fundamental difference between the two and what they can achieve." CreationDate="2015-11-19T12:35:05.170" UserId="182" />
  <row Id="1460" PostId="1674" Score="0" Text="What does, &quot;The change has to not only be locally satisfied but also satisfied on the retina,&quot; mean?" CreationDate="2015-11-19T13:05:03.053" UserId="2041" />
  <row Id="1461" PostId="1674" Score="1" Text="The eye is not recording a continious signal.  Its discrete, so even if your surface might technically meet the condition presented on a mathematical level. It might not be enough if the dicrete sample spacing does not see the change. So the slope still has to be big enough for human eye to notice." CreationDate="2015-11-19T13:38:05.733" UserId="38" />
  <row Id="1462" PostId="1674" Score="0" Text="It sounds like you're saying the derivative (of the normal) doesn't just have to be continuous, but its derivative has to be below some limit. If that's what you mean, I think that last paragraph of your answer could be clearer." CreationDate="2015-11-19T13:45:04.373" UserId="2041" />
  <row Id="1464" PostId="1674" Score="0" Text="@DanHulme its not a limit the derivate, its not a question of slope, only, but the interwall of the slope. So it is about a discrete sampling. So a very sharp angle but small difference in slope might seem continious. Likewise continious changes under a short interwall might seem sharp. Its not about mathematics its about sampling. Its just hard to qantify as its a biological system." CreationDate="2015-11-19T14:41:26.223" UserId="38" />
  <row Id="1465" PostId="1728" Score="0" Text="If you notice, the question mentions ray marching (through a grid) as an example algorithm that he has read about in the past." CreationDate="2015-11-19T14:46:45.220" UserId="56" />
  <row Id="1466" PostId="1738" Score="0" Text="Thank you very much for your help, I really appreciate you taking the time to write it.  I have implemented your suggestions.  Unfortunately, my shadows are still missing.  My most recent output is in the original qustion.  I'm assuming this means the problem is somewhere upstream in my code.  Of course, that's about 200-300 lines of setting my stage.  If you've got the stomach for it, I've put it on [pastebin](http://pastebin.com/03PHrUWJ).  If you don't, I totally understand.  I really appreciate your assistance; I've been so close to this for so long I think I've become blind to my mistakes" CreationDate="2015-11-19T23:55:15.083" UserId="2084" />
  <row Id="1467" PostId="1743" Score="4" Text="did you check the numbers with a simple triangle? 1,0,0; 0,1,0; 0,0,1 and ray from origin to 1,1,1" CreationDate="2015-11-20T08:54:33.727" UserId="137" />
  <row Id="1468" PostId="1741" Score="0" Text="Could you maybe add an example mesh and how it's being transformed? If you're actually moving the *vertices*, not the individual polygons, I'm not sure why they would pass through each other from a simple bending operation. Shouldn't the polygons on the concave side just shrink a bit?" CreationDate="2015-11-20T09:12:56.430" UserId="16" />
  <row Id="1469" PostId="1738" Score="1" Text="If you're still having trouble, just apply the usual debugging techniques. Keep simplifying your code to cut the problem space in half. Try removing all the shading computation and just set the colour to `intersections / 999999`, so your unshadowed intersections are just white. Draw an object at the light position to make sure it's correct: in both your renders, it looks like it might be in the bottom-right of the image, between the two objects. Put a breakpoint conditional on the shading co-ordinates and step through the shading of one particular point that you know ought to be in shadow." CreationDate="2015-11-20T09:19:35.913" UserId="2041" />
  <row Id="1470" PostId="1741" Score="0" Text="@MartinBüttner By the sound of it, he/she will get a similar problem to that of doing offset curves when the offset exceeds the radius of curvature. e.g. [look at the inner set of  green curves](https://en.wikipedia.org/wiki/Parallel_curve#/media/File:Evolute_and_parallel.gif) which have been displaced too far from the red." CreationDate="2015-11-20T10:44:10.333" UserId="209" />
  <row Id="1471" PostId="1741" Score="0" Text="Seems to me the subject lines ask about recalculating normals while the body  asks about self intersection." CreationDate="2015-11-20T12:50:07.063" UserId="38" />
  <row Id="1472" PostId="1744" Score="1" Text="This is a sensoring problem. The damage is done in the scanner. Photographees know a lot about these things. Its not really a computer graphics problem as a image capture problem. How to use software is out of scope." CreationDate="2015-11-20T12:52:50.583" UserId="38" />
  <row Id="1473" PostId="1744" Score="0" Text="@joojaa Maybe you have idea where to look out for help? Tried to find desktop publishing and scanning and graphic design forums, but no luck so far." CreationDate="2015-11-20T14:52:54.637" UserId="2100" />
  <row Id="1474" PostId="1744" Score="0" Text="Well there is [GD.Se](http://graphicdesign.stackexchange.com/)" CreationDate="2015-11-20T15:25:07.923" UserId="38" />
  <row Id="1475" PostId="1741" Score="1" Text="I am sorry, my subject and body are not coherent. This is because I am not sure of the exact terminology to use.  I think Simon F has interpreted my question as I intended though; I need to figure out how to handle the situation where the offset exceeds the radius of curvature.  I will upload a sketch momentarily." CreationDate="2015-11-20T16:16:57.443" UserId="2091" />
  <row Id="1478" PostId="1741" Score="0" Text="Ah yes, theres really nothing you can do about this kind of things except not bend too much." CreationDate="2015-11-20T20:53:45.187" UserId="38" />
  <row Id="1479" PostId="1745" Score="2" Text="I think you should add more details about how you actually created your color palette/table so users might be able to help you.  You might also consider one of the computer science-based Stack Exchanges." CreationDate="2015-11-19T20:29:48.510" UserDisplayName="honeste_vivere" />
  <row Id="1482" PostId="1730" Score="0" Text="This question is not exactly about OpenGL and Graphics but about GLUT and Input. Might be better suited for http://gamedev.stackexchange.com/" CreationDate="2015-11-21T23:31:38.017" UserId="528" />
  <row Id="1483" PostId="351" Score="0" Text="I don't think I get this. Isn't it basically just a minor difference in this case? E.g. what you said implies that the only difference would be that with albedo, the diffuse reflection is `(albedo * (1 - specular))` and specular is `albedo * specular`, instead of flat diffuse and specular numbers? I really don't get it :(" CreationDate="2015-11-22T04:11:00.777" UserId="2111" />
  <row Id="1484" PostId="1747" Score="0" Text="1080p is the new pixel art." CreationDate="2015-11-22T06:56:28.460" UserId="504" />
  <row Id="1485" PostId="351" Score="0" Text="@Llamageddon there are a number of differences covered in the answer but as a simple example: the albedo of a surface could be 0.8, but the RGB value of it's diffuse component could be (0.6, 0.5, 0.9). The albedo is generally just a single scalar value, whereas the diffuse component may have multiple values to give colour rather than just brightness." CreationDate="2015-11-22T12:44:58.633" UserId="231" />
  <row Id="1487" PostId="1729" Score="0" Text="Well, I guess that's the case, can't find a better explanation." CreationDate="2015-11-23T15:18:30.047" UserId="2064" />
  <row Id="1489" PostId="1751" Score="0" Text="What about when vertex shaders are used to deform meshes? (which I assume is possible... I'm a newbie to graphics programming)" CreationDate="2015-11-24T03:21:18.113" UserId="2111" />
  <row Id="1490" PostId="1751" Score="0" Text="Also, do you have any papers on GPU-backed occlusion culling? Is it a built-in feature of modern GPUs, or..?" CreationDate="2015-11-24T04:54:26.810" UserId="2111" />
  <row Id="1491" PostId="1751" Score="0" Text="@Llamageddon On modern GPU architectures, the vertex shader is always run, and is perfectly capable of deforming meshes—that's just transforming vertices non-rigidly. A more expensive vertex shader that does more work is, of course, more work that will be skipped by culling. I don't have any papers on GPU occlusion culling, academia seems to not be very enthralled with it. It is not a built-in feature of GPUs, just a creative use of compute." CreationDate="2015-11-24T08:02:07.733" UserId="196" />
  <row Id="1492" PostId="1751" Score="0" Text="Oh, I meant what about culling meshes that will be deformed? Do you cull them after running the vertex shaders? That sounds convoluted." CreationDate="2015-11-24T10:01:09.083" UserId="2111" />
  <row Id="1493" PostId="1751" Score="0" Text="@Llamageddon Generally you just cull them against a conservative volume. Or several smaller volumes that you deform with the mesh (for example, when skinning you can attach culling volumes to joints)." CreationDate="2015-11-24T16:43:05.913" UserId="196" />
  <row Id="1494" PostId="1751" Score="0" Text="Ah, I see. Would you mind getting in touch via some sort of IM/mail/direct messaging? I would really appreciate a mentor/tutor in graphics programmings stuff, since it's a topic that fascinates me greatly, but... various reasons, curb my ability to pursue it on my own." CreationDate="2015-11-25T04:58:56.910" UserId="2111" />
  <row Id="1496" PostId="1751" Score="0" Text="@Llamageddon The chat room for this site http://chat.stackexchange.com/rooms/26589/the-cornell-box is probably a good venue." CreationDate="2015-11-25T06:57:46.010" UserId="196" />
  <row Id="1497" PostId="1751" Score="0" Text="I know but I just am not good at chatrooms, I much prefer having someone to talk to and get to know x.x It's fine if you don't want to do anything such." CreationDate="2015-11-25T07:26:33.150" UserId="2111" />
  <row Id="1498" PostId="1757" Score="2" Text="I found this while searching around: http://neil-strickland.staff.shef.ac.uk/courses/algtop/pictures/sphere/ I think the animations help picture the homeomorphism between a sphere without poles and the plane/cylinder, so I thought you might want to include it" CreationDate="2015-11-26T09:02:46.857" UserId="16" />
  <row Id="1499" PostId="1757" Score="0" Text="Yeah i was planning on drawing a picture once in front of a computer." CreationDate="2015-11-26T09:10:07.183" UserId="38" />
  <row Id="1500" PostId="1754" Score="0" Text="If you solve it for still images, it ought to be the same solution for video frames right?  Also just to make sure, you are just trying to avoid the distortion at the poles when naively putting a texture on a sphere?" CreationDate="2015-11-26T18:12:40.187" UserId="56" />
  <row Id="1501" PostId="1759" Score="8" Text="+1 for showing that homework questions can be high-quality questions. :)" CreationDate="2015-11-27T12:55:58.820" UserId="16" />
  <row Id="1502" PostId="1754" Score="0" Text="I believe you are correct with the still images. However, I wasnt able to find any good material about that either.&#xA;&#xA;And yes, I'm trying mainly to solve the distortion at poles problem, since the fact that left side has to be identical to right side of the video for a good transition is a trivial problem with easy solution." CreationDate="2015-11-27T14:32:42.717" UserId="2138" />
  <row Id="1503" PostId="1754" Score="0" Text="Does this info help any? http://blender.stackexchange.com/questions/10741/what-is-the-best-way-to-unwrap-a-sphere" CreationDate="2015-11-27T15:54:58.697" UserId="56" />
  <row Id="1506" PostId="1761" Score="0" Text="Lines or line segments?" CreationDate="2015-11-29T18:33:25.253" UserId="197" />
  <row Id="1507" PostId="1762" Score="0" Text="Can we assume that the corners are 90 degree angles" CreationDate="2015-11-29T18:42:42.237" UserId="38" />
  <row Id="1508" PostId="1763" Score="0" Text="How would one calculate the aspect ratio if we weren't so close to the center?" CreationDate="2015-11-29T21:15:03.770" UserId="2162" />
  <row Id="1509" PostId="1763" Score="0" Text="@succubus there is a lengthy explanation [here](http://www.handprint.com/HP/WCL/perspect3.html) but you can do this with matrix calculation. Just didnt have time to outline the math." CreationDate="2015-11-29T22:20:47.103" UserId="38" />
  <row Id="1510" PostId="1761" Score="0" Text="It is only important, that the two new points are connected, therefore segments of curves are also OK. Question edited." CreationDate="2015-11-29T22:33:43.360" UserId="2161" />
  <row Id="1511" PostId="1756" Score="0" Text="&quot;..of a plane, cylinder or torus&quot; Klein bottle and real-projective plane feels left out :( Have a look at https://en.wikipedia.org/wiki/Fundamental_polygon" CreationDate="2015-11-29T22:58:50.367" UserId="1613" />
  <row Id="1512" PostId="1756" Score="0" Text="Mobius band is now crying..." CreationDate="2015-11-29T23:06:49.073" UserId="1613" />
  <row Id="1513" PostId="1757" Score="0" Text="Well your answer is not very consistent. If the sphere is not a sphere but a cylinder, because you are missing those two points, than the cylinder is not a cylinder but it is a plane, because you are missing the whole edge. And there are more than 3 topological families, by gluing different edges you can get sphere, cylinder, torus, mobius band, klein bottle, real-projective plane. Have a look at wiki page about Fundamental polygon." CreationDate="2015-11-29T23:13:41.173" UserId="1613" />
  <row Id="1514" PostId="1763" Score="0" Text="Thanks for your help, unfortunately I can't accept your answer because I messed up with my stackexchange account." CreationDate="2015-11-29T23:20:19.373" UserId="2162" />
  <row Id="1515" PostId="1761" Score="2" Text="You might look into path-finding algorithms for this. Use existing segments as obstacles and find a path between the two new endpoints. Maybe apply some smoothing to the resulting path to make it a nicer-looking curve." CreationDate="2015-11-29T23:36:38.467" UserId="48" />
  <row Id="1516" PostId="1764" Score="0" Text="Other tools include yEd (Free to use but no free licese), gephi... this is a NP Hard problem." CreationDate="2015-11-30T03:45:31.187" UserId="38" />
  <row Id="1517" PostId="1763" Score="0" Text="@succcubbus please refer to [official help page](http://computergraphics.stackexchange.com/help/merging-accounts) about merging your account to regain the ownership of the question." CreationDate="2015-11-30T10:38:01.683" UserId="2170" />
  <row Id="1518" PostId="1741" Score="0" Text="Well I'm pretty sure there is something that can be done, I just don't want to re-invent the wheel.  I'm pretty sure that professional applications like Maya have this solved.  Right now I'm thinking of something along the lines of checking normal directions before and after the transform to identify problem polys.  Perhaps the blender source code has something." CreationDate="2015-11-30T20:29:36.923" UserId="2091" />
  <row Id="1519" PostId="1767" Score="0" Text="Isn't this slightly under-constrained?   If you only have 3  coplanar(?),  non-colinear points, won't that allow you to have a circle? (Or do you also have the centre?)" CreationDate="2015-12-01T13:20:31.557" UserId="209" />
  <row Id="1520" PostId="1767" Score="0" Text="Are the three points any particular points on the ellipse? e.g. if one is guaranteed to be one end of the major axis, and another is one end of the minor axis, then the problem is trivial." CreationDate="2015-12-01T15:16:01.610" UserId="2041" />
  <row Id="1521" PostId="1767" Score="0" Text="Yeah, someone elsewhere pointed out it's underconstrained, and the Keplerian elements are probably more usable. That said, I can arrange for the points to be the end of the major and minor axes if that trivializes the problem." CreationDate="2015-12-01T16:43:47.290" UserId="1634" />
  <row Id="1522" PostId="1767" Score="3" Text="If you have the center and major/minor axis vectors, then the corners of the quad will just be center ± major ± minor." CreationDate="2015-12-02T01:10:19.490" UserId="48" />
  <row Id="1523" PostId="1766" Score="0" Text="[Please explain why you downvote](http://meta.stackexchange.com/questions/135/encouraging-people-to-explain-downvotes) when you do." CreationDate="2015-12-02T09:58:10.617" UserId="2173" />
  <row Id="1524" PostId="1770" Score="0" Text="I'm not, I'm referring to virtual texturing, most well known as idTech 5's [MegaTexture](https://en.wikipedia.org/wiki/MegaTexture) technology. Also see [this](http://holger.dammertz.org/stuff/notes_VirtualTexturing.html) and [this](http://silverspaceship.com/src/svt/). I've seen it mentioned in overview of many modern engines' rendering pipelines, and in a few papers that use a similar approach for shadowmaps. It does have a lot in common with texture atlases, yes, it uses them, in a way, but I'm not confusing it with texture atlases." CreationDate="2015-12-02T18:18:08.880" UserId="2111" />
  <row Id="1525" PostId="1770" Score="0" Text="Ahh. Thanks for the links. Can you add them to the question. I will update my answer accordingly" CreationDate="2015-12-02T18:49:41.500" UserId="310" />
  <row Id="1526" PostId="1770" Score="3" Text="IMO, the main drawback of simple texture atlases (not virtual textures) is you lose wrap modes like repeat and clamp, and bleeding occurs due to filtering/mipmapping - not floating-point precision. I'd be surprised to see float precision becoming a problem for ordinary (non-virtual) textures; even a 16K texture (the max allowed by current APIs) isn't big enough to really strain float precision." CreationDate="2015-12-02T20:58:17.627" UserId="48" />
  <row Id="1527" PostId="1765" Score="1" Text="&quot;What is the best solution&quot; is mostly a matter of opinion. Does your current solution work well enough, or are there problems with it that you're looking to solve? If so, what are those problems specifically?" CreationDate="2015-12-02T21:50:53.767" UserId="48" />
  <row Id="1528" PostId="1765" Score="0" Text="I wanted to make sure my method is fine. But I have a problem to project my 3D direction vector on the screen, so I asked [this question](http://computergraphics.stackexchange.com/questions/1766/how-to-use-getviewprojmatrix-transformvectorlinedirection-in-ue4)." CreationDate="2015-12-02T22:28:05.527" UserId="2173" />
  <row Id="1529" PostId="1766" Score="0" Text="I dont know why it was downvoted. but this seems like a close dupllicate to your other question." CreationDate="2015-12-02T23:48:17.757" UserId="38" />
  <row Id="1530" PostId="5" Score="0" Text="Hey OP, if possible, could you post stats of the results you got out of the accepted answer, if you did?" CreationDate="2015-12-03T09:00:19.020" UserId="2111" />
  <row Id="1531" PostId="5" Score="0" Text="@Llamageddon To be honest, I had this actual problem maybe two years ago or so and just dug it up from a past CG projects to come up with decent questions during the private beta. I haven't yet revisited said project, so I didn't get around to trying out the answer." CreationDate="2015-12-03T09:02:28.797" UserId="16" />
  <row Id="1532" PostId="1770" Score="0" Text="@RichieSams Btw, I think your answer is a good one, even if to a different question. You should make a Q&amp;A post." CreationDate="2015-12-03T09:29:11.247" UserId="2111" />
  <row Id="1533" PostId="1773" Score="0" Text="Hey, thank you for the excellent answer. I know this is typically frowned upon, but I have various issues, so I mostly just skim through things - to get an intuitive overview of topics for the future(I'm afraid properly learning and implementing things is out of my reach for the moment) - anyway, if possible, could you post a pseudocode example outlining the process itself, ideally, but not necessarily, illustrated?" CreationDate="2015-12-03T09:33:14.113" UserId="2111" />
  <row Id="1534" PostId="1774" Score="1" Text="about saving processing power: http://computergraphics.stackexchange.com/q/259/137" CreationDate="2015-12-03T10:22:03.977" UserId="137" />
  <row Id="1535" PostId="1772" Score="0" Text="Ok, thanks for this answer! I used `TransformVector` after reading the first paragraph of [this page about homogenous coordinates](http://www.opengl-tutorial.org/beginners-tutorials/tutorial-3-matrices/). I understood I should not worry about W when projecting a direction, but maybe I'm wrong." CreationDate="2015-12-03T11:30:33.810" UserId="2173" />
  <row Id="1536" PostId="1772" Score="0" Text="I thought about projecting both points on my screen, but I changed my mind because I thought it was not necessary." CreationDate="2015-12-03T11:34:06.963" UserId="2173" />
  <row Id="1537" PostId="1774" Score="0" Text="@ratchetfreak Interesting post. Stenciling isn't conditionals, though." CreationDate="2015-12-03T11:57:17.630" UserId="2111" />
  <row Id="1538" PostId="1774" Score="1" Text="But the GPU still groups (at least) 2x2 pixels together even if 3 out of 4 will fail the stencil test." CreationDate="2015-12-03T12:02:05.197" UserId="137" />
  <row Id="1539" PostId="1774" Score="0" Text="Are you certain about that? I'd imagine that that's what happens for conditionals, while with stenciles, the GPU is a bit smarter about it." CreationDate="2015-12-03T12:36:46.807" UserId="2111" />
  <row Id="1540" PostId="1774" Score="0" Text="Given that it will calculate pixels just off the triangle I doubt that it will cull the singular stencil-failing pixels." CreationDate="2015-12-03T14:40:19.997" UserId="137" />
  <row Id="1541" PostId="1774" Score="2" Text="@Llamageddon Pixel shaders are always packed together in 2x2 quads, because a pixel shader that doesn't sample at least one texture is a rare thing indeed. But if you are doing depth-only rendering, that matters less. Also, for what it's worth, shadow rendering is traditionally bound by vertices, so in traditional high-vertex-count situations this probably won't help. Also don't forget that you'd be writing to every pixel's stencil just to skip writing to that same pixel's depth (unless your pixel shader is complex)." CreationDate="2015-12-03T15:37:16.540" UserId="196" />
  <row Id="1542" PostId="1774" Score="0" Text="@JohnCalsbeek I see. What about other hypothetical scenarios, such as ones involving more complex shader code, or a more regular stencil, that discards, for example, the left half of the screen? What situations can stencils be used in to accelerate things?" CreationDate="2015-12-03T18:11:52.060" UserId="2111" />
  <row Id="1543" PostId="1772" Score="0" Text="@arthur.sw That article is OK as far as affine transforms go, but when projections get involved, things are more complicated." CreationDate="2015-12-03T18:40:23.763" UserId="48" />
  <row Id="1544" PostId="1773" Score="1" Text="@Llamageddon, it just so happens that I still had a diagram at hand ;) I'm afraid pseudo-code is going to bit a bit hard to provide, since there's quite a bit of real code to it. But I hope the expanded answer helps giving a general idea of the technique." CreationDate="2015-12-03T18:47:15.340" UserId="54" />
  <row Id="1545" PostId="1773" Score="0" Text="Amazing answer, though I still find some details unclear: If the pre-pass is low-res, isn't it possible to miss some textures altogether? What happens then? How do the shaders for the pre-pass and final render look? Is the pre-pass used only for fetching the textures, or...?" CreationDate="2015-12-03T22:03:09.053" UserId="2111" />
  <row Id="1546" PostId="1773" Score="3" Text="It's worth noting that most modern hardware now exposes programmable page tables, eliminating the need for a redirection texture.  This is exposed through e.g. [tag:directx12] [reserved resources](https://msdn.microsoft.com/en-us/library/windows/desktop/dn899181(v=vs.85).aspx), which builds on [tag:directx11] [tiled resources](https://msdn.microsoft.com/en-us/library/windows/desktop/dn786477(v=vs.85).aspx), or [tag:opengl] [sparse textures](https://www.opengl.org/registry/specs/ARB/sparse_texture.txt)." CreationDate="2015-12-04T02:32:18.077" UserId="1992" />
  <row Id="1547" PostId="1773" Score="1" Text="@Llamageddon, the feedback pre-pass can be done at a lower res to save as much computing and memory as possible, since pixels for a page will generally repeat (you can notice the big colored squares in my demo). You're correct that it might eventually miss a visible page like that, but that's not usually going to have a big visual impact because the system should always keep at least the lowest mipmap of the whole VT available in cache. That second paper I linked has all the shader examples in the appendix, you can also refer to the repo for my own project, they are similar." CreationDate="2015-12-04T03:15:23.287" UserId="54" />
  <row Id="1548" PostId="1773" Score="0" Text="The feedback pre-pass is only useful for determining the visible set of pages needed for a view/frame, but you could also combine something like depth pre-pass to it." CreationDate="2015-12-04T03:16:55.707" UserId="54" />
  <row Id="1549" PostId="1770" Score="0" Text="Hmm, this explains it quite well, though I don't really understand how it works with mip levels. I wish I could write down my specific problem with understanding it down, but it kinda eludes me..." CreationDate="2015-12-04T10:29:13.167" UserId="2111" />
  <row Id="1550" PostId="1775" Score="2" Text="cos and sin in the std lib take the angle in radians" CreationDate="2015-12-04T12:40:35.183" UserId="137" />
  <row Id="1551" PostId="1774" Score="0" Text="@Llamageddon The canonical example for stencil these days is discarding all visible sky when doing a lighting full-screen pass. Lighting is expensive, and the sky often large." CreationDate="2015-12-04T15:48:26.510" UserId="196" />
  <row Id="1552" PostId="1773" Score="0" Text="BTW, why do you say that &quot;Virtual Texturing also doesn't handle transparency in an easy way&quot;? Transparency has nothing to do with how textures are stored in memory..." CreationDate="2015-12-04T19:52:20.630" UserId="48" />
  <row Id="1553" PostId="1773" Score="0" Text="@NathanReed, it's because of the page id pre-pass. It's impossible to handle transparency there, because blending 2 or more colors would produce a different page number. What I mean is, suppose object A gets assigned page X which when encoded into a color makes a red tone, now object B gets assigned page Y, which results on green when encoded in the feedback pass. If A and B were to get blended, the color output in the pre-pass for those pixels would be a shade of yellow, translating to the wrong page id/number." CreationDate="2015-12-05T00:00:57.850" UserId="54" />
  <row Id="1554" PostId="1773" Score="1" Text="@glampert Ahh, I see; that makes sense. Still, I think there are lots of options for handling transparencies; in the page ID pass, you could dither (so histogramming would see all the pages, unless there were a huge number of transparent layers), or use a [k-buffer approach](http://www.sci.utah.edu/~csilva/papers/i3d2007.pdf), or even just base transparent texture residency on which objects are near the camera (as opposed to rendering them in a feedback pass)." CreationDate="2015-12-05T02:12:20.280" UserId="48" />
  <row Id="1555" PostId="1772" Score="0" Text="Yes, I would like to have more details about this, maybe I should ask this question on [math.stackexchange](http://math.stackexchange.com/)?" CreationDate="2015-12-05T15:34:24.700" UserId="2173" />
  <row Id="1557" PostId="1772" Score="0" Text="@arthur.sw What details are you interested in? I could add a quick example showing how the divide-by-W affects vectors, if that would be helpful." CreationDate="2015-12-05T21:12:09.347" UserId="48" />
  <row Id="1558" PostId="1772" Score="0" Text="Well I just wanted to make sure that projecting two points is the only or best way to do it." CreationDate="2015-12-06T19:26:40.830" UserId="2173" />
  <row Id="1563" PostId="1786" Score="4" Text="This sounds more like a forum discussion than a specific question. This site works best with questions that can be answered with a single, factual answer. The &quot;what is your wishlist&quot; part is definitely not suitable for an SE site." CreationDate="2015-12-08T15:09:24.190" UserId="2041" />
  <row Id="1565" PostId="1786" Score="1" Text="You can come to [the cornell box](http://chat.stackexchange.com/rooms/26589/the-cornell-box) our chatroom and fire up a discussion" CreationDate="2015-12-08T18:45:12.097" UserId="137" />
  <row Id="1567" PostId="1785" Score="0" Text="Scrolling through your output quickly, it looks reasonable. Although it's hard to tell since it's not plotted as an image. Can you clarify what the problem is?" CreationDate="2015-12-08T19:02:08.877" UserId="48" />
  <row Id="1568" PostId="1789" Score="0" Text="Ok this does answer the question. Hovewer, the explanation probably reqjires one to knpw the answer before understanding. Yould you drop down the abstraction a bit." CreationDate="2015-12-09T05:59:02.037" UserId="38" />
  <row Id="1569" PostId="1789" Score="0" Text="The theory is clear, but how would one go about implementing it? The 2 rotations I used in my example could be done without releasing the mouse inbetween, so there isn't really a clear cut line between them. The user may even go crazy and rotate up left down left which could yield something like cameraRotation.X = 0 cameraRotation.Y = 200, which says nothing about the actual ordner in which the transformations happened." CreationDate="2015-12-09T07:00:28.523" UserId="2219" />
  <row Id="1570" PostId="1791" Score="0" Text="Nice ! Your explanation gives me the intuition about it. Do you happen to have some sources where I can read a bit more about rasterization? I suppose this n-separating stuff comes from 2D where is easier to understand and then I can do more thinking to grasp the 6- and 26-separating in 3D." CreationDate="2015-12-09T14:47:13.873" UserId="116" />
  <row Id="1571" PostId="1785" Score="0" Text="Hope the addition make it more clear.." CreationDate="2015-12-09T16:44:38.587" UserId="2226" />
  <row Id="1572" PostId="1785" Score="0" Text="Hmmm...it's still not very clear to me. Is the problem that e.g. at a 45 degree angle, the line only goes out to (7, 7) and stops, instead of going all the way out to the corner at e.g. (10, 10)?" CreationDate="2015-12-09T18:07:36.510" UserId="48" />
  <row Id="1573" PostId="1785" Score="0" Text="yes exactly..   but also that the method only works for slopes within 0-1" CreationDate="2015-12-09T18:11:14.893" UserId="2226" />
  <row Id="1574" PostId="1791" Score="1" Text="@BRabbit27 I don't think the &quot;n-separating&quot; terminology is used much in 2D rasterization; I've only seen it when discussing voxelization. It just refers to the number of neighbors. I'll add a bit to the answer about that." CreationDate="2015-12-09T18:11:56.777" UserId="48" />
  <row Id="1575" PostId="1795" Score="0" Text="Nice..  it works.. :)" CreationDate="2015-12-09T19:37:20.910" UserId="2226" />
  <row Id="1576" PostId="1789" Score="0" Text="@Patrick Moving the mouse back without releasing the mouse button while rotating an object usually leads to the same result as if the mouse had not been moved e.g. up and back down at all. I added a few words about this (pretend the mouse moved instantly to the current position)." CreationDate="2015-12-09T21:50:46.807" UserId="127" />
  <row Id="1577" PostId="1795" Score="0" Text="well.. partly seem to have an issue with negative values.. Some end position contains negative values..&#xA;&#xA;the center is here 10,10.. thereby making the matrix sized 20X20.." CreationDate="2015-12-09T22:12:46.037" UserId="2226" />
  <row Id="1578" PostId="1795" Score="0" Text="endPos: (15,-2) Angle: 293" CreationDate="2015-12-09T22:17:29.263" UserId="2226" />
  <row Id="1579" PostId="1798" Score="0" Text="Thanks @NathanReed. 1 more question. Say I want to traverse the tree breadth first, I am at level 0 (i.e at the root of the tree, I haven't divided the tree yet, I have a bounding box including all the scene primitives). I divide the bounding box along an axis such that the left child node is the one closer to the camera. Then I find that a certain ray R intersects the left node. R should be early terminated with regard to the right child node and to all of its future children nodes. But I still have to test the intersection of R with the future children nodes of this left node, am I?" CreationDate="2015-12-10T07:29:19.003" UserId="2233" />
  <row Id="1580" PostId="1793" Score="2" Text="I don't think the question's necessarily too open-ended, but any numeric answer is going to be wrong within 12 months." CreationDate="2015-12-10T09:51:04.117" UserId="2041" />
  <row Id="1581" PostId="1799" Score="0" Text="I already asked this question here http://gamedev.stackexchange.com/questions/112165/brdf-and-spherical-coordinate-in-ray-tracing. Nobody seems to have an answer. I think that the arguments in my &quot;UPDATE 2&quot; could be the way to follow (because it seems that the project of vector in this way described could be the correct way to calculate the azimuth angle). Anyone could help me with an answer and maybe a canonical reference to be used as study reference?" CreationDate="2015-12-10T13:10:30.087" UserId="2237" />
  <row Id="1582" PostId="1796" Score="1" Text="Could you provide more detail on what the exact problem is? The only problem you describe is about negative coordinates. A circle of radius 10 around $(10, 10)$ barely reaches zero. Even if there are numerical errors leading to values slightly below zero, after rounding these should be gone." CreationDate="2015-12-10T16:24:12.387" UserId="127" />
  <row Id="1583" PostId="1793" Score="0" Text="@DanHulme Yeah, but the approaches used to reach that kind of efficiency stay the same. And when not, I've seen questions that require updating answers periodically on other stackexchange sites, so I think that's fine." CreationDate="2015-12-10T18:47:56.763" UserId="2111" />
  <row Id="1584" PostId="1798" Score="1" Text="@user2651062 Normally you would have built the whole tree before you start traversing it. Your comment makes it sound like you're trying to build and traverse at the same time? Or else what do you mean by &quot;future child nodes&quot;? In any case, you have to traverse all child nodes that intersect the ray or segment. If it intersects the left child, you descend into the left child and repeat the process for its children. You might need to descend into both children if they both intersect the ray/segment." CreationDate="2015-12-10T20:01:27.557" UserId="48" />
  <row Id="1585" PostId="1796" Score="0" Text="It would also be useful to see the output for a variety of different input values." CreationDate="2015-12-11T00:22:00.550" UserId="231" />
  <row Id="1586" PostId="1775" Score="0" Text="This appears to be the same user and question as [Draw angles lines in raster graphics using bresenham line algorithm](http://computergraphics.stackexchange.com/questions/1785/draw-angles-lines-in-raster-graphics-using-bresenham-line-algorithm)" CreationDate="2015-12-11T06:29:34.403" UserId="48" />
  <row Id="1588" PostId="1793" Score="6" Text="This is really impossible to answer. First of all, what is &quot;realtime&quot;—60fps? 30? Less? Second, the answer will vary hugely based on what GPU you have and what resolution you're rendering at. Third, the answer will vary hugely depending on the details of how the rendering works. Limits on scene complexity are more complicated than just the number of polygons per se, but involve such things as the number of draw calls, state changes, render passes and so on—which are affected by how the engine works, how the artists constructed the scene, and so on..." CreationDate="2015-12-11T06:39:54.973" UserId="48" />
  <row Id="1589" PostId="1800" Score="0" Text="Thank you so much for your help @NathanReed. One last question: do you have any good reference material on how to calculate the tangent space and convert my vectors wi and wo to the coordinate space for ray tracing/BRDF? At the moment I didn't find any useful one. In this way I would be able to do some comparison between the way with tangent space and the way using vectors math." CreationDate="2015-12-11T12:19:49.197" UserId="2237" />
  <row Id="1590" PostId="1793" Score="0" Text="@NathanReed And those are precisely what I'm asking about, the number itself isn't important, and likewise, realtime is loosely defined here as well, although personally I'd go with being able to maintain a framerate above 30. The question itself is about techniques and approaches used to minimize costs per frame, I just couldn't think of a better way to title and phrase it." CreationDate="2015-12-11T12:31:49.687" UserId="2111" />
  <row Id="1591" PostId="1800" Score="0" Text="@FabrizioDuroni I assume you're familiar with how to convert between coordinate systems in general? For tangent space you just have to set up coordinates using the surface normal plus some two vectors perpendicular to it as the axes. For normal mapping, the two vectors are often chosen to match the texture space U and V axes (as mapped to the particular surface). For isotropic BRDFs without normal mapping, it doesn't really matter." CreationDate="2015-12-11T19:11:04.707" UserId="48" />
  <row Id="1593" PostId="1793" Score="1" Text="@Llamageddon Considering your comments, I'm not quite sure what you actually ask for. On one hand, your question title is quite clear (max out geometry and how to do so), but as Nathan pointed out, this is kind of impossible to answer. On the other hand, in your comments you say you want to know how to minimize cost per frame. This is an extremely broad question, because you could improve/optimize your shaders, scene graph, models, textures, API usage, simply everything that does some part of your rendering. You could probably write entire books about this (if not done by someone already)." CreationDate="2015-12-12T00:39:01.593" UserId="127" />
  <row Id="1596" PostId="1800" Score="0" Text="yes I'm familiar with how to convert between coordinate system (so I know that a particular matrix that use the component of the basis vector must be used) but I'm not so familiar with tangent space and texture mapping. Searching the web it seems that the calculation vary between type of objects (sphere, triangle...). You say &quot; For isotropic BRDFs without normal mapping, it doesn't really matter.&quot;, what do you mean? So how do I choose them? Thank you again, I accepted your answer because your the only one that really give me some good hints." CreationDate="2015-12-12T17:43:10.420" UserId="2237" />
  <row Id="1597" PostId="1793" Score="0" Text="@Nero As I said, it's a broad question, I'm interested in all kinds of techniques utilized to push hardware to the limits - occlusion culling, scene graph management, batching, instantiation, virtual texturing, deferred rendering, etc. Setting aside implementing more than just the baseline of geometry, shadows, basic shading, but inclusive of everything leading up to that. I'm sorry if it's a bad question." CreationDate="2015-12-12T23:32:10.617" UserId="2111" />
  <row Id="1598" PostId="1788" Score="0" Text="This seemed a good question - did you delete it because you found the solution? If so you could undelete it and post an answer with your solution. Answering your own question is encouraged and you gain reputation for both the question and the answer. Plus it may help someone else who has a similar problem in future..." CreationDate="2015-12-13T14:11:21.907" UserId="231" />
  <row Id="1599" PostId="1793" Score="0" Text="The more you stuff every technique in one program (what usually an engine does) you are not pushing the hardware to the limits, on the contrary you give it many opportunities to breathe. The way to use the hardware at max capacity is what OCCT does. only a benchmark can do this." CreationDate="2015-12-14T02:19:57.877" UserId="1614" />
  <row Id="1600" PostId="1801" Score="0" Text="if c is a union of the projected pixels, when s1 or s2 completely obstructs the other sphere, it does not mean c gets empty. please clarify." CreationDate="2015-12-14T02:24:53.623" UserId="1614" />
  <row Id="1601" PostId="1793" Score="0" Text="@v.oddou I'd say it's still pushing it to its limits, just in a smarter way. I'm not necessarily interested in super-advanced techniques, just how to... ugh, honestly, I thought this question would be self-explanatory, I even gave examples of what I mean. I just give up, honestly. It's like you guys are literally trying to interpret the question word-by-word instead of actually considering it. Should I just write a wiki answer and let others correct me where I'm wrong or miss something? :/" CreationDate="2015-12-14T02:56:24.510" UserId="2111" />
  <row Id="1602" PostId="88" Score="1" Text="I'm going to give you a straightforward answer: GPU are turing complete. what do you conclude from this ? second answer: lux render http://www.luxrender.net/wiki/SLG" CreationDate="2015-12-14T05:13:53.773" UserId="1614" />
  <row Id="1603" PostId="1806" Score="1" Text="A million seems a bit low to me." CreationDate="2015-12-14T09:00:57.703" UserId="38" />
  <row Id="1604" PostId="1806" Score="0" Text="just take how many MPoly/s the card is capable of, and that's the FPS at which it will render 1 million. I just recalled an experiment for a terrain renderer on an ATI4800HD. If you take this list https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units they don't give Vertices/s info starting from the era of unified architecture. but 10 year old hardware seems to advertise about 40 FPS for 1 million triangles. + c.f. edit in my answer" CreationDate="2015-12-14T09:30:38.697" UserId="1614" />
  <row Id="1605" PostId="1803" Score="1" Text="Great answer and nice illustrations, thanks!" CreationDate="2015-12-14T10:44:02.737" UserId="2244" />
  <row Id="1606" PostId="1806" Score="0" Text="@v.oddou Yeah, but to get near that number you need to do batching of geometry, or instancing, in case of dynamic scenes, and **that** is what I'm asking about. How to not bottleneck yourself 2% of the way towards what hardware can do." CreationDate="2015-12-14T13:49:23.330" UserId="2111" />
  <row Id="1607" PostId="1788" Score="1" Text="Hello @trichoplax actually I found the solution, I'll share the answer with everyone answering my own question. Honestly I deleted my question because I thought none cares about this issue." CreationDate="2015-12-14T21:56:19.720" UserId="2228" />
  <row Id="1608" PostId="1788" Score="1" Text="BTW, instead of editing the question with &quot;SOLVED&quot; in the title, it's preferable to just accept your own answer. (The site might make you wait a day after posting to do that; I don't remember.)" CreationDate="2015-12-15T00:37:03.437" UserId="48" />
  <row Id="1609" PostId="1788" Score="0" Text="Hey! @NathanReed  I'll change the title, thanks about that :)" CreationDate="2015-12-15T00:38:57.447" UserId="2228" />
  <row Id="1610" PostId="1797" Score="0" Text="I'm curious - what languages like this are out there? I know there are engine-specific ones like Unity's or UE4's shader systems, plus some academic researchy things like [Spark](https://graphics.stanford.edu/papers/spark/), but I'm not aware of anything else in current use in this space." CreationDate="2015-12-15T01:22:18.650" UserId="48" />
  <row Id="1611" PostId="1806" Score="0" Text="@Llamageddon aaah, I see, THAT is a question indeed. Let me see what I can say about it. (EDIT2)" CreationDate="2015-12-15T01:47:59.120" UserId="1614" />
  <row Id="1612" PostId="1797" Score="0" Text="@NathanReed, probably Apple's [Metal](https://developer.apple.com/library/ios/documentation/Metal/Reference/MetalShadingLanguageGuide/Introduction/Introduction.html) is one of the most notable, as of now, but I haven't looked into much detail..." CreationDate="2015-12-15T02:42:46.927" UserId="54" />
  <row Id="1613" PostId="1797" Score="0" Text="Oh, OK. Metal isn't on top of HLSL or GLSL though; it's a primary shading language for Apple GPUs that compiles directly to HW microcode (via a proprietary LLVM backend I believe)." CreationDate="2015-12-15T04:42:36.130" UserId="48" />
  <row Id="1617" PostId="1762" Score="0" Text="The table's corners are, yes." CreationDate="2015-12-16T11:56:41.987" UserId="2162" />
  <row Id="1619" PostId="1806" Score="0" Text="Great in depth answer! I've made a few minor edits, as a user rather than a moderator. Feel free to roll back any/all if they don't match your intention." CreationDate="2015-12-17T12:50:50.273" UserId="231" />
  <row Id="1623" PostId="1816" Score="2" Text="wow, nice animation!" CreationDate="2015-12-18T20:46:59.857" UserId="2173" />
  <row Id="1624" PostId="1816" Score="0" Text="You can answer [here](http://stackoverflow.com/questions/34357110/how-to-triangulate-from-a-vorono%C3%AF-diagram) as well or I will delete my other question." CreationDate="2015-12-18T20:48:24.100" UserId="2173" />
  <row Id="1625" PostId="1816" Score="3" Text="@arthur.sw Cross-posting is generally discouraged on SE, so I suppose deleting it there would be the better option." CreationDate="2015-12-18T20:49:20.453" UserId="16" />
  <row Id="1626" PostId="1816" Score="0" Text="an interactive voronoï diagram creator: http://alexbeutel.com/webgl/voronoi.html" CreationDate="2015-12-19T00:47:21.030" UserId="2173" />
  <row Id="1633" PostId="1822" Score="2" Text="Sorry, but that's plain wrong. OpenGL and DirectX use approximations which are inherently faster than precise raytracing. The whole point of accelerated 3D graphics is having algorithms which balance between realism and speed, looking good enough for most practical uses: gaming, CAD, etc." CreationDate="2015-12-21T15:16:53.507" UserId="2312" />
  <row Id="1634" PostId="1822" Score="2" Text="@IMil OpenGL can be used for raytracing. Its faster because it is optimized for the hardware in question. But Maya does NOT have to ray trace. Maya and Max can use openGL and directX just as much as your game. Mayas (and 3ds) viewport is opengl or directX (your choice). The fact that your processor is slower in certain parallel processing loads is another thing. So the answer stands. The standard settings of maya is no more realistic than a standard scanline." CreationDate="2015-12-21T15:36:03.813" UserId="38" />
  <row Id="1635" PostId="1818" Score="1" Text="The short answer is that OpenGL takes shortcuts." CreationDate="2015-12-21T18:59:57.240" UserId="2316" />
  <row Id="1636" PostId="1821" Score="5" Text="This answer would be even better if you explained what the problem turned out to be and what you changed to fix it..." CreationDate="2015-12-21T20:25:45.710" UserId="231" />
  <row Id="1637" PostId="1820" Score="0" Text="The problem seems to be that all variables are `int`. In particular, `dx` and `dy` will probably get 0." CreationDate="2015-12-22T01:50:08.923" UserId="192" />
  <row Id="1645" PostId="1824" Score="4" Text="Others common renderers have this conflation problem too. See http://w3.impa.br/~diego/projects/GanEtAl14/sample.html?contour." CreationDate="2015-12-22T10:15:25.367" UserId="192" />
  <row Id="1650" PostId="1821" Score="1" Text="Ill update the question and answer now with more info." CreationDate="2015-12-22T21:22:27.333" UserId="2248" />
  <row Id="1651" PostId="1827" Score="0" Text="Its also a question of time. Even if you could render at say 60 fps and get acceptable results it rarely pans out to optimize for it. Say it takes 3 minutes per frame and you have 200 frames to render. You might be able to get the 60 fps by hiring a shader writer and by optimizing but then that takes atleast a day or two of your time. But  200 frames at 3 mins only takes 10 hours so you save that cost. In practice its cheaper to buy more hardware and not worry too much about it. Games simply can not take this approach." CreationDate="2015-12-23T05:02:36.703" UserId="38" />
  <row Id="1652" PostId="1827" Score="0" Text="@joojaa It's also a little bit more complex though. Just doing really good real-time shaders for Maya might take a year or so at the very, very least, even from an experienced shader developer (with lesser gains), because the flexibility of the nodal system there is targeted towards production rendering. It would take a reverse engineering mindset and kind of new kind of GLSL/HLSL code generation technique (like a meta programming system) to translate these general-purpose shader nodes into a real-time shading system that captures the range of effects of UE 4, e.g." CreationDate="2015-12-23T05:06:44.513" UserId="2247" />
  <row Id="1653" PostId="1827" Score="0" Text="@joojaa UE 4's shader engine is directly targeted towards an heavily-approximated PBR mindset (a very small subset of Disney's PBR shader). They designed even their material system for a fast, real-time purpose, instead of starting with something like Maya's material system which isn't at all (designed for raytracing). Even if the brightest of the UE 4 worked on VP 2.0, they'd have to work night and day for possibly years to achieve the same results against a design not intended to do this sort of stuff." CreationDate="2015-12-23T05:08:30.277" UserId="2247" />
  <row Id="1654" PostId="1827" Score="0" Text="but thats a onetime cost even if youd have that pipeline in a VFX app each scene might need that extra optimization. Theres no reason why a maya user could't render in UDK for example for same shader dev platform." CreationDate="2015-12-23T05:15:20.407" UserId="38" />
  <row Id="1655" PostId="1827" Score="0" Text="They dont have to allways be top notch just good enough for the job at hand. But yes i agree that my time factor is off by a magnitude. But even if it would take days its still not worth it. Games do not have the option." CreationDate="2015-12-23T05:17:28.650" UserId="38" />
  <row Id="1656" PostId="1827" Score="0" Text="@joojaa Yeah, tricky part is that we're often talking about production renderers that are 10+ years old, Maya which is 17+ years old -- huge legacy attached ranging from content to studio pipelines to plugins and so on. It's hard to modernize it -- game engines are always cutting-edge, always purging their previous generation engines and kind of starting a new canvas with each engine generation (though they might reuse or continue a lot of previous work). VFX companies just keep poking at their old codebase." CreationDate="2015-12-23T05:19:29.040" UserId="2247" />
  <row Id="1657" PostId="1827" Score="1" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/33352/discussion-between-joojaa-and-ike)." CreationDate="2015-12-23T05:20:41.790" UserId="38" />
  <row Id="1659" PostId="1828" Score="0" Text="Does this do the same thing except with an increase on the Z height on each iteration of the for loop?" CreationDate="2015-12-24T09:37:43.297" UserId="2248" />
  <row Id="1660" PostId="1828" Score="0" Text="Pretty much, except its always symmetrical (istead of the original a and b, I used a uniform `size` param)." CreationDate="2015-12-24T09:54:29.600" UserId="2317" />
  <row Id="1661" PostId="1830" Score="0" Text="Are you aiming for a solution with a specific programming language / library?" CreationDate="2015-12-24T13:13:16.813" UserId="2317" />
  <row Id="1662" PostId="1830" Score="0" Text="solution  can be in any language .... what is more important is algorithm" CreationDate="2015-12-24T13:23:49.630" UserId="2335" />
  <row Id="1664" PostId="1831" Score="2" Text="Something looks wrong with your first image pair...the result is mostly complete black, which doesn't seem correct. Also, it would be good to handle divide-by-zero so rows that are complete black in the input don't flip out. Maybe just replace the row by all 128s in that case?" CreationDate="2015-12-24T21:30:50.837" UserId="48" />
  <row Id="1665" PostId="1825" Score="1" Text="BTW, for a more detailed look at why alpha blending doesn't &quot;do the right thing&quot; in cases like this, check out this paper: [Interpreting Alpha](http://jcgt.org/published/0004/02/03/) by Andrew Glassner." CreationDate="2015-12-24T21:34:26.007" UserId="48" />
  <row Id="1666" PostId="1825" Score="0" Text="@NathanReed Will look thnx, but here it is simply that even if alpha worked right coverage wont know which parts cover the pixels and which not. So  Two layers with 50% alpha could mean fully opaque or only one layer visible because objects fill identical region we just dont know." CreationDate="2015-12-24T22:06:05.467" UserId="38" />
  <row Id="1667" PostId="1831" Score="0" Text="Same algorithm for 1st and 2nd img. Will check though, makes sense about the flipping, thanks." CreationDate="2015-12-24T22:20:57.027" UserId="2317" />
  <row Id="1668" PostId="1825" Score="2" Text="@joojaa Yes that's basically the point the paper is making: alpha can represent either opacity, coverage, or a combination of both. :)" CreationDate="2015-12-25T00:15:09.077" UserId="48" />
  <row Id="1669" PostId="1833" Score="3" Text="Very easy way to achieve a similar look is to just add a small value to the color or each pixel, then clamp to max (255 or 1). It will make the image look more &quot;washed out&quot;." CreationDate="2015-12-25T22:39:48.917" UserId="54" />
  <row Id="1672" PostId="1834" Score="0" Text="I edited my question so that it is more clear. I am not working in 3d btw." CreationDate="2015-12-26T18:54:49.437" UserId="113" />
  <row Id="1674" PostId="1840" Score="0" Text="I tried `float dirX = -(a + b * angle) * sin(angle) + b * cos(angle);` and `float dirZ = (a + b * angle) * cos(angle) + b * sin(angle);&#xA;` then `float newAngle = atan2(dirZ, dirX);` but this does not set the angle correctly." CreationDate="2015-12-27T14:38:35.250" UserId="2248" />
  <row Id="1676" PostId="1840" Score="0" Text="I'm multiplying the result by 180 * Pi to convert to radians" CreationDate="2015-12-27T14:52:42.093" UserId="2248" />
  <row Id="1678" PostId="1840" Score="0" Text="Programming language is c++" CreationDate="2015-12-27T14:57:16.397" UserId="2248" />
  <row Id="1679" PostId="1840" Score="0" Text="Atan2 c++ http://www.cplusplus.com/reference/cmath/atan2/" CreationDate="2015-12-27T15:02:54.920" UserId="2248" />
  <row Id="1681" PostId="1840" Score="0" Text="ill update the question with what I have so far can you have a look?" CreationDate="2015-12-27T15:13:48.623" UserId="2248" />
  <row Id="1682" PostId="1839" Score="0" Text="It would be nice if you could include in your post what libraries you use. Your using Unity right?" CreationDate="2015-12-27T15:35:39.203" UserId="38" />
  <row Id="1683" PostId="1839" Score="0" Text="Using Bullet physics and freeglut" CreationDate="2015-12-27T15:39:25.653" UserId="2248" />
  <row Id="1684" PostId="1840" Score="0" Text="Ive just noticed that your circle is spiraling in the opposite direction to mine" CreationDate="2015-12-27T16:33:02.170" UserId="2248" />
  <row Id="1685" PostId="1840" Score="0" Text="@damorton It depends on how your csys is defined and from which side of the spiral you look from. These are arbitrary definitions. I just have a right handed coordinate system in a 2d space." CreationDate="2015-12-27T17:51:48.493" UserId="38" />
  <row Id="1686" PostId="1839" Score="0" Text="Not Unity https://github.com/damorton/bullet-dominos" CreationDate="2015-12-27T22:06:55.920" UserId="2248" />
  <row Id="1688" PostId="1843" Score="1" Text="Kinda looks like you changed it to `3.2`." CreationDate="2015-12-28T07:35:46.130" UserId="457" />
  <row Id="1689" PostId="1845" Score="1" Text="'Processes' is very vague. Is that vertex shader ops? Raterizer? Shading? All of the above? None of these are meaningful, because they have a massive scene dependence. FLOPS is kind of better, but still not great because it doesnt take into account register pressure, memory latency, etc." CreationDate="2015-12-28T16:52:43.583" UserId="310" />
  <row Id="1690" PostId="1846" Score="1" Text="Hello and welcome. Are you trying to circumvent the [future MIT license](http://meta.stackexchange.com/questions/271080/the-mit-license-clarity-on-using-code-on-stack-overflow-and-stack-exchange?cb=1) by posting a image or what up with that?" CreationDate="2015-12-28T17:10:03.210" UserId="38" />
  <row Id="1691" PostId="1840" Score="0" Text="Turned out that the above was perfect, the problem was with the dimensions of each domino. Swapping the width and height values fix it :/ thanks @joojaa" CreationDate="2015-12-28T17:52:47.107" UserId="2248" />
  <row Id="1693" PostId="1843" Score="0" Text="Well, in my code its 4.1 haha" CreationDate="2015-12-28T18:32:20.553" UserId="2355" />
  <row Id="1694" PostId="1849" Score="0" Text="That's interesting, though I see from the photon-mapping link that what I'm asking about is how to approach Spectral Rendering.  Do you think it's reasonable to change the question so far as to update it with this term?  I still think I'm asking the same thing." CreationDate="2015-12-28T23:14:10.250" UserId="2360" />
  <row Id="1695" PostId="1849" Score="0" Text="@NewAlexandria Hmm...I interpreted it as being primarily about caustics. If you're really asking about spectral rendering, that's a different question than the one I answered. :) I think it wouldn't be a bad idea to post a new question, if you want to. Maybe edit this one to be specifically about caustics?" CreationDate="2015-12-29T04:13:59.447" UserId="48" />
  <row Id="1704" PostId="1845" Score="0" Text="I understand that there are all of these factors. Nonetheless, I'd be interested to know about how many triangles per second can be drawn assuming modest/reasonable/typical choices for the various factors (simple/default vertex and pixel shaders, simple lighting, big model being textured by some reasonable texture sheets)." CreationDate="2015-12-29T18:32:58.440" UserId="2358" />
  <row Id="1706" PostId="1849" Score="0" Text="Thanks, [I did, here](http://computergraphics.stackexchange.com/questions/1854/how-is-spectral-rendering-handled)" CreationDate="2015-12-29T20:25:53.253" UserId="2360" />
  <row Id="1708" PostId="1852" Score="0" Text="Thanyou so much! Now I know the how alpha blending works and fixed the particle stuff!  The solution I found worked best was making sure the function got called 60 times a second regardless of the framerate.  Thankyou much!" CreationDate="2015-12-29T23:51:03.110" UserId="2308" />
  <row Id="1714" PostId="1842" Score="0" Text="you can use glGetString(GL_SHADING_LANGUAGE_VERSION) to see which glsl versions are available to you. printf(&quot;Supported GLSL version is %s.\n&quot;, (char *)glGetString(GL_SHADING_LANGUAGE_VERSION));" CreationDate="2015-12-29T15:10:54.010" UserId="2366" />
  <row Id="1722" PostId="1831" Score="0" Text="The above solution seems correct at first glance but what if user enter the output image as input then it shows distorted output. The solution should be adaptive and learn when not to perform calculation as the output of first input is the solution so when output is again input no changes should be seen." CreationDate="2016-01-01T07:45:53.923" UserId="2383" />
  <row Id="1723" PostId="1831" Score="0" Text="@SandeepNeupane that makes sense, although that's not what the author of the question asked. I'm sure this answer can serve as the basis for a more adaptive, complete solution." CreationDate="2016-01-01T18:16:25.310" UserId="2317" />
  <row Id="1724" PostId="67" Score="1" Text="[Related question on SuperUser.](http://superuser.com/q/1019825/215723)" CreationDate="2016-01-01T19:27:16.227" UserId="16" />
  <row Id="1725" PostId="1857" Score="0" Text="Welcome to Computer Graphics Stack Exchange. You could improve your question by adding what information about the polygon and the red box you have. Do you e.g. know the precise coordinates of the vertices, or do you only have the image? Also, please describe what you already tried to do. These information will help us to write good answers and especially also one that helps you best. Some more tips can be found on our [ask] page in our Help Center." CreationDate="2016-01-01T22:09:06.727" UserId="127" />
  <row Id="1726" PostId="1859" Score="0" Text="Note that the question also requires the specified edge to be at the bottom." CreationDate="2016-01-02T23:40:30.977" UserId="231" />
  <row Id="1727" PostId="1859" Score="0" Text="@trichoplax, right. Added that bit of code." CreationDate="2016-01-03T05:34:53.337" UserId="2317" />
  <row Id="1728" PostId="1862" Score="5" Text="There are scaling algorithms like bicubic scaling which use splines to approximate the color of pixels when scaled to any size." CreationDate="2016-01-03T13:09:54.743" UserId="1683" />
  <row Id="1729" PostId="1861" Score="3" Text="These &quot;textures&quot; are called light cookies, which essentially is a texture which shows the strength of light on the projected area. This is actually really simple to implement. When the light attenuation is calculated in the shader, it looks up the cookie texture and returns the strength (which can be stored in the R, G, B, or A channels)." CreationDate="2016-01-03T13:13:00.867" UserId="1683" />
  <row Id="1730" PostId="1859" Score="0" Text="From the example in the question, I believe &quot;at the bottom&quot; means that the horizontal edge becomes the base of the shape, rather than moving it to the bottom of the screen." CreationDate="2016-01-03T14:14:13.190" UserId="231" />
  <row Id="1731" PostId="1859" Score="0" Text="For instance, your side AD is at the top of the shape, so that even when it is moved to the bottom of the screen, the rest of the shape is still below it, off screen." CreationDate="2016-01-03T14:15:43.820" UserId="231" />
  <row Id="1732" PostId="1835" Score="1" Text="You may have to use culling and only draw visible cubes if you can't do a million cubes.  Another option is to merge cubes into larger rectangular shapes." CreationDate="2016-01-05T01:16:56.063" UserId="56" />
  <row Id="1733" PostId="1865" Score="0" Text="Interesting. Could you point us to any sources, examples or results of this approach?" CreationDate="2016-01-05T09:11:27.457" UserId="385" />
  <row Id="1734" PostId="1865" Score="0" Text="I'm on my phone so can't take a screenshot, but this shadertoy uses the method and looks pretty decent: https://www.shadertoy.com/view/ltfXDM" CreationDate="2016-01-05T12:30:40.763" UserId="56" />
  <row Id="1735" PostId="1865" Score="2" Text="POV-Ray is an open-source ray-tracer that uses a similar method to simulate dispersion. It's not a ray per channel: you can configure how many rays are used, spread equally across the spectrum." CreationDate="2016-01-05T15:19:28.693" UserId="2041" />
  <row Id="1736" PostId="1854" Score="0" Text="I feel like this question is way too broad as it stands. Whole books have been written on the subject. Perhaps you could narrow it down to a specific question that's not covered by existing resources?" CreationDate="2016-01-05T15:20:30.733" UserId="2041" />
  <row Id="1737" PostId="1854" Score="0" Text="I can see this being answered along the lines of &quot;There are hundreds of ways, each of which falls into one of the following N broad categories. If you want to know specific detail about one of these categories you can ask a new question.&quot;" CreationDate="2016-01-05T16:17:38.973" UserId="231" />
  <row Id="1738" PostId="1862" Score="0" Text="@EvilTak, can you expand your comment into an small answer?" CreationDate="2016-01-05T16:37:23.030" UserId="54" />
  <row Id="1739" PostId="1866" Score="0" Text="There is a affine transform that will map each corner to its texture coordinate, you can use that to map P to its uv." CreationDate="2016-01-05T16:59:28.540" UserId="137" />
  <row Id="1740" PostId="1866" Score="0" Text="@ratchetfreak could you provide me a link plz ?" CreationDate="2016-01-05T17:06:38.397" UserId="2214" />
  <row Id="1742" PostId="1867" Score="1" Text="@Nero Thanks for Jaxing it up." CreationDate="2016-01-05T20:35:35.477" UserId="457" />
  <row Id="1745" PostId="1866" Score="0" Text="There is a good write up on how to do the intersection point calculation as well as barycentric cord calculation in one go [in this paper](http://www.cs.virginia.edu/~gfx/courses/2003/ImageSynthesis/papers/Acceleration/Fast%20MinimumStorage%20RayTriangle%20Intersection.pdf). This essentially amounts to transforming the triangle." CreationDate="2016-01-06T07:26:36.763" UserId="38" />
  <row Id="1746" PostId="1867" Score="0" Text="i sthere a error in $Bary_B$? Should the first term be $(A_y-C_y)$ or am i wrong?" CreationDate="2016-01-06T07:35:25.327" UserId="38" />
  <row Id="1747" PostId="1867" Score="0" Text="@joojaa I don't think so. It's the same in the Wikipedia article, and it seems correct from a test calculation I did." CreationDate="2016-01-06T07:52:02.270" UserId="457" />
  <row Id="1748" PostId="1862" Score="0" Text="@glampert did that. Do you want me to remove my comment?" CreationDate="2016-01-06T08:00:26.423" UserId="1683" />
  <row Id="1749" PostId="1867" Score="0" Text="ah, so it is $-(A_y-C_y)$, might be good to point out as you would pre calculate $AC_y = (A_y-C_y)$." CreationDate="2016-01-06T08:11:20.477" UserId="38" />
  <row Id="1750" PostId="1867" Score="1" Text="@joojaa The entire denominator and some of the terms in the nominator can be precalculated for each triangle, only few of the terms depend on $P$. I've added a link to a question dealing with methods of calculation. In this formula I thought it would be better to keep the notation simple and uniform rather than efficient." CreationDate="2016-01-06T08:15:03.043" UserId="457" />
  <row Id="1753" PostId="1861" Score="0" Text="I'm not really sure what part of this you don't understand. It's a relatively simple lighting environment and doesn't present any problems for a real-time renderer. Is it the way lights overlap that seems hard, or the shapes of the lights, or something else completely?" CreationDate="2016-01-06T10:13:16.560" UserId="2041" />
  <row Id="1754" PostId="1862" Score="0" Text="@EvilTak, I think you can leave it. Nice answer btw, thanks!" CreationDate="2016-01-06T14:27:11.127" UserId="54" />
  <row Id="1755" PostId="1874" Score="2" Text="The question is about software (read: CPU) rasterization. Some of the information you gave are about rasterization in general, some techniques - in my book - have nothing to do with rasterization at all. Could you please clarify in your answer how the techniques use or benefit from software rasterization?" CreationDate="2016-01-07T12:42:47.580" UserId="385" />
  <row Id="1762" PostId="1878" Score="0" Text="show us what your output is and describe what you expect it to be" CreationDate="2016-01-08T14:33:11.280" UserId="137" />
  <row Id="1763" PostId="1878" Score="0" Text="It just wont compile. It is a version of a basic pixel ilumination shader. The previous version just supports 1 light. I am trying to adapt it so that it can process 8 lights." CreationDate="2016-01-08T14:44:49.503" UserId="2425" />
  <row Id="1764" PostId="1878" Score="0" Text="we'll need to see the info log from the failed compilation/linking. It's a good practice to always log it while developing." CreationDate="2016-01-08T14:55:00.770" UserId="137" />
  <row Id="1765" PostId="1878" Score="0" Text="I am pretty much just starting with shaders...  I do get :&#xA;&#xA;&#xA;	&#xA;Cannot compile frgament shader: 0(24) : error C1101: ambiguous overloaded function reference &quot;mul(mat4, vec3&quot;) (0): mat3x4 mul(mat3x1, mat1x4) (0): mat3 mul(mat3x1, mat1x3) (0): mat3x2 mul(mat3x1, mat1x2) ..." CreationDate="2016-01-08T15:01:46.820" UserId="2425" />
  <row Id="1766" PostId="1879" Score="0" Text="I do not get matrix errors anymore but It just wont compile. I have posted the original working code for a single light. How should I approach it?" CreationDate="2016-01-08T15:36:28.617" UserId="2425" />
  <row Id="1767" PostId="1880" Score="1" Text="This question is about image / signal processing, which is not within the scope of this site. However, there is a SE for signal processing here: http://dsp.stackexchange.com/" CreationDate="2016-01-08T20:41:33.267" UserId="310" />
  <row Id="1768" PostId="1880" Score="0" Text="increase contrast add more frames from video feed. Most of it remains guesswork though" CreationDate="2016-01-08T21:19:06.850" UserId="137" />
  <row Id="1769" PostId="1880" Score="0" Text="[Meta discussion](http://meta.computergraphics.stackexchange.com/questions/201/is-this-graphics-or-image-processing-is-there-an-overlap) on whether image processing is on topic." CreationDate="2016-01-08T23:15:37.607" UserId="231" />
  <row Id="1770" PostId="1881" Score="0" Text="I haven't heard of anyone doing it yet, but I really think temporal techniques could be used to get sub pixel accuracy from video streams." CreationDate="2016-01-09T02:18:06.283" UserId="56" />
  <row Id="1771" PostId="1881" Score="0" Text="@AlanWolfe Definitely. Googling &quot;video super-resolution&quot; turns up a number of papers on that idea." CreationDate="2016-01-09T04:53:26.357" UserId="48" />
  <row Id="1772" PostId="1880" Score="0" Text="Is there a way to move this question to the other SE or should I just cut/paste it over there ?" CreationDate="2016-01-09T07:13:44.533" UserId="2427" />
  <row Id="1774" PostId="1883" Score="0" Text="Very interesting answer! I thought about this, but did not take the time to explain it. &#xA;&#xA;In my opinion, the best solution is to combine both methods: use the first method when possible and handy (when the selected axis does not point to the camera), and your method otherwise. There should be an angle threshold (the angle between the axis and the camera forward vector) to switch between those two methods. In both cases, the GUI should give a good feedback of what's going on (direction and amount of the extrusion, maybe with graduation)." CreationDate="2016-01-09T13:11:37.553" UserId="2173" />
  <row Id="1775" PostId="1883" Score="0" Text="I feel like I must accept the answer which would explain both solutions, do you want to write it?" CreationDate="2016-01-09T13:12:29.870" UserId="2173" />
  <row Id="1776" PostId="1883" Score="0" Text="Dont recommend dual approach. This kind of twitching between the modes makes worst UXp" CreationDate="2016-01-09T13:15:55.557" UserId="2433" />
  <row Id="1777" PostId="1880" Score="0" Text="Per these comments and @trichoplax 's meta discussion I went ahead and cross-posted this in two other SE forums. Here are the links to those discussions: http://dsp.stackexchange.com/questions/28168/what-is-the-state-of-the-art-on-using-computers-to-clean-up-videos?noredirect=1#comment52603_28168 and http://video.stackexchange.com/questions/17363/what-is-the-state-of-the-art-on-using-computers-to-clean-up-videos" CreationDate="2016-01-09T15:55:40.803" UserId="2427" />
  <row Id="1778" PostId="1880" Score="2" Text="Although this was with good intentions, note that [cross posting on several SE sites is not recommended](http://meta.stackexchange.com/questions/64068/is-cross-posting-a-question-on-multiple-stack-exchange-sites-permitted-if-the-qu)." CreationDate="2016-01-09T16:00:30.437" UserId="231" />
  <row Id="1779" PostId="1880" Score="0" Text="@O.M.Y. note that despite the comment about being off topic, the only response so far on meta is to say that this question is definitely on topic here. It will take time to see what other responses come in, but I don't see any reason to move this question at present unless you have your own reasons to." CreationDate="2016-01-09T16:02:31.760" UserId="231" />
  <row Id="1780" PostId="1880" Score="1" Text="Also cross posting is considered rude." CreationDate="2016-01-09T16:16:52.227" UserId="38" />
  <row Id="1781" PostId="1880" Score="0" Text="@trichoplax While it is likely that some of the people in the three SE communities will be joined in all three, there is no guarantee that experts in VIDEO will be part of DSP or CG and vice-versa. As the question seems to be on topic for all three I would like to *gather wool* from each community, give credit where credit is due to each expert, then merge the information into a really good answer for all three SE communities. I am a futurist and it seems reasonable to me that these three fields will simultaneously become both more *integrated* and more *specialized* as we progress." CreationDate="2016-01-09T16:17:25.070" UserId="2427" />
  <row Id="1782" PostId="1880" Score="0" Text="@joojaa I asked about that in a meta discussion and was told to go ahead and post it in the other forum. If I was given bad information then I apologize, but others have not complained and one even helped me to add the cross post links into the comments." CreationDate="2016-01-09T16:19:46.647" UserId="2427" />
  <row Id="1783" PostId="1880" Score="2" Text="You were not asked to post you were told there are other avenues by one user. Cross posting is not the stackexhange way never. If you have a question you need to make it worthy of the site your asking. So you cant ask the same question on video for example." CreationDate="2016-01-09T16:22:05.063" UserId="38" />
  <row Id="1784" PostId="1880" Score="1" Text="@O.M.Y. the meta post I linked to about cross posting shows community consensus is against cross posting, but there are also answers in favour of cross posting in certain rare circumstances, which are worth reading. Even then they recommend tailoring your question to different sites rather than just copy and pasting." CreationDate="2016-01-09T16:23:34.663" UserId="231" />
  <row Id="1785" PostId="1880" Score="1" Text="The reason that meta post exists is because it is not immediately obvious why cross posting would be a bad thing. It's well worth reading through. If there is going to be ill feeling on our site due to this it might be worth having a local meta discussion to see how our particular community feels about when and whether to cross post." CreationDate="2016-01-09T16:25:40.903" UserId="231" />
  <row Id="1786" PostId="1880" Score="0" Text="@joojaa This is where I was told to &quot;go ahead and post&quot; in VIDEO ... http://meta.video.stackexchange.com/questions/1467/about-asking-a-question-in-two-different-se-communities/1468#1468" CreationDate="2016-01-09T16:28:00.320" UserId="2427" />
  <row Id="1787" PostId="1880" Score="1" Text="Still i think you should re word your question anyway. Video will get you a different answer." CreationDate="2016-01-09T16:28:56.570" UserId="38" />
  <row Id="1788" PostId="1880" Score="0" Text="Also user @OlliNiemitalo (2000+ rep) helped me add the cross links in DSP without complain... again if I was provided bad information I apologize but I did ask and I did feel it was okay based on those responses. Now I feel confused and wonder if I should just delete it all and shut up." CreationDate="2016-01-09T16:31:39.993" UserId="2427" />
  <row Id="1789" PostId="1880" Score="1" Text="@O.M.Y. you certainly shouldn't feel bad about posting as you have sought advice in advance. There will always be conflicting opinions about things. If you want to talk about it further I recommend [chat] rather than trying to fit it into comments here." CreationDate="2016-01-09T16:35:06.137" UserId="231" />
  <row Id="1790" PostId="1881" Score="0" Text="I did toyed with SISR a while ago, but my mind cleverly decided to erase that information from the brain. If I can recall though one important bit (mentioned in the paper) that I would explicit in your excellent answer is that the patterns or substructures are to be searched not only in the original image but also in multiple scales of it. That IIRC lead to significantly better results, although as I said I don't remember that much details :(" CreationDate="2016-01-09T17:40:34.587" UserId="100" />
  <row Id="1792" PostId="1888" Score="1" Text="Interesting question. If there is a specific reason that you don't want a shape with planar quads, like a [rhombic dodecahedron](https://en.wikipedia.org/wiki/Rhombic_dodecahedron), mentioning that reason and explaining your motivation for using non-planar quads might help give insight into what is required." CreationDate="2016-01-10T16:12:58.640" UserId="231" />
  <row Id="1793" PostId="1889" Score="0" Text="Thanks for the answer! But I am not looking for a simple 2D shadows algorithm (there are lots on the web), I am looking for a shadow/light algorithm that can handle volumetric fog and colored transparent surfaces (as seen in the Youtube video)." CreationDate="2016-01-10T17:57:52.223" UserId="2437" />
  <row Id="1794" PostId="1888" Score="1" Text="@trichoplax Ok, updated the question :)." CreationDate="2016-01-10T17:57:59.480" UserId="2440" />
  <row Id="1795" PostId="1889" Score="0" Text="@sydd that is just a shader on the shadow volume. Easy to extend." CreationDate="2016-01-10T18:17:25.673" UserId="38" />
  <row Id="1797" PostId="1892" Score="1" Text="It's unfortunately not uncommon for drivers to contain bugs (or &quot;features&quot; to let Game X play optimally even though it doesn't use the API correctly)" CreationDate="2016-01-11T12:01:05.807" UserId="137" />
  <row Id="1799" PostId="1892" Score="0" Text="I hope it's not driver bug, but my bug with padding in that struct." CreationDate="2016-01-11T13:46:39.523" UserId="2413" />
  <row Id="1800" PostId="1892" Score="0" Text="and if you use std430 layout?" CreationDate="2016-01-11T13:50:07.660" UserId="137" />
  <row Id="1801" PostId="1892" Score="0" Text="@ratchetfreak OS X only support OpenGL [4.1](https://developer.apple.com/opengl/capabilities/) ( also, ARB_gpu_shader_fp64 is not supported on some 4.1 Mac Radeons, but it should be on my Intel ), and std430 was introduced in OpenGL 4.3" CreationDate="2016-01-11T13:59:30.073" UserId="2413" />
  <row Id="1802" PostId="1892" Score="0" Text="I've tested it now on one iMac, and it's working there ( but super slow )." CreationDate="2016-01-11T14:39:47.893" UserId="2413" />
  <row Id="1803" PostId="1892" Score="0" Text="So it looks like this shader is not compatible with Intel cards or it's bug in OS X Intel driver." CreationDate="2016-01-11T14:43:31.763" UserId="2413" />
  <row Id="1806" PostId="1889" Score="0" Text="@sydd added colored example" CreationDate="2016-01-11T17:43:40.670" UserId="38" />
  <row Id="1807" PostId="1893" Score="2" Text="What units is the radius measured in? It doesn't make a lot of sense to have a radius of 0.12 pixels (it would just be the one center pixel). It's probably 0.12 centimeters, or 0.12 ems, or something like that." CreationDate="2016-01-11T19:05:36.990" UserId="48" />
  <row Id="1808" PostId="1893" Score="0" Text="Also what does it mean to 'pick a pixel'? Given a pixel coordinate, do you want to know if it is inside the circle?" CreationDate="2016-01-11T20:44:44.263" UserId="457" />
  <row Id="1809" PostId="1893" Score="0" Text="By &quot;to pick a pixel&quot; I meant to randomly choose a pixel inside the circle. The units are not mentioned. The circle has a center C(Cx, Cy), which is a pixel. My guess is, a pixel inside the circle has coordinates P(Px, Py) such that sqrt((Px-Cx)^2+(Py-Cy)^2) &lt; 0.12 (the distance between P and the center C is less than the radius) . That's the most logical way to think about it in my opinion." CreationDate="2016-01-11T21:59:37.363" UserId="2233" />
  <row Id="1810" PostId="1896" Score="0" Text="So what's the problem, specifically? &quot;I cannot make it work&quot; doesn't give us much to go on. Are you getting compiler errors? Post them. Bad output? Show us a screenshot." CreationDate="2016-01-11T22:11:03.257" UserId="48" />
  <row Id="1811" PostId="1896" Score="0" Text="I have just started with shaders. Sorry about that, I am a little bit desperate right now...  It compiles just fine. But won't  affect the lighting in any way. The objects just stays there unaffected by the shader, same as it it was not even loaded." CreationDate="2016-01-11T22:13:12.750" UserId="2425" />
  <row Id="1812" PostId="1896" Score="0" Text="as a test, you might make the sphere have a color based on some value that you are using to calculate your lighting.  This is a really primitive way of getting an idea of the value of your variables.  Doing this, you might notice that something is constant across the sphere which shouldn't be, or a similar problem, which will then help point you in the direction of what is going wrong specifically." CreationDate="2016-01-12T01:16:59.820" UserId="56" />
  <row Id="1813" PostId="1895" Score="0" Text="Is there a reason nobody responds? Should I reformulate the question?" CreationDate="2016-01-12T08:36:11.910" UserId="2447" />
  <row Id="1815" PostId="1895" Score="1" Text="Its early days there are only so many people who have time to answer and you've only so far reached 5 views. This stackexchange is still an infant and has not got many users give it time." CreationDate="2016-01-12T10:52:36.993" UserId="38" />
  <row Id="1816" PostId="1895" Score="0" Text="OK. Thanks. I did not realize so little people were here." CreationDate="2016-01-12T12:10:08.863" UserId="2447" />
  <row Id="1817" PostId="1899" Score="1" Text="Thanks! A, B, C are vectors right? Also, I am using the scanline method because it allows me to get the exact number of points I need. Could you take a look at the code and guess why it is not working? Just the formulas. Also I would upvote the response, but I don't have 15 reputation." CreationDate="2016-01-12T13:04:15.660" UserId="2447" />
  <row Id="1818" PostId="1899" Score="0" Text="Yes, A B &amp;C are vectors; 2D in your case but it applies equally well to N dimensions. As for going through the code... as I said I don't know Lua and even getting a standard polygon scanline renderer correct can be tricky - e.g. You have to be very careful when counting crossings which lie exactly on vertex positions. When you extend that to handling Beziers directly (which I did about 20+ years ago) it's more difficult still. I'm sorry I don't have the time." CreationDate="2016-01-12T14:09:28.923" UserId="209" />
  <row Id="1819" PostId="1899" Score="1" Text="Thanks for the help. Just found the issue. The a and c in the quadratic equation were inverted." CreationDate="2016-01-12T16:22:31.000" UserId="2447" />
  <row Id="1820" PostId="1889" Score="0" Text="thanks, it looks nice! But as I wrote my post is about porting the algorithm I described to the GPU. Actually someone gave an answer on SO at http://stackoverflow.com/questions/34708021/how-to-implement-2d-raycasting-light-effect-in-glsl . This answer is what im looking for, but its slow, thus I dont think that this algorithm is feasible for a real game." CreationDate="2016-01-12T17:23:18.417" UserId="2437" />
  <row Id="1821" PostId="1889" Score="0" Text="@sydd raycasting can be accelerated by precomputing the data into a better datastructure." CreationDate="2016-01-12T18:19:55.377" UserId="38" />
  <row Id="1822" PostId="1903" Score="0" Text="in graph (A), y range is   -1.0 ~ 1.5," CreationDate="2016-01-13T08:25:40.240" UserId="2459" />
  <row Id="1825" PostId="1903" Score="3" Text="Welcome to Computer Graphics SE! I tried to improve the readability of your post a bit. However, I'm not sure it's a great fit for this site. You could probably just ask this on [so] instead. I've [started a discussion on meta](http://meta.computergraphics.stackexchange.com/q/205/16) whether questions about plotting simple graphs are considered on topic or not." CreationDate="2016-01-13T12:42:17.800" UserId="16" />
  <row Id="1828" PostId="1904" Score="0" Text="I've made some minor edits that hopefully do not change your original intention. Feel free to roll back anything that changes your question." CreationDate="2016-01-13T21:59:34.110" UserId="231" />
  <row Id="1829" PostId="1903" Score="3" Text="Given that this is specifically about R, it's likely a better fit for [Cross Validated](https://stats.stackexchange.com/)." CreationDate="2016-01-14T00:02:58.473" UserId="48" />
  <row Id="1830" PostId="1901" Score="0" Text="What's the problem with doing a dynamic loop? That seems like the natural way to solve this kind of problem. If the board has a fixed 8x8 size, the longest ray would only pass through 16 squares at most, so it's not a crazy iteration count. And if you restrict the camera angles like you said, so that people can't look straight across the board, you can cut down the longest ray length considerably." CreationDate="2016-01-14T00:09:16.400" UserId="48" />
  <row Id="1831" PostId="1901" Score="0" Text="I've been going down that route without having a better solution, so it's nice to hear your support of it, thanks Nathan." CreationDate="2016-01-14T00:17:58.143" UserId="56" />
  <row Id="1835" PostId="1909" Score="1" Text="Hello, it could be a particular issue so, can you edit your post and add your GPU code (vertex/fragment shaders) as well as your specific CPU's code on which you are dispatching your shadow map to GPU? this is in order to do a fast debug." CreationDate="2016-01-15T04:07:54.017" UserId="2228" />
  <row Id="1836" PostId="1909" Score="0" Text="I added the code but as you can see, there's nothing really advanced here. I also precise that I am using Unity." CreationDate="2016-01-15T07:27:49.133" UserId="2372" />
  <row Id="1838" PostId="1901" Score="0" Text="Have you tried using raymarching instead of raytracing? Raytracing is more of a CPU/software rendering oriented algorithm, whereas raymarching with signed distance fields is much more easy to implement on a GPU, IMO." CreationDate="2016-01-16T06:24:50.037" UserId="1683" />
  <row Id="1839" PostId="1901" Score="0" Text="For sphere and axis aligned box, directly finding the intersection with raytracing is the better way to go.  For more complex shapes, sure, ray marching is definitely a nice option.  Ray marching exists to get around having to analytically solve line segment vs arbitrary shape." CreationDate="2016-01-16T06:35:37.547" UserId="56" />
  <row Id="1840" PostId="1909" Score="0" Text="This should be the same issue as in http://stackoverflow.com/questions/1513383/texturing-error-on-a-sphere" CreationDate="2016-01-16T13:53:15.863" UserId="2476" />
  <row Id="1841" PostId="1912" Score="0" Text="transform feedback buffers" CreationDate="2016-01-16T17:02:05.970" UserId="137" />
  <row Id="1842" PostId="1912" Score="0" Text="Sounds very good. The constraints of the cloth simulation make it necessary that I can access all neighbours of a particle (to calculate the spring forces). Can this be done with such buffers? In a geometry shader maybe? to pervent the feedback buffer to print each vertex multiple times I would like to use GL_POINTS. After a first look it seems that this makes it harder to calculate the springs." CreationDate="2016-01-16T19:18:54.543" UserId="273" />
  <row Id="1845" PostId="1907" Score="0" Text="This seems to be helpful .... Thanks cheers!!!!!!!1" CreationDate="2016-01-18T03:34:44.603" UserId="2383" />
  <row Id="1846" PostId="1911" Score="1" Text="Your question could probably be improved by including some more information about the actual algorithm instead of just linking to it. Posts should generally be self-contained so as not to be susceptible to link rot (even though that's unlikely for Wikipedia) and so that people can understand the full question without having to read an external link first." CreationDate="2016-01-18T09:10:28.977" UserId="16" />
  <row Id="1849" PostId="1916" Score="1" Text="Depends heavily on what you trace, but this isnt a good fit for the site in my opinion written in this form. Edit the post to be about backface culling in raytracing." CreationDate="2016-01-18T16:08:59.780" UserId="38" />
  <row Id="1856" PostId="1921" Score="1" Text="I think even in the silhouette case, using *signed* projected area (as you noted) means that assumption 3 isn't violated, so long as the microsurface's boundaries match the macrosurface's. Even if there are overhangs beyond the silhouette, the signed projected area of facets on the front and back sides of the overhang will cancel out." CreationDate="2016-01-19T18:30:58.430" UserId="48" />
  <row Id="1857" PostId="1921" Score="0" Text="(Also, maybe this goes without saying, but I think the assumptions also guarantee that the microsurface is a nice, 2-manifold surface without any holes or other weird stuff.)" CreationDate="2016-01-19T18:51:22.653" UserId="48" />
  <row Id="1858" PostId="1924" Score="2" Text="Could you elaborate on what you mean by sampling? The data is just a LUT. The simple way to use it would be to look up the closests input / output vectors and LERP the spectral data between them. Or do you mean how to importance sample?" CreationDate="2016-01-19T23:32:20.063" UserId="310" />
  <row Id="1859" PostId="1924" Score="0" Text="Hi @RichieSams i update my question with more specific problems. Ok for the linear interpolation (I want to keep thing simple at the moment :))" CreationDate="2016-01-19T23:55:21.643" UserId="2237" />
  <row Id="1860" PostId="1925" Score="2" Text="Modern cards differ from previous generation cards as they nolonger have a fixed pipeline for triangles and thus its harder to say what the rate is as it waries with conditions outside the card." CreationDate="2016-01-20T06:22:05.103" UserId="38" />
  <row Id="1861" PostId="1925" Score="0" Text="I was worried that might be the case for NV. AMD did have a fixed triangle setup engine in Hawaii, but I wouldn't be surprised if it went away in the next architecture revision." CreationDate="2016-01-20T06:39:17.713" UserId="2500" />
  <row Id="1862" PostId="1926" Score="0" Text="Thank you @RichieSams. So what about that transformation matrix? Is it some kind of change of coordinate matrix, for example like the one used for transform input data to a camera coordiante system?" CreationDate="2016-01-20T07:23:31.213" UserId="2237" />
  <row Id="1863" PostId="1921" Score="0" Text="@NathanReed That's true, I should have been more precise about that. As for what the assumptions guarantee, I think of it the other way round: the fact that a surface, however faceted, has to be the whole of a boundary between some &quot;inside&quot; and some &quot;outside&quot; forces it to have the three properties." CreationDate="2016-01-20T09:58:42.097" UserId="2041" />
  <row Id="1864" PostId="1926" Score="1" Text="Exactly. It's a change of coordinate matrix. The data is in world coordinate space. And specifically, the input vector is a small portion of the space. So transform to world, then rotate to the input vector domain" CreationDate="2016-01-20T14:24:10.900" UserId="310" />
  <row Id="1865" PostId="1927" Score="2" Text="How are you representing the rectangle, and how are you moving it? Can you share the relevant sections of your code?" CreationDate="2016-01-20T18:53:01.593" UserId="48" />
  <row Id="1866" PostId="1927" Score="1" Text="We will need more detail on what you are trying to achieve, and what you have tried so far so we don't duplicate effort." CreationDate="2016-01-20T20:20:24.237" UserId="231" />
  <row Id="1867" PostId="1927" Score="0" Text="I'm new to this code base so I'm still figuring things out. I'll update the question with some more information when I get time." CreationDate="2016-01-20T22:27:34.393" UserId="2506" />
  <row Id="1868" PostId="1925" Score="0" Text="Again it seems to me that you can make certain default assumptions that take a lot of the uncertainly out of what's &quot;outside the card&quot; for the purpose of comparing card speeds (simple/default vertex and pixel shaders, simple lighting, big model being textured by some reasonable texture sheets). But I'll take this fixed-pipeline estimate of 2 billion per second as being similar to what you'd get with those assumptions. Thanks." CreationDate="2016-01-21T01:26:13.003" UserId="2358" />
  <row Id="1869" PostId="1925" Score="0" Text="I'm going to study up on NV's white papers - I need to catch up on their architecture (I worked for AMD a few years back). Ideally, there would be be synthetic benchmarks which would help everybody understand where the bottlenecks are, but there was a long history of cheating on them..." CreationDate="2016-01-21T03:31:10.087" UserId="2500" />
  <row Id="1870" PostId="1927" Score="1" Text="There is no such thing as a straight line on a sphere. Thus no rectangle. But most likely also your coordinate system is cylindrical causing the cordinates not to be uniform." CreationDate="2016-01-21T05:25:11.487" UserId="38" />
  <row Id="1871" PostId="1925" Score="0" Text="If it's an operation that always (or nearly always) needs to be done (and triangle set up is one such thing) then there can be large power/area/efficiency reasons for using dedicated hardware." CreationDate="2016-01-21T10:31:56.017" UserId="209" />
  <row Id="1872" PostId="1927" Score="0" Text="@joojaa: A rectangle is mapped on to a sphere. The coordinates on the globe are LatLon but from what I can tell, the X and Y used aren't related." CreationDate="2016-01-21T15:26:45.447" UserId="2506" />
  <row Id="1873" PostId="1931" Score="0" Text="Is the dipole approximation at all related to the Beer–Lambert law?" CreationDate="2016-01-22T03:16:56.483" UserId="2457" />
  <row Id="1874" PostId="1931" Score="0" Text="@maogenc The dipole approximation concerns homogeneous materials with a very high scattering coefficient, such that they're dominated by multiple scattering. The Beer–Lambert law would also apply to such materials." CreationDate="2016-01-22T06:38:14.047" UserId="48" />
  <row Id="1875" PostId="1937" Score="0" Text="How sure are you that the light source is parameterized in the same way?" CreationDate="2016-01-22T09:27:04.150" UserId="385" />
  <row Id="1876" PostId="1937" Score="0" Text="Pretty sure, because and Lux uses diffuse IES data on light sources by default and I only support diffuse material. Also, both light sources have the same strength/color of 6.0." CreationDate="2016-01-22T09:49:04.440" UserId="2448" />
  <row Id="1877" PostId="1937" Score="1" Text="Still it might be that you are using a different [photometric quantity](http://mentalraytips.blogspot.de/2007/03/understanding-photometric-and.html) somewhere, that your tone mapping is still different despite Gamma 2.2, that you're [missing a pi](https://seblagarde.wordpress.com/2012/01/08/pi-or-not-to-pi-in-game-lighting-equation/) somewhere, or any other reason. It's hard to tell, please give some more details about your shading, show some code etc. Besides, the camera is not the same in both images, you might wanna adjust that too :)" CreationDate="2016-01-22T10:20:14.420" UserId="385" />
  <row Id="1879" PostId="1937" Score="0" Text="I have edited in some code. Hopefully it helps. Also, here is the whole repo. More code could be found here. (Please use the &quot;dev&quot; branch. It's where all the new codes are.) https://bitbucket.org/Seanstone5923/simpleray5cpu. OK, I'll fix the camera as soon as possible." CreationDate="2016-01-22T15:03:49.677" UserId="2448" />
  <row Id="1880" PostId="101" Score="0" Text="When they are available you can use SSBOs to get a more flexible output format where you don't need to encode in colors. However, the big drawback of this approach is that it alters the code which can hide/alter bugs, especially when UB is involved. +1 Nevertheless, for it is the most direct method that is available." CreationDate="2016-01-22T15:10:21.610" UserId="2521" />
  <row Id="1882" PostId="1940" Score="3" Text="You may need to add relevant code but this should be pretty easy to paralellisize and use the GPU for calculation. How many objects are we talking about." CreationDate="2016-01-22T16:39:56.777" UserId="38" />
  <row Id="1885" PostId="1939" Score="1" Text="Before/after screenshots would be helpful, otherwise we're all trying to imagine what you mean. Also, is the question about how to enlarge a known region (i.e. you have already identified where the eyes are), or about how to find/track the objects to enlarge? Because those are very different questions. :)" CreationDate="2016-01-22T19:13:21.767" UserId="48" />
  <row Id="1886" PostId="1939" Score="0" Text="Welcome to Computer Graphics Stack Exchange. Asking for off-site resources is off-topic for this site, that's why I removed that part of your question." CreationDate="2016-01-22T19:18:26.767" UserId="127" />
  <row Id="1887" PostId="1940" Score="1" Text="Welcome to Computer Graphics Stack Exchange. As joojaa already said, it's quite hard to help you to improve your code without seeing it. Please edit your question and add relevant parts of your code. Additionally, a screenshot of the result then also helps understanding what your code does." CreationDate="2016-01-22T19:26:54.350" UserId="127" />
  <row Id="1888" PostId="1940" Score="0" Text="Off the cuff, I'd say render the objects in a floating-point texture using additive blending to accumulate their [gravitational potential](https://en.wikipedia.org/wiki/Gravitational_potential). Then use a second pass to convert the texture into a color map." CreationDate="2016-01-22T19:39:39.613" UserId="48" />
  <row Id="1889" PostId="1946" Score="0" Text="Hi @Dragonseel thank you for your answer. I read somewhere on the web that you could also need the texture coordinate to calculate the tangent space. Are there other procedure that could be used to calculate the tangent space?" CreationDate="2016-01-22T20:49:11.507" UserId="2237" />
  <row Id="1890" PostId="1946" Score="1" Text="Nice answer. I edited it to apply MathJax. A minor note: You can't use $DIR$ for the pixel that looks directly at the sphere, because of linear dependency to $N$. But actually you could use any vector instead of $DIR$ that is linearly independent. Or am I missing something?" CreationDate="2016-01-22T20:51:29.750" UserId="127" />
  <row Id="1891" PostId="1946" Score="0" Text="I don't know about the texture coordinate, but given that you already have the intersection-point and the normal calculating two cross-products seems to be very cost effective way. Yes, any vector that is linearly independent can be used. I wrote $DIR$ because it is an easily usable vector. You could just invent a way to generate a non-linear dependent vector (switch two components comes to mind, but I'm not that sure)." CreationDate="2016-01-22T21:18:56.590" UserId="273" />
  <row Id="1892" PostId="1946" Score="0" Text="If the tangent space is to be used for texture mapping (e.g. filtering, normal mapping etc) then it's convenient for the tangent space axes to be aligned with the texture UV axes. But if any arbitrary tangent space is fine, then [an arbitary vector orthogonal to the normal](http://lolengine.net/blog/2013/09/21/picking-orthogonal-vector-combing-coconuts) can be used." CreationDate="2016-01-22T23:15:56.423" UserId="48" />
  <row Id="1893" PostId="1939" Score="0" Text="@Nero no worries." CreationDate="2016-01-23T04:56:08.257" UserId="2520" />
  <row Id="1894" PostId="1939" Score="0" Text="@NathanReed I have already tracked the region of the video for enlargement. I am only looking at enlarging the regions are blend well with the rest of the video content." CreationDate="2016-01-23T04:58:07.287" UserId="2520" />
  <row Id="1895" PostId="1940" Score="0" Text="@NathanReed Right my thoughts exactly..." CreationDate="2016-01-23T07:57:50.153" UserId="38" />
  <row Id="1896" PostId="1948" Score="1" Text="Your point is well taken as a matter of practice: when I used to work at AMD, to test architectural limits, I would do things like clock down the GPU so I could simulate &quot;infinitely&quot; fast memory. However, such architectural limits are all part of a well designed, balanced GPU; understanding gives insight into GPU design, implementation and programming." CreationDate="2016-01-23T15:58:04.627" UserId="2500" />
  <row Id="1897" PostId="1951" Score="1" Text="Just out of curiosity didnt we agree that [software recommendations are offtopic](http://computergraphics.stackexchange.com/help/on-topic)? I dont nesseserily mind but..." CreationDate="2016-01-24T01:21:46.423" UserId="38" />
  <row Id="1898" PostId="1951" Score="1" Text="@joojaa The question is about using scripting to automate image operations, which seems on-topic and generally useful to me. Moreover I don't think I should hold back my answer just because it refers to some external software." CreationDate="2016-01-24T03:04:22.223" UserId="48" />
  <row Id="1899" PostId="1951" Score="0" Text="Its a great answer! Thanks so much!" CreationDate="2016-01-24T07:51:48.717" UserId="2537" />
  <row Id="1900" PostId="1951" Score="0" Text="Love you man. You saved so much of my time :) Hope you have a great day!" CreationDate="2016-01-24T08:27:08.883" UserId="2537" />
  <row Id="1901" PostId="1951" Score="1" Text="No thats not nesseserily what im saying, im just pointing out that have closed questions of simililar nature due to them being software recommendations." CreationDate="2016-01-24T08:40:07.400" UserId="38" />
  <row Id="1904" PostId="1953" Score="0" Text="I read that in a raster display a picture is kind-of jaggy but in random we get a smooth line. why and how does this happen when both work on pixel level anyhow?" CreationDate="2016-01-24T14:13:13.327" UserId="2540" />
  <row Id="1905" PostId="1953" Score="0" Text="Both systems produce some aliasing (I think this is what you mean with 'jaggy') on pixel-level since there is only a finite discrete resolution available. I don't know how the amount of aliasing compares, since in the background of a rasterizer geometric primitives also get converted into pixel-grid, and a random scan has to do the same conversion. I can only assume that because a random scan can directly follow the line it is easier to anti-alias." CreationDate="2016-01-24T14:36:38.283" UserId="273" />
  <row Id="1906" PostId="1957" Score="1" Text="I dont think the memory type matters, however you may want to get a new chipset so that you can do most of the state of the art stuff. Since the memory type may a indicator of the chipset... Memory per see should not matter." CreationDate="2016-01-25T07:32:58.423" UserId="38" />
  <row Id="1911" PostId="1959" Score="0" Text="Statistical tests for random number generators should be useful. Computing the expected number of in order (reverse order) pairs might be a good place to start with a test. This paper has lots of references: http://csrc.nist.gov/groups/ST/toolkit/rng/documents/nissc-paper.pdf." CreationDate="2016-01-25T10:30:09.687" UserId="2500" />
  <row Id="1912" PostId="1960" Score="0" Text="Welcome to Computer Graphics SE! Re your second point, I looked at the cyclic decomposition of the permutation table used by Perlin. It consists of multiple cycles of lengths `{4, 121, 89, 12, 4, 15, 4, 6}`, so apparently that's good enough? (Or maybe it isn't and a different permutation table would be even &quot;better&quot;? Although I'm not sure a human could perceive the difference. Or is actually better to have multiple cycles?)  I'm not following your third point. A uniform random distribution of what? And what step distance do you mean?" CreationDate="2016-01-25T11:29:40.773" UserId="16" />
  <row Id="1913" PostId="1960" Score="0" Text="Thanks! Yeah, that was quite cryptic I guess. It was years ago I experimented with this, so I don't recall the exact implementation, but when you have the implementation for the optimal path generation, it should become evident that you pick random positions of what is left of unused indices - it is that random step length I refer to. I updated the answer" CreationDate="2016-01-25T11:53:09.523" UserId="1790" />
  <row Id="1914" PostId="1957" Score="0" Text="&quot;Maxwell&quot; is NV's current architecture - it has some new graphics features which are described in NVs GeForce GTX 980 Whitepaper. I believe the lowest end Maxwell is the 950." CreationDate="2016-01-25T12:23:23.247" UserId="2500" />
  <row Id="1915" PostId="1936" Score="0" Text="Note that there is a fundamental difference: supersampling (with jittering or any other technique) requires _additional samples_, while a post-filter (like the suggested low-pass filter) works on the _rendered image_. Nevertheless, this is valuable additional information and both techniques go hand in hand." CreationDate="2016-01-25T12:47:23.523" UserId="385" />
  <row Id="1916" PostId="1936" Score="0" Text="I answered initially without seeing yours (antialias with some kind of stochastic supersampling), but your answer was better. I still thought the older paper was still worth referencing, so I changed my answer instead of deleting it. I hope that's reasonable etiquette?" CreationDate="2016-01-25T17:34:39.693" UserId="2500" />
  <row Id="1917" PostId="1957" Score="0" Text="It's really very simple now, especially for Nvidia GPUs: the more you pay, the more you get. Check out this [Price/performance ratio table](http://www.videocardbenchmark.net/gpu_value.html). It roughly shows that if you are low on budget, GeForce GTX 950 is the best you can get. Also it's based on the latest architecture, so you get DirectX 12 and CUDA compute capability 5.2. Alternatively you may look into laptops. Low-end discrete GPUs are almost free there." CreationDate="2016-01-25T17:45:15.380" UserId="2115" />
  <row Id="1918" PostId="1937" Score="0" Text="I'm afraid I dont know enough about it to be really helpful, however, I would probably try to adjust the light intensity so the images look equal, then see with which factor you needed, maybe it can give a clue as what went wrong. Or maybe you cant get it to look the same, like the whole image gets to light to get the centre the right brightness, then maybe some angle calculation is not the same.. just some ideas to try out" CreationDate="2016-01-25T19:11:02.740" UserId="2422" />
  <row Id="1919" PostId="1958" Score="0" Text="So there're no architectural differences whatsoever?" CreationDate="2016-01-26T01:31:12.990" UserId="1870" />
  <row Id="1920" PostId="1958" Score="1" Text="It's just memory. Of course there are some hardware-level differences in how the chip talks to the memory, but it makes no difference for programming it. In CPU programming, you don't write different code for DDR3 vs DDR4 memory either." CreationDate="2016-01-26T02:15:08.723" UserId="48" />
  <row Id="1922" PostId="1963" Score="3" Text="Is your actual texture also just black dots on white? If so, you might be able to generate the texture procedurally, e.g. with [Poisson disc sampling](http://devmag.org.za/2009/05/03/poisson-disk-sampling/) with varying radii. Otherwise, if your texture is actually something more continuous, blending might be an option (that wouldn't work well for crisp textures like your example though). So if you could clarify what your actual pattern/texture looks like that might help getting more helpful answers." CreationDate="2016-01-26T07:37:53.980" UserId="16" />
  <row Id="1923" PostId="1936" Score="0" Text="Totally, your answer is a fine addition :) I just wanted to make sure nobody gets confused." CreationDate="2016-01-26T08:43:32.147" UserId="385" />
  <row Id="1924" PostId="1951" Score="0" Text="The question is about a scripting tool, perhaps making it an &quot;API recommendation&quot; question? It would probably be at home in game development. Maybe I've got a meta meta question here - can questions be recommended for transfer to or crossposted on other stack exchange sites?" CreationDate="2016-01-26T12:53:28.380" UserId="2500" />
  <row Id="1925" PostId="1937" Score="0" Text="Have you tried setting `FACTOR_CUT_OFF` to 0 (introduces bias, LuxRender is unbiased)? Have you tried excluding techniques like Russian Roulette, to see where the culprit may be?" CreationDate="2016-01-26T13:31:53.990" UserId="385" />
  <row Id="1926" PostId="1936" Score="0" Text="Understood. It really sinks in that aliasing is a result of the initial samples being at regular intervals: if every sample hits the pickets of a fence, you're stuck with a white wall. Applying a low pass filter to a regularly sampled image just isn't universally effective (the high frequency picket fence aliases down to a low frequency artifact - a white wall)." CreationDate="2016-01-26T14:00:57.493" UserId="2500" />
  <row Id="1927" PostId="1936" Score="0" Text="The post filtering approach also seems like a really interesting modern CPU (with integrated GPU) load balancing question. Ray trace using CPU cores, which walk such data structures efficiently, post process the images on the GPU. Fun project!" CreationDate="2016-01-26T14:02:20.203" UserId="2500" />
  <row Id="1928" PostId="1937" Score="0" Text="Yes, I did test that. In fact, the image I shown is rendered under a unbiased condition. Also, Russian Roulette is enabled in the image that I shown. I have set `BOUNCE_DEPTH` to -1(which makes my renderer to ignore bounce limits)." CreationDate="2016-01-26T15:10:53.090" UserId="2448" />
  <row Id="1929" PostId="1967" Score="0" Text="So essentially, you want to remove the hand and pencil from the drawing? So you can have just a time-lapse of the drawing over time? Did I understand you correctly?" CreationDate="2016-01-26T16:37:49.393" UserId="310" />
  <row Id="1930" PostId="1967" Score="0" Text="Actually, I want to know how the painter draw. That is, the sequence of its drawing stroke. The painter will draw by hands on paper. Like that http://imgur.com/FeN1IPy" CreationDate="2016-01-26T16:40:09.933" UserId="2551" />
  <row Id="1931" PostId="1967" Score="0" Text="Remove the hand and pencil may benefit to analyze and it is good also." CreationDate="2016-01-26T16:40:52.333" UserId="2551" />
  <row Id="1932" PostId="1963" Score="1" Text="Post process? Something like a variant on edge detection: write out the distance to the nearest dot center then, in screen space, search each pixel's dot sized neighborhood for any (close to) zero values. This gives screen space dots, so you'd have to write and mess with UV derivatives to get back to texture space... Note that you still have a continuity problem - as the p high/low boundary moves across the surface, dots appear and disappear. If p can be continuous (a soft boundary) you might be able to shrink/grow the dots along the boundary..." CreationDate="2016-01-26T16:50:03.830" UserId="2500" />
  <row Id="1933" PostId="1967" Score="0" Text="So, sort of a vector field drawing showing the path the pencil takes on the paper? For example, each stroke would be an individual curve/arrow." CreationDate="2016-01-26T19:17:59.540" UserId="310" />
  <row Id="1935" PostId="1963" Score="1" Text="Another option is to look into Wang tiling.  [introduction](http://procworld.blogspot.com/2013/01/introduction-to-wang-tiles.html), [tile genetics](http://procworld.blogspot.com/2013/01/tile-genetics.html), [GPU gems](http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter12.html)" CreationDate="2016-01-27T02:06:31.343" UserId="2457" />
  <row Id="1936" PostId="1963" Score="0" Text="@MartinBüttner good point, I've added a line about the type of texture" CreationDate="2016-01-27T04:13:21.460" UserId="31" />
  <row Id="1937" PostId="1967" Score="0" Text="Yes. I just record how painters they paint in daily life actually." CreationDate="2016-01-27T07:02:16.060" UserId="2551" />
  <row Id="1938" PostId="1963" Score="1" Text="Have you looked at texture bombing? http://http.developer.nvidia.com/GPUGems/gpugems_ch20.html" CreationDate="2016-01-27T08:24:37.303" UserId="2463" />
  <row Id="1939" PostId="1967" Score="0" Text="Any demo videos so we can test this? You could simply track the ne dof the pencil. But if it gets occluded then no dice. You could also track the evolution of the shape over frames by difference keying. Or just put the paper on a wacom tablet and change to a wacom ink pen." CreationDate="2016-01-27T08:33:28.030" UserId="38" />
  <row Id="1940" PostId="1971" Score="0" Text="One could paint on a semitransparent surface and capture image from behind, use wacom tablet trough paper and so on... Reverse difference matte should also give the evolution." CreationDate="2016-01-27T08:42:03.433" UserId="38" />
  <row Id="1941" PostId="1967" Score="0" Text="Its my test video, a very simple one. No overlap. http://1drv.ms/1UpF4x3" CreationDate="2016-01-27T09:06:40.550" UserId="2551" />
  <row Id="1942" PostId="1971" Score="0" Text="I understood he had the finished video from somewhere and now wants to reconstruct the process." CreationDate="2016-01-27T10:14:24.280" UserId="273" />
  <row Id="1943" PostId="1971" Score="0" Text="Sure but sometimes, one really can not get a good measurement in hindsight. Anyway for the test video tracing works well, and since he has a test video maybe he also hasnt set up the system yet." CreationDate="2016-01-27T10:21:52.227" UserId="38" />
  <row Id="1944" PostId="1973" Score="1" Text="Thanks for the reply Nathan, I did expect that MSDN page was incorrect from my initial experiments. I might give the clip planes solution another try when I get some time, i'll post another question when I have more information on what's going wrong. Thanks again for your help." CreationDate="2016-01-27T11:40:47.010" UserId="288" />
  <row Id="1945" PostId="1974" Score="0" Text="Im not sure it exists as such, determining wetehr or theres a possibility for 0-1 or 2 or more is pretty trivial but the formulation does not really make it ieasy to make sure its 0 or 1 without actually checking." CreationDate="2016-01-28T13:44:13.640" UserId="38" />
  <row Id="1946" PostId="1974" Score="0" Text="What is the runtime requirements? An solution that should be able to produce pretty accurate results would be to approximate both curves by a large number of short straight segments and then intersecting them in a pairwise fashion. But that costs much time and memory." CreationDate="2016-01-28T14:05:10.967" UserId="273" />
  <row Id="1947" PostId="1974" Score="0" Text="@Dragonseel Well, I would be happy for any solution, really, but since you asked O(1) would be nice. But approximating the curves with line segments leads to the same problems as the test for bounding box overlap..." CreationDate="2016-01-28T15:00:20.193" UserId="141" />
  <row Id="1948" PostId="1974" Score="0" Text="Interesting problem. I don't think there's an easy answer but I'd like to be wrong. Do you have a link for the Sederberg and Meyers paper?" CreationDate="2016-01-28T15:21:11.820" UserId="2500" />
  <row Id="1949" PostId="1974" Score="0" Text="@DanielMGessel Yes, see the edit above." CreationDate="2016-01-28T15:25:10.350" UserId="141" />
  <row Id="1950" PostId="1977" Score="0" Text="Using the identity matrix is equivalent to placing the object at the origin, with no scaling, and no rotation." CreationDate="2016-01-28T19:29:07.043" UserId="310" />
  <row Id="1951" PostId="1977" Score="0" Text="@RichieSams yes, same as putting object to world coordinates (or other parent), except theres a matrix in between to manipulate." CreationDate="2016-01-28T19:46:41.473" UserId="38" />
  <row Id="1952" PostId="1967" Score="1" Text="[Meta discussion](http://meta.computergraphics.stackexchange.com/questions/210/is-reconstruction-of-pen-strokes-from-real-life-video-on-topic) on whether this is on topic." CreationDate="2016-01-28T21:42:55.493" UserId="231" />
  <row Id="1953" PostId="1974" Score="0" Text="Are the control points of your curves 2D or 3D (or, for that matter, greater)?" CreationDate="2016-01-29T09:41:23.870" UserId="209" />
  <row Id="1954" PostId="1974" Score="0" Text="@SimonF 2D, sorry, see the edit." CreationDate="2016-01-29T09:48:31.333" UserId="141" />
  <row Id="1955" PostId="1974" Score="0" Text="(Sigh. Caught by the comment edit timeout). You also say _&quot;I also would like to use floating-point numbers in the implementation&quot;_  I suspect that you might be limited on how &quot;reliable&quot; the test will be by whatever rounding goes on in floating point. For example, if there is a huge dynamic range in the control point values, you may lose too much precision." CreationDate="2016-01-29T09:50:10.977" UserId="209" />
  <row Id="1956" PostId="1974" Score="0" Text="@Eric: Being pedantic here:  By planar do you mean the _same_ plane?" CreationDate="2016-01-29T09:55:32.927" UserId="209" />
  <row Id="1957" PostId="1974" Score="0" Text="@SimonF Yes, same plane." CreationDate="2016-01-29T09:58:28.413" UserId="141" />
  <row Id="1958" PostId="1974" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/35018/discussion-between-simon-f-and-ecir-hana)." CreationDate="2016-01-29T10:03:58.910" UserId="209" />
  <row Id="1959" PostId="1963" Score="1" Text="Can you give an idea of the range of shapes/patterns you need to cover and how much flexibility there can be in the approach? @DanielMGessel's suggestion of changing the circle radii instead of their density might make things simpler and give smoother animation but it's not yet clear how much your approach is constrained." CreationDate="2016-01-29T12:19:35.967" UserId="231" />
  <row Id="1963" PostId="1979" Score="0" Text="Do you understand how frame buffers work? https://en.wikipedia.org/wiki/Framebuffer (Caveat, it's got alot of history.) Is that some of what you are looking for?" CreationDate="2016-01-29T14:11:53.177" UserId="2500" />
  <row Id="1964" PostId="1982" Score="0" Text="Yes, but im personally more interested in the case where  about the case where the Bm and/or B0 are Both within the volume of A's max and min bound but does not pierce it then you need to subdivide and in worst case scenario calculate the intersection point. Better ways would be to use the minimum bounding box also known as thick line approximation." CreationDate="2016-01-29T15:23:46.557" UserId="38" />
  <row Id="1965" PostId="1982" Score="0" Text="Given that, with every binary subdivision, the difference between the curve and the segment connecting the end points goes down by reasonable factor (and, off the top of my head, I think it might have been 4x for quadratics) surely the bounds are going to converge to a &quot;thin&quot; ribbon fairly rapidly." CreationDate="2016-01-29T15:44:00.973" UserId="209" />
  <row Id="1966" PostId="1982" Score="0" Text="Yes but worst case scenario is that the other bezier starts at the other." CreationDate="2016-01-29T15:54:48.087" UserId="38" />
  <row Id="1967" PostId="1982" Score="0" Text="You mean, for example, _An_ == _B0_.  Do you define that as an intersection or not?" CreationDate="2016-01-29T16:01:40.370" UserId="209" />
  <row Id="1968" PostId="1982" Score="0" Text="No more like B0 is At somewhere on the curve. Or even a just minimally crossing" CreationDate="2016-01-29T16:08:39.820" UserId="38" />
  <row Id="1969" PostId="1982" Score="0" Text="Thanks a lot for the effort but I think the problem is the 6th case. If you propose to subdivide the curves further then this is not better than the overlapping bboxes because it could happen than after many subdivisions we still wont be able to answer &quot;yes&quot; or &quot;no&quot; definitely. But in ordered not to end up in infinite loop or floating point rounding errors we would have to stop at some point, basically guessing." CreationDate="2016-01-29T16:54:07.137" UserId="141" />
  <row Id="1971" PostId="1984" Score="0" Text="Blender supports Nurbs surfaces and a &quot;spin&quot; operator for curves. Is that the operation you want? If so you could maybe lookup how Blender implemented the operation." CreationDate="2016-01-29T23:53:59.457" UserId="273" />
  <row Id="1972" PostId="1973" Score="0" Text="Finally figured out why the VR prototype I did wasn't working. Turns out my prototype was a bit too simple, I was just doing the squish and move code to all vertex shaders and also doing all draws with an instance count of 2. This meant that when the last BLIT ran to copy the render target to the back buffer happened it also squish, moved and rendered twice causing the 4 views :) figured it out pretty quick once I'd captured in RenderDoc." CreationDate="2016-01-30T00:40:49.583" UserId="288" />
  <row Id="1973" PostId="1973" Score="0" Text="BTW: Thanks again for the information on the SV_RenderTargetArrayIndex in the pixel shader now I've thought about it it makes sense that the RT output would need to be consistent across all pixels." CreationDate="2016-01-30T00:41:03.577" UserId="288" />
  <row Id="1975" PostId="1984" Score="0" Text="@Dragonseel No, i know how to make a revolved surface, im only interested in how to make a silhouette curve in my own 2d software. I can do a discrete solution for this like blender would in maya, max, creo, solidworks. The only app that ive ever seen to do this acceptably is Rhino. But since this is a special case seems to me there should be a analytic solution." CreationDate="2016-01-30T06:56:11.890" UserId="38" />
  <row Id="1976" PostId="1984" Score="1" Text="@Dragonseel updated the question to refelect what i said" CreationDate="2016-01-30T08:29:24.650" UserId="38" />
  <row Id="1978" PostId="1984" Score="0" Text="If you figure out what happens in your first image when rotating around a vertical axis, the answer will generalize to all cases (others are rotations of that case in the image plane) - is that right?" CreationDate="2016-01-31T11:10:45.940" UserId="2500" />
  <row Id="1979" PostId="1984" Score="0" Text="@DanielMGessel Most likely yes, at least in the case where your shape gets thinner towards you. When the situation gets thinker towards you you get all kinds of other possible self occlusion problems. Basically this case is most likely some evolution of the swept ovals shape. But im not really certain how to find the intersection point other than by binary search which isn't really good for analytical stuff ;)" CreationDate="2016-01-31T11:16:24.907" UserId="38" />
  <row Id="1995" PostId="1984" Score="0" Text="It's a good problem. I don't know the answer - obviously ;) - just feeling it out as a problem to solve. Self occlusion - of course. That'll introduce discontinuities." CreationDate="2016-01-31T17:10:12.063" UserId="2500" />
  <row Id="1996" PostId="1986" Score="0" Text="Why is it 6th degree polynomial, and not 3rd, if we are talking about cubic Beziers? And the two methods you linked to, are they amenable to finding solutions only in $[0,1]^2$, as opposed to whole $R^2$?" CreationDate="2016-01-31T19:51:51.933" UserId="141" />
  <row Id="1997" PostId="1940" Score="0" Text="I'd still have to calculate for each point gravity for each object. This seems to be the expensive part" CreationDate="2016-01-31T20:33:44.037" UserId="2524" />
  <row Id="1998" PostId="1986" Score="1" Text="@EcirHana It's 6th degree because it's the squared distance. (You could square-root it, but then it's no longer polynomial, and will be non-smooth at the zeroes.) Note that the $[0,1]$ is the parameter space, not the space the splines live in, i.e. these are splines with endpoints. In any case, the methods will work fine in $\mathbb{R}^2$, but they only &quot;travel downhill&quot; from an initial guess and find a local minimum; something more is needed to examine the whole parameter region and find the _global_ minimum. Constraining the parameter space is probably helpful there." CreationDate="2016-01-31T22:00:52.463" UserId="48" />
  <row Id="2000" PostId="1986" Score="1" Text="Nathan - nice formulation! I'm rusty, but: I think you can divide each bezier curve into at most 5 segments, by where $x$ or $y$ change direction in the curve. $x$, as a function of $c_i$ changes direction at most twice (roots of the derivative) breaking the curve into 3 segments, 2 of which may be divided again by changes in direction of $y$. Now you have, not straight segments, but segments that &quot;don't curve too much&quot;. I think if you start your search at 25 points, chosen by segment pairs, you could be always find the global minima, but I can't quite see how to prove (or disprove) it." CreationDate="2016-02-01T03:47:09.483" UserId="2500" />
  <row Id="2001" PostId="1982" Score="0" Text="Oops! I'd renumbered the initial list but not the solution cases!" CreationDate="2016-02-01T08:50:38.250" UserId="209" />
  <row Id="2002" PostId="1986" Score="0" Text="@Nathan: I had considered that but, having spent much time writing code to find minima in texture compression formats, it all seemed a bit hideous." CreationDate="2016-02-01T13:30:27.697" UserId="209" />
  <row Id="2003" PostId="1987" Score="0" Text="What's the paper you mentioned in your first sentence? It might give helpful context. Otherwise, it does sound like these are two mutually incompatible uses of the word &quot;fluence&quot;." CreationDate="2016-02-01T17:29:09.283" UserId="48" />
  <row Id="2004" PostId="1987" Score="0" Text="I made an edit to include a link to the paper. Is there a preferred format for referencing papers?" CreationDate="2016-02-01T18:00:59.183" UserId="2457" />
  <row Id="2007" PostId="1493" Score="1" Text="This should go somewhere in this thread, so adding it here :) Inigo Quilez's analytic sphere projection: https://www.shadertoy.com/view/XdBGzd" CreationDate="2016-02-02T12:11:49.200" UserId="2463" />
  <row Id="2008" PostId="1995" Score="0" Text="The only thing I can think of is doing the perspective projection on the CPU, and using an ortho matrix... Does that work?" CreationDate="2016-02-03T14:07:19.980" UserId="2500" />
  <row Id="2009" PostId="1995" Score="5" Text="why not use glsl? You are going to need to learn to program a shader eventually why not now." CreationDate="2016-02-03T14:54:31.353" UserId="137" />
  <row Id="2010" PostId="1995" Score="5" Text="It should be possible to do this using projective texture mapping, which can be done in fixed-function by providing 4-dimensional texcoords. I'd have to sit down and do the math to work out the details though." CreationDate="2016-02-03T16:48:34.523" UserId="48" />
  <row Id="2021" PostId="1999" Score="0" Text="Awh shoot, just realized [Geometry shaders aren't supported in WebGL](http://stackoverflow.com/questions/8641119/webgl-geometry-shader-equivalent). :|" CreationDate="2016-02-04T05:57:24.597" UserId="71" />
  <row Id="2022" PostId="1999" Score="2" Text="Why not make that your answer. Then this question is not as purposeless as it seems. (or delete offcourse)" CreationDate="2016-02-04T08:26:01.440" UserId="38" />
  <row Id="2023" PostId="1999" Score="0" Text="@joojaa done and done :)" CreationDate="2016-02-04T09:29:33.060" UserId="71" />
  <row Id="2024" PostId="1997" Score="0" Text="@trichoplax which part of it you did not understand?" CreationDate="2016-02-04T23:53:47.483" UserId="2602" />
  <row Id="2025" PostId="1997" Score="0" Text="`transform bounding sphere radius and center of sphere in 0-1` doesn't make clear whether you want the radius normalised to 1, or transformed to some value between 0 and 1 (in which case, how is that value determined?). The same for the centre - is it to be moved to the origin or translated to somewhere within a distance 1 from the origin or something else?" CreationDate="2016-02-05T04:56:14.470" UserId="231" />
  <row Id="2026" PostId="1997" Score="0" Text="`somebody pointed out to me that its not possible to transform radius into 0-1` From the comments on your [initial question](http://computergraphics.stackexchange.com/questions/1996/calculate-bounding-sphere-radius-in-normalized-space) it doesn't appear that anyone is saying that it is not possible to transform the radius. We just need to know what you mean by &quot;0-1&quot;." CreationDate="2016-02-05T05:00:12.693" UserId="231" />
  <row Id="2027" PostId="1997" Score="0" Text="@trichoplax okay. I changed my algorithm. Can you check it" CreationDate="2016-02-05T05:35:09.910" UserId="2602" />
  <row Id="2028" PostId="2004" Score="0" Text="I'm accepting this answer because the paper itself is good and its &quot;Related work&quot; section seems pretty comprehensive. Even if I don't end up using this technique exactly, I'm sure I'll be able to tailor something for my use case from this and its references." CreationDate="2016-02-05T09:42:17.403" UserId="2041" />
  <row Id="2029" PostId="1963" Score="0" Text="How is define the pattern ? as an image of pixels, or as a noise function, a procedural field, etc, with some threshold to make in B&amp;W ? in the letter case you could adapt the value of the threshold.&#xA;&#xA;Anyway, we need more information about what is the texture to be able to answer." CreationDate="2016-02-06T06:25:19.577" UserId="1810" />
  <row Id="2030" PostId="1997" Score="1" Text="This seems clearer now. I've edited to add in links and hovertext for the acronyms. If this changes your intention please edit to correct this." CreationDate="2016-02-06T13:18:48.847" UserId="231" />
  <row Id="2031" PostId="2006" Score="1" Text="There's a detailed look into Quake's engine [in this article](http://fabiensanglard.net/quakeSource/index.php). You'll also find one on [Quake 2](http://fabiensanglard.net/quake2/index.php) on the same site. I think there's a link there somewhere to a GDC presentation talk by Michael Abrash where he briefly talks about the visibility problem and how it was solved using portals." CreationDate="2016-02-06T18:01:34.113" UserId="54" />
  <row Id="2032" PostId="2006" Score="1" Text="Ah found it! That's the video I mentioned: http://www.gdcvault.com/play/1014234/Excerpt-Quake-Postmortem-Optimizing-Level" CreationDate="2016-02-06T18:08:16.550" UserId="54" />
  <row Id="2035" PostId="1995" Score="0" Text="@DanielMGessel I think that's a good idea. I'll try." CreationDate="2016-02-07T05:05:58.510" UserId="2600" />
  <row Id="2036" PostId="1995" Score="0" Text="@ratchetfreak Actually it's a project requirement..." CreationDate="2016-02-07T05:06:37.673" UserId="2600" />
  <row Id="2037" PostId="2008" Score="2" Text="Maybe your model of the box has a wrong normal." CreationDate="2016-02-07T10:05:01.643" UserId="1888" />
  <row Id="2038" PostId="1995" Score="1" Text="Nathan's right - you can undo perspective projection with a 4th coordinate (I too would have to work out the math); look into texture coordinate generation. I have vague memories that on (very) old hardware precision can crop up as an issue (I think implementing stipple this way was error prone - I never had to do it personally) - so if old hardware is why you're not able to use shaders, you may bump into some oddities at the pixel level. But it is a better approach, especially if you have alot of geometry or if you are using vertex buffers." CreationDate="2016-02-07T12:54:25.723" UserId="2500" />
  <row Id="2039" PostId="2008" Score="2" Text="This might not be the error, but you have a uniform &quot;viewPos&quot; which is, I assume, the camera position. But you do all calculations in Camera-Space, so this should always be $(0.0, 0.0, 0.0)$ and thereby the **viewDir** becomes $-1 * fragPos$." CreationDate="2016-02-07T14:30:11.230" UserId="273" />
  <row Id="2040" PostId="2011" Score="1" Text="the first parameter of glVertexAttribPointer should be queried with glGetAttribLocation" CreationDate="2016-02-07T23:50:21.737" UserId="137" />
  <row Id="2042" PostId="2012" Score="0" Text="Do you see that working well when you have N grid cells, each of which may or may not have a sphere in it that you need to test your ray against?  Maybe a bvh would be better when the grid is mostly empty, but have a worse worst case? Hrm.." CreationDate="2016-02-08T02:40:57.633" UserId="56" />
  <row Id="2043" PostId="2012" Score="0" Text="I was thinking using it for the (real) sphere set only, after a first cut to the plate &quot;skin&quot;, as you said.&#xA;&#xA;Note that dynamics loops are not totally an issue as long as they are bounded; you can break/return/continue using an if inside. only the longest length for a given warp will be applied to all of its pixel.s The only issue is the code length, since loops and functions are unrolled." CreationDate="2016-02-08T03:13:24.527" UserId="1810" />
  <row Id="2044" PostId="2014" Score="0" Text="Is it possible that the solid red is a hole?" CreationDate="2016-02-08T14:12:08.473" UserId="137" />
  <row Id="2046" PostId="2014" Score="0" Text="No I have checked that, and it is not" CreationDate="2016-02-08T15:14:53.877" UserId="2630" />
  <row Id="2047" PostId="2014" Score="1" Text="Can you upload your code somewhere (gist, pastebin, etc.) and link it? We might be able to spot the error in the code." CreationDate="2016-02-08T15:37:48.797" UserId="310" />
  <row Id="2048" PostId="2008" Score="0" Text="@Dragonseel  Yup!! That was the problem! Thank you for taking out some time for this question!" CreationDate="2016-02-08T17:41:38.177" UserId="2096" />
  <row Id="2049" PostId="2007" Score="0" Text="I can not calculate min max corner in modelspace as after multiplying with view and projection matrices, min and max corner does not remains the same. that is why I am iterating over all the vertices.Previously I thought just using bounding box would suffice but looks like it is changing in clip coordinate space." CreationDate="2016-02-08T18:09:17.637" UserId="2602" />
  <row Id="2050" PostId="2014" Score="4" Text="Problem is most likely in your normal computation. The end is a singularity in the ubderlying parametric shape which can yeld bad results if you dont know what your doing. The visible artefact number 2 is pretty common in these cases." CreationDate="2016-02-08T22:25:12.420" UserId="38" />
  <row Id="2051" PostId="2007" Score="0" Text="when I checked NDC coordinates at the end in both ways, I get very different numbers. with iteration over vertices I get vertex cooridinates in range of minus thousands to plus thousands whereas with bounding boxes they are in range of -10 plus 10 approx." CreationDate="2016-02-09T00:48:54.703" UserId="2602" />
  <row Id="2053" PostId="2014" Score="0" Text="@joojaa, could you explain more on the second sentence?" CreationDate="2016-02-09T08:06:49.157" UserId="2630" />
  <row Id="2055" PostId="1995" Score="0" Text="@DanielMGessel I figured it out. Just use(sz,tz,0,z) and it works, where z is z-coordinate in eye space." CreationDate="2016-02-09T08:48:56.447" UserId="2600" />
  <row Id="2056" PostId="2016" Score="2" Text="The normal depth value is not linear and goes against 1 relatively quickly. Are you sure it is really white as in value 1.0? Could be that the values are just close to 1.0 so that you cannot visibly distinguish them? (there are only 255 white-gray-black values but far more values in a big depth-buffer)." CreationDate="2016-02-09T10:30:13.477" UserId="273" />
  <row Id="2058" PostId="2018" Score="2" Text="Oh dear, 2 answers and no picture computer graphicists on their best." CreationDate="2016-02-09T14:03:32.957" UserId="38" />
  <row Id="2064" PostId="2020" Score="1" Text="@Nicol Thank you. I edited the answer accordingly." CreationDate="2016-02-09T23:54:48.630" UserId="273" />
  <row Id="2067" PostId="2023" Score="2" Text="The range of numbers on the axes change between the three diagrams, and the 2nd and 3rd have more vertices labelled. Are these transformed versions of the same cube, or three separate shapes?" CreationDate="2016-02-10T03:43:37.407" UserId="231" />
  <row Id="2068" PostId="2023" Score="0" Text="There is [information about syntax highlighting on Meta Stack Exchange](http://meta.stackexchange.com/questions/184108/what-is-syntax-highlighting-and-how-does-it-work) but it appears that Matlab is not supported." CreationDate="2016-02-10T03:54:42.653" UserId="231" />
  <row Id="2071" PostId="2023" Score="0" Text="The first shape is an example of one of the composite worldspace shapes in its local coordinates with normals drawn on. The middel and right hand plots is the same worldspace shape transformed" CreationDate="2016-02-10T11:38:40.393" UserId="2646" />
  <row Id="2072" PostId="2023" Score="3" Text="Just FYI, in a GPU, the back face culling is done (or can be considered to be done) on the post-projection values of the vertices, and thus only needs to consider the X&amp;Y values of the 3 triangle vertices. (There are a few reasons for this: avoiding problems with floating point rounding being one)" CreationDate="2016-02-10T11:53:46.063" UserId="209" />
  <row Id="2073" PostId="2026" Score="0" Text="Thanks for the comment, updated my post. This is my first post on stackoverflow so I appreciate all constructive criticism haha" CreationDate="2016-02-10T15:26:21.163" UserId="2651" />
  <row Id="2074" PostId="2026" Score="1" Text="This is a significant improvement! Note that [so] and [computergraphics.se] are different sites. They are both part of the Stack Exchange network but they have different rules about what is on topic. If there's any confusion feel free to drop in to [chat]." CreationDate="2016-02-10T15:37:30.790" UserId="231" />
  <row Id="2075" PostId="2029" Score="1" Text="In principle, I think automatic has an advantage: when using the same vertex shader with a variety of fragment shaders, some attributes may not propogate to the output of a given pixel shader and the compiler can detect this and inform the application, allowing optimization of the vertex layout. In practice, I don't take advantage of this (for a few reasons). I too am curious to hear what the downside to automatic is..." CreationDate="2016-02-11T05:21:48.507" UserId="2500" />
  <row Id="2076" PostId="2030" Score="1" Text="That's just a warning that the vram is getting over-committed and could cause more memory transfers." CreationDate="2016-02-11T11:06:37.830" UserId="137" />
  <row Id="2077" PostId="2014" Score="1" Text="Your question would definitely be more answerable if you included relevant parts of the code." CreationDate="2016-02-11T12:36:04.533" UserId="16" />
  <row Id="2078" PostId="2031" Score="2" Text="Where have you seen that term? Stereo 3D reconstruction is a special case of multi-view reconstruction as far as I know. I have never seen the term &quot;multi-view stereo&quot;." CreationDate="2016-02-11T14:23:38.730" UserId="273" />
  <row Id="2080" PostId="2031" Score="1" Text="For example http://dl.acm.org/citation.cfm?id=1153518" CreationDate="2016-02-11T14:31:27.930" UserId="2670" />
  <row Id="2081" PostId="2016" Score="0" Text="I tried it from different view points and with different entities. But still all I can see is a wait blank texture." CreationDate="2016-02-11T14:59:10.137" UserId="2637" />
  <row Id="2082" PostId="2016" Score="0" Text="How do you check the framebuffer texture? And have you tried to leave out the 'out_color' from the fragment shader?" CreationDate="2016-02-11T15:16:53.333" UserId="273" />
  <row Id="2083" PostId="2030" Score="0" Text="@ratchetfreak: That's just a side effect then? I still don't understand why the program stops working though." CreationDate="2016-02-11T17:08:21.130" UserId="2666" />
  <row Id="2084" PostId="2030" Score="0" Text="Can you share OS + GPU details?" CreationDate="2016-02-12T02:19:05.593" UserId="2500" />
  <row Id="2086" PostId="2034" Score="6" Text="https://en.wikipedia.org/wiki/ClearType or more generally https://en.wikipedia.org/wiki/Subpixel_rendering" CreationDate="2016-02-12T08:44:20.430" UserId="457" />
  <row Id="2088" PostId="2014" Score="0" Text="What is the source of your teapot? Glut-teapot?" CreationDate="2016-02-12T09:57:09.787" UserId="2463" />
  <row Id="2090" PostId="2035" Score="0" Text="If you know the final display size, scaling your original, highest quality image to the display size with a good graphics application should produce as good as or better than letting the display system do it for you (which is likely to cut corners for performance/simplicity)." CreationDate="2016-02-12T15:08:23.783" UserId="2500" />
  <row Id="2091" PostId="2016" Score="0" Text="I'm using a gui system to show the texture and then I screenshot it to check it with better detail in a editor program. I removed the 'out_color' but still it is just blank white space." CreationDate="2016-02-12T15:35:00.743" UserId="2637" />
  <row Id="2092" PostId="2030" Score="0" Text="@DanielMGessel: Yes, I am using Linux Mint 17.2 and my GPU is a GTX 980Ti with 6GB of VRAM." CreationDate="2016-02-12T17:00:26.943" UserId="2666" />
  <row Id="2093" PostId="2037" Score="1" Text="The object in the right hand image does appear to be at a similar angle to the middle image. It's just that the axes are not the same. Is it only the axes you are asking about, or does the object itself have a problem?" CreationDate="2016-02-12T19:20:25.223" UserId="231" />
  <row Id="2094" PostId="2030" Score="0" Text="The message seems to be that a buffer in VRAM is getting straight up discarded - unless you are using a ton of VRAM for something else, or you have locked up the GPU - this could be a symptom what would be a TDR on windows - if you submit a command buffer that takes too long. But that should just kill your command buffer, not your data. You could try changing the bits passed to glBufferStorage to make it dynamic and see what happens. Crosspost on an NV message board and/or file a bug report." CreationDate="2016-02-12T20:02:26.917" UserId="2500" />
  <row Id="2095" PostId="2037" Score="0" Text="It's essentially the axes - I would have thought that the axes should be rotated to look like the central figure. Instead, it's slightly off, as in the right figure" CreationDate="2016-02-12T20:36:08.070" UserId="2646" />
  <row Id="2096" PostId="2016" Score="0" Text="Okay.  My last idea is that you try to bind a color attachment that you write a static random color into. Maybe there is some optimisation bug. I got my shadow mapping running with such a &quot;fake&quot; texture attached." CreationDate="2016-02-12T21:20:02.317" UserId="273" />
  <row Id="2097" PostId="2016" Score="0" Text="When you say GUI-system do you mean some software or a self written texture renderer with orthogonal projection? If self written you may have a slight error there an hour could give the relevant shader that reads the shadow map." CreationDate="2016-02-12T21:22:34.177" UserId="273" />
  <row Id="2101" PostId="2042" Score="0" Text="I was writing ray-tracing instead of raytracing, that was the error" CreationDate="2016-02-14T00:52:34.470" UserId="2684" />
  <row Id="2103" PostId="2041" Score="2" Text="As for the code, there are many examples online. Take a look at SO: http://stackoverflow.com/questions/1659440/32-bit-to-16-bit-floating-point-conversion" CreationDate="2016-02-14T16:57:40.007" UserId="54" />
  <row Id="2105" PostId="2045" Score="0" Text="Cool - it's nice to have an alternative to AVX. f11_f11_f10 is my priority right now, as I use it for all my linear space/HDR rendering." CreationDate="2016-02-14T22:52:45.997" UserId="2500" />
  <row Id="2108" PostId="2030" Score="0" Text="@DanielMGessel: I think it has something to do with how long my compute shader is working. When I am using a simple compute shader where I iterate through the whole buffer and compute an average over all the values inside, then it works with bigger buffer sizes. Are there any rules on how long a shader can be working? My error appeared when the shader was working for more than ~8 seconds." CreationDate="2016-02-15T07:55:24.873" UserId="2666" />
  <row Id="2109" PostId="2045" Score="1" Text="I've added some links to R11G11B10_Float conversion implementations." CreationDate="2016-02-15T08:29:16.037" UserId="93" />
  <row Id="2114" PostId="2045" Score="0" Text="Beautiful! Thanks!" CreationDate="2016-02-15T10:59:09.793" UserId="2500" />
  <row Id="2115" PostId="2043" Score="1" Text="&quot;The only difference is that half edge uses directional edge and winged edge uses undirectional edge.&quot; From my understanding, more like: [Half-edge](https://en.wikipedia.org/wiki/Doubly_connected_edge_list) is doubly linked (and each direction may contain additional information), while [winged edge](https://en.wikipedia.org/wiki/Winged_edge) is, most commonly, counter-clockwise only." CreationDate="2016-02-15T11:28:57.173" UserId="385" />
  <row Id="2116" PostId="88" Score="1" Text="The best way to do this is probably using atomic operations for the scattered accumulation operations (when multiple rays may hit the same point), but you could render your entire &quot;ray cloud&quot; as points and do the intersection and occlusion calculations in a vertex shader, use a passthrough pixel shader and let the HW blender do the accumulation." CreationDate="2016-02-15T13:55:00.150" UserId="2500" />
  <row Id="2117" PostId="2049" Score="0" Text="It is customary for the camera to face in the negative z direction. Perhaps you have view in positive direction? Sorry no time to debug." CreationDate="2016-02-15T20:57:51.090" UserId="38" />
  <row Id="2118" PostId="2049" Score="0" Text="I'm unsure if I got the camera facing in the right direction - my main thought was that the view frustum was still point towards the object from the origin so that should be satisfactory? Either way I dont think it transforms correctly...." CreationDate="2016-02-15T21:16:40.003" UserId="2646" />
  <row Id="2120" PostId="2046" Score="0" Text="Links aren't allowed to be answers by themselves,  but here is a link that explains it pretty well, and has simple working source code: http://blog.demofox.org/2015/07/28/rectangular-bezier-patches/" CreationDate="2016-02-16T00:37:18.353" UserId="56" />
  <row Id="2121" PostId="2050" Score="1" Text="Follow your answer, I found out that the Modified Conjugate Gradient method is proposed to deal with the non-symmetric semi-positive-definite linear equation(section 5.2). Thanks a lot." CreationDate="2016-02-16T01:14:47.317" UserId="2593" />
  <row Id="2123" PostId="2043" Score="0" Text="So, you mean the way they use doubly linked simply to add more info explicitly? Because I think by using Half Edge there might be some performance gained for specific query from the mesh. But till now, I still cannot figure which query.." CreationDate="2016-02-16T07:28:43.660" UserId="2687" />
  <row Id="2124" PostId="2047" Score="0" Text="I don't think it is what I had to begin with - although on a brief glance it should seem that e &lt;=&gt; C and  x,y,z &lt;=&gt; U,V,N, when I change the code to use your method, the results are starting to seem more intuitively correct!" CreationDate="2016-02-16T10:13:23.190" UserId="2646" />
  <row Id="2125" PostId="2046" Score="0" Text="@AlanWolfe , that's a great article which explains much, but nevertheless it says about 16 control points for bicubic surface, which I'm trying to evade" CreationDate="2016-02-16T10:43:47.063" UserId="2694" />
  <row Id="2126" PostId="2052" Score="0" Text="I've performed the perspective divide - (see update) - and my result is now a cube that has coordinates of +/- 1 in all axes (seems right!) although the z plane has been reflected such that the far plane is now at +1 and the near plane at -1 - is this correct? Thanks very much" CreationDate="2016-02-16T10:58:02.583" UserId="2646" />
  <row Id="2127" PostId="2047" Score="2" Text="In your matrix you have a &quot;c&quot; superscript on the x, y and z components (camera space). I added a description of how to derive the eye local space based on the gluLookat parameters. I think that pyramid in front is pretty confusing; I'd use one box to start with. Try plotting looking down the axi first, make sure you get the correct rectangles. Also, anything that can let you animate changing the parameters will help clear things up immensely. A single wireframe picture is always hard to decode..." CreationDate="2016-02-16T12:31:51.497" UserId="2500" />
  <row Id="2128" PostId="2046" Score="0" Text="Ah, sorry for missing that part" CreationDate="2016-02-16T14:25:53.657" UserId="56" />
  <row Id="2129" PostId="2052" Score="0" Text="Additionally - should the aspect ratio of the view volume be taken into account in the size - or does this simply lead to a slight deformation of the shape rather than the [1, -1] cube deformation?" CreationDate="2016-02-16T15:16:48.743" UserId="2646" />
  <row Id="2130" PostId="2048" Score="0" Text="thank you. Do I understand you correctly, that in my case using NURBS the user could first specify points on surface, and then fine-tune the surface by fine-tuning section curves? And I still cannot get how I can make Bezier (or NURBS) patch with control points and knots only on edges. Can you show your second image with control points and edges?" CreationDate="2016-02-16T16:43:55.223" UserId="2694" />
  <row Id="2131" PostId="2048" Score="0" Text="@morodeer theres are infinite ways of doing this. But i was drawing a image for you allready just dont have time to complete it" CreationDate="2016-02-16T17:19:11.770" UserId="38" />
  <row Id="2132" PostId="2052" Score="1" Text="@davidhood2 That sounds right, yes. Post-projective space is left-handed (+x = right, +y = up, +z = into screen). As for the aspect ratio, that should be handled by x and y scale factors in the projection matrix. Post-projection, x and y should be in [−1, 1] regardless of aspect ratio." CreationDate="2016-02-16T17:28:29.150" UserId="48" />
  <row Id="2133" PostId="2048" Score="0" Text="I think, I understood almost everything: should I use the same algorithm for mid-points? PS: I think there are some mistakes in points and lengths, but image makes it clear." CreationDate="2016-02-16T18:40:20.657" UserId="2694" />
  <row Id="2134" PostId="2048" Score="0" Text="@morodeer yes same algo works fine" CreationDate="2016-02-16T19:00:10.883" UserId="38" />
  <row Id="2135" PostId="2048" Score="0" Text="ok, and the algorithm you described looks more like blending between 3 curves, but not as interpolating between curve network, right? Do you know equally elegant way to interpolate mid-points in case of curve network?" CreationDate="2016-02-16T19:02:51.053" UserId="2694" />
  <row Id="2136" PostId="2048" Score="0" Text="@morodeer Thats what you asked. It is forming a interpolation, curve networks just split up and insert knots in the underlying surface least if its a square case," CreationDate="2016-02-16T19:27:37.363" UserId="38" />
  <row Id="2137" PostId="2043" Score="0" Text="While we're on edge-representations, this is a great paper, generalising a lot of them: http://graphics.cs.ucdavis.edu/~joy/ecs178/Unit-9/resources/Tahoe-Paper.pdf" CreationDate="2016-02-16T19:56:33.423" UserId="2463" />
  <row Id="2138" PostId="2058" Score="0" Text="Though in general it is far easier to just use the construction joojaa explained in his answer." CreationDate="2016-02-16T23:27:35.177" UserId="273" />
  <row Id="2139" PostId="2060" Score="3" Text="I don't know a lot about voxel-based rendering, but I always got the impression voxel-based raytracers intersected the rays directly with the voxels, instead of constructing a polygonal representation of the voxels first." CreationDate="2016-02-17T15:14:39.980" UserId="16" />
  <row Id="2140" PostId="2060" Score="0" Text="Neither am I, I just read about rendering using voxelization so I might have a wrong idea on the technique." CreationDate="2016-02-17T15:26:39.277" UserId="116" />
  <row Id="2141" PostId="1861" Score="0" Text="The problem for me is projecting the actual beam, like if the light is passing through fog, from the source to the floor... The simplest way i think would be &quot;projecting&quot; a fake beam, a cone of a fixed lenght, made of triangles, from the lens of the light, towards the direction in which the light is facing, with some alpha blending." CreationDate="2016-02-17T17:28:01.917" UserId="2388" />
  <row Id="2142" PostId="1861" Score="0" Text="But this way the beam would pass through the objects, so i need a way to stop it at the first intersection... a very simple way would be raytracing the distance from the center of the lens to the floor, and then making the cone of that lenght. but its not a good way..." CreationDate="2016-02-17T17:34:00.240" UserId="2388" />
  <row Id="2143" PostId="2053" Score="0" Text="Where the lens are located in your model? Onto the screen plane? Where is the aperture size?" CreationDate="2016-02-17T18:15:08.913" UserId="2684" />
  <row Id="2144" PostId="2059" Score="0" Text="Thank you very much for your answer, could you tell me more about stratified sampling and low-discrepancy sequence ?" CreationDate="2016-02-17T18:38:36.007" UserId="2372" />
  <row Id="2145" PostId="2029" Score="0" Text="The accepted answer on this post argues for the pro-explicit side, maybe you could take a look at why he thinks so.&#xA;http://stackoverflow.com/questions/4635913/explicit-vs-automatic-attribute-location-binding-for-opengl-shaders" CreationDate="2016-02-17T19:30:35.047" UserId="2651" />
  <row Id="2146" PostId="2029" Score="0" Text="Reusing VAOs with multiple shaders that use the same vertex layout. If you let the compiler decide, you have potentially need a VAO for each VB for each compiled program, even if they use exactly the same attributes. Sounds good to me! I should look up what Vulkan does, now that the spec is out..." CreationDate="2016-02-18T02:01:04.087" UserId="2500" />
  <row Id="2147" PostId="2063" Score="0" Text="What transformations are you applying? Or are these coordinates directly in clip space with no transformations? And how are you triangulating the square?" CreationDate="2016-02-18T04:21:46.547" UserId="48" />
  <row Id="2148" PostId="2063" Score="0" Text="No transformations, I edited my post to show code" CreationDate="2016-02-18T04:25:14.960" UserId="2651" />
  <row Id="2157" PostId="2068" Score="0" Text="Wouldn't `norm` be better at normalizing also you can write `R[U, 0; V, 0; -N, 0; 0, 0, 0, 1]`" CreationDate="2016-02-18T22:23:28.900" UserId="38" />
  <row Id="2158" PostId="2068" Score="1" Text="You are right about norm, but sqrt(dot(x,x)), is the same (I do not know octave/matlab very good, so I use math). Regarding matrix - not really, because U, V, N are columns, correct way would be R = [U', 0; V', 0; -N', 0; 0,0,0,1] :-) but I tried to keep original formatting." CreationDate="2016-02-18T22:34:29.107" UserId="43" />
  <row Id="2159" PostId="2068" Score="0" Text="Its the same but really ease of reading would make the code much better... As the original code is a totall mess. Which is mostly why there were not many takers. And the fact that its matlab..." CreationDate="2016-02-18T22:47:25.633" UserId="38" />
  <row Id="2160" PostId="2062" Score="0" Text="Thanks for the great answer! Would it be possible to allocate texture or geometry data to just individual cards then? The link seemed to indicate that computing could be distributed, which is makes sense, but I'm still confused as to how or if geometry or textures could be split." CreationDate="2016-02-19T02:12:22.923" UserId="2707" />
  <row Id="2161" PostId="2062" Score="0" Text="@aces Yes, you can allocate geometry or textures to just individual cards—you'd just create those resources only on one node. Every time you create a resource, you specify which node to put it on, so you have total control over which GPUs get copies of which resources." CreationDate="2016-02-19T02:20:00.133" UserId="48" />
  <row Id="2162" PostId="2062" Score="0" Text="That makes sense, but would that be feasible in a game? I would think you would have to synchronize all the fragments if you did that somehow." CreationDate="2016-02-19T02:22:04.977" UserId="2707" />
  <row Id="2163" PostId="2062" Score="0" Text="@aces &quot;Synchronize all the fragments&quot;? You've lost me. :)" CreationDate="2016-02-19T02:22:49.977" UserId="48" />
  <row Id="2164" PostId="2062" Score="0" Text="Sorry :). In a simple example, if you have one model with a certain texture on one GPU and another model with a different texture on another GPU, they would both need to be rasterized and shaded, so the frame buffer would need to contain both of this information in the end for depth tests. I guess I'm just confused on how that could utilize VRAM stacking." CreationDate="2016-02-19T02:25:53.217" UserId="2707" />
  <row Id="2165" PostId="2062" Score="3" Text="@aces OK, yeah. If you distribute the rendering across GPUs in some way, you have to put the results back together somehow afterward. So in your example, after rendering, you could copy the color &amp; depth buffers from GPU2 back to GPU1, and use GPU1 to composite the two frames together. That would take some extra time, which eats into the time you saved by distributing the rendering in the first place, so it might or might not be an overall perf win depending on circumstances." CreationDate="2016-02-19T02:33:24.280" UserId="48" />
  <row Id="2166" PostId="2062" Score="0" Text="Ah ok, so I guess if the bus latency/bandwidth isn't too bad that could work. Thanks for the clarification!" CreationDate="2016-02-19T02:36:16.770" UserId="2707" />
  <row Id="2167" PostId="2071" Score="0" Text="Ah cool neat idea. And yeah, I'm just doing nearest neighbor sampling. I'll edit the question to reflect that." CreationDate="2016-02-19T05:14:05.223" UserId="56" />
  <row Id="2168" PostId="2071" Score="1" Text="Given you are storing 0-1 values where, probably, having high precision near 0 is of little value, the exponents of the f16 may be wasted space. Something to think about if you are looking to tighten things up as much as possible. Bit packing can be especially useful in vertex data before any interpolation happens; in one case, I store texture array coordinates in 4 bytes and use the alpha channel to add extra bits to x &amp; y; this gives me texel precise addressing for up to a 4k x 4k x 256 texture." CreationDate="2016-02-19T05:29:18.677" UserId="2500" />
  <row Id="2169" PostId="2071" Score="0" Text="Good to know! In my case unfortunately all I have is a full screen pixel shader, I don't have geometry. But, on the plus side, the buffer can store values outside of 0 to 1.  It's a full 16 bit floating point number which is nice." CreationDate="2016-02-19T05:33:37.603" UserId="56" />
  <row Id="2170" PostId="2071" Score="0" Text="When you talk about the exponent bits being wasted are you talking about doing something like storing two values in a single float, where you make them be of different scales and add then, so that you can use mod and div to get the values out again? Do you have any working details of that by chance?" CreationDate="2016-02-19T05:52:11.697" UserId="56" />
  <row Id="2171" PostId="2072" Score="0" Text="Slerp is a possibility for normals. It stands for &quot;spherical lerp&quot;—unit vectors lie on a sphere as much as unit quaternions do." CreationDate="2016-02-19T07:08:04.263" UserId="196" />
  <row Id="2172" PostId="2071" Score="1" Text="Basically, the exponent scales an otherwise fixed point value (the mantissa) with the size of the value. If you don't benefit from that, then fixed point numbers (ints or GL &quot;norms&quot;) utilize the bits better. It may not apply in your case for any number of reasons (does WebGL support integers? I'm out of date). I do use mod and div to unpack when I need to (also, before GLSL supported ints)." CreationDate="2016-02-19T13:28:07.083" UserId="2500" />
  <row Id="2173" PostId="2071" Score="1" Text="I can rant on floats, so I won't go there. Fixed point can be a better choice (ask if more precision near 0 is useful - with positional values fp is often more of a convenience; in a linear color space, fp matches our perceptual system beautifully). Just something to consider when concerned with bandwidth." CreationDate="2016-02-19T13:56:30.313" UserId="2500" />
  <row Id="2174" PostId="2072" Score="1" Text="I've found out that nlerp is a decent method: just interpolate your components and re-normalize.  I can do a bicubic version of this by doing each component individually.  Found some decent links too: https://keithmaggio.wordpress.com/2011/02/15/math-magician-lerp-slerp-and-nlerp/  and http://number-none.com/product/Understanding%20Slerp,%20Then%20Not%20Using%20It/ ." CreationDate="2016-02-19T14:50:07.500" UserId="56" />
  <row Id="2175" PostId="2072" Score="1" Text="If you know your grid vectors don't vary more than, say, 120 degrees  from grid point to grid point (i.e. trying to avoid problems with &quot;which way do you interpolate), why wouldn't bicubic interpolation with renormalisation be &quot;good enough&quot;?" CreationDate="2016-02-19T14:55:46.257" UserId="209" />
  <row Id="2176" PostId="2069" Score="0" Text="I am currently using the code to provide 2d rendering support for sprites that position the image pixel perfect to the desired position on the screen in pixel coordinates instead of opengl coordinates. As such I currently do not plan to allow batch rendering of multiple quads but instead use the quad as a &quot;stamp&quot; to render the image to the screen. As such each quad drawn has of course a different texture or different texture coordinates to read from. As such I also never know how many quads are drawn beforehand." CreationDate="2016-02-19T19:19:33.343" UserId="2623" />
  <row Id="2177" PostId="2069" Score="1" Text="@salbeira OK. You could still combine sprites in an atlas to allow instancing. Even if you don't do that, though, you can still batch the uniform buffer updates. And you can batch even while not knowing how many quads are drawn beforehand—I added a paragraph to the answer about that." CreationDate="2016-02-19T20:08:47.280" UserId="48" />
  <row Id="2178" PostId="2069" Score="0" Text="I can imagine how this should work but it still seems a bit like a mystery to me. Like when I supply a method to &quot;render a single spirte&quot; that has a texture attached to it and information about where to find the sprite within that texture, I'd need to pile that data up and store it in a huge prepared array. When I want to draw that I'd still need to call a drawElements for each sprite, and bind each texture that contains that sprite sparately. Do I magically combine each texture with a framebuffer to a huge atlas before rendering? Or should I just say to the application dev to use an atlas?" CreationDate="2016-02-20T03:04:03.583" UserId="2623" />
  <row Id="2179" PostId="2074" Score="0" Text="After you fix the main problem, if you want better image quality you should try bicubic interpolation instead of bilinear.  http://blog.demofox.org/2015/08/15/resizing-images-with-bicubic-interpolation/" CreationDate="2016-02-20T05:41:31.877" UserId="56" />
  <row Id="2180" PostId="2069" Score="0" Text="@salbeira Yeah, designing the interface for this sort of thing is tricky. You could supply a method to &quot;add a sprite to the render list&quot; and another method to &quot;render all the sprites on the list&quot;, so the user would be responsible for calling that when the rendering was finished. If the rendering order of the sprites doesn't matter, you could also sort them by texture, so that you don't have to switch textures so many times and you can use instancing in a group of sprites with the same texture. Then your system would work with or without a user-provided atlas (but faster with)." CreationDate="2016-02-20T07:45:05.573" UserId="48" />
  <row Id="2181" PostId="2068" Score="0" Text="Thanks very much - I know the original code is a mess - please accept my apologies! I've tried to do it using only simple matlab functions because I will be porting it to an FPGA (in VHDL) - and am simply trying to model the maths!" CreationDate="2016-02-20T18:26:26.127" UserId="2646" />
  <row Id="2185" PostId="2060" Score="1" Text="You don't want to do voxel rendering, right ? since you would no longer need triangles. You wan't to do an optimisation structure sorting the scene triangles withing regular grid cells, right ? (but there are several variants, e.g. spliting the triangles or not. which is yours ?)" CreationDate="2016-02-21T00:49:57.720" UserId="1810" />
  <row Id="2186" PostId="2069" Score="0" Text="I guess this will also not work since I'd like the user of these functions to be able to apply the painters algorithm if needed and also the bound framebuffer, color modifiers and stencil information all need to be applied in order" CreationDate="2016-02-21T04:17:07.203" UserId="2623" />
  <row Id="2187" PostId="2081" Score="0" Text="how the objects in the scene are defined? In ray tracing you calculate intersections using the implicit function, ie, sphere, plane, cone, etc, how is it in ray marching? also can you expaling the meaining of the variables that you wrote in your post?" CreationDate="2016-02-21T04:21:20.363" UserId="2684" />
  <row Id="2188" PostId="2081" Score="0" Text="I mimicked your very definition, which did not defined meshes :-). For volume rendering, the scene is made of density stored in each voxel, i.e. each cell of the 3D grid." CreationDate="2016-02-21T09:39:25.080" UserId="1810" />
  <row Id="2189" PostId="2081" Score="0" Text="I modified the equation and completed the definition. Now for such basic definitions, you should rather google or open a book. ;-) https://en.wikipedia.org/wiki/Volume_rendering" CreationDate="2016-02-21T09:42:41.673" UserId="1810" />
  <row Id="2191" PostId="2078" Score="0" Text="What you are saying may be a valid thing I haven't heard of before, but for the purposes of high performance, I've always heard of scan line conversion working like you convert 3d object to 2d screenspace objects and then use something like bressenham (for triangles, drawing a line down each edge, one y step at a time) to know where to start and end on the x axis for the current y.  In your case I could see similar working, where you do that N times, where each index of N is a slice of the Z axis.  I haven't ever done what you are trying though so can't say for sure if that is a good idea." CreationDate="2016-02-21T21:31:20.850" UserId="56" />
  <row Id="2192" PostId="2083" Score="1" Text="NB:  they don't answer emails about these questions :-)" CreationDate="2016-02-21T21:45:19.160" UserId="1810" />
  <row Id="2193" PostId="2078" Score="0" Text="Whenever it comes to scanline conversion for 2D graphics, definitely it is not a good idea. But in a particular technology of 3d printing, this type of printing head manuevoring has been deemed to effective to generate better printing outputs. This is why I want to find out intersection between scan line segment [(x1,y1)------(x2,y2)] parallel to x-axis or y-axis and polygon edges." CreationDate="2016-02-21T23:33:27.227" UserId="2712" />
  <row Id="2194" PostId="2073" Score="0" Text="Thanks for taking the time, on both the answer and the ShaderToy example!" CreationDate="2016-02-22T14:16:06.993" UserId="457" />
  <row Id="2196" PostId="397" Score="0" Text="Why do you think Wavelet based Denoising would be best?" CreationDate="2016-02-23T07:04:54.733" UserId="234" />
  <row Id="2204" PostId="2088" Score="0" Text="Even if we know the order theres till a infinite number of curves that fit trough the points. Even if we add additional constraints, then the open ends are problematic as their tangent orientation can be arbitrary. A [picture here](http://i.imgur.com/nlWqkRT.png)" CreationDate="2016-02-23T16:17:59.463" UserId="38" />
  <row Id="2207" PostId="2088" Score="0" Text="@joojaa Yes, you are right. But since the packing of points is very dense, I don't expect it to be exact. If I do get to have the right order, I was planning to connect the sequence of points as a polyline." CreationDate="2016-02-23T16:41:32.440" UserId="2562" />
  <row Id="2209" PostId="2088" Score="0" Text="In the code that needs to order the points, are you even aware of the parametric form of the curve? (If not, I'll delete my first answer, because it requires you to know the parametric form.)" CreationDate="2016-02-23T16:49:33.153" UserId="16" />
  <row Id="2210" PostId="2088" Score="0" Text="@MartinBüttner Yes, I do have access to the parametric form of the curve, if it's needed." CreationDate="2016-02-23T16:51:17.707" UserId="2562" />
  <row Id="2211" PostId="2088" Score="0" Text="@andrea.al i do secondary curve fitting for cad data all the time and I was not entirely happy with the results every second time. Thats why I have started to inject normal's to the starting points for the fitting. Problem is that curves tend to lose it in certain situations." CreationDate="2016-02-23T17:29:49.320" UserId="38" />
  <row Id="2214" PostId="397" Score="0" Text="@poor, What make you think that (Not that I think the other way, I'm just wondering)?" CreationDate="2016-02-24T06:42:29.020" UserId="234" />
  <row Id="2215" PostId="2088" Score="1" Text="Please show a typical point set !" CreationDate="2016-02-24T09:26:57.800" UserId="1703" />
  <row Id="2217" PostId="2078" Score="0" Text="Your question is still unclear but I figure that you are trying to fill a polygon with a Hilbert curve, right ? So you are asking about an algorithm that finds the useful portions of a Hilbert curve inside a given polygon ? If yes, do you want whole segments only or precise clipping to the polygonal window ?" CreationDate="2016-02-24T11:03:19.857" UserId="1703" />
  <row Id="2218" PostId="397" Score="0" Text="@Drazick I've read this somewhere 2-3 years ago. Not sure, but I guess it's probably a mixture of different algorithms for different situations. May I ask why you are interested in this? Is there any better approach? *Note: I'm not a physicist. However I'm using denoisers for video very frequently and I'm just curious how they work* :) Also see: http://dsp.stackexchange.com/questions/20086/denoise-images-with-wavelets" CreationDate="2016-02-24T12:26:51.637" UserId="18" />
  <row Id="2219" PostId="2078" Score="0" Text="You guessed right!. I am looking for the whole segments that reside inside the given polygon contour. In the usual scan line conversion we have parallel scan lines that fill the inside  region and in my specific case I am using Hilbert curve to fill the inside region.  Here goes a sample [image](http://i.imgur.com/NTtPYoB.jpg) that shows the pattern. As you can see that intersection test has to be performed with hilbert line segments and polygon outer and inner borders to define the inner region and I need tips in intersection test." CreationDate="2016-02-24T21:07:50.960" UserId="2712" />
  <row Id="2220" PostId="397" Score="0" Text="I would also guess Wavelets, but here - https://ni.neatvideo.com/overview/how-does-it-work the imply something else. Regarding the approach, I'm not sure, Non Local Means should also be good but harder to tune and slower to run." CreationDate="2016-02-25T06:56:27.483" UserId="234" />
  <row Id="2221" PostId="397" Score="0" Text="@Drazick That's neat image, a stand alone app to denoise *images*." CreationDate="2016-02-25T10:46:52.927" UserId="18" />
  <row Id="2222" PostId="397" Score="0" Text="It is the same for Video. But you deleted the comment which creates the context." CreationDate="2016-02-25T11:02:13.980" UserId="234" />
  <row Id="2224" PostId="2094" Score="0" Text="Everything looks ok. Bugs like these are the worst. :( Your quad coordinates are ok. DX Clip space is from (-1.0, 1.0). 2 suggestions: First, enable a debug Device using D3D11_CREATE_DEVICE_DEBUG. And see if anything interesting pops up in the output. Second, try capturing a frame in RenderDoc: https://github.com/baldurk/renderdoc Look at the call graph and see if anything is fishy." CreationDate="2016-02-25T23:28:56.167" UserId="310" />
  <row Id="2225" PostId="2096" Score="0" Text="Jason Allen is also correct. You will need to do both to get your program rendering." CreationDate="2016-02-26T00:25:59.260" UserId="2752" />
  <row Id="2226" PostId="2095" Score="0" Text="Fixed it, no change. See my comment on icStatic's answer" CreationDate="2016-02-26T01:41:21.753" UserId="2726" />
  <row Id="2227" PostId="2096" Score="0" Text="I added more code that is in my project. My D3D11 Initialization and my swap chain resize. Initialization creates rasterization states and also sets one of them. Swap chain makes/remakes the viewport according to screen current size." CreationDate="2016-02-26T01:42:39.307" UserId="2726" />
  <row Id="2228" PostId="2094" Score="0" Text="Have you tried temporarily turning off back-face culling (`D3D11_CULL_NONE` instead of `D3D11_CULL_BACK`) to see if there's a winding order problem?" CreationDate="2016-02-26T04:00:33.473" UserId="48" />
  <row Id="2229" PostId="2094" Score="0" Text="just did, didn't work" CreationDate="2016-02-26T04:04:04.400" UserId="2726" />
  <row Id="2233" PostId="2094" Score="0" Text="I posted a picture, could those warnings be causing an issue?" CreationDate="2016-02-26T12:18:30.087" UserId="2726" />
  <row Id="2234" PostId="2096" Score="0" Text="I posted a picture, could those warnings be causing an issue?" CreationDate="2016-02-26T12:18:34.900" UserId="2726" />
  <row Id="2238" PostId="2099" Score="0" Text="Its a bit hard to say what something looks like depends on where your camera is and whet Leese you have going on in the scene." CreationDate="2016-02-26T14:20:59.543" UserId="38" />
  <row Id="2239" PostId="2099" Score="0" Text="There is only this one cube in the scene. It is slowly rotating around its z axis and slowly going into the distance (this seems to work fine)." CreationDate="2016-02-26T14:39:16.877" UserId="2508" />
  <row Id="2240" PostId="2099" Score="1" Text="Yes, but that does not help us if we cant see your matrix, etc. You dont have any code that i can help you with. Your supposed to give minimal code to reproduce the problem otherwise theres no debugging to be had. Yes cubes can look very elongated if they are very close to the camera plane." CreationDate="2016-02-26T14:58:51.237" UserId="38" />
  <row Id="2241" PostId="2099" Score="0" Text="@joojaa sorry. I was a bit reluctant to provide any additional code because I am writing all of this in assembly. I supposed that would not help if I posted it here :)" CreationDate="2016-02-26T15:52:36.163" UserId="2508" />
  <row Id="2245" PostId="2104" Score="2" Text="To expand a little on the solution you found: with values like 1366 and 768 in `glFrustum`, you were effectively setting an extremely wide field of view (nearly 180 degrees), which caused extreme perspective distortion; that's why the cube looked weirdly stretched out. You might prefer to use `gluPerspective` instead of `glFrustum`, as the parameters there are more intuitive." CreationDate="2016-02-27T01:22:06.027" UserId="48" />
  <row Id="2246" PostId="2103" Score="0" Text="I don't get it. What is the relation between the MERL database (and the mystery of it's references for colors) and BRDF viewer ? Or do you think there is a special mode dedicated to MERL ? but just browsing the BRDF does not require to accurately depict the colors, right ?" CreationDate="2016-02-27T02:55:41.603" UserId="1810" />
  <row Id="2247" PostId="2103" Score="0" Text="I hoped they would interpret the colours giving you a hint.  They also use two databases so the might have to interpret to unify. Sounds like they don't." CreationDate="2016-02-27T03:01:03.483" UserId="2748" />
  <row Id="2248" PostId="2093" Score="0" Text="What do you consider ligtweight and why not use opengl or matplotlib?" CreationDate="2016-02-27T09:49:06.657" UserId="38" />
  <row Id="2249" PostId="2106" Score="0" Text="How are the objects represented? Surfaces modeled with triangles?" CreationDate="2016-02-28T03:44:26.493" UserId="2500" />
  <row Id="2250" PostId="2106" Score="0" Text="Yes, they are triangular poly meshes." CreationDate="2016-02-28T04:18:50.750" UserId="2748" />
  <row Id="2251" PostId="2105" Score="0" Text="Are you working with integers or floating point numbers?" CreationDate="2016-02-28T04:34:39.430" UserId="56" />
  <row Id="2252" PostId="1662" Score="0" Text="Not a full answer but the smaller amount of memory you use the better, as it will be more likely to fit in caches and have fewer cache misses.  If you have interpolatable values, like you are baking out points on a curve into textures, you might check this out as a way to get higher quality curve lookup tables with less memory: http://blog.demofox.org/2016/02/22/gpu-texture-sampler-bezier-curve-evaluation/" CreationDate="2016-02-28T04:51:46.943" UserId="56" />
  <row Id="2253" PostId="2105" Score="0" Text="Floating point numbers." CreationDate="2016-02-28T11:44:50.393" UserId="2712" />
  <row Id="2254" PostId="2105" Score="0" Text="ah ok, assumed so.  The answer I gave is appropriate for your usage case then.  If using integer coordinates there would be a way to get an EXACT answer without thresholding, but with floating point it's basically unavoidable." CreationDate="2016-02-28T21:04:44.877" UserId="56" />
  <row Id="2255" PostId="2108" Score="0" Text="The above explanation is focused on clarity, but there are some other optimizations you can do if you need it to be even faster.  For instance, instead of using a distance threshold, you can use a squared distance threshold, to avoid a square root.  You could also avoid the normalization step, and clamp between 0 and 1 instead of from 0 to length.  The above explanation should be plenty fast for most needs though!" CreationDate="2016-02-28T21:06:22.633" UserId="56" />
  <row Id="2256" PostId="2107" Score="0" Text="This is for the LUT, but it tells nothing for the gamut, right ? or does it ? (moreover I understood that their capture device was more home-made than the standard camera - and not all camera captors have the same raw gamut anyway)." CreationDate="2016-02-28T21:19:20.050" UserId="1810" />
  <row Id="2257" PostId="2106" Score="0" Text="approximate (conservative or not ?) or exact ? GPU assisted of pure CPU ? rigid object or deformable ? are the 2 objects arbitrary complex or at least one of both has a simple shape ?" CreationDate="2016-02-28T22:02:38.030" UserId="1810" />
  <row Id="2258" PostId="2106" Score="0" Text="Minkowski portal refinement and also GJK  are algorithms commonly used for this." CreationDate="2016-02-29T04:47:15.370" UserId="56" />
  <row Id="2260" PostId="2103" Score="0" Text="If the tool you mention can be used to provide an answer, it would help to explain how. As it stands, this is more of an additional question than an answer." CreationDate="2016-02-29T12:05:49.693" UserId="231" />
  <row Id="2262" PostId="2108" Score="0" Text="You are suggesting a dot product between a point and vector. Should it not be between two vectors. In that case, we have to derive vector from point. We can do it by generating a vector between the point and the start point of the line segment" CreationDate="2016-02-29T16:15:17.363" UserId="2712" />
  <row Id="2265" PostId="2108" Score="0" Text="You are right, good eye.  Fixed!" CreationDate="2016-02-29T16:23:50.907" UserId="56" />
  <row Id="2266" PostId="2093" Score="0" Text="@joojaa Saying &quot;lightweight&quot;, I mean not a game framework, because I don't need a lot of features just to visualize models. Matplotlib took several seconds to render the model; mayavi does it fast, but I don't need a frame with image on it -- I need just a matrix with colors. I started meeting with PyOpenGL and didn't find, how to render triangulated model and get buffer as a matrix instead of displaying it." CreationDate="2016-02-29T17:09:13.360" UserId="2750" />
  <row Id="2267" PostId="2117" Score="0" Text="Here is the texture I'm using: &#xA;http://i.stack.imgur.com/SEi0G.png" CreationDate="2016-02-29T19:19:59.807" UserId="2775" />
  <row Id="2268" PostId="2119" Score="1" Text="I've started experimenting with some of this already.  I've found that sampling in a + sign (5 reads) instead of the full 9 showed no differences in my testing, but im sure with more complex situations, there would be differences.  Doing a full x jfa and then a full y jfa does make lots of errors.&#xA;&#xA;I'll be interested to hear more details/info if you have it, but accepting your answer :P" CreationDate="2016-02-29T20:43:56.783" UserId="56" />
  <row Id="2269" PostId="2119" Score="1" Text="Forgot, here's link to one of my experiments:&#xA;https://www.shadertoy.com/view/Mdy3D3" CreationDate="2016-02-29T20:44:08.737" UserId="56" />
  <row Id="2270" PostId="2119" Score="0" Text="Interesting that it works apparently just as well with only 5 reads - especially since they can't be parallelised. Since the paper lists the cases that lead to error maybe you could deliberately set these up and see if 5 jump directions is still as good." CreationDate="2016-02-29T20:49:41.167" UserId="231" />
  <row Id="2271" PostId="2119" Score="0" Text="Sounds like you are ready to post your own answer..." CreationDate="2016-02-29T20:50:13.827" UserId="231" />
  <row Id="2272" PostId="2118" Score="1" Text="Thanks, I'll try this. It was one of my original methods of building the polygon, however much more complex mathematically so I went with the simpler method. I'll give it another shot and report back." CreationDate="2016-02-29T20:58:35.013" UserId="2775" />
  <row Id="2273" PostId="2119" Score="0" Text="my info supplements yours :P" CreationDate="2016-02-29T21:07:27.703" UserId="56" />
  <row Id="2274" PostId="2108" Score="0" Text="Dont we have to normalize the vector &quot;PRelative&quot; ?" CreationDate="2016-02-29T21:10:42.860" UserId="2712" />
  <row Id="2275" PostId="2108" Score="0" Text="No, you don't and shouldn't!  The length of PRelative is what is projected onto the direction vector (direction must be normalized though).  Are you having problems getting it working still?  If easily done you might try visualizing the various steps of finding the closest point on the line to see where it's breaking down." CreationDate="2016-02-29T21:14:21.913" UserId="56" />
  <row Id="2276" PostId="2108" Score="0" Text="I am defining the vector between closestPoint and P and then getting the length of the vector . If the length of the vector is less than equal to std::numerical_limit&lt;float&gt;::epsilon() , I return true, else return false. Is there anything worng with the concept ? I am still getting the wrong answer." CreationDate="2016-02-29T22:25:58.587" UserId="2712" />
  <row Id="2277" PostId="2108" Score="0" Text="i would make the epsilon larger than that.  The error that creeps in from floating point calculations is likely much larger than the smallest possible difference between two floating point numbers.  I'd try a larger than needed number to start out, and then shrink it down to the smallest value that feels right." CreationDate="2016-02-29T22:27:40.800" UserId="56" />
  <row Id="2280" PostId="2123" Score="4" Text="Don't apologise, I think it's a fair question, seeing that GLM's function naming is quite misleading here. I expect this could be a useful (and concise) reference in the future." CreationDate="2016-03-01T09:52:48.017" UserId="16" />
  <row Id="2282" PostId="2108" Score="0" Text="It works!, Some of the issues are not clear though. When to or not to normalize vectors. Since the normalization of a vector does not change its direction other than clamping its value between zero and one, it would be nice if some one elaborate more over this issue of when to normalize." CreationDate="2016-03-01T13:54:24.103" UserId="2712" />
  <row Id="2283" PostId="2108" Score="1" Text="You might consider asking some new questions about that, perhaps even on the math stack exchange sites.  You might also give dot product / vector projection a Google and read up on that a little bit, there is some great info on the net on that stuff.  Congratulations though, I'm really glad to hear you got it working (:" CreationDate="2016-03-01T14:53:03.910" UserId="56" />
  <row Id="2284" PostId="2117" Score="1" Text="I assume the issue is the shearing of the image? I see this happens where you have inconsistent spacing between the vertices of the triangular subdivision. My guess is you are using consistent steps for your uv mapping, giving you the shearing effect." CreationDate="2016-03-01T23:15:41.800" UserId="2500" />
  <row Id="2285" PostId="2128" Score="3" Text="I don't really think you can be more efficient than a equality check for one coordinate of two points. And as far as I see this check is correct. So... What is your issue with this solution?" CreationDate="2016-03-02T11:57:06.680" UserId="273" />
  <row Id="2286" PostId="2128" Score="2" Text="It may (or may not) be *slightly* faster to subtract the points and check whether one component of the difference is zero. Dragonseel has a valid question though." CreationDate="2016-03-02T12:26:55.480" UserId="16" />
  <row Id="2288" PostId="2128" Score="1" Text="@MartinBüttner Yes, its also easier to deal with floating point inaccuracy that way. Possibly more readable algo... Sounds like premature optimization to me." CreationDate="2016-03-02T13:01:06.953" UserId="38" />
  <row Id="2289" PostId="2130" Score="0" Text="Just thinking aloud... Could you model the negative part of a filter separately to generate two sets of samples, one to be treated as positive and the other as negative? Would this allow arbitrary filters for your second approach (generate in the shape of a filter)?" CreationDate="2016-03-02T20:02:08.893" UserId="231" />
  <row Id="2290" PostId="2073" Score="0" Text="Lerp and nlerp seem “good enough” for angles &lt;90°, but when you start getting higher (especially around 180°), the inaccuracies of lerp/nlerp become extremely pronounced.  I modified your ShaderToy example to have the vectors be 178.2° (180° * 0.99) apart from each other and the lerp/nlerp results are garbage: https://www.shadertoy.com/view/4sGGWd  If you can guarantee that the angle delta will be &lt;90° lerp/nlerp may be a fair optimization; if not, I think using slerp in combination with heavy memoization or a pre-computed lookup table is a safer and fast-enough way to go." CreationDate="2016-03-02T20:23:53.287" UserId="2785" />
  <row Id="2291" PostId="2073" Score="0" Text="I think you missed the fact that you can click the mouse to change the vector angles! and yeah, it gets real bad at larger angles.  You can even see that in the top image in my answer!" CreationDate="2016-03-02T20:40:26.713" UserId="56" />
  <row Id="2292" PostId="2073" Score="0" Text="@AlanWolfe: Side-note: I can write up an answer than pre-computes the `slerp` results into a texture and looks-up the values at runtime, if you're interested.  Given that the color value is the output normal, and if we can assume both lookup vectors are normal-or-nearly-normal-length, I believe the lookup table can be reduced to only plausible inputs, without covering the whole input space (4-dimensional, 2 for each input vector) and without resorting to using vector angles as inputs (2-dimensional, but still incurs the `atan2` cost)." CreationDate="2016-03-02T20:40:34.917" UserId="2785" />
  <row Id="2293" PostId="2073" Score="0" Text="@AlanWolfe Ha, yah, I totally did.  You're one step ahead of me!  _Deleting my shadertoy clone (link above is now dead)._" CreationDate="2016-03-02T20:41:45.603" UserId="2785" />
  <row Id="2294" PostId="2130" Score="0" Text="Maybe? Lemme fiddle with it for a bit" CreationDate="2016-03-02T20:47:44.223" UserId="310" />
  <row Id="2295" PostId="2130" Score="1" Text="Ok, if you track the zeros of the function, you can abs() the output into the pdf. Then when sampling, you can check if you're negative. Sample code here: https://gist.github.com/RichieSams/aa7e71a0fb4720c8cb41" CreationDate="2016-03-02T22:31:26.867" UserId="310" />
  <row Id="2296" PostId="2117" Score="0" Text="I actually fixed that shortly after posting this by making the subdivisions consistent and smaller, it fixes the shearing somewhat (although not completely). However the texture is still very distorted. I'm working on @nathan suggestion below, which at the very least will allow for a much finer subdivision." CreationDate="2016-03-03T00:06:08.263" UserId="2775" />
  <row Id="2297" PostId="2135" Score="0" Text="Try a game engine like unity, etc... not sure this is the best place for this question though" CreationDate="2016-03-03T12:37:20.770" UserId="38" />
  <row Id="2298" PostId="2135" Score="0" Text="Thanks, I have thought of Unity already. But I think it would be overkill for my purpose. I was hoping to find something not so complex." CreationDate="2016-03-03T14:11:00.490" UserId="2789" />
  <row Id="2299" PostId="2134" Score="2" Text="Are you looking to measure details smaller than a pixel by looking at several different instances of the pattern that have different offsets from the pixel grid? Or are you looking to use the arrangement of the red, green and blue subpixels to improve accuracy with a single instance of the pattern?" CreationDate="2016-03-03T14:48:16.543" UserId="231" />
  <row Id="2300" PostId="2137" Score="0" Text="Have you tried tessellating the environment map and associating depth with each vertex? Then crossfading as you move from one point to the other." CreationDate="2016-03-03T15:43:11.610" UserId="2500" />
  <row Id="2301" PostId="2132" Score="0" Text="Thanks! These are great resources. So, in the end, there are 3 methods? 1. Generate and Weigh with splatting 2. Generate and Weigh without splatting 3. Generate in the Shape of a Filter" CreationDate="2016-03-03T15:54:38.817" UserId="310" />
  <row Id="2302" PostId="2132" Score="0" Text="Do you know of any papers, blogs, etc. that explore how to parallelize Generate and Weight *with* splatting? Off the top of my head, you could have a mutex per tile, or make each pixel atomic." CreationDate="2016-03-03T15:57:59.273" UserId="310" />
  <row Id="2304" PostId="2131" Score="0" Text="It isn't clear why static objects and empty cells should allow the deletion of rows and columns. Are you setting these rows and columns to zero or removing them altogether to give a smaller matrix?" CreationDate="2016-03-03T17:13:24.183" UserId="231" />
  <row Id="2305" PostId="2131" Score="0" Text="In case the problem is somewhere other than where you guess, it would help to see the code, if this is something you are happy to share. Ideally an [MCVE](http://stackoverflow.com/help/mcve)" CreationDate="2016-03-03T17:14:50.840" UserId="231" />
  <row Id="2307" PostId="2131" Score="0" Text="Hey trichoplax. A matrix with an all zero row or column would be singular, as far as I know, so I instead remove them from the matrix to make a smaller matrix (as well as their corresponding entries in the b vector)." CreationDate="2016-03-03T17:57:59.703" UserId="2786" />
  <row Id="2308" PostId="2131" Score="0" Text="I will edit an MCVE in tonight when I am near my computer with the source." CreationDate="2016-03-03T17:58:37.703" UserId="2786" />
  <row Id="2309" PostId="2131" Score="0" Text="I also suspected that I was maybe making a wrong assumption somewhere else in the code, however this only pertains to the matrix structure (and whether or not it's singular). The only thing I can think of is what qualifies as a &quot;surface cell&quot; vs an air cell or a liquid cell. If this is a liquid cell adjacent to an air cell, is there something different that I should be doing with its corresponding columns/rows?" CreationDate="2016-03-03T18:00:50.033" UserId="2786" />
  <row Id="2310" PostId="2132" Score="2" Text="@RichieSams I don't know why you'd use &quot;generate and weigh without splatting&quot;, actually—that seems like it would be worse in any case than filter importance sampling. I was assuming that &quot;generate and weigh&quot; implies splatting. As for parallelization of splatting, off the top of my head, one way would be to split the image into tiles, but give each tile a 2‒3 pixel border to catch splats that cross the tile edge. Then in a final pass, additively composite the bordered tiles together into the final image." CreationDate="2016-03-03T18:20:25.620" UserId="48" />
  <row Id="2311" PostId="2131" Score="0" Text="I edited the question to include my code for generating the matrix. It is in Processing/Java." CreationDate="2016-03-03T19:49:29.360" UserId="2786" />
  <row Id="2312" PostId="2134" Score="0" Text="I'm trying to measure details smaller than a pixel so by several instance you mean shifting it to left or right and then match .Thanks for your response." CreationDate="2016-03-04T03:24:09.633" UserId="2788" />
  <row Id="2314" PostId="2135" Score="0" Text="if it's really for trivial shapes and to play with positions and orientations, why base OpenGL is not good ?" CreationDate="2016-03-04T07:06:06.737" UserId="1810" />
  <row Id="2315" PostId="2139" Score="0" Text="I don't see a problem, doubles can represent numbers as small as 10^-308." CreationDate="2016-03-04T09:00:09.867" UserId="1703" />
  <row Id="2316" PostId="2128" Score="0" Text="You will get the most appropriate answers by tell us **why** you want to detect such segments." CreationDate="2016-03-04T09:03:50.460" UserId="1703" />
  <row Id="2317" PostId="2134" Score="0" Text="By several instances I mean an image with a repeat pattern as described [here](http://computergraphics.stackexchange.com/questions/1880/what-is-the-state-of-the-art-on-using-computers-to-clean-up-images/1881#1881). By red, green and blue subpixels I mean taking advantage of the placement of different colours within a single pixel (the pixel geometry) as described [here](http://computergraphics.stackexchange.com/questions/424/subpixel-rendering-for-a-ray-tracer). It sounds like you probably mean the first approach." CreationDate="2016-03-04T12:30:21.587" UserId="231" />
  <row Id="2319" PostId="2118" Score="0" Text="Thank you. I was able to modify your suggestion into a solution. See my edit above." CreationDate="2016-03-04T17:16:09.480" UserId="2775" />
  <row Id="2320" PostId="2131" Score="0" Text="I added more information to the post." CreationDate="2016-03-04T18:02:45.977" UserId="2786" />
  <row Id="2321" PostId="2139" Score="1" Text="Perhaps your problem might stem from the order of ops in your calculation. You might have something like  (BigValue + SmallValue) - BigValue, and instead of getting &quot;SmalValue&quot; you get 0." CreationDate="2016-03-04T18:29:46.107" UserId="209" />
  <row Id="2322" PostId="2139" Score="1" Text="Just occurred to me that you seem to, effectively, have a rotation, R, followed a translation T.  If you can keep these separate, to get Inverse(R*T) you can just do Inverse(T)*Inverse(R), both of which are trivial. Would that help?" CreationDate="2016-03-04T18:46:03.690" UserId="209" />
  <row Id="2323" PostId="2139" Score="0" Text="Simon, you're right about the problem stemming from multiplying numbers of different magnitudes. The matrix is more complex than an R*T; it's also a scale, and can be a multiplication of any affine.. I managed to fix the problem, however, see below. Thanks for your help." CreationDate="2016-03-04T21:10:12.203" UserId="2792" />
  <row Id="2324" PostId="2139" Score="0" Text="You should ask on the math forum !" CreationDate="2016-03-04T21:16:23.010" UserId="1810" />
  <row Id="2325" PostId="2144" Score="0" Text="But the other line x = x1 is an infinite line parallel to the y-axis. Is not there a significant difference between a line and a line segment where both are parallel to y-axis ?" CreationDate="2016-03-04T21:55:48.563" UserId="2712" />
  <row Id="2326" PostId="2144" Score="0" Text="woops, you want segments. then verify that y match, and that $\lambda$ match." CreationDate="2016-03-05T00:47:07.520" UserId="1810" />
  <row Id="2329" PostId="2140" Score="0" Text="Is it known which line segment is axis parallel and which axis it is parallel to? Are you given two line segments, where one of them is parallel to one of the axes but you don't know which line segment or which axis, or are you given the first line segment, knowing it is always parallel to the x axis, and the second one may be any arbitrary line segment?" CreationDate="2016-03-05T18:04:10.747" UserId="231" />
  <row Id="2330" PostId="2140" Score="0" Text="These seemingly subtle differences may make a large difference to the approach, and so answerers will need to know which is the case." CreationDate="2016-03-05T18:05:11.483" UserId="231" />
  <row Id="2331" PostId="2140" Score="0" Text="It is confirmed that one of the line segment is either parallel to x-axis or y-axis and the other line segment may be or may be not parallel to either of the axis." CreationDate="2016-03-05T18:28:54.393" UserId="2712" />
  <row Id="2332" PostId="2133" Score="0" Text="24 bit shifters are used in single precision floating point to align mantissas, so the compiler might generate a few, but I don't think you'll see 30." CreationDate="2016-03-05T18:34:46.297" UserId="2500" />
  <row Id="2333" PostId="1893" Score="1" Text="I'm betting the 0.12 is in &quot;uv space&quot; meaning it's 12% of the image size (on each axis)." CreationDate="2016-03-07T04:11:23.850" UserId="56" />
  <row Id="2337" PostId="2139" Score="0" Text="@solendil sorry yes I meant to mention scaling as well, but missed the edit timeout. Scaling is, obviously, also trivial to include particularly if it's the same for both dimensions. FWIW I used this trick in an API for some early PC graphics chips. Anyway, glad you've resolved your issues." CreationDate="2016-03-07T08:52:34.917" UserId="209" />
  <row Id="2338" PostId="316" Score="0" Text="Of course I don't have all the details, but it seems to me that this person merely used a pseudo-distance field in place of a regular one, which has already been demonstrated in a 2006 paper by Qin, McCool and Kaplan, &quot;Real-time texture-mapped vector glyphs&quot;, which is also referenced in the Valve paper. It only affects the miters of outlines and does nothing to improve the appearance of corners. I suspect the reason it looks sharp is because he uses unpractically large distance field textures. I might be wrong though." CreationDate="2016-03-07T12:21:17.253" UserId="2811" />
  <row Id="2339" PostId="2151" Score="4" Text="Great first answer, welcome to the Computer Graphics SE! :) Is your thesis publicly available? (Or will it be after you've finished said paper?) If so it would probably be very helpful to link to that, too." CreationDate="2016-03-07T13:57:41.343" UserId="16" />
  <row Id="2340" PostId="2151" Score="0" Text="It is supposed to be publicly available, but it seems the school hasn't put it up yet. Anyway, I would prefer not to spread it right now, since the article I'm writing will really explain the important parts much better and focus on how to implement it, and it should be complete very soon." CreationDate="2016-03-07T14:26:47.413" UserId="2811" />
  <row Id="2341" PostId="2151" Score="0" Text="@Detheroc Please notify here and on the gamedev Q when you are done with the article. Explanation's still not 100% clear for me. I would suggest showing the composition step by step in images." CreationDate="2016-03-07T15:09:17.600" UserId="101" />
  <row Id="2342" PostId="2151" Score="1" Text="would love to be able to replicate your current results even if they are not as good as your future results, +1 to sharing whatever details you can.  very exciting.  Have you considered either technique's application towards ray marching (sphere tracing)?  In volume textures or similar..." CreationDate="2016-03-07T19:36:45.700" UserId="56" />
  <row Id="2343" PostId="2151" Score="0" Text="Also, obviously would love to see how it compares to just having a single channel texture that has 3x as many pixels (;" CreationDate="2016-03-07T23:18:19.107" UserId="56" />
  <row Id="2344" PostId="2150" Score="1" Text="You might try asking on the math stack exchange site if you don't get an answer here." CreationDate="2016-03-08T04:43:30.923" UserId="56" />
  <row Id="2345" PostId="2150" Score="2" Text="I must say that i dont understand what the inaccuracy is. So I cant help. Basically your saying Im doing A but A does not work. Without explaining what thing that does not work is." CreationDate="2016-03-08T07:17:27.507" UserId="38" />
  <row Id="2346" PostId="2150" Score="0" Text="@joojaa, When i fit single surface from a set of sampled points (Each sampled point was sampled from one of the source surface), the resulting surface fails to achieve good accuracy (In sense of maximum deviation between resulting surface and the set of sampled points). So I'm asking if there is some another method to do same thing (Get single surface from different trimmed surfaces) with smaller loss in accuracy, because least squares method gives too rough results." CreationDate="2016-03-08T11:37:15.227" UserId="2644" />
  <row Id="2347" PostId="2150" Score="1" Text="Make a picture showing the error." CreationDate="2016-03-08T11:42:02.893" UserId="38" />
  <row Id="2348" PostId="2150" Score="0" Text="@joojaam, it's just an error of NLib library, which tells that i cannot approximate set of points with desired tolerance. It's programming related, not from CAD package, sorry, i forgot to mention this." CreationDate="2016-03-08T16:02:01.437" UserId="2644" />
  <row Id="2350" PostId="2150" Score="1" Text="Even if the error is not clearly visible, it may help to include in the question an image of a curved surface for which the accuracy is unacceptable, and the (possibly textual) evidence that the accuracy is not sufficient. This will give answerers a better idea of what you are dealing with." CreationDate="2016-03-08T20:00:50.473" UserId="231" />
  <row Id="2351" PostId="2153" Score="2" Text="What are you trying to do with the array and loop? I'm asking because this somehow sounds like an [XY Problem](http://meta.stackexchange.com/questions/66377/what-is-the-xy-problem) to me. Since the best way to use conditions and loops on the GPU is to refrain from using them, maybe there are even better ways instead of using arrays and loops in your case." CreationDate="2016-03-08T21:38:13.670" UserId="127" />
  <row Id="2352" PostId="2153" Score="0" Text="I am implementing a screenspace subsurface scattering effect which currently work. But I have some doubts about the way I use the kernel according to performances. I've choose to do a maximum array size and fill only a part and use a dynamic loop with a dynamic number of iteration which is related to the currently used array content.&#xA;I think that there are things to do or know when programming shaders according to performances for example. And in my opinion loops is a common performance topic which might follow some rules and maybe &quot;good practices&quot; but I didn't found any good answer about it." CreationDate="2016-03-09T07:38:31.413" UserId="2372" />
  <row Id="2354" PostId="2156" Score="0" Text="Out of curiosity, what was the precision before? Low or Medium?" CreationDate="2016-03-09T09:33:26.220" UserId="209" />
  <row Id="2355" PostId="2152" Score="0" Text="Doh:  actually I'm wrong about the method not being uniform...working on a rewrite." CreationDate="2016-03-09T10:57:04.507" UserId="2831" />
  <row Id="2356" PostId="2152" Score="0" Text="OK, I put up a first revision." CreationDate="2016-03-09T12:17:30.033" UserId="2831" />
  <row Id="2363" PostId="2156" Score="0" Text="I found a document saying it's low for samplers and high for float/int by default on ES. Oh, ints depend on shader type I've added link to my answer." CreationDate="2016-03-09T21:16:58.943" UserId="2840" />
  <row Id="2364" PostId="2163" Score="0" Text="To be clear, are you looking for an existing software renderer for vector graphics, or are you looking to learn how to write a software renderer for vector graphics on your own?" CreationDate="2016-03-10T04:58:27.120" UserId="48" />
  <row Id="2365" PostId="2163" Score="0" Text="i am looking to write a software renderer for vector graphics on my own" CreationDate="2016-03-10T04:59:42.887" UserId="2853" />
  <row Id="2366" PostId="2163" Score="0" Text="Are you trying to make a program that outputs an image file? Or are you trying to make a program that shows the vector shapes on the screen and lets you edit them in real time?  What operating system are you using?" CreationDate="2016-03-10T06:19:25.380" UserId="56" />
  <row Id="2367" PostId="2152" Score="0" Text="Added the area-distortion note" CreationDate="2016-03-10T09:13:11.253" UserId="2831" />
  <row Id="2368" PostId="2159" Score="0" Text="The scene graph technique that I am looking forward to implement is very similar to the http://www.openscenegraph.org/ . I have used it for a while and I am familiar to it. The scene that you created with it can be stored as a trivial file format which can be processed further by this library." CreationDate="2016-03-10T10:53:03.073" UserId="2712" />
  <row Id="2369" PostId="2163" Score="1" Text="I'm afraid it is not possible for us to narrow the question down for you, as we do not know specifically what you want. Is there a similar application that already exists for comparison, so we can see what you intend to build? What will the finished program be used for? Will a shape be displayed as a wire frame, a solid, with flat shading or realistic lighting? Do you want to produce images in real time or slowly generate high quality images?" CreationDate="2016-03-10T12:08:33.670" UserId="231" />
  <row Id="2370" PostId="2156" Score="0" Text="OK. Was just wondering if you'd tried mediump as there could be performance (and power) benefits." CreationDate="2016-03-10T12:30:13.953" UserId="209" />
  <row Id="2371" PostId="2163" Score="0" Text="i wan't to create a 3D program where the only thing it does is showing a box with 8 vertex points , 12 edges in wire frame . This program will be an executable for windows operating system 10 . I wan't to create this 'sample' structure program without using OpenGL, DirectX or Vulkan , only with custom code for the entire input , output of the program . That means creating the class methods for vertex and edges and a ouput system to the screen with vector graphics. It may be in CLI or MFC , it does not matter for me." CreationDate="2016-03-10T14:03:53.507" UserId="2853" />
  <row Id="2372" PostId="2168" Score="0" Text="The option to queue the frame but not wait on it, would that be considered tripple (and higher) buffering?" CreationDate="2016-03-10T14:19:45.650" UserId="56" />
  <row Id="2373" PostId="2168" Score="0" Text="@AlanWolfe possibly, you need at least 3 buffers for a non-tearing and non-blocking view, one to display, one as the next to display and one that's being rendered to." CreationDate="2016-03-10T14:21:21.993" UserId="137" />
  <row Id="2375" PostId="2159" Score="0" Text="I would either look into the javascript library http://osgjs.org/, which also is based on openscenegraph.org. But if you want to have more control over the interaction with the library i would suggest trying to compile the openscenegraph using emscripten (as it is open source), or leaving the compiled library as is and creating javascript to c++ bindings for it (also possible with emscripten iirc)." CreationDate="2016-03-10T16:31:28.303" UserId="64" />
  <row Id="2376" PostId="2162" Score="0" Text="Thanks for your suggestion. Must try it, there are a good few demos on youtube, but I had never seen any mentioning live imput from mike." CreationDate="2016-03-10T19:23:51.333" UserId="2848" />
  <row Id="2377" PostId="2151" Score="0" Text="@Detheroc This is great. I'm also interested to get notified when you release it publicly." CreationDate="2016-03-11T00:55:40.090" UserId="250" />
  <row Id="2378" PostId="2163" Score="0" Text="what is your reasoning for not wanting to use a graphics API? Without a graphics API you will be doing CPU software rendering which is slower and not the modern way to do graphics. The graphics APIs utilize the graphics card's hardware, that is all.  You really are best off using opengl, directx or similar, unless you have a really strange reason why you can't use them." CreationDate="2016-03-11T03:37:11.020" UserId="56" />
  <row Id="2379" PostId="2163" Score="0" Text="i want to learn the work flow and understand the logic in details. It's only for educational purpose . There is not any big work going on or idea . You can say that i am a maniac in knowing this things how they work down in detail and code. I can use a API and i know how , it's just for my own personal reason." CreationDate="2016-03-11T05:30:16.603" UserId="2853" />
  <row Id="2380" PostId="2163" Score="0" Text="So I have re-interpreted the question as follows: *How to make the rasterizer of vector graphics yourself?* Does that sound about right? It hard to find a modern operating system functionality that allows you to manage everything yourself. Even the lowest level modern apis know how to draw lines for you." CreationDate="2016-03-11T08:42:40.920" UserId="38" />
  <row Id="2382" PostId="2170" Score="1" Text="Will you want to repeatedly test many different angles of ray from the same starting point, against the same polyline? Or will the starting point and angle both be variable? (What I'm getting at is it's probably possible to build an acceleration structure that would speed up these queries, but which structure is best will depend on how it's going to be used.)" CreationDate="2016-03-11T21:45:17.817" UserId="48" />
  <row Id="2386" PostId="2172" Score="0" Text="Putting online and use the google image advanced search ? :-p ( beside kidding, they might have publish (white) papers on that )." CreationDate="2016-03-12T00:10:26.250" UserId="1810" />
  <row Id="2387" PostId="2175" Score="1" Text="I really like this idea, I'll have to try it out. It makes perfect sense. Thanks!" CreationDate="2016-03-12T02:49:48.353" UserId="31" />
  <row Id="2388" PostId="2170" Score="0" Text="Nathan, I will be testing many from a single starting point. And then test multiple angles from another starting point and another and etc. I actually have segments of two contour lines and am trying to draw a line in between the two of them. My current strategy is to draw a series of lines at semi-regular intervals between the two lines. I can then create a polyline between all the short crossing lines.  I'm currently working on how to create the lines that cross between the two contours, which is surprisingly difficult due to how curvy and convoluted some of them are." CreationDate="2016-03-12T03:41:19.340" UserId="2863" />
  <row Id="2390" PostId="2172" Score="0" Text="Questions asking us to recommend or find a book, tool, software, tutorial or other off-site resource are off-topic as they tend to attract opinionated answers and spam. Instead, describe your problem or need and the steps, if any, you've taken to solve it." CreationDate="2016-03-12T11:25:11.537" UserId="231" />
  <row Id="2391" PostId="2172" Score="0" Text="As it stands, this question does not specify what types of images will need to be considered, so I am closing as unclear. The question may be reopened if it can be edited to clarify what types of non-photographic images will be presented, and to request an algorithm rather than an off site resource recommendation." CreationDate="2016-03-12T11:28:02.647" UserId="231" />
  <row Id="2392" PostId="2172" Score="0" Text="See also [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic)" CreationDate="2016-03-12T11:29:43.333" UserId="231" />
  <row Id="2394" PostId="2135" Score="0" Text="Not sure if this is what you're looking for, but [OpenSceneGraph](http://www.openscenegraph.org/) or [Vtk](http://www.vtk.org/) are [scene graph toolkits](https://en.m.wikipedia.org/wiki/Scene_graph). You may want to check licensing. I think they are LGPL but I'm not sure." CreationDate="2016-03-05T13:39:03.330" UserId="2562" />
  <row Id="2395" PostId="2135" Score="0" Text="Questions asking us to recommend or find a book, tool, software, tutorial or other off-site resource are off-topic as they tend to attract opinionated answers and spam. Instead, describe your problem or need and the steps, if any, you've taken to solve it. See also [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic)" CreationDate="2016-03-12T11:38:51.463" UserId="231" />
  <row Id="2397" PostId="1758" Score="0" Text="Questions asking us to recommend or find a book, tool, software, tutorial or other off-site resource are off-topic as they tend to attract opinionated answers and spam. Instead, describe your problem or need and the steps, if any, you've taken to solve it. See also [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic)" CreationDate="2016-03-12T12:23:28.780" UserId="231" />
  <row Id="2399" PostId="2170" Score="0" Text="So your triong to find the closest point on the curve?" CreationDate="2016-03-12T15:51:57.763" UserId="38" />
  <row Id="2400" PostId="2170" Score="0" Text="I tested out closest point on the curve and while it can work there are too many cases where it doesn't work at all or works poorly. The approach I'm currently considering is based on angles. When a line is drawn between two other lines 4 angles are created. I'm thinking the &quot;best&quot; line is created when all 4 angles are as close to 90 degrees as possible. So at a given position I intend to draw every possible line (intervals of 5 or 10 degrees or similar) and select the best line based on a ranking of the 4 angles." CreationDate="2016-03-12T16:51:03.637" UserId="2863" />
  <row Id="2401" PostId="2163" Score="0" Text="joojaa , yes that is about what i wan't ." CreationDate="2016-03-12T17:02:06.673" UserId="2853" />
  <row Id="2402" PostId="2170" Score="0" Text="Would you be interested in solutions that use precomputed acceleration structures? Bsp trees and possibly vornoi diagrams seem likely useful (:" CreationDate="2016-03-12T17:26:06.243" UserId="56" />
  <row Id="2403" PostId="2163" Score="0" Text="Ah ok Roger.  To help you out in googling or asking further questions, this might be referred to as software rendering or software rasterization." CreationDate="2016-03-12T20:08:41.263" UserId="56" />
  <row Id="2406" PostId="2178" Score="1" Text="If you take a screenshot using Alt and printscreen, instead of printscreen alone, it will just copy the current window instead of the whole screen, so you can show just the relevant part of your screen." CreationDate="2016-03-13T13:51:29.133" UserId="231" />
  <row Id="2408" PostId="1614" Score="0" Text="Hi cupe, is it possible to take a look to the code somewhere? Ps: are you checking also for multiple entries?" CreationDate="2016-03-13T18:35:18.960" UserId="1561" />
  <row Id="2409" PostId="1983" Score="0" Text="@trichoplax 1. Why now, after all this time? 2. Moreover, the response to the [meta-question](http://meta.computergraphics.stackexchange.com/questions/211/are-requests-for-reputable-sources-on-topic/212#212), while not firm, was leaning towards allowing the question. 3. Finally, as I have explained there, the argument about &quot;opinionated answers and spam&quot; is completely inapplicable in the case of requests for reputable sources for a specific fact. Either there is such a source or not." CreationDate="2016-03-13T20:29:40.783" UserId="2574" />
  <row Id="2412" PostId="1983" Score="0" Text="If anyone disagrees that off site resource requests should be off topic, they can have their say in [Are questions asking for off site resources on topic?](http://meta.computergraphics.stackexchange.com/questions/146/are-questions-asking-for-off-site-resources-on-topic) If anyone thinks requests for reputable sources should be an exception to the off site resources rule, they can have their say in [Are requests for reputable sources on topic?](http://meta.computergraphics.stackexchange.com/questions/211/are-requests-for-reputable-sources-on-topic)" CreationDate="2016-03-14T00:10:10.030" UserId="231" />
  <row Id="2415" PostId="2181" Score="0" Text="You might also want to read up on signed distance fields in general by the way. Seems relevant as you might possibly be able to skip the distance transform step and just draw sdf's" CreationDate="2016-03-14T03:12:19.650" UserId="56" />
  <row Id="2416" PostId="2177" Score="0" Text="Thank you for the suggestions Nathan, I'll look into BVH. Delaunay triangulation could be useful, but I'm seeing potential issues in the example you posted. I'm having the greatest issues where one contour is significantly longer than the other. On the ENE ridge in the image the triangles are spanning the same contour, which is the type of location that I really need the lines to span both contours to interpolate a smooth curve. I may have to go with a ray tracing type approach." CreationDate="2016-03-14T17:05:09.320" UserId="2863" />
  <row Id="2418" PostId="2183" Score="0" Text="I got the problem!" CreationDate="2016-03-14T18:26:00.437" UserId="2096" />
  <row Id="2420" PostId="2177" Score="0" Text="@MikeBannister, yes, there are places where triangles span the same contour, but you could detect those and filter them out, and only look at triangles that span two adjacent contours." CreationDate="2016-03-14T21:17:01.287" UserId="48" />
  <row Id="2421" PostId="2156" Score="0" Text="mediump doesn't help in my case, and also I don't notice any significant performance difference either." CreationDate="2016-03-14T21:22:20.267" UserId="2840" />
  <row Id="2422" PostId="2181" Score="0" Text="Thanks, it seems to work quite well. Using OpenCV, applying a distance transform, thresholding the image and finally normalizing it back to 0 to 255 values seems to give quite a good result. Some jagged lines are really visible though. I tried a Gaussian blur which kind of works but I would really like to know if there is a better solution." CreationDate="2016-03-15T02:54:12.533" UserId="2319" />
  <row Id="2423" PostId="2181" Score="0" Text="There is a better solution.  The topic kind of warrants it's own question but here's a short answer.  First step is to make it fade from white to black over a specific distance.  Like if 10 was the distance that went from black to white, you could make it fade between 9 and 11 where you use the distance value to figure out how light or dark to make the pixel based on distance.  Next, you take that pixel shade, which should be between 0 and 1, and put it through the smoothstep function.  That will turn it from a linear fade to something a lot more appealing. Adjust fade distance to taste (:" CreationDate="2016-03-15T03:10:46.917" UserId="56" />
  <row Id="2424" PostId="2181" Score="0" Text="Here is the details of the smoothstep function:  https://en.m.wikipedia.org/wiki/Smoothstep  for a better or more detailed answer you might ask how to do anti aliasing when rendering using a distance field." CreationDate="2016-03-15T03:12:21.340" UserId="56" />
  <row Id="2425" PostId="2181" Score="0" Text="oh sorry, to address more towards your original question.  I believe the curve just says how to turn the linear distance into a pixel shade.  like if your width was 10 pixels, and you were 5 pixels away (either on the positive or negative side), that it would take that to mean you were at 50% distance, and do a lookup on the curve at 50% to see how white or black to make the output pixel." CreationDate="2016-03-15T03:47:51.517" UserId="56" />
  <row Id="2426" PostId="2189" Score="2" Text="You can (and probably should) include images straight in the post (which I've done for your post now). Just hit Ctrl+G while editing the post and drag your image file into the browser. That way it will be hosted on imgur (on a specific Stack Exchange subdomain), which is a lot less likely to be subject to link rot at some point in the future (plus, people don't have to follow a link to your image to see what the problem)." CreationDate="2016-03-15T15:37:43.417" UserId="16" />
  <row Id="2427" PostId="2189" Score="0" Text="@MartinBüttner oh, thanks! I didn't know that I can do that here :)" CreationDate="2016-03-15T15:40:11.260" UserId="2508" />
  <row Id="2428" PostId="2188" Score="0" Text="Given this and your previous answers is there a reason why you do not use nurbs and B-Rep?" CreationDate="2016-03-15T17:35:16.007" UserId="38" />
  <row Id="2429" PostId="1963" Score="0" Text="I just came across this paper from SIGGRAPH 2006 that talks about a way to do this using wang tiles.&#xA;Recursive Wang Tiles for Real-Time Blue Noise&#xA;http://johanneskopf.de/publications/blue_noise/" CreationDate="2016-03-15T18:53:54.283" UserId="56" />
  <row Id="2430" PostId="2181" Score="0" Text="Sorry if my comment was a bit unclear.&#xA;&#xA;here is an [image](http://i.imgur.com/RKxyKtH.png) of the result I get.&#xA;&#xA;As you can see in the result, jagged strips are quite visible.&#xA;Like I mentioned, I used the distance transform + threshold + normalize to &quot;make it fade from white to black over a specific distance&quot;.&#xA;&#xA;You mentioned &quot;anti aliasing when rendering using a distance field&quot;. Is this what corresponds to the issue here?" CreationDate="2016-03-15T20:24:52.497" UserId="2319" />
  <row Id="2431" PostId="2181" Score="0" Text="Did you put the shade color through smoothstep to make the blending non linear? If so that is really weird. Probably worth another question about just that specifically. (Using terminology of rendering using a distance field)" CreationDate="2016-03-15T20:28:58.600" UserId="56" />
  <row Id="2432" PostId="2188" Score="0" Text="Yes. Whatever I build must then be uploaded in CSG format to a very old piece of software that then does raytracing on it. And the old software is setting up some STRONG restrictions on functions. However I seem to have found a workaround. As soon as I get approval on the figures I'll post my current solution." CreationDate="2016-03-16T08:14:08.810" UserId="2858" />
  <row Id="2433" PostId="2188" Score="0" Text="Yes but if you could tell what the old ray tracer is then we can come up with better solutions. I mean first you tell use it has to be torus and sphere cylinder etc. than you relax it to hull and Malinowski functions. Neither one of those alone are perfect solutions. But given artificial restrictions that are there because you think you are restricted in this way is not effective communication.   B-Rep is for most part CSG." CreationDate="2016-03-16T08:28:55.827" UserId="38" />
  <row Id="2434" PostId="2188" Score="0" Text="You are indeed correct. The final object must be constructed in Craig E. Kolb's Rayshade program and it must be inputted as a text file. I am still not allowed to use commands such as Hull or Minkowski since they are not available in Rayshade. I was looking for a way to simulate them. Up to now I've had (in my opinion) relative success by taking three projections of the objects and then intersecting them.   Documentation about Rayshade can be found in here: http://graphics.stanford.edu/~cek/rayshade/doc/guide/guide.html" CreationDate="2016-03-16T09:19:58.120" UserId="2858" />
  <row Id="2435" PostId="2190" Score="1" Text="I've been thinking about this myself and I'm not entirely sure, but here's some inspiration that might or might not be valid... So can there be a continuity? Ontologically no. Phenomenologically yes." CreationDate="2016-03-16T09:26:56.747" UserId="385" />
  <row Id="2436" PostId="2188" Score="0" Text="Howabout using sweptsph that gives you quite some options." CreationDate="2016-03-16T10:32:09.303" UserId="38" />
  <row Id="2437" PostId="2188" Score="0" Text="It turns out that intersecting the projections from three different views gives me an acceptable quality. I will check swepthsph if I need extra smoothness in the figures but for now the method I'm using is enough. Thank you very much for taking the time to help." CreationDate="2016-03-16T12:58:44.057" UserId="2858" />
  <row Id="2438" PostId="2190" Score="2" Text="BRDF space is definitely continuous. The classes of BRDFs you mention are simplified slices of BRDF space and it would take careful analysis of the formulas to decide if there is a parameter for specular in Cook-Torrence that gives a lambertian result when used in a physically based renderer. I think of a surface with both specular and diffuse reflection as being &quot;layered&quot; - like a gloss coating on a magazine, a polished outermost layer (specular) with crevices - all plastics seem translucent, underneath a shiny layer, light bounces around before being reemitted. Thus the divided model." CreationDate="2016-03-16T13:31:28.293" UserId="2500" />
  <row Id="2440" PostId="2193" Score="0" Text="And this discrete nature of depth map is created because of floating point precision. Right?" CreationDate="2016-03-16T20:12:31.963" UserId="2096" />
  <row Id="2441" PostId="2193" Score="1" Text="No its created because images are discrete as in have only one value for a area that varies." CreationDate="2016-03-16T21:00:08.700" UserId="38" />
  <row Id="2442" PostId="2193" Score="0" Text="Images are different in camera and light space?" CreationDate="2016-03-16T21:04:51.713" UserId="2096" />
  <row Id="2443" PostId="2176" Score="0" Text="thanks a lot for your help in this subject . It covers the basics that i need to continue what im searching for . i am grateful  for the help." CreationDate="2016-03-17T00:03:55.267" UserId="2853" />
  <row Id="2444" PostId="2176" Score="0" Text="Glad to hear! Sometimes it's hard to know what to search for." CreationDate="2016-03-17T00:14:55.893" UserId="56" />
  <row Id="2445" PostId="2189" Score="2" Text="I'd suggest to proceed step by step: try to display the framebuffer (directly with a quad) to validate its content first. Also, it looks like the UV on your cube might be wrong, so you may want to display your cube with its texture coordinates (`gl_FragColor = vec4(gl_TexCoord[0].xy, 0., 1.);`)." CreationDate="2016-03-17T03:01:52.287" UserId="182" />
  <row Id="2446" PostId="2187" Score="0" Text="Great question, I'm also curious." CreationDate="2016-03-17T03:48:40.693" UserId="56" />
  <row Id="2447" PostId="2187" Score="0" Text="I suggest you do a test by outputting a grey scale band, and see how it looks on all your available platforms. then try the same thing with `pow(c, 1/2.2)` at the end of the pipeline. Your trained eye will immediately see which is good and which is over-done. Over-done gamma should result in banding." CreationDate="2016-03-17T08:23:56.873" UserId="1614" />
  <row Id="2448" PostId="2174" Score="0" Text="Yeah. I see how detecting toy story images could classify, but Octane render/maxwell render or any path traced image is just going to shit in the fan." CreationDate="2016-03-17T08:26:35.730" UserId="1614" />
  <row Id="2449" PostId="2187" Score="0" Text="I came here *because* of the test I've done :-) https://www.shadertoy.com/view/4stSRN&#xA;The real world of webGLSL is incredibly messy and unrobust: behaviors can depend on driver, browser, OS, settings (Angle vs native OpenGL, display settings - soft and hard), plus the versions of all these. ( Of course here I had to trust people telling there system is well calibrated. )" CreationDate="2016-03-17T08:28:42.067" UserId="1810" />
  <row Id="2450" PostId="2181" Score="0" Text="Smoothstep does not solve the jagged lines problem which is not really surprising. It remaps the values in a smoother curve but it does not fix the aliasing in any way. I will address another question targeting this specific issue." CreationDate="2016-03-17T11:51:21.250" UserId="2319" />
  <row Id="2451" PostId="2193" Score="0" Text="Ok is that zigzag line represents depth map?" CreationDate="2016-03-17T14:14:02.127" UserId="2096" />
  <row Id="2452" PostId="2193" Score="0" Text="It represents the function of the depth map, the dashed lines represent the pixel samples of the depth map." CreationDate="2016-03-17T14:16:55.647" UserId="38" />
  <row Id="2453" PostId="2197" Score="0" Text="One can also be more clever and render mid distance maps." CreationDate="2016-03-17T14:18:53.710" UserId="38" />
  <row Id="2454" PostId="2193" Score="0" Text="Ok so when we sample from a depth map..Its not necessary you will sample from the exact same position..but the different one which can be either higher or lower than the depth map?" CreationDate="2016-03-17T14:25:36.870" UserId="2096" />
  <row Id="2455" PostId="2197" Score="1" Text="I would have thought it's called Peter Panning [because some films depict Peter Pan's shadow as having a mind of its own and detaching itself from Peter](http://movies.stackexchange.com/q/13552/15400)." CreationDate="2016-03-17T14:27:12.473" UserId="16" />
  <row Id="2456" PostId="2181" Score="0" Text="I'm looking forward to seeing the details in the new question. The artifacts you are seeing is strange. Fwiw the smoothstep-ing of the fade is a common technique. It might not help the specific issue you are hitting but it is useful / widely used. Just wanted to let you know I didn't just make it up :p" CreationDate="2016-03-17T15:04:59.550" UserId="56" />
  <row Id="2457" PostId="2197" Score="0" Text="@MartinBüttner Well, yes. That seems to be a sensible reason to call it that way. The tutorial I refered to uses the explanation that I gave." CreationDate="2016-03-17T16:12:43.823" UserId="273" />
  <row Id="2458" PostId="2181" Score="0" Text="Don't  worry! I believe you haha! I read about it and it seems to be used in many situations. This is simply not useful in this case since I need to preserve the linear interpolation. Here is [another post](http://dsp.stackexchange.com/questions/530/bitmap-alpha-bevel-algorithm) I found that has similar request and technology in use. I think the artifacts could be related to the implementation of distance transform in OpenCV." CreationDate="2016-03-17T18:27:42.700" UserId="2319" />
  <row Id="2459" PostId="2187" Score="0" Text="You cold add that to your post ;) Unfortunately this is all just a case of futility." CreationDate="2016-03-17T20:23:01.877" UserId="38" />
  <row Id="2460" PostId="2198" Score="0" Text="Do you want the insets to travel on the surface itself?" CreationDate="2016-03-17T21:41:28.000" UserId="38" />
  <row Id="2461" PostId="2198" Score="0" Text="Yes, on the surface.&#xA;&#xA;I've found papers for insetting 3D shapes in 3D, but I need to adhere to the surface." CreationDate="2016-03-17T22:35:39.817" UserId="2896" />
  <row Id="2462" PostId="2193" Score="0" Text="Yes you got it." CreationDate="2016-03-18T05:39:50.807" UserId="38" />
  <row Id="2465" PostId="2188" Score="0" Text="The reason nobody suggest the 3 projection boolean is because it does in fact not work for nearly any sensible case." CreationDate="2016-03-18T10:07:37.767" UserId="38" />
  <row Id="2467" PostId="2198" Score="0" Text="This is easier for cad applications as they have mathematically better surfaces than polygon meshes. But yes you could extrude tubes of varying sizes along edges then repeat and you'd get a good approximation. Its just that defining what distance along internal polygons is shortest and valid one is a bit hard." CreationDate="2016-03-18T18:37:10.940" UserId="38" />
  <row Id="2468" PostId="2201" Score="0" Text="OpenGL doesn't seem like it would help here; it's a computational geometry problem, not rendering. Can you edit the question and define your terms better? What do you mean by a &quot;maximum continual convex patch&quot; exactly?" CreationDate="2016-03-19T02:22:18.800" UserId="48" />
  <row Id="2469" PostId="2194" Score="0" Text="What is a &quot;dioptre material&quot;? AFAIK a dioptre is a unit of measurement of optical power. :)" CreationDate="2016-03-19T02:25:40.427" UserId="48" />
  <row Id="2471" PostId="2194" Score="1" Text="a dioptre between material 1 / material 2 is the surface between  2 optical materials of different index of refraction.  You confuse with the dioptry." CreationDate="2016-03-19T03:28:32.193" UserId="1810" />
  <row Id="2472" PostId="2194" Score="0" Text="Diopter/dioptre is [a unit of measurement](https://en.wikipedia.org/wiki/Dioptre). The surface between two materials is usually called an &quot;interface&quot;, AFAIK...I've never heard any term similar to &quot;diopter&quot; for that..." CreationDate="2016-03-19T03:30:54.830" UserId="48" />
  <row Id="2473" PostId="2194" Score="1" Text="possibly a problem of translation, then. The french wikipedia offer no english equivalent: https://fr.wikipedia.org/wiki/Dioptre  . And online translation suggest to use the same world is english. :-/ . Ok, I replace by &quot;interface&quot; but this world is less precise." CreationDate="2016-03-19T04:42:14.507" UserId="1810" />
  <row Id="2475" PostId="2189" Score="0" Text="@JulienGuertault I tried rendering the FB directly to screen and I've attached the output of this to my original post (at the end). It seems to be drawing to the FB just fine - I'm still not getting what is wrong here. Also, I switched to quads (2 triangles) and put in the coords by hand just to be sure - so the coords are now right, but the texture, when applied to the second quad, seems to be stretched badly in the Y direction - the square looks like a very tall rectangle. I don't get it..." CreationDate="2016-03-19T14:57:51.800" UserId="2508" />
  <row Id="2477" PostId="2201" Score="0" Text="@nathanreed opengl will be used to render the objects, processing algorithms are to be done in C++. I have to apply different colors to the different convex patches on the object to signify the selection. Say I have a sphere then the whole sphere is one maximal convex patch. Any portion of the sphere surface will be a convex patch, by maximal I mean the maximum continuous convex patch that can be found. Well in the rendering, depending on the viewing angles, the maximal convex patches visible to the viewer will have to colored." CreationDate="2016-03-20T04:01:00.113" UserId="2898" />
  <row Id="2478" PostId="2201" Score="0" Text="@nathanreed Below I have posted an answer that should work, can you improve upon it, or can you provide a better solution?" CreationDate="2016-03-20T04:02:01.553" UserId="2898" />
  <row Id="2483" PostId="2204" Score="0" Text="If a patch ends, and not all triangles have been visited/tested, you have to start anew with another random triangle that is not part of a patch yet. So you find all convex patches and not just the one you happen to begin with. Then chose the biggest one. (Note: Patches can be just a single triangle)" CreationDate="2016-03-20T13:13:42.493" UserId="273" />
  <row Id="2484" PostId="1983" Score="1" Text="@trichoplax You yourself were uncertain whether this question is really a request for off-site resources, and you say that uncertainty remains. Is it a standard practice on this stackexchange that, when there is uncertainty about whether a question is off-topic, to err on the side of it being off-topic?" CreationDate="2016-03-20T14:23:28.137" UserId="2574" />
  <row Id="2490" PostId="2204" Score="0" Text="@dragonseel well for practical purposes I think single triangle patches should be ruled out, as for us to determine convexity, it has to span across several triangles.&#xA;well we do get all patches, but when a convex patch continues further, it's size keeps on increasing by including the neighbouring triangles that satisfy convexity, until the patch reaches the maximum size it can attain." CreationDate="2016-03-20T16:41:13.333" UserId="2898" />
  <row Id="2492" PostId="2204" Score="0" Text="Yea. I just wanted to emphazise that in order to find the biggest convex patch, you have to keep sampling until you tested everything, since after the first one there might be a even bigger one disconnected that you haven't found jet." CreationDate="2016-03-20T20:30:55.647" UserId="273" />
  <row Id="2493" PostId="2206" Score="0" Text="It is perfectly OK if your material looks dark gray if you illuminate it with weak light source. What is the brightness of the light source?" CreationDate="2016-03-20T23:14:39.753" UserId="2479" />
  <row Id="2494" PostId="2206" Score="0" Text="@ivokabel but my materials should look white and light gray. The SPD of the illuminant used is the D65. Do i need to tweak the spd of the light in some way?" CreationDate="2016-03-20T23:17:02.907" UserId="2237" />
  <row Id="2495" PostId="2206" Score="0" Text="@ivokabel Do i need to define a brightness paramter and use it somewhere?" CreationDate="2016-03-20T23:17:50.843" UserId="2237" />
  <row Id="2496" PostId="2206" Score="0" Text="If I am not mistaken, D65 only defines the shape of the spectrum, not the intensity. Therefore, you will really have to add a parameter telling the amount of emitting radiance, or something similar. Related topic is the renderer exposure value, but I saw that you take 1 as the limit value, so you don't have to bother with this one." CreationDate="2016-03-20T23:37:29.820" UserId="2479" />
  <row Id="2497" PostId="2206" Score="0" Text="Thank you @ivokabel for the suggestion about the parameter radiance. Could it be just a constant that will be multiplied with the spd of the illuminant during the tracing of rays? Or do i need to multiply the spd of the illuminant during the conversion from spd to cie xyz? Also I don't understand what you mean with renderer exposure value. Where do I take 1 as its value?" CreationDate="2016-03-20T23:45:26.890" UserId="2237" />
  <row Id="2498" PostId="2206" Score="0" Text="Yes, multiplying the SPD of your illuminant with a value (whether constant or variable) during or before the ray tracing is the way to go." CreationDate="2016-03-20T23:55:55.353" UserId="2479" />
  <row Id="2499" PostId="2206" Score="0" Text="@ivokabel what about the render exposure value? Where do i take 1?" CreationDate="2016-03-20T23:58:54.817" UserId="2237" />
  <row Id="2500" PostId="2206" Score="0" Text="...and sorry for the confusion about exposure. What I meant is the image value which maps onto maximum value in the resulting picture, 255 in your case. Don't worry about that at this point." CreationDate="2016-03-20T23:58:58.063" UserId="2479" />
  <row Id="2501" PostId="2206" Score="0" Text="@ivokabel can you just write a response to my question? In this way your answer an the other (if someone else would response) will remain as reference, and the user will not have to search in the  :) thank you." CreationDate="2016-03-21T00:05:41.957" UserId="2237" />
  <row Id="2502" PostId="2206" Score="0" Text="I might get to it tomorrow..." CreationDate="2016-03-21T00:11:36.470" UserId="2479" />
  <row Id="2503" PostId="2206" Score="0" Text="After re-reading your code, it is unclear to me what is the relation between scene-&gt;light-&gt;spectrum and material-&gt;le in `PathBRDF::shade`. Is it the same value? If yes, do you allow just one light source in your scene? Moreover, why do you normalize your XYZ values with illuminant luminance in `CIE1931XYZ::tristimulusValues()`?" CreationDate="2016-03-21T10:01:09.643" UserId="2479" />
  <row Id="2504" PostId="2206" Score="0" Text="@ivokabel scene-&gt;light-&gt;spectrum and material-&gt;le contain the same value, the D65 SPD. In the scene init I give to the material of light object the SPD of the scene light. My scenes supports only one light. The normalization in CIE1931XYZ::tristimulusValues() follows the standard conversion from spd to CIE XYZ that you can found here http://www.scratchapixel.com/old/lessons/3d-basic-lessons/lesson-5-colors-and-digital-images/color-spaces/ or on the wiki CIE XYZ page. Is this passage not correct? I don't think so because my engine supports also the whitted ray tracing model that seems to be ok." CreationDate="2016-03-21T10:09:52.457" UserId="2237" />
  <row Id="2505" PostId="2206" Score="0" Text="This is an example scene generated with a different illuminant SPD and the Whitted model https://raw.githubusercontent.com/chicio/Spectrum-Clara-Lux-Tracer/master/Screenshots/03_scene4_whittedSpectrum_fl9.png" CreationDate="2016-03-21T10:10:25.363" UserId="2237" />
  <row Id="2506" PostId="2206" Score="0" Text="In fact if I multiply the material-&gt;le with a multiplier the scene become more bright. Here are some rendered images https://drive.google.com/drive/u/1/folders/0BxeVnHLvT8-7Ty1jTVM5U1JJdms. They are not totally correct (as I expect the floor to be white), but maybe i just need a higher multiplier. Do you see any error in code that could avoid this multiplier (some error in the pdf/BRDF calculation)? Thank you very much again @ivokabel." CreationDate="2016-03-21T10:13:59.067" UserId="2237" />
  <row Id="2508" PostId="2206" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/37281/discussion-between-ivokabel-and-fabrizio-duroni)." CreationDate="2016-03-21T12:29:26.397" UserId="2479" />
  <row Id="2509" PostId="2211" Score="1" Text="Not to mention it needs to render the image from 2 slightly different perspectives to accommodate 3D." CreationDate="2016-03-22T09:09:30.180" UserId="2933" />
  <row Id="2510" PostId="2211" Score="2" Text="@Tom.Bowen89 I'd file that under needs higher resolution or higher framerate. Given that you are either using geom shader to emit 2 sets of vertices to the rasterizer or simply rendering twice." CreationDate="2016-03-22T09:12:49.417" UserId="137" />
  <row Id="2513" PostId="2210" Score="1" Text="My laptop which has dual 980m cards is inferior too fyi. The mobile cards add latency apparently due to Intel optimus technology. I was very sad to find that out." CreationDate="2016-03-22T16:49:55.247" UserId="56" />
  <row Id="2514" PostId="2160" Score="0" Text="Questions asking us to recommend or find a book, tool, software, tutorial or other off-site resource are off-topic as they tend to attract opinionated answers and spam. Instead, describe your problem or need and the steps, if any, you've taken to solve it. See also [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic)" CreationDate="2016-03-22T17:19:54.807" UserId="231" />
  <row Id="2517" PostId="1983" Score="0" Text="You make a good point, which I've taken some time to think about. In the absence of any community support for excluding such questions, I have reopened this one." CreationDate="2016-03-23T15:34:44.407" UserId="231" />
  <row Id="2519" PostId="2222" Score="0" Text="This question is a cross-post from http://gamedev.stackexchange.com/questions/118744/how-to-convert-non-axis-aligned-bounding-boxes-to-aabb" CreationDate="2016-03-23T21:30:58.277" UserId="2954" />
  <row Id="2521" PostId="1720" Score="0" Text="The historical terms are &quot;that's the difference between flat and gouraud's shading&quot;. In practice this relates to normals like explained by joojaa" CreationDate="2016-03-24T05:17:13.580" UserId="1614" />
  <row Id="2522" PostId="13" Score="3" Text="I believe this answer is completely false. You are free to multisample outgoing rays and weight their result when combining. You obtain the same truth than the russian roulette technique, but it's generally accepted that the former method is more expensive." CreationDate="2016-03-24T05:27:59.740" UserId="1614" />
  <row Id="2523" PostId="1983" Score="0" Text="This is allmost certainly never going to be answered. Besides what does it have to do with 3d graphics." CreationDate="2016-03-24T05:29:00.690" UserId="38" />
  <row Id="2524" PostId="2223" Score="0" Text="This is (too) many questions rolled into one. Should it possibly be split up?" CreationDate="2016-03-24T05:33:11.800" UserId="38" />
  <row Id="2525" PostId="2223" Score="0" Text="They are all algorithms which apply to the same topic in computational geometry, specifically, computation concerning non-convex enclosed polyhedrons. (Thank you, @nathan-reed, for adding that tag.)" CreationDate="2016-03-25T01:28:16.253" UserId="2957" />
  <row Id="2526" PostId="1983" Score="0" Text="@joojaa 1. you are probably right that if it hadn't been answered up to this point, it's has low probability of ever being answered. But (a) &quot;low&quot; is not zero, and (b) there was no way to estimate this probability before posting the question." CreationDate="2016-03-25T02:07:53.613" UserId="2574" />
  <row Id="2527" PostId="1983" Score="0" Text="@joojaa 2. I have explained my rationale for submitting this question to this stackexchange in the post itself, section **The reason I'm posting in Computer Graphics stackexchange**. Moreover, a book (_Multiple View Geometry in Computer Vision_), which isn't quite computer graphics but is surely quite related to it, does come pretty close to giving the answer, I would suggest that this is a reasonably strong argument that the question was indeed appropriate (see section **Two sources that come close**)." CreationDate="2016-03-25T02:09:05.650" UserId="2574" />
  <row Id="2528" PostId="1983" Score="0" Text="@trichoplax Thank you!" CreationDate="2016-03-25T02:12:48.827" UserId="2574" />
  <row Id="2529" PostId="2093" Score="0" Text="Can i ask you how you defined colors for each triangle when you have already a variable defined in 3D-space ? Cheers, Aurel" CreationDate="2016-03-24T22:51:04.317" UserId="2962" />
  <row Id="2530" PostId="2232" Score="4" Text="`See the number of pixels you need to update grows exponetially when the sides grow.` Quadratically, i think." CreationDate="2016-03-25T11:45:34.313" UserId="2970" />
  <row Id="2531" PostId="2232" Score="0" Text="&quot;Copying the data over from the CPU to the graphics card is a relatively slow operation.&quot; This is true, but irrelevant. Copying over a few million pixels at 60 fps is easily achievable over even modest PCI-E links (it would only require a few hundred megabytes per second.)" CreationDate="2016-03-25T13:06:31.973" UserId="2972" />
  <row Id="2532" PostId="2234" Score="0" Text="My laptop's APU has 256 shader cores @ 686MHZ while my tablet has 192." CreationDate="2016-03-25T13:38:36.217" UserId="2964" />
  <row Id="2533" PostId="2232" Score="0" Text="So APIs make things faster by letting the GPU do the work?" CreationDate="2016-03-25T13:42:15.433" UserId="2964" />
  <row Id="2534" PostId="2232" Score="0" Text="Yes - the GPU is capable of doing hundreds of times as many calculations as the CPU." CreationDate="2016-03-25T13:44:59.973" UserId="2971" />
  <row Id="2535" PostId="2232" Score="0" Text="@pjc50 its not so much that the cpu is faster per core it just has extrenely many cores. Since pixels pushing is a embarassingly parallel business it works." CreationDate="2016-03-25T14:19:36.793" UserId="38" />
  <row Id="2536" PostId="2232" Score="0" Text="@Coxy yes but on top of that copying you need to prepare that image. So it limits what i can prepare. Nothing stops you from doing this, with opengl. Prepare image upload prepare upload." CreationDate="2016-03-25T14:21:10.750" UserId="38" />
  <row Id="2537" PostId="2232" Score="2" Text="@pjc50 That's not wrong, but also not exactly true. A GPU is specialized to run a single program (usually a shader) in parallel on a large amount of data. So, you need to perform the same operations on lots of data to actually use the computing power of a GPU. If your program doesn't, you better run the program on the CPU." CreationDate="2016-03-25T14:22:53.633" UserId="127" />
  <row Id="2538" PostId="2189" Score="1" Text="Other debugging techniques: Use a non-FBO texture to see if it shows up as you'd expect. Make sure that your FBO is complete by calling [`glCheckFramebufferStatus`](http://docs.gl/gl3/glCheckFramebufferStatus)" CreationDate="2016-03-25T14:57:08.820" UserId="197" />
  <row Id="2539" PostId="1983" Score="0" Text="Have you tried posting this in some approriate journal. This should immediately bring oout the trolls that know it has been addressed." CreationDate="2016-03-25T15:25:50.633" UserId="38" />
  <row Id="2540" PostId="2212" Score="0" Text="Did this solve the problem?" CreationDate="2016-03-25T17:06:21.783" UserId="2479" />
  <row Id="2541" PostId="2232" Score="0" Text="@Cthulhu, quadratic can equal exponential (think about it, exponential = `y=x^2` and quadratic = `y=x^2+2x+1`, they looks the same, but they're in different places on the graph)" CreationDate="2016-03-25T18:00:36.997" UserId="2977" />
  <row Id="2542" PostId="2234" Score="0" Text="Hey, the Titan X has 5760 cores in it." CreationDate="2016-03-25T18:02:04.110" UserId="2977" />
  <row Id="2543" PostId="2232" Score="1" Text="@Daniel $x^2$ is a quadratic function. An exponential function would be $2^x$. And exponential functions grow way faster than quadratic functions." CreationDate="2016-03-25T18:14:13.497" UserId="127" />
  <row Id="2544" PostId="2232" Score="0" Text="Gah, my bad - this is what I get for not waking up fully before typing." CreationDate="2016-03-25T18:14:43.663" UserId="2977" />
  <row Id="2545" PostId="2232" Score="0" Text="many words, don't go to the point. @pjc50 answer is most to the point and understandable" CreationDate="2016-03-25T20:18:55.960" UserId="2981" />
  <row Id="2546" PostId="2234" Score="0" Text="@Daniel Are there any games that the Titan X goes &lt;  30fps at ultra high .The Titan X is very powerful" CreationDate="2016-03-26T02:33:38.857" UserId="2964" />
  <row Id="2547" PostId="2234" Score="0" Text="Ummm, One of these, perhaps: http://www.maximumpc.com/10-most-graphically-demanding-pc-games/" CreationDate="2016-03-26T02:46:36.110" UserId="2977" />
  <row Id="2548" PostId="2231" Score="0" Text="In general doing one operation on a lot of things is more efficient and easier to think about than doing it on every thing individually." CreationDate="2016-03-26T03:29:37.357" UserId="2988" />
  <row Id="2549" PostId="2231" Score="2" Text="Because every game would need to be rewritten for every graphics card. Unless they didn't use the graphics card, but then they'd be slow." CreationDate="2016-03-26T04:18:47.940" UserId="2316" />
  <row Id="2550" PostId="2231" Score="0" Text="I think that GPU companies have to create their own DirectX drivers" CreationDate="2016-03-26T04:35:21.333" UserId="2964" />
  <row Id="2551" PostId="2235" Score="0" Text="So not good for excel :)" CreationDate="2016-03-26T05:31:50.900" UserId="38" />
  <row Id="2552" PostId="2093" Score="0" Text="@AurelienSanchez sure. I want to use normal map as a color of pixels, and it's another problem -- to find normal vectors fast. Now I use glfw to render the model and planning to use glReadPixels to achieve matrix with pixels, and calculating normal vectors by myself" CreationDate="2016-03-26T06:35:48.780" UserId="2750" />
  <row Id="2553" PostId="2232" Score="1" Text="You talk about copying and moving, but what's more important is the actual image generation. You have to determine which object will be visible at which point, how it will be lighted, what will be the effects of smoke, etc, etc. GPUs are highly optimized to perform these operations fast and in parallel. APIs make it easy to express common operations." CreationDate="2016-03-26T06:47:47.153" UserId="2312" />
  <row Id="2554" PostId="2232" Score="0" Text="@IMil yesIi commented about this. It is more meant to be a lies to children type of answer than a this is the exact reason. 3D is by no reason the only reason we use graphics accelerators." CreationDate="2016-03-26T07:32:34.387" UserId="38" />
  <row Id="2555" PostId="2232" Score="0" Text="The operating system could a API which allows everything to be drawn by pixels" CreationDate="2016-03-26T07:48:40.713" UserId="2964" />
  <row Id="2556" PostId="2232" Score="0" Text="@SuiciDoga there is an api for that both directX and opengl can do this as can direct2D, GDI and numerous others. Just because the api can do more complex things does not mean it cant do simple things too. Just you do not write directly to the buffer on screen somebody else does that for you." CreationDate="2016-03-26T07:58:57.253" UserId="38" />
  <row Id="2557" PostId="2232" Score="0" Text="@joojaa what do you mean by 'lies to the children'? There are two options. 1) you calculate every pixel by hand, 2) you say: here are the coordinates, here is the texture, here are the light sources, go ahead and draw them. Option 2 is easier, and it allows graphic chip designers and driver developers to make even old games faster." CreationDate="2016-03-26T08:11:28.833" UserId="2312" />
  <row Id="2558" PostId="2232" Score="0" Text="@IMil Lies to children is a simplified reality, i know full and well that the answer is not this simple. Its not granted that the CPU is slower than the GPU. It can be faster IF the image you make is not parallelizable. Its just that in general pixels are independent of each other, if this is not the case then it does not hold true. So it is not certain and definitive that this is fastest in a general sense. But since it is there you rarely see other kinds of things. Too many dimensions to consider." CreationDate="2016-03-26T08:27:29.920" UserId="38" />
  <row Id="2559" PostId="2234" Score="0" Text="@Daniel The developers of those games must have need about 2-4 Titan X cards :)" CreationDate="2016-03-26T13:30:40.113" UserId="2964" />
  <row Id="2560" PostId="2234" Score="0" Text="Yep.  I'm going to start developing in UE4, and that engine is a beast.  For max settings in a big world you basically need 4 Titan X's to get 60fps @ 1080p." CreationDate="2016-03-26T14:48:27.263" UserId="2977" />
  <row Id="2561" PostId="2231" Score="0" Text="&quot;But why would we need all these frameworks and GPU features when we could just draw everything pixel by pixel?&quot; *that's* how it was done in good ol' days. Wolfenstein 3D, Doom, Duke Nukem 3D, Quake and most other games of late '90 used pure software rendering (Quake offered OpenGL renderer as an option)." CreationDate="2016-03-26T17:07:22.950" UserId="2997" />
  <row Id="2563" PostId="2222" Score="0" Text="If the current answer is sufficient you can close the question otherwise you can point at possible shortcomings or aspects to elude more deeply." CreationDate="2016-03-26T17:59:48.630" UserId="2287" />
  <row Id="2564" PostId="1983" Score="0" Text="@joojaa No, but I would be thankful indeed if your can think of some journal which might be appropriate---I'm certainly willing to try...  By the way, I'm not familiar with journal websites offering this sort of discussion as an option... do you happen to know of examples?" CreationDate="2016-03-26T19:52:22.037" UserId="2574" />
  <row Id="2566" PostId="2240" Score="1" Text="Are you sure you want N·H, or do you want R·L i.e. reflection vector dotted with light source vector? The latter would be the classic Phong equation (N·H is Blinn), and the figure appears to mark the R·L angle: 15 degrees." CreationDate="2016-03-27T06:01:07.813" UserId="48" />
  <row Id="2568" PostId="2240" Score="0" Text="Should the question not point out which Bidirectional Reflectance Distribution Function (BRDF) you need to use? You have Phong, Blinn-Phong, Modified Phong, Modified Blinn-Phong and a whole variety of physically-based ones." CreationDate="2016-03-27T10:17:51.400" UserId="2287" />
  <row Id="2569" PostId="2240" Score="0" Text="@NathanReed how can i find R.L if i dont know the vectors? Do i just use 15 degrees?" CreationDate="2016-03-27T13:28:42.073" UserId="2359" />
  <row Id="2570" PostId="2240" Score="2" Text="@user2976568 The dot product is the cosine of the angle between the vectors (for unit vectors). Just as you remarked that N·L = cos(theta). :)" CreationDate="2016-03-27T17:24:35.710" UserId="48" />
  <row Id="2571" PostId="2234" Score="0" Text="I am a beginner with developing games and creating some Unity games using my laptop's APU.I haven't experienced much lag because my laptop has a dedicated class APU which almost as good as a dedicated video card.My APU is a AMD A8-4500M.I do not think that this laptop can use UE4 without a external GPU card (using USB3)." CreationDate="2016-03-28T02:44:14.047" UserId="2964" />
  <row Id="2572" PostId="2234" Score="0" Text="And to get 4K they must need about 8 Titan X GPUs" CreationDate="2016-03-28T02:46:51.953" UserId="2964" />
  <row Id="2573" PostId="2231" Score="1" Text="@MatthewRock You're not being helpful. You can certainly bundle an OS with a game inside a docker container and distribute the container. That way, the user doesn't need to install library dependencies for their distro." CreationDate="2016-03-28T03:55:44.500" UserId="2968" />
  <row Id="2574" PostId="67" Score="1" Text="FWIW, I've heard of displays (usually very large displays in stadiums) that use a delta-nabla configuration. (Named for the Greek delta letter &quot;Δ&quot; and the Hebrew nabla letter &quot;∇&quot; because the pixels were alternating triangles with the point going up, then down, then up, then down.) One example is the Philips Vidiwall." CreationDate="2016-03-28T05:07:44.823" UserId="3003" />
  <row Id="2575" PostId="1741" Score="0" Text="@user3531082 Maya does not in fact solve this problem" CreationDate="2016-03-28T08:52:08.703" UserId="38" />
  <row Id="2576" PostId="1892" Score="0" Text="@Marqin Could you convert your last comment into an aswer so we can get this post out of the unanswered queue?" CreationDate="2016-03-28T09:03:15.333" UserId="38" />
  <row Id="2578" PostId="1892" Score="0" Text="@joojaa But frankly it's not valid answer. Some time ago I've reorganized my shader, changed types/order of  struct fields and now it &quot;miraculously&quot; work. And I still want to know why it was not working the old way - maybe I just made some error with padding? I cannot find that bug :/" CreationDate="2016-03-28T09:36:20.210" UserId="2413" />
  <row Id="2579" PostId="1892" Score="0" Text="@Marqin then answer that, the question does not contain your code so theres no way to verify your bug from this anyway." CreationDate="2016-03-28T09:41:19.163" UserId="38" />
  <row Id="2580" PostId="2243" Score="0" Text="Thanks for the reference !  In case of web application, this adds one more level of difficulties. The fact is that as shadertoy shows, not all people see the same result. Plus in all API already providing a texture loading, I guess we can just hope a linearization is done." CreationDate="2016-03-28T10:43:15.020" UserId="1810" />
  <row Id="2581" PostId="2243" Score="0" Text="Beside, when preparing a slide on googleDoc comprising imported images, exporting in pdf, and displaying via acroread on the same screen, I already don't have the same gamma for the 2 copies of the image !" CreationDate="2016-03-28T10:44:21.600" UserId="1810" />
  <row Id="2583" PostId="2240" Score="1" Text="@NathanReed sounds like the answer to me" CreationDate="2016-03-29T08:07:37.823" UserId="38" />
  <row Id="2584" PostId="2246" Score="0" Text="They may do some edge checking to maintain the crispness" CreationDate="2016-03-29T12:54:16.693" UserId="137" />
  <row Id="2585" PostId="2247" Score="1" Text="What level of authenticity to early-90s games are you looking for? You could just alpha-blend the sprites over the background and it will look &quot;right&quot;. If it must be done using palettes, it's going to be a lot more difficult and limited. Or are you asking about how to do alpha blending? If so, we'll need more info about how your renderer works, which API(s) you're using, etc." CreationDate="2016-03-29T16:26:05.513" UserId="48" />
  <row Id="2586" PostId="2247" Score="0" Text="Alpha blending should work(it's not 100% implemented yet), by the way of 50% blending every two color combination in the palette, finding the best fit color from the palette and writing that to an array. This array is then indexed by the background color and the new pixel's color to get the color that is then put on the screen." CreationDate="2016-03-29T17:37:58.307" UserId="3020" />
  <row Id="2587" PostId="2247" Score="0" Text="OK, assuming you have a palette that has close enough fits for all the blended colors, that could work. But then, what's the question about? Sounds like you already have a plan for how to do it." CreationDate="2016-03-29T17:44:44.280" UserId="48" />
  <row Id="2588" PostId="2247" Score="0" Text="Whether to first apply lighting to the sprite texel and then blend it with the background or first blend with the background and then apply lighting. (Or possibly something else entirely?)" CreationDate="2016-03-29T19:56:43.433" UserId="3020" />
  <row Id="2589" PostId="1741" Score="0" Text="...that you know of." CreationDate="2016-03-29T21:15:07.007" UserId="2091" />
  <row Id="2590" PostId="2234" Score="0" Text="Further to [pcj50's answer](http://computergraphics.stackexchange.com/a/2234/3007), you had to account for the peculiarities of every graphics card out there. While they claimed to support standards like EGA, VGA &amp; VESA VBE, there were peculiarities &amp; limitations with their implementations that you had to account for. I remember several graphics programs (not games) didn't work properly on my ATI Mach 32 card because ATI's implementation of VGA (or was that VESA VBE) was notoriously dodgy." CreationDate="2016-03-27T12:23:36.130" UserDisplayName="user3007" />
  <row Id="2592" PostId="2030" Score="0" Text="It would be useful to include the additional information (such as the comment mentioning OS and GPU) in the question so it is more accessible. Also, comments are not intended to last long term. If there's any information you want to add to the question, just [edit]." CreationDate="2016-03-30T02:52:37.057" UserId="231" />
  <row Id="2593" PostId="2252" Score="0" Text="Thanks. After reading of Vulkan doc I could find the similar for directx [fine](https://msdn.microsoft.com/en-us/library/windows/desktop/hh446950(v=vs.85).aspx) and [coarse](https://msdn.microsoft.com/en-us/library/windows/desktop/hh446948(v=vs.85).aspx) derivatives." CreationDate="2016-03-30T12:00:36.940" UserId="386" />
  <row Id="2595" PostId="2256" Score="1" Text="Sorry wrote this quickly on my phone. Need to add links and stuff. Maybe organize and a few rounds of google. Feel free to fix my typos." CreationDate="2016-03-30T21:37:12.947" UserId="38" />
  <row Id="2597" PostId="1983" Score="0" Text="Not sure if this helps, but the book [3D Engine Design for Virtual Globes](http://www.virtualglobebook.com/) includes a chapter on GPU ray-casting the globe, which involves computing the horizon in the fragment shader.  Disclaimer, I know the authors of this book, after they wrote it they went on to be the founders of [Cesium](http://cesiumjs.org)." CreationDate="2016-03-31T21:08:16.023" UserId="1908" />
  <row Id="2600" PostId="2030" Score="0" Text="There are usually two limits, one having &quot;native&quot; in the name. Beyond native limits the driver is likely to emulate behavior in software. MB of shader data storage is never good. I had similar problem in DX11 a few years back. You should try move that data to a buffer object of some kind or divide the issue into more passes." CreationDate="2016-03-31T18:53:43.617" UserId="3041" />
  <row Id="2601" PostId="2030" Score="0" Text="Why is a buffer object better than a shader data storage? I thought it was the other way around." CreationDate="2016-03-31T23:39:04.643" UserId="2666" />
  <row Id="2602" PostId="2030" Score="0" Text="@gartenriese Better? How do you mean?" CreationDate="2016-04-01T06:07:30.627" UserId="3041" />
  <row Id="2603" PostId="2030" Score="0" Text="@Andreas: If you look [here](https://www.opengl.org/wiki/Shader_Storage_Buffer_Object), three out of four points are in favor of SSBOs over UBOs." CreationDate="2016-04-01T12:08:45.787" UserId="2666" />
  <row Id="2604" PostId="2201" Score="0" Text="The comments on your answer suggest some confusion over what is meant by &quot;maximal&quot;. I believe you are using &quot;maximal&quot; to mean a convex surface which is not a subset of a larger convex surface, rather than to mean the largest convex surface that exists in the triangle mesh. This is covered in your question but due to the confusion it might be worth editing to clarify." CreationDate="2016-04-02T07:51:33.137" UserId="231" />
  <row Id="2605" PostId="2201" Score="0" Text="It would be helpful to know what the purpose is, to give a better idea of exactly what is required. For example, does a convex patch need to be strictly convex (never flat), or does it count as convex provided it is nowhere concave? A cylinder is nowhere concave. It is convex everywhere, but it is not strictly convex in every direction (it is flat in the axial direction). Do you want the surface of a cylinder to count as one convex patch?" CreationDate="2016-04-02T07:59:20.887" UserId="231" />
  <row Id="2606" PostId="2259" Score="0" Text="Could you clarify what you mean by &quot;360 degree stereo video&quot;? You can create a stereo image by taking two images from  different positions (approximating one from each eye). However, if you take two 360 degree images then the stereo effect will be strongest in the directions perpendicular to the offset between the images, and zero in the directions parallel to the offset. To get a stereo effect in all directions I would expect two images in each of a wide variety of directions to be necessary. Answering your question will require knowing more about the input that is provided." CreationDate="2016-04-02T08:12:36.327" UserId="231" />
  <row Id="2608" PostId="2259" Score="0" Text="In the sense of what Kolor Eyes uses. You have a video, e.g. 1920x1080, split into two halves (side by side), each half (960x1080) then needs to be mapped onto a sphere and then you render the viewpoint of a viewer at the centre of that sphere looking out in a particular direction." CreationDate="2016-04-03T00:51:46.103" UserId="3038" />
  <row Id="2611" PostId="2201" Score="0" Text="@trichoplax thanks for the suggestion! yes, a convex surface which is not a subset of a larger convex surface, is what is meant. And yes, it needs to be strictly convex. For a cylinder, there would be one convex patch in the whole figure which is maximal, and this patch would not include the axial flat surfaces." CreationDate="2016-04-03T08:43:49.370" UserId="2898" />
  <row Id="2613" PostId="2259" Score="0" Text="Kolor Eyes appears to provide 360 degree video, but not in stereo. Are you looking to make your 360 degree stereo video by combining two 360 degree videos from two similar viewpoints, or by combining a large number of 360 degree videos from many similar viewpoints?" CreationDate="2016-04-03T10:54:20.407" UserId="231" />
  <row Id="2614" PostId="2259" Score="0" Text="You mention rendering in your comment. Are you working with recorded real world video here, or a rendered artificial scene?" CreationDate="2016-04-03T10:54:59.813" UserId="231" />
  <row Id="2615" PostId="2259" Score="0" Text="Note that requests for software recommendations and off site resources are off topic. See [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic) I recommend editing to remove the request for existing software or libraries, keeping just the request for how to program this yourself, in order to avoid the question being closed." CreationDate="2016-04-03T11:05:03.730" UserId="231" />
  <row Id="2616" PostId="2201" Score="0" Text="I meant that the curved surface of the cylinder is itself flat in the direction parallel to its central axis. Depending on whether you move to the next triangle around the cylinder or along the cylinder, it will appear to be either strictly convex (moving around), or flat (moving along). Do you require the surface to be strictly convex in every direction, or only one direction?" CreationDate="2016-04-03T11:15:01.443" UserId="231" />
  <row Id="2617" PostId="2204" Score="0" Text="A possible problem with this approach is that for some surfaces there will exist a path between two adjacent triangles that is made up only of convex steps, even though the two adjacent triangles touch along a concave edge. In sufficiently smooth examples this may not be a problem as the inaccuracy will only tend to be one triangle wide, but it is possible to construct examples with arbitrarily large concavities that are accessible by purely convex paths. For example, imagine the shape made by pushing a pin into a balloon so that the surface dips inwards (assuming it doesn't burst)." CreationDate="2016-04-03T11:30:31.910" UserId="231" />
  <row Id="2618" PostId="2201" Score="0" Text="It may help to consider a torus. Do you require an algorithm that makes the outer half of the torus a single convex patch, with the triangles on the inner half (around the hole) not counted as being in any convex patch? Or do you require an algorithm that makes the whole surface of the torus a single convex patch, since even on those parts of the surface that are concave in one direction, they are still convex in the perpendicular direction?" CreationDate="2016-04-03T11:38:01.053" UserId="231" />
  <row Id="2620" PostId="2189" Score="1" Text="Assuming that the content of the FBO is correct, and based on your comment and the look of the cube on your screenshot, it sounds to me your texture coordinates are simply incorrect. You could use a regular texture as Mokosha suggests, and switch to FBO only after you have the cube textured correctly." CreationDate="2016-04-03T12:32:00.377" UserId="182" />
  <row Id="2622" PostId="2266" Score="1" Text="You can actually turn Photoshop effects to be profile  aware its just not on by default because of backwards compatibility and least surprise for old users. Mind you though its not perfect. I would say that in general ALL of our color correction workflow is totally whacked because if the tacked on by later date nature. so for 5 i propose (d) because its hard and time consuming to rebuild a new all encompassing standard that can replace the old ones." CreationDate="2016-04-04T07:12:00.370" UserId="38" />
  <row Id="2623" PostId="2201" Score="0" Text="@trichoplax strictly convex is what I am looking for. In the example of the Torus, I require an algorithm that makes the outer half of the torus a single convex patch, with the visible (the inner surface of the other side of the torus will be visible to the viewer) triangles on the inner half (around the hole) , that forms a concave lateral arc to the viewer, it should show convex strips perpendicular to the concave arc. I hope I was able to communicate the idea effectively." CreationDate="2016-04-04T07:57:35.103" UserId="2898" />
  <row Id="2624" PostId="2204" Score="0" Text="@trichoplax I am myself not sure that the algorithm I have posted is optimal or the best one. If you have a better algorithm, please do share. I would love receive it! If another answer turns out to be better, I will unaccept my answer the accept the better one. And for your example, as I said above in coment to the question, if there are local convex strips, they should be shown as local convex strips." CreationDate="2016-04-04T08:00:49.620" UserId="2898" />
  <row Id="2627" PostId="2201" Score="0" Text="Your description of how the torus should be divided is perfectly clear, but I can't see a way of translating that into a rigorous requirement that would apply to other types of surface. The strips are distinct from each other due to concave edges, but they all seem to be connected to the large outer convex patch, and therefore part of it. I think the main question here is how to unambiguously define what a maximal convex patch is." CreationDate="2016-04-04T12:39:02.793" UserId="231" />
  <row Id="2629" PostId="2204" Score="0" Text="I understand that your answer is just an example to get things started. I was just giving some feedback in the hope that it will trigger an idea for improvement from someone." CreationDate="2016-04-04T12:40:59.857" UserId="231" />
  <row Id="2630" PostId="2204" Score="0" Text="@trichoplax thanks :)" CreationDate="2016-04-04T13:43:22.513" UserId="2898" />
  <row Id="2634" PostId="2272" Score="0" Text="Get it! You are a great guy. Thanks a lot." CreationDate="2016-04-05T09:40:31.677" UserId="3058" />
  <row Id="2635" PostId="2271" Score="0" Text="For VR you need super low latency, any type of network hop will be too long." CreationDate="2016-04-05T10:18:00.380" UserId="137" />
  <row Id="2636" PostId="2271" Score="0" Text="Only if the cloud is very close to the vr gear like, within tens of meters. But given that everything and their dad is going to be a computer that is certainly possible." CreationDate="2016-04-05T12:59:35.223" UserId="38" />
  <row Id="2639" PostId="2272" Score="0" Text="There are two causes of tearing: rendering to the visible buffer (front buffer rendering) and swapping to the back buffer in the middle of a display refresh. Vsync assumes you are doing at least double buffered rendering and holds the swap until vblank - there is no tearing whatsoever. Since rendering rarely completes during vsync, two buffers can be held up waiting on display (one currently displayed, one waiting for vsync) thus requiring triple+ buffering to keep rendering continuous..." CreationDate="2016-04-05T17:50:14.567" UserId="2500" />
  <row Id="2644" PostId="2271" Score="0" Text="@ratchetfreak,  I added more detail info above. That explains what I'm thinking" CreationDate="2016-04-06T00:59:47.590" UserId="3058" />
  <row Id="2645" PostId="2273" Score="0" Text="I added more detail info above. That explains what I'm thinking" CreationDate="2016-04-06T01:00:01.760" UserId="3058" />
  <row Id="2646" PostId="2271" Score="0" Text="@HaoZhang well if your calculations that the total time to display a frame to the user from the cloud is 15.5ms then things should work out. In order to have a game running at 60FPS you need to display a frame on the screen every 16.6ms. Are you sure its going to take 15.5ms to display both frames(one for each screen of the VR system)?" CreationDate="2016-04-06T07:18:05.233" UserId="204" />
  <row Id="2647" PostId="2271" Score="0" Text="I give it 5ms for transmission to OLED display of 2K*2@200Hz, that would need at least 40Gbps interface and cable, as display port 1.4. (https://en.wikipedia.org/wiki/DisplayPort) supports. Yes it would be a little costly but it is achievable." CreationDate="2016-04-06T08:45:59.250" UserId="3058" />
  <row Id="2648" PostId="2272" Score="0" Text="Yes, I heard triple buffer technology. That's cool. Thanks." CreationDate="2016-04-06T09:00:12.287" UserId="3058" />
  <row Id="2650" PostId="2275" Score="1" Text="The reason they're using a FPGA, is they require hardware features that the GPU does not have. So they emulate a GPU using a FPGA." CreationDate="2016-04-06T14:24:19.277" UserId="310" />
  <row Id="2653" PostId="2008" Score="3" Text="@Dragonseel: since it was the correct observation, could you turn the comment into an answer?" CreationDate="2016-04-07T02:53:05.380" UserId="182" />
  <row Id="2654" PostId="2276" Score="0" Text="Hi Nathan, nice to meet you here. I have read some of your presentations on Nvidia Gameworks VR. &quot;Timewarp&quot; can reduce the latency but with image quality loss. And I think, if the head rotated very fast,  it's very hard to warp the 2D image to cheat the eyes. Motion blurring may be help in this case. Local client or remote server could send an unclear image to the display in very low latency. You mentioned Cloud could do some offloadings on VR. That's a cool. What's the requirement for the network bandwidth and latency? Does it still need high bandwidth and low latency? Thanks." CreationDate="2016-04-07T06:08:09.030" UserId="3058" />
  <row Id="2655" PostId="2277" Score="0" Text="The second paper use &quot;ray casting&quot; -- a simplified version of  &quot;ray tracing&quot;. It could reduce the computing power dramatically. I think the rendering quality is not as good as full ray tracing rendering. But, is it acceptable in most cases? And, as what you said, frameless rendering, or racing the beam, could be implemented as spliting the screen into several blocks and &quot;racing the block&quot; instead of &quot;racing the beam&quot;. Of course it need modify the GPU and display. It seems wonderful, does it?" CreationDate="2016-04-07T06:31:30.850" UserId="3058" />
  <row Id="2656" PostId="2275" Score="0" Text="It seems that in second paper the authors used &quot;ray casting&quot; instead of rasterization. So they used FPGA directly, not emulate a GPU? And it also seems they didn't support any API like OpenGL or DirectX. Do you mean the authors just need FPGA to &quot;race the beam&quot;?" CreationDate="2016-04-07T06:38:59.827" UserId="3058" />
  <row Id="2657" PostId="2276" Score="0" Text="You are right, current internet access are mostly based on dialup or DSL. But, in the near future, e.g. 5-10 years later, could something be changed? Google has deployed 1Gbps fiber in some cities and I heard Google also tried to introduce 10Gbps to home." CreationDate="2016-04-07T06:48:39.943" UserId="3058" />
  <row Id="2658" PostId="2276" Score="0" Text="haha, Nathan, I got some information from your blog: (http://www.reedbeta.com/blog/2014/04/03/vr-and-multi-gpu/#more-591), &quot;That being said, what if the high-spec dGPUs were far away from you? Cloud-based rendering is a topic that’s been getting interest lately, and the same kind of latency reduction strategies could be applicable there. Imagine having your VR headset driven by the little GPU in your phone, but streaming down source frames or shading cache updates from a dGPU in the cloud (or your home PC)! &quot;" CreationDate="2016-04-07T07:23:32.787" UserId="3058" />
  <row Id="2660" PostId="2279" Score="0" Text="I can see shadows. Therefore i do not concurr with your analysis. However secondary light might have too high reflectivity. Could you try using a simpler scene." CreationDate="2016-04-07T13:10:39.740" UserId="38" />
  <row Id="2661" PostId="2279" Score="1" Text="@joojaa edited the description with a target reference image" CreationDate="2016-04-07T13:31:07.520" UserId="3069" />
  <row Id="2662" PostId="2279" Score="0" Text="Using a simpler scene helps you debug the effect. To me it looks like the shadows are there your scene is just not color corrected and you bounce too nuch energy." CreationDate="2016-04-07T13:57:06.897" UserId="38" />
  <row Id="2663" PostId="2275" Score="0" Text="As Nathan mentioned in his anwer, in order to &quot;race the beam&quot;, you need a very tight synchronization of the &quot;graphics hardware&quot; and the OS/CPU. Current GPUs and Graphics APIs don't have this support. So the authors created the hardware themselves using a FPGA. In theory, the hardware they created in FPGA could be manufactured into &quot;real&quot; hardware." CreationDate="2016-04-07T14:59:28.613" UserId="310" />
  <row Id="2664" PostId="2008" Score="0" Text="@JulienGuertault yes. Done" CreationDate="2016-04-07T14:59:52.257" UserId="273" />
  <row Id="2665" PostId="2008" Score="0" Text="@Dragonseel: thank you." CreationDate="2016-04-07T15:41:55.693" UserId="182" />
  <row Id="2666" PostId="2271" Score="0" Text="Although this question as phrased appears to be broad and opinion based, the answers show that the possibilities are not as arbitrary as they may at first appear, and objective restrictions can be placed on what is likely to happen." CreationDate="2016-04-07T16:21:10.270" UserId="231" />
  <row Id="2667" PostId="2279" Score="0" Text="@joojaa I just posted some of the code because I am using a much more complex framework. That's why I was avoiding rendering more simple scenes, because it requires me some effort finding out how exactly the scene loader works.  Ok, may you suggest me code-wisely what I could change to color the scene correctly?" CreationDate="2016-04-07T16:27:09.340" UserId="3069" />
  <row Id="2668" PostId="2281" Score="0" Text="First of all thanks for your answer :)&#xA;&#xA;So, the method to check the intersection between the ray and the square light is right. I have already been asking about it cause of a small error I couldn't find. The method and the relative solution can both be found here:&#xA;&#xA;http://stackoverflow.com/questions/36180741/intersection-of-ray-and-rectangle-in-c/36186088#36186088&#xA;&#xA;I also edited the question adding a picture that shows the result in case I omit the **depth &gt; 1** check. Maybe it can help understanding where the problem is.. thanks once more" CreationDate="2016-04-07T19:52:56.443" UserId="3069" />
  <row Id="2669" PostId="2279" Score="0" Text="It might help to restrict your image to a very small region in which you expect to have a very obvious shadow. Then you can get a more detailed look at that region in a much shorter rendering time, to see if it really is zero shadow, or just a less obvious softened shadow." CreationDate="2016-04-07T23:24:54.983" UserId="231" />
  <row Id="2670" PostId="2275" Score="0" Text="You say &quot;real hardware&quot;, does it mean ASIC?" CreationDate="2016-04-08T00:31:58.077" UserId="3058" />
  <row Id="2671" PostId="2283" Score="2" Text="Pro tips, unrelated to your question: you can expect `(foo * foo)` to be a lot faster than `pow(foo, 2.0f)`; if your data is organized row by row (as opposed to column by column), you should swap the for loops to traverse data in a more coherent order." CreationDate="2016-04-08T02:02:55.560" UserId="182" />
  <row Id="2672" PostId="1519" Score="0" Text="Do you have sources to recommend on the fact that metals do transmit light? What is the order of magnitude of the contribution to the perceived color?" CreationDate="2016-04-08T08:03:34.527" UserId="182" />
  <row Id="2674" PostId="2289" Score="1" Text="Great answer, thanks a lot!" CreationDate="2016-04-08T12:19:37.890" UserId="182" />
  <row Id="2676" PostId="1519" Score="0" Text="@JulienGuertault It's fairly standard knowledge, and Google returns useful results. E.g. [this](https://www.reddit.com/r/askscience/comments/37ktye/would_it_be_possible_to_make_a_thin_enough_sheet/crnsmgw) &quot;Below 50 nm thickness, the light transmitted through gold is green (imagine all the colours that aren't reflected are coming through . . .).&quot; Probably appearance depends. Metals reflect a lot, even with thin coatings. The amount refracted depends on the extinction coefficient and thickness." CreationDate="2016-04-08T16:25:32.930" UserId="523" />
  <row Id="2682" PostId="2288" Score="1" Text="Could you narrow things down a bit. Like what have you tried. What is your level of expertise. Think of it thisway, if I were to answer you as briegly as you ask would you be happy?" CreationDate="2016-04-09T05:14:46.607" UserId="38" />
  <row Id="2683" PostId="2288" Score="0" Text="I am pretty much a newbie, never touch cg before, just try to get some keywords so that I can google around and move forward. But details are welcomed too." CreationDate="2016-04-09T10:27:03.123" UserId="3074" />
  <row Id="2684" PostId="2288" Score="1" Text="Problem is that since your question is high level you also get a high level answer that pobably does not help you much. The first step is hard. Not the what to do once you have it done." CreationDate="2016-04-09T10:54:49.477" UserId="38" />
  <row Id="2685" PostId="2030" Score="0" Text="@gartenriese (Sry for delay, didnt have rep to comment) Oh I see. SSBO have additional features, and that is &quot;better&quot;. Ok. My idea of a &quot;buffer storage&quot; was not SSBO/UBO. This post mentions &quot;texture buffer objects&quot; that are quite large without the requirement to be writeable: http://stackoverflow.com/questions/7954927/glsl-passing-a-list-of-values-to-fragment-shader" CreationDate="2016-04-09T15:15:36.510" UserId="3041" />
  <row Id="2687" PostId="2294" Score="0" Text="Did you get to try this? Did it work?" CreationDate="2016-04-10T07:33:33.293" UserId="3083" />
  <row Id="2691" PostId="2275" Score="0" Text="No, just any chip. It could be a modified GPU. When I say 'real', I just mean that it's silicon logic, rather than programmed logic, ie. a FPGA." CreationDate="2016-04-10T15:17:58.693" UserId="310" />
  <row Id="2692" PostId="2294" Score="0" Text="I just did it now. I originally had the pos defined in the 4x4 view matrix, but i took it out and did it at the end, and it worked! Thankyou!" CreationDate="2016-04-10T16:05:34.780" UserId="2953" />
  <row Id="2693" PostId="2291" Score="0" Text="your answer was very useful, thanks first of all ! You where right, **distFromLight** and  **distFromObj** where wrongly calculated. I fixed it. Something has been improved but the problem is not fixed yet. I posted one picture showing the new results. I thought maybe the error relies on too soft shadows due to a too big light, therefore I tried to lower the dimension of the light. No matter how increase the power of the light, but in this case results are bad (posted a second picture about it). Hope it helps you. In the meantime I am working on the .obj loader for a simpler scene." CreationDate="2016-04-10T17:15:00.353" UserId="3069" />
  <row Id="2695" PostId="2300" Score="0" Text="Looks fine to me. Is anything not working as it should, or do you have any specific questions about the code or Euler angles? Asking for &quot;your opinion about this approach&quot; is very vague and will probably lead to questions like this being closed on this site." CreationDate="2016-04-10T19:45:35.710" UserId="48" />
  <row Id="2696" PostId="2300" Score="1" Text="Thanks @NathanReed for your reply and reviewing my code. You were right about the 'open endedness' of the question, thus, rephrased it." CreationDate="2016-04-10T20:40:28.353" UserId="3098" />
  <row Id="2697" PostId="2291" Score="1" Text="A smaller light means more of the rays will never reach the light, so you will need to increase the number of rays to give a better quality image. This will also increase the time it takes to render. It might be worth instead making the light as large as possible (an infinite or very large plane). This way, if the problem is transparent surfaces, you should see even more evidence for it, with the added benefit that the largest possible light will reduce the graininess of the image." CreationDate="2016-04-10T22:45:39.950" UserId="231" />
  <row Id="2698" PostId="2291" Score="0" Text="I've added an additional final paragraph about putting the light underground as a test (see edited answer) that may help if you are unable to change the scene, but able to move the light." CreationDate="2016-04-10T22:49:17.140" UserId="231" />
  <row Id="2699" PostId="2279" Score="1" Text="Seeing your updated screenshot (3rd edit), is it still rendered with 16spp? With a only small area light, and no bidirectional path tracing or next event estimation, it's expected to have a lot of noise. Have you tried a much higher number, like 1000spp?" CreationDate="2016-04-11T01:55:11.087" UserId="182" />
  <row Id="2700" PostId="2301" Score="3" Text="Your question looks like it would belong to StackOverflow rather than here." CreationDate="2016-04-11T04:40:07.833" UserId="182" />
  <row Id="2701" PostId="2030" Score="0" Text="@Andreas: Thanks for the link, very informative. However the top answer says that SSBOs are even larger than TBOs, so I don't think this should be the problem. But I can try anyways!" CreationDate="2016-04-11T06:24:15.123" UserId="2666" />
  <row Id="2702" PostId="2030" Score="0" Text="@gartenriese That's not what I read. SSBO are limited to 16MB. TBO is limited to VRAM. And you do have more VRAM than that, right? In any case you should check those parameters for you platform (glGetInteger)." CreationDate="2016-04-11T06:34:53.510" UserId="3041" />
  <row Id="2703" PostId="2300" Score="1" Text="[Meta discussion about review questions.](http://meta.computergraphics.stackexchange.com/q/223/16)" CreationDate="2016-04-11T07:04:09.383" UserId="16" />
  <row Id="2704" PostId="2030" Score="1" Text="@Andreas: No, they are not limited to 16MB. 16MB is the minimal size it is guaranteed to have. Usually they are limited to VRAM. If you look at my question I already checked the size with `GL_MAX_SHADER_STORAGE_BLOCK_SIZE` and it's 2GB (which isn't actually my VRAM size, but still big enough)." CreationDate="2016-04-11T09:53:47.163" UserId="2666" />
  <row Id="2705" PostId="2030" Score="0" Text="@gartenriese Oh right, my bad." CreationDate="2016-04-11T09:55:06.207" UserId="3041" />
  <row Id="2706" PostId="2279" Score="0" Text="@JulienGuertault yes it is still rendered with 16 spp. No I didn't try with 1000spp because it would take a huge amount of time and this project is about a research topic which aims to good quality images with low number of spp." CreationDate="2016-04-11T12:59:48.033" UserId="3069" />
  <row Id="2707" PostId="2291" Score="0" Text="Ok, I followed your suggestion. I tried to put the light beneath the scene and nothing is lit up. No light passes through surfaces and therefore surfaces are not transparent. At least we can exclude this possibility.. so it can be that shadows are too soft or washed out by too much light reflected from the rest of the scene. How do you suggest me to proceed?" CreationDate="2016-04-11T13:06:12.773" UserId="3069" />
  <row Id="2708" PostId="2291" Score="0" Text="Have you tried rendering just a small section of the image yet? For example, the bottom right sixteenth of the image contains are strong shadow in the target image. Rendering just that small image would allow you to use a much higher number of samples per pixel, to give a clearer image and more idea of what is going on." CreationDate="2016-04-11T14:03:53.410" UserId="231" />
  <row Id="2709" PostId="2291" Score="0" Text="just did it, shadows are not there.. you can find the piece of image above at EDIT 4" CreationDate="2016-04-11T14:38:21.090" UserId="3069" />
  <row Id="2710" PostId="2291" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/38237/discussion-between-tarta-and-trichoplax)." CreationDate="2016-04-11T15:39:07.487" UserId="3069" />
  <row Id="2736" PostId="2302" Score="3" Text="I would just add that Euler angles work very well for FPS-style or orbiting cameras, where you can only rotate on 2 axes, and controls map directly to the yaw and pitch angles. There are no gimbal lock problems then. If your camera can rotate on all 3 axes, then quaternions may make more sense." CreationDate="2016-04-13T06:06:54.480" UserId="48" />
  <row Id="2738" PostId="2287" Score="0" Text="But how does the OS ask the GPU for memory to write to when you plugged in an HDMI port. Do OSes manage their own framebuffer when doing &quot;naive&quot; drawing operations and send the 1920*1080 pixel big framebuffer to the GPU with some arbitrary and common standard that can be implemented or does one need a driver for each GPU type so that one can send a simple text message over HDMI ? How does  a tty or xserver send it's content through HDMI to the screen?" CreationDate="2016-04-13T10:06:14.970" UserId="2623" />
  <row Id="2739" PostId="2287" Score="0" Text="The video card in a PC will handle encoding VRAM to a video signal, the CPU is never involved in this. For unaccelerated OS's they will keep a system memory based frame buffer and use the CPU to draw into it, and then send regions of it over the PCIE bus to the card (typically 'dirty' regions or just areas of the screen that need updating), so some GPU driver/motherboard driver is required to do this." CreationDate="2016-04-13T10:19:22.433" UserId="3073" />
  <row Id="2740" PostId="2287" Score="0" Text="So implementing this in my very own OS will not really be possible using open and known standards?" CreationDate="2016-04-13T10:34:27.023" UserId="2623" />
  <row Id="2741" PostId="2305" Score="1" Text="If you plan to immediately answer your own question please at least mention it in the question so others don't waste their time." CreationDate="2016-04-13T12:50:45.033" UserId="457" />
  <row Id="2742" PostId="2305" Score="2" Text="Every good question should have 2 or 3 good answers. You did not waste your time and I really appreciated your effords. I need this thread to explain the logic to a project partner and your post will help him alot." CreationDate="2016-04-13T12:53:50.467" UserId="361" />
  <row Id="2743" PostId="2279" Score="0" Text="Comments are not for extended discussion; this conversation has been [moved to chat](http://chat.stackexchange.com/rooms/38357/discussion-on-question-by-tarta-path-tracer-not-rendering-shadows)." CreationDate="2016-04-13T22:48:22.993" UserId="231" />
  <row Id="2744" PostId="2279" Score="0" Text="@SlippD.Thompson and Tarta, this is not an appropriate place to have a discussion about behaviour on an external site. You are both very welcome on this site, but complaints relating to other sites will need to be addressed there, not here. If you use the chat room created to continue discussing the original path tracing question, please do so respectfully. Heated conflict over differences tends to decrease understanding rather than increase it." CreationDate="2016-04-13T23:00:09.740" UserId="231" />
  <row Id="2745" PostId="2307" Score="0" Text="Thanks for your reply.&#xA;Idea with writing directly to color buffer sounds great. I will give it a try." CreationDate="2016-04-14T05:13:19.750" UserId="3123" />
  <row Id="2746" PostId="2309" Score="0" Text="Yes this tracing works, but if the object was sealed in the first place wouldt the contour have a sidedness?" CreationDate="2016-04-14T09:17:09.850" UserId="38" />
  <row Id="2747" PostId="2309" Score="0" Text="I'm not sure I follow. Isn't the point of the question to determine this sidedness? It's not like you magically get it as a parameter after the CSG slice." CreationDate="2016-04-14T09:23:23.287" UserId="2817" />
  <row Id="2748" PostId="2309" Score="1" Text="You do get the sidedness of  the model, 3d already has a sidedness which is almost allways the case. When you slice the model you know which side is outwards so for each loop you also know which side of it is inside (jsut record normal in 2D also). This means that if you loop around the contour clockwise (from above) and the predominate cross product of each edge is away from you then its a interior edge. But the trace trick can be faster as its a O(log(n)) operations while this test is n." CreationDate="2016-04-14T09:43:29.437" UserId="38" />
  <row Id="2749" PostId="2309" Score="0" Text="Ah, so you basically mean using the surface normal for that? I can see that working, but only if you have a way of knowing what &quot;away&quot; means. :) And while it's trivial for convex meshes (centroid should be fair enough), concave ones will be tricky. The ray trace method sidesteps this issue." CreationDate="2016-04-14T10:37:14.187" UserId="2817" />
  <row Id="2750" PostId="2311" Score="0" Text="Yes i know this, but wondering if there could be a alternate strategy to avoid division by 0." CreationDate="2016-04-14T10:37:54.963" UserId="38" />
  <row Id="2751" PostId="2309" Score="1" Text="that's whats i was alluding winding rules can handle this for you but tracing is less work." CreationDate="2016-04-14T10:40:02.013" UserId="38" />
  <row Id="2752" PostId="2287" Score="0" Text="You could have a look here for some more specifics: http://wiki.osdev.org/VGA_Hardware" CreationDate="2016-04-15T06:26:36.063" UserId="3073" />
  <row Id="2759" PostId="2305" Score="1" Text="@5chdn [Self answering is highly encouraged](http://blog.stackoverflow.com/2012/05/encyclopedia-stack-exchange/). Thanks for adding this answer. If you have an answer in mind at the time you post a question, there is a tick box below the question which will allow you to write your answer before posting the question, so both will appear at the same time. This is just to let you know in case it is useful - you certainly don't need to do this and self answers are still very welcome at any interval of time after posting the question." CreationDate="2016-04-16T09:48:50.590" UserId="231" />
  <row Id="2761" PostId="2319" Score="0" Text="This is what I was looking for! Thanks!!" CreationDate="2016-04-17T00:18:11.207" UserId="3144" />
  <row Id="2762" PostId="2311" Score="0" Text="You could avoid divide by zero by pushing in or out values that would cause one." CreationDate="2016-04-17T00:24:11.360" UserId="56" />
  <row Id="2763" PostId="2318" Score="0" Text="Also look into GJK and MPR (minkowski portal refinement)" CreationDate="2016-04-17T01:05:03.843" UserId="56" />
  <row Id="2764" PostId="2291" Score="0" Text="EDIT 6 contains the final explanation" CreationDate="2016-04-17T10:22:27.487" UserId="3069" />
  <row Id="2765" PostId="2310" Score="0" Text="Imagine having a 3d object INSIDE your eye. How would that look like in your opinion? What do your buddies think?" CreationDate="2016-04-17T11:06:14.913" UserId="3041" />
  <row Id="2766" PostId="2310" Score="0" Text="@Andreas yes but that only how the linear algebra trick works we could use numerous other tricks." CreationDate="2016-04-17T11:17:16.863" UserId="38" />
  <row Id="2767" PostId="2310" Score="0" Text="What other tricks?" CreationDate="2016-04-17T11:21:15.167" UserId="3041" />
  <row Id="2768" PostId="2310" Score="0" Text="@Andreas we could do spherical projections for example. Just because we have one mathematical model that works does not mean there are no other ways to solve problems." CreationDate="2016-04-17T11:50:04.330" UserId="38" />
  <row Id="2772" PostId="2310" Score="0" Text="Removed my answer" CreationDate="2016-04-17T14:59:11.590" UserId="3041" />
  <row Id="2773" PostId="2311" Score="0" Text="@joojaa Raytracing does not divide by zero." CreationDate="2016-04-17T15:01:07.537" UserId="3041" />
  <row Id="2774" PostId="2311" Score="0" Text="@Andreas Yes i know that neither does reality ;)" CreationDate="2016-04-17T15:01:44.980" UserId="38" />
  <row Id="2776" PostId="2326" Score="0" Text="Temporal algorithms ought to help, by reusing information from frame to frame, thus amortizing rendering costs over time." CreationDate="2016-04-18T04:45:53.690" UserId="56" />
  <row Id="2777" PostId="2323" Score="0" Text="Ok now we are getting somewhere. I am not really concerned in how most rasterizers work but rather if there is a novel way to do it that does not have this problem. I am thinking of accepting this answer as it hints at the answer, and would avoid me having to show that in fact one can do this by approaching the problem differently than how current pipelines do it. Its not a feature of projection but rather the matrix operations and how we read the data in the projection." CreationDate="2016-04-18T07:18:21.897" UserId="38" />
  <row Id="2778" PostId="2332" Score="1" Text="Are you looking to replace regions of pure red with pure blue? Or are you looking to change the red component of each colour to blue (so an orange region becomes cyan)? Somewhere in between these two approaches, you could identify regions of the image that are &quot;sufficiently red&quot; and only apply the change to those regions. If you edit the question to describe more specifically what you require, we will be better able to suggest algorithms." CreationDate="2016-04-18T09:33:19.480" UserId="231" />
  <row Id="2779" PostId="2332" Score="0" Text="I want to change pure red with pure blue maintaing the lightning and shades of color" CreationDate="2016-04-18T09:40:28.387" UserId="2383" />
  <row Id="2780" PostId="2332" Score="0" Text="This sounds like you want to change more than just one precise colour. In this [picture of an apple](https://upload.wikimedia.org/wikipedia/commons/2/20/Civni-Rubens_apple.jpg) would you want all of the reddish parts to become bluish? Or would you want only the very reddest parts to become blue, and the rest of the reddish parts to remain reddish?" CreationDate="2016-04-18T11:13:05.583" UserId="231" />
  <row Id="2781" PostId="2332" Score="0" Text="i want to all red to become blue? for both wouldn't the algorithm differ only in the threshold taken for red" CreationDate="2016-04-18T11:33:21.457" UserId="2383" />
  <row Id="2782" PostId="2332" Score="0" Text="Do you have an example before and after image?" CreationDate="2016-04-18T13:31:29.027" UserId="231" />
  <row Id="2783" PostId="2167" Score="0" Text="Very interesting post. Could you post some images using different constants?" CreationDate="2016-04-18T18:07:41.437" UserId="3041" />
  <row Id="2785" PostId="2334" Score="0" Text="What I am trying to do is Color Replacement Tool in Photoshop. I donot know what Channel Mixer does. I am trying to replicate the Color Replacement Tool." CreationDate="2016-04-19T03:36:26.743" UserId="2383" />
  <row Id="2786" PostId="2332" Score="0" Text="What i am trying to do is done in this tool http://explorug.net/coloranything/   I want to know what are the best methods to achieve this. This is same as PhotoShop's Color Replacement Tool." CreationDate="2016-04-19T03:37:48.830" UserId="2383" />
  <row Id="2788" PostId="2343" Score="1" Text="some type of [linear regression](https://en.wikipedia.org/wiki/Linear_regression) would help find the parameters" CreationDate="2016-04-19T14:04:13.627" UserId="137" />
  <row Id="2789" PostId="2342" Score="0" Text="Do you have any physical interpretation for the two normal maps? e.g. one is high-frequency and one is low-frequency and applied &quot;on top of it&quot; (which is the Reoriented Normal Mapping case)?&#xA;&#xA;I would expect that you would need to feed the mesh's normal/tangent/bitangent into the equation somehow. You might consider simply transforming the world space normal into tangent space, applying RNM, then transforming the result to world space—that's only one extra 3x3 mul." CreationDate="2016-04-19T14:17:41.263" UserId="196" />
  <row Id="2790" PostId="2342" Score="0" Text="Thanks for your answer @JohnCalsbeek but using baked tangent space normal doesn't provide good enough results that's why I am using a world space normal, especially a world space bent normal map, and I try to apply on top of if the regular normal map." CreationDate="2016-04-19T19:24:20.670" UserId="2372" />
  <row Id="2791" PostId="2342" Score="0" Text="I've added some screenshot to illustrate the issue I am having." CreationDate="2016-04-19T20:01:02.903" UserId="2372" />
  <row Id="2793" PostId="2346" Score="4" Text="Maybe a silly question, but you're setting up two separate VAOs and two separate arrays of VBOs for the two meshes, right? If you're accidentally re-using them, that would explain why setting-up B screws up the rendering of A." CreationDate="2016-04-19T22:46:37.597" UserId="48" />
  <row Id="2794" PostId="2341" Score="0" Text="Hi, we try to avoid link only answers. Can you summarize the contents of the link. Thisway your answer is not nearly as prone to link rot." CreationDate="2016-04-20T04:58:39.273" UserId="38" />
  <row Id="2795" PostId="2341" Score="0" Text="@joojaa I've corrected the answer." CreationDate="2016-04-20T05:44:47.307" UserId="2372" />
  <row Id="2797" PostId="2350" Score="0" Text="What Icetigris said, with the correction that although nails aren't always reflective, they have a high Fresnel constribution, especially on the tops of the lengthwise ridges. These ridges tend to be polished through normal wear, even if the valleys of the ridges are more diffuse." CreationDate="2016-04-20T17:58:44.800" UserId="3184" />
  <row Id="2798" PostId="2328" Score="0" Text="Thanks for your answer Julien! I do agree with you, most likely there is some error in the code.. I am not sure whether I am doing some wrong calculation or there is some error at the level of the algorithm itself. That's why I posted my code.. unfortunately I tried to implement some variance reduction algorithm like next event prediction.. the results are not better. Indeed those algorithm should give back a &quot;better&quot; image, but something is wrong at the base" CreationDate="2016-04-21T07:53:12.640" UserId="3069" />
  <row Id="2800" PostId="2208" Score="0" Text="Questions asking us to **recommend or find a book, tool, software, tutorial or other off-site resource** are off-topic for Computer Graphics as they tend to attract opinionated answers and spam. I have edited to remove that request, so that the question need not be closed. If this changes your intention in other ways, please edit to correct this." CreationDate="2016-04-21T11:20:42.770" UserId="231" />
  <row Id="2801" PostId="2355" Score="1" Text="There is [rotating calipers](https://en.wikipedia.org/wiki/Rotating_calipers) but that only works for convex polygons. Otherwise you can use it to as a base for a brute force solution." CreationDate="2016-04-22T09:51:58.693" UserId="137" />
  <row Id="2802" PostId="2355" Score="2" Text="Well if O(n^2) isnt a problem then test all point pairs" CreationDate="2016-04-22T09:52:26.363" UserId="38" />
  <row Id="2804" PostId="2355" Score="0" Text="Yes, diameter, intra-shape width, not sure if theres a proper name for it. E.g. how'd you measure the maximum length of a 5-point star shape?" CreationDate="2016-04-22T10:18:05.417" UserId="3204" />
  <row Id="2805" PostId="2355" Score="0" Text="was hoping for a simpler solution than this: https://gis.stackexchange.com/questions/32552/how-to-calculate-the-maximum-distance-within-a-polygon-in-x-direction-east-west" CreationDate="2016-04-22T11:18:34.263" UserId="3204" />
  <row Id="2806" PostId="2355" Score="2" Text="Actually it's a bit more involved: imagine 2 rooms connected by a narrow corridor. The largest diameter will end on the walls in the different rooms and won't end on any points." CreationDate="2016-04-22T11:19:25.323" UserId="137" />
  <row Id="2807" PostId="2355" Score="1" Text="Are you looking for an algorithm that works in the most general case or can it be restricted to e.g. the 2D case? This might be easier to solve with some more information or restrictions about the input. You use the word polygon which may hint at 2D-only, also the question you linked suggests the 2D case. Also, is it enough to consider vertex-vertex distances or do you need correct results for cases like ratchet freak mentioned in [his comment](http://computergraphics.stackexchange.com/questions/2355/find-the-longest-straight-line-between-two-points-on-surface-of-polygon#comment2806_2355)?" CreationDate="2016-04-22T20:02:12.510" UserId="127" />
  <row Id="2808" PostId="2355" Score="0" Text="@Nero 2D is fine. Shapes are mostly organic. Cancer nodules in ultrasound. I suppose vertex vertex is OK." CreationDate="2016-04-23T05:26:46.540" UserId="3204" />
  <row Id="2810" PostId="2355" Score="0" Text="Is your raw data raster (pixel) data rather than a list of vertices? If so a raster approach which cuts out the intermediate step of converting to vertices may increase accuracy." CreationDate="2016-04-23T11:58:07.300" UserId="231" />
  <row Id="2811" PostId="2309" Score="0" Text="Let me see if I have understood you correctly. Ray tracing parallel to the slicing planes seems to not only help me get the inside/ouside of the contour , but also pave a way to implement GPU slicer for 3D model made of triangular meshes." CreationDate="2016-04-23T13:04:45.360" UserId="2712" />
  <row Id="2812" PostId="2355" Score="0" Text="@trichoplax yes, but I thought that would be slower and messier. The ROI was drawn in paint by hand. Convert to vertexes was just noting the coordinates of each white pixel." CreationDate="2016-04-23T14:10:34.943" UserId="3204" />
  <row Id="2813" PostId="2320" Score="0" Text="In your `Sample` function, there is the statement `float ndotl = dot(I.getNormal(), L);`. Not sure, but `I.getNormal()` looks a bit like it denotes the normalized `I` vector, instead of the surface normal at position `I`." CreationDate="2016-04-25T08:03:36.617" UserId="1835" />
  <row Id="2814" PostId="2212" Score="0" Text="Hi @ivokabel, sorry for the late reply. You advice was good, and point me in the right direction, but was not enough. Indeed adding a brightness parameter to the light to manage the spectrum of the light. But i also added another parameter to manage the total luminance of a pixel: the tristimulus value obtained from the path tracer is multiply with the total radiance of the pixel (sum of each SPD wavelenght of the pixel). In this way the image brightness is good." CreationDate="2016-04-26T11:49:18.877" UserId="2237" />
  <row Id="2815" PostId="2212" Score="0" Text="This is an example: https://drive.google.com/open?id=0BxeVnHLvT8-7MFNuN1pDcG1DZjg. Anyway thank you again. I hope to find a completely physically based solution before the end of my thesis time." CreationDate="2016-04-26T11:49:24.143" UserId="2237" />
  <row Id="2816" PostId="2151" Score="0" Text="@Detheroc Looks really inteestingif you could notify me too when  you release publicly." CreationDate="2016-04-26T14:18:21.603" UserId="204" />
  <row Id="2817" PostId="2151" Score="0" Text="@UriPopov Well, I have released the source code publicly, please see my new answer." CreationDate="2016-04-26T15:28:18.787" UserId="2811" />
  <row Id="2818" PostId="2360" Score="5" Text="You can use any standard image downsampling method. There's nothing special about raytraced images that requires a different method." CreationDate="2016-04-26T17:29:20.133" UserId="48" />
  <row Id="2819" PostId="2360" Score="1" Text="Is part of the question whether or not downsampling should be done in linear or gamma corrected space? Linear space is always appropriate - GPUs can apply a degamma-regamma when resolving gamma-corrected multisample buffers." CreationDate="2016-04-26T18:08:32.773" UserId="2500" />
  <row Id="2820" PostId="2326" Score="0" Text="The question states &quot;acoording to some sources, less than 1% of computers in use...&quot;. Could you be more precise in what sources you are thinking of?" CreationDate="2016-04-26T18:50:18.367" UserId="3041" />
  <row Id="2821" PostId="2326" Score="1" Text="@Andreas http://goo.gl/YqMpwQ&#xA;http://goo.gl/2oQBPw&#xA;&#xA;I did not believe that this statement was relevant enough to the question to require a source. Especially considering that (while the actual numbers are debatable), it is obvious and well known that VR is performance heavy and thus won't run on slower PCs." CreationDate="2016-04-26T19:29:28.087" UserId="3156" />
  <row Id="2822" PostId="2360" Score="0" Text="I'm not asking about whether it should be done in linear or gamma space (I know that it should be done in linear space). @NathanReed could you suggest an easy to implement one?" CreationDate="2016-04-26T19:42:32.240" UserId="1924" />
  <row Id="2823" PostId="2326" Score="0" Text="I was not sure if &quot;some&quot; were some guys you met on reddit rambling or, as the links points out, quite respectable companies and news networks. As for obvious well that is matter of today and tomorrow. Today is 1%. 2020 is estimated 8%. The latter may well cover everyone still interested in VR by that time so it may or may not be a problem." CreationDate="2016-04-26T19:55:03.180" UserId="3041" />
  <row Id="2825" PostId="2362" Score="0" Text="is `y2 - y1` close to 0?" CreationDate="2016-04-27T10:53:42.743" UserId="137" />
  <row Id="2826" PostId="2362" Score="0" Text="it is already checked that y2 != y1. One more issue may be to check how the algorithm behaves when both the line segments overlap. I checked that the algorithm does not filter it out. Should it be a intersection in this case ?" CreationDate="2016-04-27T11:28:13.740" UserId="2712" />
  <row Id="2827" PostId="2362" Score="1" Text="subtracting 2 nearly equal values and then dividing with it is generally bad for precision." CreationDate="2016-04-27T11:37:38.387" UserId="137" />
  <row Id="2829" PostId="2362" Score="0" Text="Initially it just returns if the two line segments does intersect or not and the algorithm is edited. If there any intersection I proceed with next part of the algorithm to find the point where it intersects and then I use getXIntersection(..) function. I am afraid that the issue of two overlapped line segments are not identified in the initial part of the algorithm . What do you think ? Do you suggest any changes in the initial part ?" CreationDate="2016-04-27T12:07:49.303" UserId="2712" />
  <row Id="2834" PostId="2362" Score="0" Text="I tried with several point values with the help of geogebra and I get valid result as you asked." CreationDate="2016-04-27T12:22:56.343" UserId="2712" />
  <row Id="2839" PostId="2362" Score="0" Text="I've deleted my answer now that I understand that the initial step checks for segment intersection, not just line intersection. Sorry for the confusion. I've checked the pseudocode in the book and your code seems to match it. So your code for both steps looks correct to me. Since I can't see what is wrong, could you include the values you are testing with, showing the input values and returned result? The segment end points in the code appear to be identical (the same segment twice, rather than distinct segments): (5,1),(5,3) and (5,1),(5,3) again." CreationDate="2016-04-27T21:23:03.610" UserId="231" />
  <row Id="2840" PostId="2362" Score="0" Text="The result shows that it has the intersection with those points you mentioned - in other words overlapping line segments intersects. In my case loverlapping line segments must not be decided as intersection and I , believe that I have to add some extra condition to it. Need some hint on those conditions. At last but not the least, once the initial test mentions that there is intersection between the line segments, Do I have test if the calculated intersection point reside on both the line segments ?" CreationDate="2016-04-27T21:34:02.133" UserId="2712" />
  <row Id="2841" PostId="2361" Score="0" Text="I wonder if there are any high performance libraries out there instead of stb? Would halide support doing something like this?" CreationDate="2016-04-27T21:52:01.653" UserId="1924" />
  <row Id="2842" PostId="2362" Score="0" Text="Could you share the input values that result in your X-intersection value of -32768?" CreationDate="2016-04-27T22:25:30.363" UserId="231" />
  <row Id="2843" PostId="2362" Score="0" Text="No, if the initial test confirms that the segments intersect, then the intersection point automatically lies on both segments." CreationDate="2016-04-27T22:26:07.070" UserId="231" />
  <row Id="2844" PostId="2362" Score="0" Text="Lets name the line segments. Surrogate Segment - (0,1762)---(1057,1762) and Axis Aligned Segment - (1066,1762)----(1038,1762). As you can see both the segments overlap each other. The current algorithm does not deal with these type of scenarios" CreationDate="2016-04-27T22:48:44.470" UserId="2712" />
  <row Id="2845" PostId="2356" Score="2" Text="_&quot;Matlab is not the choice of language for this particular task&quot;_... nor mine. I'm not familiar with the language nor have access to Matlab.&#xA;&#xA;Having said that, I see you have the eigenvectors of the covariance matrix and are taking the dot product of each vertex against each of those vectors. The mins and maxs give the bounds of a box.&#xA;&#xA;However, I don't think this is guaranteed to give you a *good* bound. Imagine a dense central cluster of vertices with a few outliers.  The directions of your axes are going to governed by the dense cluster but your box really only depends on the outliers." CreationDate="2016-04-28T08:14:31.777" UserId="209" />
  <row Id="2846" PostId="2362" Score="0" Text="Does your code work correctly for segments that do not overlap (that is, segments that do not share the same line)? If so, then the original question is solved and you can ask about detecting overlapping segments as a new question. If not, then overlapping segments is best left until the code is fixed - let's get it working before adding extra exceptions. Please don't change the aim of a question - feel free to ask a new question for a new problem." CreationDate="2016-04-28T08:50:19.533" UserId="231" />
  <row Id="2847" PostId="2356" Score="1" Text="There seems to be a problem with matlab on this site, every time somebody puts up a question with matlab code it seems to rot forever. I have matlab installed but honestly would not care to fire it up for debugging" CreationDate="2016-04-28T08:54:03.000" UserId="38" />
  <row Id="2848" PostId="2356" Score="0" Text="@SimonF Surely though, as a bounding box, it still has to include those outliers - unless of course you have multiple bounding boxes. However, I'm only attempting to model the mathematics and the shape in question is just one I quickly drew up while I had the time..." CreationDate="2016-04-28T12:24:01.427" UserId="2646" />
  <row Id="2849" PostId="2356" Score="0" Text="Well, yes, it will still include the outliers, but one aim of OBBs over AABBs is to get a much smaller bounding volume. I just meant to point out that using PCA might not give a good result.  I must admit, though, I don't really have an efficient scheme for doing better :-|" CreationDate="2016-04-28T13:09:56.963" UserId="209" />
  <row Id="2850" PostId="2361" Score="0" Text="@Decave I don't know off the top of my head, but I did find [this image-resizing sample](https://github.com/halide/Halide/blob/master/apps/resize/resize.cpp) in the Halide repository." CreationDate="2016-04-28T15:54:44.777" UserId="48" />
  <row Id="2851" PostId="2361" Score="0" Text="Very cool, thanks @NathanReed" CreationDate="2016-04-28T19:11:08.077" UserId="1924" />
  <row Id="2852" PostId="2151" Score="3" Text="The thesis is publicly available here: https://dspace.cvut.cz/bitstream/handle/10467/62770/F8-DP-2015-Chlumsky-Viktor-thesis.pdf" CreationDate="2016-04-28T22:00:30.783" UserId="3255" />
  <row Id="2853" PostId="2370" Score="1" Text="Could you tell also where you found the &quot;mentions of using Interval Arithmetics&quot;?" CreationDate="2016-04-29T12:11:46.873" UserId="2074" />
  <row Id="2854" PostId="2374" Score="0" Text="Yes! Thanks! This solved the problem. As an additional note, &quot;Deterministic  Monte Carlo&quot; is (now) known as &quot;Brute Force&quot; in Vray. I thought I tried this already, but suspect I may have failed to set it in both my primary and secondary GI engines" CreationDate="2016-04-29T16:38:53.410" UserId="3251" />
  <row Id="2855" PostId="2370" Score="0" Text="@rych [In this paper.](http://www.cs.utah.edu/~knolla/dagstuhlHijazi.pdf)" CreationDate="2016-04-29T16:55:01.940" UserId="3233" />
  <row Id="2856" PostId="2327" Score="2" Text="Thanks! You were right, but I should also make note for future readers that I had misunderstood the inverse of the matrix. What I got from that inverse matrix (variable called x) was actually 3 scalars, not a coordinate. Think about that and everything else will click once this is realized. The first value (x.x) is actually the scalar distance from the start vector along the direction vector where the intersection occurs!" CreationDate="2016-04-29T22:44:42.077" UserId="3153" />
  <row Id="2857" PostId="2372" Score="0" Text="Two lines cannot partially overlap, but two line segments can." CreationDate="2016-04-30T00:42:17.940" UserId="231" />
  <row Id="2858" PostId="2364" Score="0" Text="It would be helpful to see your code in this question to make the post self-contained. I'll add an answer that refers to your code but it will make more sense if the code is visible in the question." CreationDate="2016-04-30T00:52:51.550" UserId="231" />
  <row Id="2859" PostId="2372" Score="0" Text="Ah, good point. I was thinking infinite lines. I'll update when I have a minute." CreationDate="2016-04-30T01:03:54.483" UserId="3003" />
  <row Id="2860" PostId="2366" Score="1" Text="It seems you have the answer you need, but if you ever want to look up the mathematics for anything else, the 2d donut-like shape you describe is called an [annulus](https://en.wikipedia.org/wiki/Annulus_%28mathematics%29)" CreationDate="2016-04-30T01:21:28.413" UserId="231" />
  <row Id="2861" PostId="2377" Score="1" Text="I've seen both, unfortunately. I'm not fond of ray per second as meaning exclusively primary rays and I'd suggest &quot;paths per second&quot; or  &quot;samples per second&quot; instead. &quot;Complete ray&quot; is not a name you'll find elsewhere: a ray is an unbounded line segment. &quot;Rays per second&quot; is ill specified for a path tracer: do shadow rays count, for instance? It's a useful metric for an acceleration framework (i.e. Embree or OptiX) but not a renderer." CreationDate="2016-04-30T21:20:15.810" UserId="3075" />
  <row Id="2862" PostId="2377" Score="0" Text="Also, be aware that samples per second still isn't a great metric of actual performance since sample quality will vary wildly depending on implementation details. It's probably the best thing you can do starting out though, as the better solutions involve fairly complex variance estimates." CreationDate="2016-04-30T21:30:43.067" UserId="3075" />
  <row Id="2863" PostId="2377" Score="0" Text="@KarlSchmidt I think you should post those comments as an answer ;)" CreationDate="2016-04-30T22:05:09.677" UserId="48" />
  <row Id="2864" PostId="2377" Score="0" Text="Probably a good idea, yes. :)" CreationDate="2016-04-30T22:56:45.753" UserId="3075" />
  <row Id="2865" PostId="2381" Score="0" Text="Can the pieces be of arbitrary shape, or do you require concave pieces to be decomposed into convex pieces?" CreationDate="2016-05-01T08:26:58.217" UserId="231" />
  <row Id="2866" PostId="2381" Score="2" Text="When you say &quot;clipped&quot;, can we assume that the inside of the square is discarded, and we are counting the number of disconnected pieces left outside the square?" CreationDate="2016-05-01T08:28:36.027" UserId="231" />
  <row Id="2867" PostId="2365" Score="0" Text="I can't find where it says there are two hardware queues in the programming guide. Could you post a quote from the document? Mention which chapter that says there are two queues? Is number of hardware queues queryable in runtime using OpenCL?" CreationDate="2016-05-01T12:06:03.263" UserId="3041" />
  <row Id="2868" PostId="2359" Score="0" Text="Can you be more precise about what your problem is (time, memory, logic)? What is stopping you from &quot;just do it&quot;?" CreationDate="2016-05-01T12:16:20.813" UserId="3041" />
  <row Id="2869" PostId="2380" Score="0" Text="Great answer. To add to it, since it is hard to come up with one metric that is universally meaningful, I'd suggest you pick one that makes most sense / is most honest for your usage case, and make sure you explain what you mean by your terminology." CreationDate="2016-05-01T13:59:28.150" UserId="56" />
  <row Id="2870" PostId="2365" Score="0" Text="I've updated my post. It does say *possible* execution, but if it can do a few why can't it do them all? Also the OpenCL runtime has no notion of hardware queue, so it's not something that you can query." CreationDate="2016-05-01T16:05:48.387" UserId="197" />
  <row Id="2872" PostId="2375" Score="1" Text="Thank you for the answer, also sorry for being so vague in my question. The goal is to create diverging lens using the CSG difference operator \ in the following manner: (C \ A) \ B. You've made it much more clearer to me on the general approach, but I'm still struggling to implement \. Can you please give an example for this operator as well?" CreationDate="2016-05-01T19:19:19.040" UserId="3233" />
  <row Id="2873" PostId="2359" Score="1" Text="Hey @Andreas I've updated the question with how am I doing it right now. The main issues are listed. I was wondering if a &quot;canonical&quot; best way of solving this problem is known. Typically when I cannot find them in google is because i am searching for &quot;the wrong keywords&quot;." CreationDate="2016-05-02T09:03:18.050" UserId="3225" />
  <row Id="2876" PostId="2387" Score="0" Text="A image might do wonders." CreationDate="2016-05-03T04:43:22.337" UserId="38" />
  <row Id="2880" PostId="2375" Score="1" Text="(Hoping I've got this right) Would it be easier for if you constructed the difference operator from *Intersection* and *Not* i.e.  A \ B  =  A ^ ~B.  To generate ~B you just need the &quot;gaps&quot;. For example, if the intersection along a ray for B is  { [b1, b2] }, then for ~B it'd be { [-infinity, b1] , [b2, +inf] }." CreationDate="2016-05-03T10:50:38.390" UserId="209" />
  <row Id="2881" PostId="2388" Score="0" Text="I'd upvote if there was also an example of a (possibly unrelated) soft shadow technique for contrast, as part of the question was what is the difference between them." CreationDate="2016-05-03T14:56:57.397" UserId="231" />
  <row Id="2882" PostId="2375" Score="0" Text="That did it, thanks a lot!" CreationDate="2016-05-03T17:48:02.430" UserId="3233" />
  <row Id="2883" PostId="2388" Score="1" Text="Soft shadows with shadow volumes makes me think of Instant Radiosity - essentially approximate area light sources as multiple point light sources and accumulate the results. Many shadow (100+) passes are doable in real time on a modern GPU with stacked memory if the shadow casting geometry can be limited." CreationDate="2016-05-03T19:09:00.507" UserId="2500" />
  <row Id="2884" PostId="2311" Score="0" Text="@joojaa But have you ever tried looking at something that's been pushed through your eyeball? :-)" CreationDate="2016-05-04T08:00:46.650" UserId="209" />
  <row Id="2885" PostId="2311" Score="0" Text="@SimonF Actually I have, although i tried to not concentrate on it.. But the projectile geometry actually pushes everything that crosses your view plane trough one point which reality does not ;)" CreationDate="2016-05-04T08:04:12.513" UserId="38" />
  <row Id="2886" PostId="2367" Score="0" Text="Hey thanks a lot for sharing ur knowledge. I wrote a code but it seems to have some error ive edites the post with an image. I would appreciate if u cab give me help with that. Im constantly running into problems so i searched for ur contact number cuz i cant post image in a comment lol." CreationDate="2016-05-04T10:24:59.313" UserId="3248" />
  <row Id="2887" PostId="2367" Score="1" Text="@quinnavery ...did you just copy/paste the shadertoy into your code? :D You're going to need a bit more plumbing than that. It's written in GLSL, not C++, which is where most of the errors are coming from. If you're not familiar with using shaders in Cocos2d-x, you should look up some tutorials on the subject first, such as [this one](https://www.raywenderlich.com/10862/how-to-create-cool-effects-with-custom-shaders-in-opengl-es-2-0-and-cocos2d-2-x)." CreationDate="2016-05-04T16:17:46.320" UserId="48" />
  <row Id="2888" PostId="2392" Score="0" Text="AMD's GCN will execute graphics and compute concurrently even when both are issued on the graphics queue, but generally not across multiple command buffers (multiple draw calls might even be sketchy). The driver (or application - I think in DX12 or Vulkan) must check for data dependencies and block between draw (graphics) and dispatch (compute) if needed. Multiple command queues would probably be useful if you have compute that is truly asynchronous from graphics (like physics for the next frame), but I have no direct experience with this." CreationDate="2016-05-05T00:22:33.503" UserId="2500" />
  <row Id="2892" PostId="2397" Score="1" Text="You could also have white bars. You can also use some image synthesis function to fill the pixels you can overlay a picture with itself  one that is cropped and one that stretches for example. here simply is a infinite amounts of ways to deal with no data. I think most of us just assumed its going to be a nonuniform scale." CreationDate="2016-05-06T08:23:02.803" UserId="38" />
  <row Id="2893" PostId="2394" Score="0" Text="Thanks for writing an answer! I'll take a look into BVH! Yes determining whether the voxels are inside/outside/intersected by the mesh is important for me. Could you elaborate a bit or point to a resource for using ray casting to determine inside/outsideness? From where to where do the rays go?" CreationDate="2016-05-06T11:27:18.987" UserId="3225" />
  <row Id="2895" PostId="2397" Score="0" Text="Very true. I assumed the original poster specifically didn't want to fill with a solid color and was interested only in the scaling aspect, since that's what they asked about. But there are lots of other fill methods. There are also non-linear scaling methods, where the center maintains its aspect ratio and the edges are stretched out, for example." CreationDate="2016-05-06T15:44:30.103" UserId="3003" />
  <row Id="2896" PostId="2394" Score="0" Text="@gnzlbg To test if a point P is inside the mesh, you can fire a ray starting at P in any direction, to infinity. If P is inside, it will hit the mesh an odd number of times; if P is outside, the ray will hit the mesh an even number of times (counting zero as an even number). [This Wikipedia article](https://en.wikipedia.org/wiki/Point_in_polygon#Ray_casting_algorithm) explains it for the 2D case, but it's the same idea in 3D." CreationDate="2016-05-06T19:43:02.843" UserId="48" />
  <row Id="2897" PostId="2403" Score="0" Text="This is a quick answer over the phone, spelling corrections and tex formulas welcome." CreationDate="2016-05-06T20:15:31.890" UserId="38" />
  <row Id="2900" PostId="2404" Score="0" Text="And the neat geometric interpretation is what I used. Personally I prefer to build the matrixes with a few number of atomic operations because then I only need to remember 3 rules of construction." CreationDate="2016-05-07T06:05:53.883" UserId="38" />
  <row Id="2901" PostId="2395" Score="0" Text="This is a great and informative answer, but Daniel's answer talks more about the tesselation in a historical context, which is what my question was really about. But thanks your answer. :)" CreationDate="2016-05-07T10:07:40.883" UserId="88" />
  <row Id="2905" PostId="2409" Score="0" Text="I'm not familiar with the framework, but wouldn't it be more efficient to create an acceleration structure for each model?" CreationDate="2016-05-07T12:39:30.493" UserId="457" />
  <row Id="2908" PostId="2402" Score="0" Text="explain your API. I would expect to see something like vertices A1,B1, A2,B2. Beside, what about the case they are not even aligned ? what kind of error threshold do you plan ?" CreationDate="2016-05-08T10:17:12.200" UserId="1810" />
  <row Id="2909" PostId="2412" Score="1" Text="The lines in plane $y = y_0$ are not all parallel." CreationDate="2016-05-08T13:06:10.150" UserId="137" />
  <row Id="2910" PostId="2412" Score="0" Text="Of course, I need a group of parallel lines in plane $y=y_0$ that their vanishing point is some point, say $(x_0,0,1)$ (the camera position and direction and the view plane are defined in the question)" CreationDate="2016-05-08T13:07:41.200" UserId="3305" />
  <row Id="2911" PostId="2414" Score="0" Text="I have tough about that, so if my camera in $(0,0,0)$ and it's looking to $(0,0,1)$, the view plane is $z=1$ and lets say that the vanishing point is $(10,0,1)$. Then the line from the camera to the vanishing point is $(0,0,0)+t(10,0,1)$. So all the parallel lines that are going to that vanishing point will be $(a_1,a_2,a_3) + t(10,0,1)$? And then all the lines in plane $y=y_0$ that have that vanishing point will be $(a_1,y_0,a_3) + t(10,0,1)$?" CreationDate="2016-05-08T13:58:57.273" UserId="3305" />
  <row Id="2912" PostId="2414" Score="1" Text="stop focusing on the $y = y_0$ plane, just having the vanishing point on the view plane and the camera position is enough." CreationDate="2016-05-08T14:08:22.040" UserId="137" />
  <row Id="2913" PostId="2407" Score="0" Text="What about the case where one corner of the square is inside the polygon and all others are outside?" CreationDate="2016-05-08T14:08:51.740" UserId="310" />
  <row Id="2914" PostId="2414" Score="0" Text="OK, so $(a_1,a_2,a_3)+t(10,0,1)$ are all the lines that their vanishing point is $(10,0,1)$?" CreationDate="2016-05-08T14:09:50.400" UserId="3305" />
  <row Id="2915" PostId="2407" Score="1" Text="@RichieSams its the same case as splits into one shard, it exits 2 times and results in one concave shard." CreationDate="2016-05-08T14:56:47.270" UserId="38" />
  <row Id="2916" PostId="2367" Score="0" Text="hi, Im having some trouble with my developer.... he really doesnt know how to code (Just my opinion) I'm considering to hire somebody else to create the game for me. You really seem like you can code since u shared your knowledge in less than a day. I think it will be quite easy for you? ^^ I'm trying to create for android platform and dont mind what program u use cocos, unity...., Will you be interested? I do pay upfront LOL" CreationDate="2016-05-08T17:03:50.183" UserId="3248" />
  <row Id="2917" PostId="2409" Score="0" Text="Okay. Yes, but then I have to switch out which mesh is the currently active one... I think that would be doable... switching just works by setting a variable in the graphics memory I think. I would have to try if this influences perfomance in a big way. Thank you for your suggestion." CreationDate="2016-05-08T17:10:12.533" UserId="273" />
  <row Id="2918" PostId="2367" Score="0" Text="if u are interested, plz let me know and we can proceed to further negotiations via email or something. I can pay you via paypal just to let ya know~" CreationDate="2016-05-08T17:10:20.763" UserId="3248" />
  <row Id="2919" PostId="2411" Score="1" Text="BTW, what is the &quot;*&quot; notation in this formula? I've never seen that in the rendering equation before." CreationDate="2016-05-08T18:32:01.207" UserId="48" />
  <row Id="2920" PostId="2411" Score="0" Text="@NathanReed p* is just a point from another surface, L(p*,-wi) is just the radiance from point p in direction wi" CreationDate="2016-05-08T18:53:54.177" UserId="2359" />
  <row Id="2923" PostId="2380" Score="0" Text="Yeah right, thank you for clearing my thoughts! Currently I choose the &quot;samples per second&quot; since it is easy to understand and hard to be misunderstood IMO. I'll eventually implement something that can calculate variance and measure rendering performance based on that." CreationDate="2016-05-08T19:42:39.173" UserId="3267" />
  <row Id="2924" PostId="2407" Score="1" Text="@trichoplax right i can add that. There shouldn't be less,  but see i added the or less for your comment.  I am not sure i have proven there isn't a smaller case (i could), there shouldn't be but i haven't laid out the proof very well for that case, not rigorous enough. I have however proven that there are no more cases than 4 and since i can find a 4 it proves that is maximum which is enough to answer the question at hand." CreationDate="2016-05-08T20:11:39.083" UserId="38" />
  <row Id="2925" PostId="2407" Score="0" Text="@joojaa fair point." CreationDate="2016-05-08T20:24:26.147" UserId="231" />
  <row Id="2926" PostId="2398" Score="0" Text="The C version to the above code is in the link at description of question." CreationDate="2016-05-09T01:32:42.353" UserId="2383" />
  <row Id="2928" PostId="2409" Score="0" Text="@Rotem I tried to just create everything before displaying, and then only switching it out... It turns out each time tell Optix to use another model this takes nearly half a second and my &quot;animation&quot; looks really unsmooth." CreationDate="2016-05-09T11:41:06.550" UserId="273" />
  <row Id="2929" PostId="2402" Score="0" Text="If integers are 32 bits, the multiplies in orientation could overflow if p1's coordinates are large negatives and p2, p3's coordinates are large positives. Other than that, this looks correct - cross products are zero, h's vertexes are within the bounding box of c." CreationDate="2016-05-09T13:40:06.563" UserId="2500" />
  <row Id="2931" PostId="2402" Score="0" Text="will it solve the issue if d1, d2 , d3 and d3 are stored as 64-bit integer type instead ?" CreationDate="2016-05-09T15:40:48.973" UserId="2712" />
  <row Id="2932" PostId="2402" Score="0" Text="&quot;orientation&quot; would have to compute with 64 bit integers too; the math there is currently promoted to integers, but they won't quite hold your worst case multiply result (if 32 bits). If you know your original data is limited (e.g. I think if you only had positive values - effectively positive 15 bit values, assuming 2's compliment integers), this may not be a problem. Start with a test that fails due to overflow, then work modify to fix. That is, prove my thinking isn't wrong before using the extra bits (or another strategy)... ;)" CreationDate="2016-05-09T17:46:49.560" UserId="2500" />
  <row Id="2934" PostId="2398" Score="0" Text="I have removed the &quot;//&quot; and C# implementation of above code is this much only" CreationDate="2016-05-10T00:05:01.570" UserId="2383" />
  <row Id="2935" PostId="2406" Score="0" Text="`combination of a quite dark texture with very bright lighting or an extremely overexposed camera setting can show banding in the final frame` very good insight. But we may note that gamma encoding is here precisely to mitigate this point. If he has the problem why not try a superior gamma exponent ? that would inhibit usage of hardware sRGB samplers though." CreationDate="2016-05-10T00:50:33.420" UserId="1614" />
  <row Id="2936" PostId="2419" Score="3" Text="The magic keyword to search for is &quot;polygon offsetting&quot;. The curves you're trying to generate are called offset curves. [Here is a StackOverflow question](http://stackoverflow.com/questions/1109536/an-algorithm-for-inflating-deflating-offsetting-buffering-polygons) with some good background and links to some libraries that can do it. You can also find plenty of material with a web search." CreationDate="2016-05-10T02:16:25.833" UserId="48" />
  <row Id="2938" PostId="2406" Score="0" Text="Thank you for your input. But to clarify, I'm mostly interested in this from a photo realistic rendering point of view, with ray trace renderers (like mental ray, V-Ray, Arnold, etc.) with full light simulations and global illumination, rather then for real-time game engines." CreationDate="2016-05-10T07:32:58.720" UserId="2736" />
  <row Id="2939" PostId="2420" Score="0" Text="Thank you for your input. But to clarify, I'm mostly interested in this from a photo realistic rendering point of view, with ray trace renderers (like mental ray, V-Ray, Arnold, etc.) with full light simulations and global illumination, rather then for real-time game engines. I've updated my original post." CreationDate="2016-05-10T07:33:27.423" UserId="2736" />
  <row Id="2940" PostId="2406" Score="0" Text="@KristofferHelander Good to know, but I think what I wrote probably applies just as well to offline rendering. But I admit that I don't have much direct experience in that area." CreationDate="2016-05-10T07:39:04.373" UserId="48" />
  <row Id="2941" PostId="2406" Score="0" Text="@NathanReed Yes, you most certainly made some really good points there :)" CreationDate="2016-05-10T07:54:49.367" UserId="2736" />
  <row Id="2942" PostId="2401" Score="0" Text="This doesn't really seem to adress the actual question much at all." CreationDate="2016-05-10T09:51:32.520" UserId="6" />
  <row Id="2943" PostId="2401" Score="0" Text="I added it as a comment and the original questioner asked me to promote it to an answer. See questioner's comment below..." CreationDate="2016-05-10T14:28:51.530" UserId="2500" />
  <row Id="2944" PostId="2422" Score="2" Text="I didn't read your code because I saw this:&#xA;&#xA;*&quot;For example, when one of the vertices is behind the camera, the texture is stretched.&quot;*&#xA;&#xA;You *have* to clip your polygons to a Z=constant plane in front of the camera or else all sorts of chaos will ensue :-)" CreationDate="2016-05-10T15:47:29.307" UserId="209" />
  <row Id="2945" PostId="2422" Score="0" Text="@SimonF I am already clipping on the Z axis, see _ZCLIP_ in the 3rd code." CreationDate="2016-05-10T16:32:41.590" UserId="3330" />
  <row Id="2946" PostId="2422" Score="0" Text="My apologies! I saw that opening paragraph and it &quot;raised a red flag&quot;.    So given *&quot;I am guessing that it has something to do with the texture mapping not being perspective-correct.&quot;* .. I tried to understand your &quot;map&quot; function but got lost. For example, I don't understand why you need a square root.  IIRC there's an initial set up from triangle coords to create 9 params, a,b,c d e f  p q r, then @ pixel x,y,  U= (a x +by + c)/(px+qy+r)  and V = (d x +ey + f)/(px+qy+r).   To determine the abc..pqr requires adjoint of a simplified &quot;3x3&quot; matrix (i.e. inverse w/out det) and mat muls." CreationDate="2016-05-10T16:55:31.523" UserId="209" />
  <row Id="2947" PostId="2416" Score="1" Text="Hi headbanger, image warping transformations are certainly possible but it's a bit unclear from your question what you're looking for. An example before/after screenshot or diagram would help." CreationDate="2016-05-10T17:08:59.257" UserId="48" />
  <row Id="2948" PostId="2422" Score="0" Text="@SimonF My *map()* function is adapted from [this StackoverFlow answer](http://stackoverflow.com/questions/808441/inverse-bilinear-interpolation). I thought it was perspective-correct, but I guess it isn't. Does all perspective-correct texture mapping techniques require to break down the polygon into triangles? Is there any formula that is not in a matricial form? Thanks!" CreationDate="2016-05-10T17:49:48.373" UserId="3330" />
  <row Id="2949" PostId="2423" Score="1" Text="Wow, thanks @LarryGritz! Considering you work for Sony Pictures Imageworks, I'll take you as a very reliable source! :)&#xA;&#xA;But how do you capture these textures? Most cameras only shoot in 14bit RAW files, do you have special cameras with 16bit linear sensors? Or do you take multiple exposures for textures, just like one does for HDRI imaged based lighting? Or do you simply capture with 14bit camera RAW and saves it as &quot;16bit&quot;? Ow, and what file format do you use, .tiff (.tx, .tex) or .exr?&#xA;Thanks again for your input!! :)" CreationDate="2016-05-10T19:27:15.013" UserId="2736" />
  <row Id="2950" PostId="2401" Score="1" Text="My question was about why a seemingly underused feature like tesselation has gotten so much API attention. RichieSams' answer talks more about what tesselation is, rather than explaining how it got to be so important. The above answer is the best answer I've gotten so far." CreationDate="2016-05-11T04:52:39.783" UserId="88" />
  <row Id="2951" PostId="2422" Score="0" Text="Why on earth do people want to avoid the matrix calculations. Triangles make it easy to transform the coordinates." CreationDate="2016-05-11T10:22:36.163" UserId="38" />
  <row Id="2952" PostId="2425" Score="0" Text="But how can I use this with the Rendering Equation which involves radiance ?" CreationDate="2016-05-11T10:36:33.913" UserId="3339" />
  <row Id="2953" PostId="2422" Score="0" Text="@joojaa My reason is that creating a matrix object, calling and updating members is going to take longer to the JVM than simply executing some bytecode. Okay, I understand. Does it work w/o breaking it down into triangles though?" CreationDate="2016-05-11T10:38:23.343" UserId="3330" />
  <row Id="2954" PostId="2426" Score="0" Text="Hi @sajis997, it's hard to understand what you're talking about from words alone. Could you add a screenshot or diagram to show what you mean by inner and outer loops and tree depth?" CreationDate="2016-05-11T16:43:11.213" UserId="48" />
  <row Id="2955" PostId="2401" Score="0" Text="Another aspect is that geometry shaders (flexible enough to implement tessellation, and available in DX10), are extremely hard to implement efficiently in HW. My understanding is the main issue is the in-order processing of the output from each input primitive means executing geometry shaders in parallel requires substantial buffering. A fixed function tessellator turned out to be far more efficient." CreationDate="2016-05-11T16:52:41.040" UserId="2500" />
  <row Id="2956" PostId="2426" Score="0" Text="Hi @NathanReed, The initial question is edited with some image snapshot. I hope it wil be clear now." CreationDate="2016-05-11T19:24:10.857" UserId="2712" />
  <row Id="2957" PostId="2424" Score="0" Text="In short, the number of photos you'll encounter drops as you get farther from a point light source. You'll encounter 1/(distance^2) in fact (;" CreationDate="2016-05-12T03:28:08.710" UserId="56" />
  <row Id="2959" PostId="2416" Score="0" Text="I think the question is pretty clear in intention. Just dont expect people to publish code for you as the question sounds more like do my work for me the way its presented." CreationDate="2016-05-12T08:38:12.673" UserId="38" />
  <row Id="2960" PostId="2422" Score="0" Text="*&quot;Does it work w/o breaking it down into triangles though&quot;*. If your surface is planar and the texturing, in world space, is linear, then &quot;yes&quot;. You just need 3 non-colinear points on that surface along with their matching UVs." CreationDate="2016-05-12T14:01:57.077" UserId="209" />
  <row Id="2961" PostId="2431" Score="0" Text="The code appears to return a point uniformly selected from the *interior volume* of the unit sphere. This will give different results from the surface of the sphere. Is this what you intend? I ask because you mention picking a point &quot;on&quot; the light, which sounds like a point on the surface." CreationDate="2016-05-12T17:04:48.740" UserId="231" />
  <row Id="2963" PostId="2431" Score="0" Text="But.. _centerOfSphere + Vec3&lt;float&gt;(x, y, z) * radius;_ should give me a point on the surface of the sphere! right? cause I move from the center towards the surface of _radius_" CreationDate="2016-05-12T20:01:05.323" UserId="3069" />
  <row Id="2966" PostId="2431" Score="1" Text="If the points are already on the surface of the sphere, then multiplying by radius will give the surface of a sphere of that radius. If the points are already in the interior volume of the sphere, then multiplying by radius will put them in the interior volume of a sphere of that radius. Multiplying by, for example, radius 2, will give a sphere twice as large. However, whether the points are on the surface or in the volume of this larger sphere depends on whether they were on the surface or in the volume of the original unit sphere" CreationDate="2016-05-12T22:18:24.223" UserId="231" />
  <row Id="2968" PostId="2436" Score="0" Text="I believe you are correct. I will check my PBRT book when I get home and correct the code. IIRC, I need to add a square root." CreationDate="2016-05-12T23:09:45.017" UserId="310" />
  <row Id="2969" PostId="2426" Score="0" Text="Do you have access to the tree shown in the image? If so my answer would be &quot;the outer loops are at even depths, the inner loops are at odd depths&quot;. Is your requirement to produce this tree, or to interpret its results?" CreationDate="2016-05-13T01:01:46.353" UserId="231" />
  <row Id="2970" PostId="2436" Score="2" Text="Yes, @trichoplax is right, sampling both angles uniformly will lead to points bunching at the poles. The simplest way to fix it is sample theta and Y uniformly, then set the XZ radius = $\sqrt{r^2 - y^2}$. Which, BTW, is exactly what you're doing in the second code snippet when you sample `cosPhi` (which is proportional to Y) uniformly. See also [this MathWorld article](http://mathworld.wolfram.com/SpherePointPicking.html)." CreationDate="2016-05-13T02:01:09.303" UserId="48" />
  <row Id="2971" PostId="2436" Score="0" Text="Fixed and updated." CreationDate="2016-05-13T02:36:22.383" UserId="310" />
  <row Id="2972" PostId="2424" Score="0" Text="By photos I meant photons by the way" CreationDate="2016-05-13T04:21:23.637" UserId="56" />
  <row Id="2973" PostId="2426" Score="0" Text="After the slicing operation, I have closed loops . I want separate into separate layer parts when each layer part contain an outer loop, one/several loops and infill pattern. I want to use Tree data structure for this purpose as mentioned in the paper." CreationDate="2016-05-13T07:38:12.677" UserId="2712" />
  <row Id="2975" PostId="2438" Score="1" Text="You could pass the color data in a constant buffer rather than vertex data. If they are all the same." CreationDate="2016-05-13T11:55:50.033" UserId="310" />
  <row Id="2979" PostId="2441" Score="0" Text="Do you require a neighbourhood that extends beyond immediate neighbours or would you be interested in approximations based on repeated application of a nearest neighbour approach?" CreationDate="2016-05-13T16:18:50.413" UserId="231" />
  <row Id="2980" PostId="2441" Score="0" Text="@trichoplax As long as the number of iterations required to reach a 10-hop standard deviation isn't going to be prohibitive. (I guess it wouldn't be, because at least in the 1D case a 10-hop deviation can be reached by mixing nearest neighbors 100 times.) I just wouldn't know how to weight the neighbors in this case." CreationDate="2016-05-13T17:11:29.177" UserId="3294" />
  <row Id="2981" PostId="2438" Score="1" Text="If GL ES 3.0 is a possibility, you could also use geometry instancing, but it doesn't seem to be available in GL ES 2.0 as far as I can tell from some googling." CreationDate="2016-05-13T18:02:20.797" UserId="48" />
  <row Id="2985" PostId="2438" Score="0" Text="@Ratchet freak OK I re-strategised my plan! Thank you for letting me know.  If you are willing I made a new one because I am stuck on that one, I need to get good at vertex buffers somehow." CreationDate="2016-05-14T00:12:35.043" UserId="2308" />
  <row Id="2986" PostId="2438" Score="0" Text="http://stackoverflow.com/questions/37221027/how-to-write-complicated-vertex-buffers" CreationDate="2016-05-14T00:12:37.637" UserId="2308" />
  <row Id="2990" PostId="2441" Score="0" Text="@joojaa It is separable when applied on a flat surface/grid but is it still separable when you have edges going off in arbitrary directions from each vertex?" CreationDate="2016-05-14T11:56:59.197" UserId="3294" />
  <row Id="2991" PostId="2443" Score="0" Text="Yes, MIS is an important production-verified technique, which helps a lot and I employ it in my solution (I guess, I should have stated that more clearly in the question).&#xA;&#xA;However, the overall performance of a MIS-based estimator depends on the quality of its partial sampling strategies. What I am trying to do here is to improve one of the the sub-strategies to improve the overall performance of the estimator. In my experience, it is usually more efficient to use less high-quality samples than may be more expensive to generate than more easily-generated low-quality ones." CreationDate="2016-05-14T12:28:28.730" UserId="2479" />
  <row Id="2992" PostId="2444" Score="0" Text="Thank you. My mention of the 3D Euclidean distance may have done more harm than good. I only meant it as an expensive (and conditional) approximation that is simple to explain. I wouldn't pursue the approximation if it is less attainable than the ideal answer." CreationDate="2016-05-14T23:28:35.730" UserId="3294" />
  <row Id="2993" PostId="2444" Score="0" Text="What I am ideally after is what you call &quot;using 2D distance&quot;. On a regular grid (with consistent local geometry) it is easy because the same symmetrical 1-hop-neighborhood kernel can be applied (repeatedly) throughout the grid. My real problem is determining the kernel-weights of 1-hop neighbors around *each* vertex *from the local geometry* of that 1-hop neighborhood." CreationDate="2016-05-14T23:28:54.640" UserId="3294" />
  <row Id="2994" PostId="2444" Score="0" Text="Each vertex has valence 4 but edge lengths and directions vary. (Alternatively (if for some reason planar faces make the problem tractable) each quadrilateral face can be broken along its shortest diagonal into two (now planar) triangles, in which case vertex valence varies from 4 to 8, and edge lengths become even more varied.)" CreationDate="2016-05-14T23:29:34.767" UserId="3294" />
  <row Id="2995" PostId="2444" Score="0" Text="Ultimately, I have a vertex with a ring of neighbors at different distances and angles, and I need to know the local kernel weights over these neighbors. Constraining the mean/centroid of the kernel to lie &quot;in the center&quot; (which in this case presumably means along the mean curvature normal's span from the vertex) already constrains 2DoF in the kernel's weights. Choosing some variance provides another constraint. But the system is still underdetermined since valence&gt;3. How to choose the weights?" CreationDate="2016-05-14T23:30:08.583" UserId="3294" />
  <row Id="2998" PostId="2444" Score="0" Text="Yes, &quot;using 2D distance&quot; is a little vague. I guess we should distinguish between &quot;shortest edgewise distance&quot; and &quot;shortest distance within the surface&quot; (which won't necessarily stay on edges). Fortunately the difference becomes less relevant the more times you apply the blur (even a box filter will approximate a Gaussian blur if applied several times). If you can determine whether you need a Gaussian blur with a precise parameter or just a blur that is Gaussian with an adjustable parameter, then you'll be able to decide whether a repeated box-style filter will be sufficient." CreationDate="2016-05-15T00:44:22.903" UserId="231" />
  <row Id="3003" PostId="2444" Score="0" Text="If the blur is to be performed by repeated convolution with a local (1-hop-neighborhood) kernel, that kernel (or &quot;box&quot;) would have to vary from vertex to vertex, because the valence and/or neighborhood geometry may vary from vertex to vertex. I think what matters most is that the (per-vertex) kernels' means/centroids and variances/moments of inertia must be consistent. These constraints would fully determine a valence-3 vertex's kernel's weights, but in cases of higher valence perhaps any kernel complying with these constraints will eventually converge to the effect of a Gaussian kernel." CreationDate="2016-05-15T07:42:21.683" UserId="3294" />
  <row Id="3004" PostId="2446" Score="0" Text="1. Does glGetError return anything else than no error? 2. &quot;Blown up&quot; does not really mean anything to me. Maybe it's just me but I'm having hard time figuring out what the problem is and/or expected behavior." CreationDate="2016-05-15T11:30:47.090" UserId="3041" />
  <row Id="3005" PostId="2444" Score="0" Text="I see. Rereading the question I see I misread the start and thought the main focus was a 3D distance approach. I'll have another look and edit the answer in a while." CreationDate="2016-05-15T11:38:20.457" UserId="231" />
  <row Id="3007" PostId="2446" Score="0" Text="As far as I can understand, the images each show some aspect working as it is intended. It isn't clear to me which one (if any) shows the problem you are having. Could you edit to either add an image showing the problem, or indicate which existing image already shows the problem?" CreationDate="2016-05-15T11:42:12.197" UserId="231" />
  <row Id="3008" PostId="2441" Score="0" Text="@Museful Ok whereabout  iterative smaller blurs that are a bigger that way you can do it only over edge connections and let the iterations propagete the effect. This is used in FEM and fluid sims to high efficiency" CreationDate="2016-05-15T15:49:26.570" UserId="38" />
  <row Id="3009" PostId="2446" Score="0" Text="@Andreas so sorry this wasn't clear! I have made some edits that hopefully make things a bit easier to understand.  The simulation I am doing is just a bunch of moving particles that are going to leave a trail behind them. Thankyou so much for looking into this." CreationDate="2016-05-15T16:18:39.363" UserId="2308" />
  <row Id="3010" PostId="2446" Score="0" Text="@trichoplax Same thing for you, I have edited it to make that bit more clear." CreationDate="2016-05-15T16:18:58.380" UserId="2308" />
  <row Id="3011" PostId="2446" Score="0" Text="@Andreas also glGetError does not return anything, except for one error every time I create a shader.  That has not proven to be significant.  Also this error happens on the code apple wrote for their template so It is probably a non-issue." CreationDate="2016-05-15T16:24:14.013" UserId="2308" />
  <row Id="3012" PostId="2446" Score="0" Text="This is a little clearer. If I understand correctly, you want the texture shown in image 1 to cover the image, but without being scaled up as in image 2. So you want to tile the image with the texture shown in image 1?" CreationDate="2016-05-15T17:35:22.787" UserId="231" />
  <row Id="3013" PostId="2441" Score="0" Text="@joojaa Yes I think that is the best approach but how exactly does the small blur work, as the connections/edges of a mesh are not generally regular, like they are between, say, pixels in an image. The mesh geometry should somehow influence how &quot;conductive&quot; each edge is relative to other edges in its neighborhood. How?" CreationDate="2016-05-15T17:38:45.307" UserId="3294" />
  <row Id="3014" PostId="2441" Score="1" Text="Im not sure, im not so deeply invested in the lore of FEM. All I know that they can solve the diffusion over irregular meshes and that is entirely analogous with gaussian blur." CreationDate="2016-05-15T18:04:02.593" UserId="38" />
  <row Id="3015" PostId="2449" Score="0" Text="Thank you **very** much for your explanation. This is maybe off-topic, but I'll ask anyway; in some source codes that compute noise, people use vector vec3(1, 57, 113) to compute dot product with current coordinate (I suppose the aim is also to obtain a hash). Why this particular choice of constants (57 is approx. 1 radian in degrees, 133 = approx. 2*radian in degrees)? Is it because of periodicity in trig functions? I'm unable to google this." CreationDate="2016-05-15T18:55:16.090" UserId="3365" />
  <row Id="3016" PostId="2449" Score="3" Text="@sarasvati I'm not really sure, but a guess is that 57 and 113 are chosen because they're prime-ish numbers. (113 is prime; 57 isn't, but it's 3*19, so still kinda primey...if that's a thing.) Multiplying or modding by a prime-ish number tends to jumble up the bits, so it's not an uncommon ingredient in hashes." CreationDate="2016-05-15T19:15:58.503" UserId="48" />
  <row Id="3017" PostId="2446" Score="0" Text="@trichoplax No, I am thinking that the texture in the FBO is not big enough." CreationDate="2016-05-15T19:19:35.480" UserId="2308" />
  <row Id="3018" PostId="2449" Score="0" Text="Wow, I didn't think about it this way. Thanks." CreationDate="2016-05-15T19:20:40.767" UserId="3365" />
  <row Id="3019" PostId="2446" Score="1" Text="As far as opengl errors go all indicate something did not have desired effect. It matters not who wrote the code. Fix it. Btw what does the FBO look like in the debugger? Anything unusual?" CreationDate="2016-05-15T19:55:22.667" UserId="3041" />
  <row Id="3020" PostId="2449" Score="0" Text="What are the magic numbers?" CreationDate="2016-05-15T20:41:06.323" UserId="3367" />
  <row Id="3021" PostId="2449" Score="0" Text="@cat Not sure how Mikkel came up with them. They may just be random / arbitrarily chosen values." CreationDate="2016-05-15T21:04:08.753" UserId="48" />
  <row Id="3022" PostId="2449" Score="0" Text="@NathanReed Doesn't GLSL have an RNG / PRNG? Why not use that or hardware entropy over hardcoded arbitrary constants?" CreationDate="2016-05-15T21:05:05.290" UserId="3367" />
  <row Id="3023" PostId="2449" Score="0" Text="@cat Well any PRNG or hash function has hardcoded constants in its implementation. And anyway a PRNG is not what you want for this application; you want a hash function. You could use a PRNG as a hash by re-seeding it every time, but a lot of PRNGs don't give good results when used that way; they're primarily designed to give good results in the sequence generated from one seed." CreationDate="2016-05-15T21:10:23.537" UserId="48" />
  <row Id="3024" PostId="2446" Score="0" Text="@Andreas Yeah for some reason the &quot;Depth Attachment&quot; appears to also have a renderbuffer." CreationDate="2016-05-15T21:21:42.247" UserId="2308" />
  <row Id="3025" PostId="2449" Score="1" Text="@cat I doubt GLSL has a PRNG, given that GLSL programs are deterministic." CreationDate="2016-05-15T22:20:40.880" UserId="2316" />
  <row Id="3026" PostId="2449" Score="0" Text="@immibis Shows what I know, I thought GLSL was at least turing-complete." CreationDate="2016-05-15T22:32:31.670" UserId="3367" />
  <row Id="3028" PostId="2449" Score="1" Text="Looks like there are several potential new questions in this comment thread..." CreationDate="2016-05-16T00:22:52.760" UserId="231" />
  <row Id="3029" PostId="2449" Score="0" Text="@cat Being deterministic does not preclude being Turing-complete." CreationDate="2016-05-16T01:28:36.453" UserId="2316" />
  <row Id="3030" PostId="2449" Score="0" Text="@immibis I think I misinterpreted your comment as &quot;it's decidable whether a given program will halt&quot; not &quot;deterministic side-effects&quot;, because I can read. I'll shut up now :P" CreationDate="2016-05-16T01:33:17.547" UserId="3367" />
  <row Id="3031" PostId="2444" Score="0" Text="I have a feeling that [this discretization of the Laplace-Beltrami operator](http://computergraphics.stackexchange.com/a/1721/3294) (&quot;cotan formula&quot;) may give the right weights to attribute to the edges in the mesh. When applied to (ambient space) coordinates of the surface itself, the operator yields the mean curvature normal, but in general when applied to some function defined over the surface, the operator yields the (generalized) Laplacian, or divergence of the function's gradient on that surface, I think." CreationDate="2016-05-16T18:02:46.193" UserId="3294" />
  <row Id="3033" PostId="2444" Score="0" Text="Once we have the Laplacian we can diffuse the initial function over the surface just like heat i.e. $\frac{\partial f}{\partial t}=-\Delta_S f$. Doing so with explicit Euler method may be tantamount to the local mixing we had in mind. Not sure what time-step to choose for good numerical properties. A large time-step means higher variance per iteration and therefore fewer iterations needed, but it would also mean we aren't staying true to the PDE, and I'm not sure at what step-size that begins to matter." CreationDate="2016-05-16T18:14:58.950" UserId="3294" />
  <row Id="3034" PostId="2441" Score="0" Text="@joojaa Thanks for helping me realize it's just diffusion ala $\frac{\partial f}{\partial t}=-\nabla_S f$ with the Laplace-Beltrami operator. I'm thinking of running explicit Euler using [this discretization of the operator](http://computergraphics.stackexchange.com/a/1721/3294). Not sure what time-step to use." CreationDate="2016-05-16T18:26:40.580" UserId="3294" />
  <row Id="3035" PostId="2451" Score="0" Text="Haha, when I posted the answer, SE asked me if I'm a human or a robot, the site wasn't sure :D I hope it is not because of the length of the answer, It got a little bit out of hand." CreationDate="2016-05-16T20:33:39.170" UserId="1613" />
  <row Id="3036" PostId="2451" Score="0" Text="you want to make my brain melt, don't you. ;-) BTW: I already managed to read two of the papers/presentations so I'll hopefully extend the question or write a superficial answer at the end of this week. And now, GoT FTW!" CreationDate="2016-05-16T20:39:10.357" UserId="2479" />
  <row Id="3037" PostId="2449" Score="0" Text="The math speak here is: 57 and 113 are coprimes.  GCD(57,133)=1.  And 57 is square free." CreationDate="2016-05-17T09:11:15.923" UserId="2831" />
  <row Id="3038" PostId="2436" Score="0" Text="Also in MathWorld article: Marsaglia (1972) - point in disk method should trump trig based." CreationDate="2016-05-17T09:18:09.863" UserId="2831" />
  <row Id="3039" PostId="2453" Score="0" Text="Exactly how do you &quot;know&quot; that? Reference maybe?" CreationDate="2016-05-17T10:16:01.747" UserId="3041" />
  <row Id="3040" PostId="2453" Score="0" Text="@Andreas the linked article states this, although doesn't provide a reference. However, it does provide evidence that this affects performance, unless that can be explained in terms of some other cause." CreationDate="2016-05-17T10:44:45.463" UserId="231" />
  <row Id="3041" PostId="2453" Score="0" Text="@trichoplax My bad. Did not read question properly." CreationDate="2016-05-17T10:47:51.343" UserId="3041" />
  <row Id="3042" PostId="2453" Score="0" Text="Also I saw several times that instead of fullscreen quad people use big triangle that covers the screen." CreationDate="2016-05-17T10:48:44.857" UserId="386" />
  <row Id="3044" PostId="2453" Score="0" Text="@trichoplax I saw this explanation several times. For example https://www.reddit.com/r/gamedev/comments/2j17wk/a_slightly_faster_bufferless_vertex_shader_trick/." CreationDate="2016-05-17T11:17:57.233" UserId="386" />
  <row Id="3045" PostId="2453" Score="1" Text="@nikitablack Oh I see - I thought you meant 2 triangles instead of a quad - but you mean 1 oversized triangle that encompasses the whole screen. That's interesting. I apologise - that is very much related to this question :)" CreationDate="2016-05-17T11:20:34.220" UserId="231" />
  <row Id="3046" PostId="2436" Score="0" Text="I would need to do some profiling, but I'm not entirely convinced that rejection sampling would beat trig." CreationDate="2016-05-17T11:56:21.650" UserId="310" />
  <row Id="3047" PostId="2454" Score="1" Text="Enabling GL_POLYGON_SMOOTH would cause multiple rasterization." CreationDate="2016-05-17T16:55:41.177" UserId="3041" />
  <row Id="3048" PostId="2456" Score="0" Text="A geometry shader may add more primitives than found in the original mesh. Does this do it for you?" CreationDate="2016-05-17T17:41:03.227" UserId="3041" />
  <row Id="3049" PostId="2450" Score="0" Text="I believe you need to define the &quot;edge&quot; more in detail. How does it differ from a simple wireframe? In cifz answer there is a good description of screen space post-process, but from your question it is difficult to determine if it is applicable." CreationDate="2016-05-17T19:07:58.727" UserId="3041" />
  <row Id="3050" PostId="2391" Score="0" Text="I have one question. Is the second picture generated only by sampling the EM? Or is it MISed version of sampling cosine and sampling EM? I really hope that it is the MISed version, because if so, then I might have a remedy for the high noise in the shadowy part." CreationDate="2016-05-17T20:42:02.350" UserId="1613" />
  <row Id="3051" PostId="2391" Score="0" Text="No @tom, it uses sperical EM sampling only, ignoring both the (Lambert) BRDF and the cosine factor. 64 samples were used and no image-space filtering applied, just averaging over pixel area. When MIS is applied to combine the EM sampling with the cosine sampling, the noise in the shadow decreases a lot, but slightly increases in the sunlit part." CreationDate="2016-05-17T22:17:39.087" UserId="2479" />
  <row Id="3052" PostId="2450" Score="0" Text="Well, with edge I mean the &quot;creases&quot; and &quot;ridges&quot; that form the solids. A wireframe would display all triangle faces, which is not what I want." CreationDate="2016-05-18T07:27:05.180" UserId="3377" />
  <row Id="3053" PostId="2452" Score="0" Text="Very nice explanation cifz, but what if no gradient is visible in a certain situation? For example, there is no light source at the back of the cube and therefore, no gradient is visible. Then this image based edge detection process you describe wouldn't work, am I right?" CreationDate="2016-05-18T07:31:15.297" UserId="3377" />
  <row Id="3054" PostId="2436" Score="0" Text="super nice answer! thanks a lot.. not giving back my opinion yet about it because I still have to really check and implement your answer.. I will get back to you soon.. but in the meantime a big thanks!" CreationDate="2016-05-18T09:28:37.203" UserId="3069" />
  <row Id="3055" PostId="2450" Score="0" Text="Ok, exactly what I asked for :-) I would generate a wireframe from those creases and ridges. The tricky part is still to determine what a crease/ridge is. Do you have any idea of how to do that?" CreationDate="2016-05-18T10:44:20.217" UserId="3041" />
  <row Id="3057" PostId="2460" Score="0" Text="How do area lights create sampling bias?" CreationDate="2016-05-18T14:32:42.377" UserId="3385" />
  <row Id="3058" PostId="2436" Score="0" Text="Everything is more than clear, thanks a lot for your awesome explanation!" CreationDate="2016-05-18T14:36:19.800" UserId="3069" />
  <row Id="3059" PostId="2436" Score="0" Text="If I may ask you, if you feel like of course, I have another question that might definitely solve my project.. Thanks in any case for your precious help so far:&#xA;http://computergraphics.stackexchange.com/questions/2461/resulting-probabilty-density-in-path-tracer-for-paths-using-next-event-estimatio" CreationDate="2016-05-18T14:38:39.343" UserId="3069" />
  <row Id="3060" PostId="2460" Score="0" Text="@Jake: In the same way as the rest: you have to cast more than one ray, so you need to decide what distribution to use." CreationDate="2016-05-18T19:34:14.567" UserId="196" />
  <row Id="3061" PostId="2460" Score="0" Text="I suppose I thought area lights where directional so that light was only emitted along the normal of the light surface. Is that not how area lights work?" CreationDate="2016-05-18T22:27:16.833" UserId="3385" />
  <row Id="3062" PostId="2460" Score="1" Text="@Jake No, area lights generally emit light into the entire hemisphere of directions at each point on their surface. You can see this if you imagine what a real-world area light looks like (such as a computer display showing solid white). You can see the emitted light from any direction." CreationDate="2016-05-18T23:24:53.520" UserId="48" />
  <row Id="3063" PostId="2460" Score="3" Text="I'd add that [distribution ray tracing](https://en.wikipedia.org/wiki/Distributed_ray_tracing) is sort of a middle ground between recursive ray tracing and path tracing. It allows one to _somewhat_ have things like glossy reflection or depth of field in the recursive framework. The limitation is it's easy to run into problems with the branching factor: imagine if every ray that hits a surface spawns 10-100 secondary rays; the ray count would grow exponentially with each bounce! So path tracing is much more efficient if you want to do more than a very limited amount of stochastic phenomena." CreationDate="2016-05-18T23:42:16.707" UserId="48" />
  <row Id="3064" PostId="2460" Score="0" Text="Yea the laptop example really clears things up. Of course it's hemispherical from each point now that you put it that way. Thanks so much!" CreationDate="2016-05-19T00:07:55.837" UserId="3385" />
  <row Id="3065" PostId="2452" Score="0" Text="If you use  a deferred renderer you can do the same but on the normal buffer or again, if you have a prepass you could apply the algorithm to the depth. &#xA;Still you are right, a screen space approach might be not ideal in all cases :) What solution is viable depends a lot on what is your budget for this effect, how complex are your scene." CreationDate="2016-05-19T07:24:06.327" UserId="100" />
  <row Id="3066" PostId="2452" Score="0" Text="Potentially if this is really central for your game, a very easy, but potentially taxing, would be computing a gradient on your neighboring normals for each vertex (offline at load time) and pass this factor as additional vertex attribute." CreationDate="2016-05-19T07:40:19.280" UserId="100" />
  <row Id="3067" PostId="2462" Score="2" Text="The standard approach is to use a transformation $T$ that does the conversion form space to space. In this case Homogeneous coordinates could be used. The process is pretty well described [here](http://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/building-basic-perspective-projection-matrix). Feel free to write a proper answer so that the question wont hang here forever." CreationDate="2016-05-19T08:12:02.633" UserId="38" />
  <row Id="3068" PostId="2452" Score="0" Text="Thangs again cifz for your help.&#xA;Well, what I want to achieve is to develop a CAD/CAM solution that looks similar to this: https://www.youtube.com/watch?v=-qTJZtYUDB4&#xA;And I really would like to know how they managed it to render all the black edges, creases and ridges.&#xA;cifz, do you think as a graphics programming specialis that they achieved this look by using one of your screen space approaches? Or maybe by computing a gradient on neighboring normals? I really want to know how this can be done. Thanks again!" CreationDate="2016-05-19T09:01:21.427" UserId="3377" />
  <row Id="3069" PostId="2430" Score="0" Text="Thanks for this Joojaa.  It does sound like do my work for me doesn't it? But it was genuinely just as idle thought on the train.  You'll given me some cool clues to get started with (I've never fiddled with image manipulation like this before (which is why it sounds a bit 'DMWFM' - I didn't have a starting point.  Now, thanks to you, I do)).  I'll probably fiddle with this in idle moment - and I may be back with further questions (and my own source code!) later.  Thanks again!" CreationDate="2016-05-19T09:18:53.697" UserId="3319" />
  <row Id="3070" PostId="2452" Score="0" Text="As a premise, I have didn't look that closely to the video as I am at work and super busy :) But judging from the cylindrical green shape at the beginning of the video, they are not using a screen space approach on the final image, as you don't see edges highlighted between that green shape and the white background. Not sure if that is a problem for you, as a first try I'd try a screen space edge detection combining information from depth and normals that should be quite fast to prototype, and if it is failing you go for the offline version or keep experimenting in SS :)" CreationDate="2016-05-19T09:48:16.243" UserId="100" />
  <row Id="3071" PostId="2452" Score="0" Text="Also, update your question to a reference image of what you are looking for :) Maybe it will attract someone with a precise answer. i'll try and think about it and have a better look at the video if I ever stop being so busy :(" CreationDate="2016-05-19T09:59:06.440" UserId="100" />
  <row Id="3072" PostId="2452" Score="0" Text="Thank's very much cifz for your hints. The thing is that I'm a total noob regarding OpenG, GLSL and Computer Graphics in general and don't know how to start. I know you haven't that much time but I would be very thankful if you could help me starting programming the shaders. Of course I would gladly pay you for your efforts." CreationDate="2016-05-19T15:16:49.877" UserId="3377" />
  <row Id="3074" PostId="2463" Score="0" Text="Thanks! I have been doing what you've said at the beginning, using geometry shaders to create geometry around the points and so enable to draw pixels around the point. It works, of course, but its a more cumbersome solution. I got particularly interested in the last bits of what you've said: if I understood you correctly, what you mention is one could render the whole scene and then use a screen shader aimed at the area of interest and then do a pass over the rendered frame in order to work on how many pixels one wants around the points in the center?" CreationDate="2016-05-20T00:06:26.130" UserId="3383" />
  <row Id="3075" PostId="60" Score="0" Text="I seriously doubt the ROP adds any performance overhead if blending is disabled." CreationDate="2016-05-20T03:33:03.143" UserId="3386" />
  <row Id="3076" PostId="60" Score="0" Text="@GroverManheim Depends on the architecture! The output merger/ROP step also has to deal with ordering guarantees even if blending is disabled. With a full-screen triangle there aren't any actual ordering hazards, but the hardware may not know that. There might be special fast paths in hardware, but knowing for certain that you qualify for them…" CreationDate="2016-05-20T06:05:40.787" UserId="196" />
  <row Id="3077" PostId="2465" Score="0" Text="There are also cases where a read-only buffer will do better than storing 4kb of read-only data in shared local memory. For example, storing it in local memory may mean that there is a unique copy of your data for every thread group. If the buffer fits in cache, it's quite possible that the cache performs better than local memory for read-only access patterns." CreationDate="2016-05-20T06:17:13.907" UserId="196" />
  <row Id="3078" PostId="2463" Score="0" Text="You could render the first pass in a texture. Then in the second pass on full screen, in each pixel you could inspect the texture content in a disk around the pixel and do what you want (e.g. color according to the closest filled pixel, or paint white only if distance is below some threshold). Of course for just drawing disks, this is far from being the most efficient way ;-)" CreationDate="2016-05-20T07:12:31.913" UserId="1810" />
  <row Id="3079" PostId="2463" Score="0" Text="Sure, I just gave the simples example I could think of. But yes, I think what you say is in line with what I've been thinking, i.e. rendering to a texture in one pass and then later we have the information for, in other passes, alter pixels in relation to what lays in surrounding pixels. Many thanks." CreationDate="2016-05-20T09:04:10.657" UserId="3383" />
  <row Id="3080" PostId="2452" Score="1" Text="Don't need to pay anyone, just start reading few tutorials online, buy some books and study hard :) It will be very rewarding at the end!" CreationDate="2016-05-20T13:41:32.590" UserId="100" />
  <row Id="3081" PostId="2450" Score="1" Text="The cad programs do not do this with a shader mostly. Instead they know the hard edges from the model and draw a line on top of the mesh form that info." CreationDate="2016-05-20T14:51:40.600" UserId="38" />
  <row Id="3082" PostId="2469" Score="2" Text="Additionally, the code for `vSize()` does not return very precise results as the square root of an integer is not necessarily also an integer, but the method returns an `int`. Actually, in most cases the given method will return imprecise results." CreationDate="2016-05-20T15:36:14.207" UserId="127" />
  <row Id="3083" PostId="2469" Score="0" Text="It might be used as a hash of some kind. Or maybe they are using a integer quantized field." CreationDate="2016-05-20T20:22:29.413" UserId="38" />
  <row Id="3084" PostId="2469" Score="0" Text="Basically the integer is 64-bit. Will it make any difference in that case ?" CreationDate="2016-05-21T09:56:39.100" UserId="2712" />
  <row Id="3085" PostId="2469" Score="0" Text="64 bit integers allow for a [larger maximum number](https://en.wikipedia.org/wiki/9223372036854775807) than 32 bit integers, but they can still only take whole number values. You could ask why integers are used as a separate question, but that would require access to more of the code and some context on what it is used for. Without that context it is difficult to judge whether this would be on topic for Computer Graphics." CreationDate="2016-05-21T12:48:31.720" UserId="231" />
  <row Id="3086" PostId="2471" Score="0" Text="There are several different meanings for &quot;OSG&quot;. Please could you specify which one this question refers to?" CreationDate="2016-05-21T13:04:33.680" UserId="231" />
  <row Id="3087" PostId="2471" Score="0" Text="I dont think there is a good equation for a prism as such." CreationDate="2016-05-21T13:54:29.767" UserId="38" />
  <row Id="3088" PostId="2474" Score="1" Text="Thank you for your help, successfully created the prism. :)&#xA;&#xA;I think, set the 8 points manually and draw with triangles is easier solution in this case. I believed it could be solved with a similar parametric equation, like cylinder.&#xA;&#xA;Like you said, first create a cube, and scale it to the proper size solved the problem." CreationDate="2016-05-21T16:19:53.773" UserId="3408" />
  <row Id="3089" PostId="2479" Score="0" Text="Many thanks for your answer! So, in the meanwhile I was able to come with the exact same solution for the cross-shape problem, but in a messier way. So, I am glad I was not thinking it wrong, but happy to see a cleaner solution. Thanks! About the performance, it's currently terrible: ~14ms now, which is of course unacceptable for real time simulations. I got interested in both your suggestions. Would care to elaborate a bit more? (just a PS: I am trying to implement the intensity of halos of point lights depending on the aggregation of how many point lights are close to each other)." CreationDate="2016-05-22T01:45:46.643" UserId="3410" />
  <row Id="3090" PostId="2479" Score="0" Text="By down-sampling (with which I would be totally fine and would be even welcome) you mean something like this http://stackoverflow.com/questions/14366672/how-can-i-improve-this-webgl-glsl-image-downsampling-shader or something simpler? And I the spatial hierarchy was what I though in the first place, but I though there wasn't a way to save an array of arrays from one pass to the next that would hold the quad-tree. Isn't that the case? I don't see how I would populate the quad-tree in the first pass but then store the info for the second pass (unless using a compute shader)." CreationDate="2016-05-22T01:47:52.310" UserId="3410" />
  <row Id="3091" PostId="2472" Score="0" Text="What do you mean by &quot;random sampling&quot;? Does it mean &quot;uniform random sampling&quot; or &quot;cosine-weighted random sampling&quot;? What does `createRandomReflect` do? How do you compute the PDF of the generated sample? How does your Monte Carlo estimator look like?" CreationDate="2016-05-22T13:05:05.427" UserId="2479" />
  <row Id="3093" PostId="2472" Score="0" Text="Ohh... Sorry,&#xA;&quot;random sampling&quot; means &quot;uniform random sampling&quot;. createRandomReflect creates sample on a hemisphere for uniform random sampling. As for the PDF, I did some integral(on paper) and google/wolfram|alpha search. I'm using a basic Monte Carlo estimator which could be found in any basic Path Tracer(eg. smallpt/smallpaint). It's a loop that throws samples at the scene and averages whatever I get back from the samples." CreationDate="2016-05-22T13:20:25.433" UserId="2448" />
  <row Id="3094" PostId="2472" Score="0" Text="Of course, but in a naive path tracer you usually use a simple Monte Carlo estimation, which consists of picking the direction, evaluating the BRDF for that direction and dividing the result by probability density of generating the direction. You showed us the sampling and evaluation of the BRDF, but we cannot see how you compute the PDF and how you use the generated sample later on. We need more code to understand your implementation." CreationDate="2016-05-22T15:49:06.213" UserId="2479" />
  <row Id="3095" PostId="2479" Score="0" Text="@GiovannS For this I think you would start with a simple 2x box filter downsample. It can be done by just copying a texture into a 2x smaller render target, using bilinear filtering; that will average together 2x2 pixels in the source to 1 pixel in the destination. For the spatial hierarchy approach, if the points are generated on the GPU then you probably want to build the data structure on the GPU. It's going to be a more complicated endeavor with append buffers, compute shaders, etc. If you google &quot;build quadtree on GPU&quot; or similar, there's a number of articles and papers on it." CreationDate="2016-05-22T21:02:41.577" UserId="48" />
  <row Id="3096" PostId="2481" Score="0" Text="WOW! That last link hit the spot! Things are looking great.  However the code they have makes no sense to me as I am a glsl programmer.  Thankfully I found two pass code somewhere." CreationDate="2016-05-23T06:44:48.920" UserId="2308" />
  <row Id="3097" PostId="2481" Score="0" Text="My confusion is the following, how exactly do you increase/decrease the size of the blur? Increasing how many texels away each sample is, or increasing the amount of texels sampled? And have you seen any good code for this working inside of a for loop based on glow radius?" CreationDate="2016-05-23T06:46:00.707" UserId="2308" />
  <row Id="3098" PostId="2481" Score="1" Text="_&quot;I would think rendering to a downsampled FBO would be much faster than generating mipmaps. glGenerateMipmap will generate the complete mipmap pyramid, which seems wasteful:&quot;_&#xA;Surely that might depend on whether there is custom hardware or not? Also, given that the total MIP map chain is a 33% overhead on the original image, but the 1st MIP map level is 25% of that, the storage for the remainder is only an additional 8%." CreationDate="2016-05-23T12:13:15.337" UserId="209" />
  <row Id="3099" PostId="2483" Score="1" Text="This is interesting but I'm confused by one point. Could you clarify what it means to &quot;divide the result for that direction by probability of picking the direction&quot;? If it is not a binary choice but a direction chosen from a continuous distribution, won't the probability be zero?" CreationDate="2016-05-23T15:00:26.530" UserId="231" />
  <row Id="3100" PostId="2483" Score="0" Text="Thank you! This helps a lot. The method you describe will work for ideal Fresnel surfaces. And I just discovered Walter et. al. [2007] and Burley [2015], which can be used for rough BSDFs." CreationDate="2016-05-23T15:10:04.173" UserId="310" />
  <row Id="3101" PostId="2483" Score="1" Text="@trichoplax: Yes it would, but in that paragraph I was describing the sampling technique just for a (dielectric) Fresnel BSDF - ideally smooth surface, which is a sum of two Dirac delta functions. In such case you are picking one of the directions with some discrete probability. In case of a non-delta (finite) BSDF, you generate directions according to a probability density function. Unfortunately, delta and non-delta cases have to be handled separately, which makes the code a little messy. More details on sampling microfacet BSDFs can be found, for example in the Walter et. al. [2007] paper." CreationDate="2016-05-23T16:37:09.823" UserId="2479" />
  <row Id="3102" PostId="2483" Score="2" Text="@RichieSams: Walter et. al. [2007] is basically still the state-of-the art for dielectric rough surfaces, but to make it work well you need a good sampling which was published just recently by Heitz and D'Eon the 2014 paper &quot;Importance Sampling Microfacet-Based BSDFs using the&#xA;Distribution of Visible Normals&quot;. And note that it is a single-scattering model which neglects inter-reflections between microfacets making it visibly dark for higher roughness values. See my question &quot;Compensation for energy loss in single-scattering microfacet BSDF models&quot; for more details." CreationDate="2016-05-23T16:53:04.740" UserId="2479" />
  <row Id="3104" PostId="2483" Score="4" Text="Just wanted to point out that if you choose probability = fresnel() as the question suggested, then when you divide by the probability, you cancel out the Fresnel factor that would normally be multiplied in. So (in the discrete, two-Dirac case) you end up with the ray contribution not including any Fresnel factor at all. It's standard importance-sampling theory, but I thought I'd point that out as a potentially confusing issue." CreationDate="2016-05-23T18:22:23.363" UserId="48" />
  <row Id="3105" PostId="2485" Score="2" Text="As an example two edge sharing triangles may perfectly splitting the same pixel in two but an OpenGL implementation shall guarantee only one of the triangles cover the pixel center. Which one is implementation dependent." CreationDate="2016-05-23T18:27:22.187" UserId="3041" />
  <row Id="3106" PostId="2483" Score="0" Text="@Nathan, that's actually a very good point. I'm glad you mentioned it." CreationDate="2016-05-23T18:32:49.610" UserId="2479" />
  <row Id="3107" PostId="2212" Score="0" Text="@FabrizioDuroni, does it mean, that adjusting the power of your light source with the brightness parameter doesn't make the resulting picture bright enough? I think I am missing something and I'm afraid you are adding to much alchemy into it ;-) Anyway, I'm glad it helped at least partially." CreationDate="2016-05-23T20:04:32.153" UserId="2479" />
  <row Id="3108" PostId="2481" Score="0" Text="Good point, I spoke too broadly there.  It is possible that generating the chain is faster on some systems." CreationDate="2016-05-24T03:43:14.057" UserDisplayName="user3412" />
  <row Id="3109" PostId="2481" Score="0" Text="To increase the size of the blur you increase the amount of pixels sampled.  See here for a 7x7 blur: https://software.intel.com/en-us/blogs/2014/07/15/an-investigation-of-fast-real-time-gpu-based-image-blur-algorithms" CreationDate="2016-05-24T03:48:38.897" UserDisplayName="user3412" />
  <row Id="3110" PostId="2487" Score="0" Text="Oh awesome, I guess I should have dug a little deeper on the corporation websites.  I assumed that since apitrace didn't offer these details, there was some sort of reversing going on.  Thanks!" CreationDate="2016-05-24T06:06:11.660" UserDisplayName="user3412" />
  <row Id="3111" PostId="2483" Score="2" Text="@Nathan, I incorporated your notice into the answer." CreationDate="2016-05-24T11:27:33.883" UserId="2479" />
  <row Id="3112" PostId="2490" Score="0" Text="That first case sounds more like the  non-zero rule (https://en.wikipedia.org/wiki/Nonzero-rule) (except using  the normal rather than 2D edge direction)" CreationDate="2016-05-24T16:40:01.747" UserId="209" />
  <row Id="3114" PostId="2212" Score="0" Text="I was missing something too, and in fact i remove that strange &quot;alchemy&quot; that i though was the right thing to do (web is not always a good source :D) Just a brightness multipler is good. This parameter could be substituted with more physically correct values, like the light power (using a Rienmann sum to calculate the integrate the light SPD ) and its area. But for my thesis has been decided that the brightness parameter is enough." CreationDate="2016-05-24T20:06:11.123" UserId="2237" />
  <row Id="3115" PostId="2212" Score="0" Text="This is an example of a final rendered image that will be included in the thesis https://drive.google.com/open?id=0BxeVnHLvT8-7TVBZZE44VTRuT1k Keep watching my github repo for the final result ;)" CreationDate="2016-05-24T20:06:16.093" UserId="2237" />
  <row Id="3116" PostId="2212" Score="0" Text="@fabrizio, but that is almost exactly what I suggested you to do. ;-) And yes, having a more physically meaningful parameter (e.g. power) in a physically-based renderer would be definitely a nicer thing than some unitless brightness parameter. Especially when you have a spectral renderer (it is not clear how to do it in a colour-space renderer). The image looks pretty good to me. Good luck with the thesis!" CreationDate="2016-05-24T20:39:39.613" UserId="2479" />
  <row Id="3118" PostId="2493" Score="0" Text="Requests for off site resources are off topic, but by removing the final sentence this is now an on topic question. Feel free to override my edit if I've changed your intent too much." CreationDate="2016-05-25T10:26:30.260" UserId="231" />
  <row Id="3119" PostId="2498" Score="0" Text="I don't understand, can you be more precise ?" CreationDate="2016-05-25T16:28:13.513" UserId="3339" />
  <row Id="3120" PostId="2499" Score="0" Text="Does that mean that the geometry term cancels the cosine term ?" CreationDate="2016-05-25T17:35:20.943" UserId="3339" />
  <row Id="3121" PostId="2499" Score="0" Text="No. The geometry term is a normalization factor that accounts for the fact that the microfacets will shadow and mask each other. The 'visibility' term I mention in answer is the integral itself. The integral adds up all *visible* incoming light." CreationDate="2016-05-25T17:45:59.950" UserId="310" />
  <row Id="3122" PostId="2499" Score="0" Text="However, in the book Real Time Rendering I read that for the Blinn-Phong model, the geometry term is (n.v)(n.l) which cancels the denominator." CreationDate="2016-05-25T20:06:31.837" UserId="3339" />
  <row Id="3123" PostId="2500" Score="0" Text="&quot;As you can see this causes an issue because of the segments.&quot;. Could you describe what problem you are seeing? What about the picture should be different?" CreationDate="2016-05-25T21:28:56.693" UserId="231" />
  <row Id="3124" PostId="2500" Score="0" Text="@Trichoplax the line has clear segments inside of it. The line should be smooth." CreationDate="2016-05-25T21:30:22.467" UserId="2308" />
  <row Id="3125" PostId="2502" Score="0" Text="What does n mean? Why 0.95?" CreationDate="2016-05-25T21:31:44.270" UserId="2308" />
  <row Id="3126" PostId="2500" Score="0" Text="Do you mean that there should be no visible colour difference between the segments?" CreationDate="2016-05-25T21:50:14.087" UserId="231" />
  <row Id="3127" PostId="2499" Score="1" Text="@Livetrack: The part (n.v)(n.l) is not usually called &quot;geometry factor&quot;, that is the G in the formula. The only part which gets cancelled when this BRDF is placed into the rendering equation is (n.l)." CreationDate="2016-05-25T21:54:58.507" UserId="2479" />
  <row Id="3128" PostId="2500" Score="0" Text="@trichoplax I just want the color to be smooth. I want the new segment to math nicely with the previous faded one.  Just so that everything looks continuous in both shape and color." CreationDate="2016-05-25T22:22:25.927" UserId="2308" />
  <row Id="3130" PostId="2495" Score="0" Text="Oh I had totally missed that this was already a result in the paper. That certainly clears it. :) I'll have to reread it to get a better grasp of how it fits within the BRDF though." CreationDate="2016-05-26T09:00:09.833" UserId="182" />
  <row Id="3131" PostId="2503" Score="0" Text="could you apply your thinking to my example as stated above? I did not get the grasp of your method." CreationDate="2016-05-26T09:00:55.963" UserId="2214" />
  <row Id="3133" PostId="2507" Score="0" Text="The second method is Phong shading model ? And could you elaborate more on how you conclude that in Gouraud Shading we cannot have brighter points and in the second method we can have brighter points ?" CreationDate="2016-05-26T09:17:50.953" UserId="2214" />
  <row Id="3134" PostId="2507" Score="0" Text="@johnjohn gouraud shading is taking a weighted average of the colors of the vertices. While with per pixel you can have a point light near the middle of a large surface. The vertices won't get much light but the middle should have a lot of light." CreationDate="2016-05-26T10:08:00.483" UserId="137" />
  <row Id="3135" PostId="2509" Score="0" Text="Thanks. And how big are these groups?" CreationDate="2016-05-26T12:10:58.630" UserId="386" />
  <row Id="3136" PostId="2509" Score="1" Text="@nikitablack depends on the hardware and is kinda impossible to extract naively from any marketing information." CreationDate="2016-05-26T12:13:00.820" UserId="137" />
  <row Id="3137" PostId="2509" Score="0" Text="And also a question. Lets's say one thread in a group reads from some buffer in a branch based on that thread's id. Lets's say the buffer size is 1, so only the first thread can read, all other threads will cause out of bounds read. Is it safe? And one more thing - the read from the system memory is very slow, but all threads will read from it, even if they don't need, right?" CreationDate="2016-05-26T12:16:15.593" UserId="386" />
  <row Id="3138" PostId="2509" Score="1" Text="@nikitablack that's going to depend on a few things; 1) if the driver is smart enough to forward the constant in the test into the then clause, 2) if the hardware can do a &quot;masked read&quot;, 3) what exactly happens on out of bounds reads" CreationDate="2016-05-26T13:12:26.167" UserId="137" />
  <row Id="3139" PostId="2507" Score="0" Text="@johnjohn just to add to ratchet's comment, WRT Gouraud, just consider two points on a 2D graph where height represents brightness and connect them by a straight line segment. No point along the segment can be higher (i.e. brighter) than the higher end point.  If you consider the edges of your triangle it shows that no point on an edge can be brighter than the end vertices.  Now for any point inside the triangle just draw, say, a line through the point from edge to edge - the point inside can't be brighter.&#xA;&#xA;With per-pixel shading, however, you don't have linear interpolation." CreationDate="2016-05-26T13:53:55.863" UserId="209" />
  <row Id="3140" PostId="2503" Score="0" Text="If fact I'm not sure what you meanhere by &quot;interpolate&quot;. If what you aim at is a mesh, your formula directly give the *position* where the isoval cross an edge. Then you obtain the mesh by connecting these points." CreationDate="2016-05-26T15:50:39.080" UserId="1810" />
  <row Id="3141" PostId="2502" Score="0" Text="opacity 0.05 means transparency 0.95, i.e. the intensity scaling. n is the number of passes (i.e. number of times you apply a 0.05 opacity)." CreationDate="2016-05-26T15:52:19.933" UserId="1810" />
  <row Id="3142" PostId="2498" Score="0" Text="The other answer state the same differently in its last paragraph :-)" CreationDate="2016-05-26T15:54:02.783" UserId="1810" />
  <row Id="3143" PostId="2509" Score="2" Text="@nikitablack The lockstep group size is 32 for NVIDIA and 64 for AMD (they call them &quot;warps&quot; and &quot;wavefronts&quot; respectively). For Intel the size is variable from 8 to 32, determined by the shader compiler. Also, out-of-bounds reads are defined to just return zero in DirectX, and I'm pretty sure it's the same for GL and the other APIs." CreationDate="2016-05-26T17:13:09.697" UserId="48" />
  <row Id="3144" PostId="2492" Score="0" Text="The diagrams are nice, but you might want to label the points (p0, p1, p2) to make them easier to understand. :)" CreationDate="2016-05-26T17:16:53.060" UserId="48" />
  <row Id="3145" PostId="2492" Score="1" Text="There are just sketches @NathanReed I threw away the sources, but yes i can do that." CreationDate="2016-05-26T18:29:57.653" UserId="38" />
  <row Id="3156" PostId="2509" Score="1" Text="For the reason Nathan just described, you should always make your work group size a multiple of 64 to avoid wasting shader invocations in the last warp / wavefront of each group. If you don't need to use shared variables, exactly 64 is usually a good size." CreationDate="2016-05-27T07:57:30.677" UserId="1937" />
  <row Id="3157" PostId="2503" Score="0" Text="What I do not understand, is where you put what in the formula. I know that in V1 and in V2 I should replace the values from the vertices but in P1 and P2 I have no clue what I should put there? Can you give me a hint how it works in the current example?" CreationDate="2016-05-27T08:08:52.733" UserId="2214" />
  <row Id="3158" PostId="2512" Score="1" Text="I do not understand what you mean though in the last paragraph. Could you elaborate more on that ? For example, what do you mean by 'negative' and 'positive' end ?" CreationDate="2016-05-27T09:02:37.070" UserId="2214" />
  <row Id="3159" PostId="2510" Score="1" Text="It looks to me that it's just finding runs of monotonic edges. Given a defined winding order (in this case anticlockwise), then you can identify P6 through P0 as a decreasing run, as is P2 through P4.  Since the *left most* vertex, P8, is on a decreasing run, the *decreasing* runs define left boundaries (and therefore increasing runs, right boundaries)" CreationDate="2016-05-27T09:59:49.447" UserId="209" />
  <row Id="3160" PostId="2503" Score="0" Text="P1 and P2 are the coordinates of the segment extremities." CreationDate="2016-05-27T11:18:50.863" UserId="1810" />
  <row Id="3161" PostId="2507" Score="0" Text="@SimonF So, to recap there are three methods of finding the brightness:&#xA;1) Firstly, calculate the dot product of normals and light vector for each vertex, then interpolate over triangle.&#xA;2) Interpolate normals then calculate the dot product with light vector for each point.&#xA;3) Interpolate gradient vectors then calculate the dot product with light vector.&#xA;&#xA;From these methods, which can give brightness bigger than the vertices?" CreationDate="2016-05-27T18:50:58.750" UserId="2214" />
  <row Id="3162" PostId="2514" Score="0" Text="what if we assume a directional&#xA;light, whose direction does not depend on the position on the surface. What happens then ?" CreationDate="2016-05-27T19:47:59.517" UserId="2214" />
  <row Id="3163" PostId="2514" Score="0" Text="@johnjohn same thing, the view direction is still affected by the surface." CreationDate="2016-05-27T20:35:11.993" UserId="38" />
  <row Id="3165" PostId="2498" Score="0" Text="I don't understand : Imagine that I have a single point source, then I can simplify the equation like this : $$ L = f(\omega_o, \omega_i) E(\omega_i) $$. Why don't I need to bother about the cosine term ?  PS : J'ai vu que vous êtes chercheur à l'Inria. Je ne veux pas trop vous déranger mais vous pourriez faire l'explication en français ? Merci." CreationDate="2016-05-27T21:02:36.193" UserId="3339" />
  <row Id="3166" PostId="2499" Score="0" Text="I don't understand how the integral (which is about incoming lights) can cancel the cosine term." CreationDate="2016-05-27T21:05:33.693" UserId="3339" />
  <row Id="3167" PostId="2515" Score="0" Text="So there are three methods which you can find the brightness .&#xA;1) Firstly, calculate the dot product of normals and light vector for each vertex, then interpolate over triangle. 2) Interpolate normals then calculate the dot product with light vector for each point. 3) Interpolate gradient vectors then calculate the dot product with light vector. &#xA;&#xA;From these methods, which can give brightness bigger than the vertices?" CreationDate="2016-05-28T08:00:48.460" UserId="2214" />
  <row Id="3168" PostId="2515" Score="1" Text="@johnjohn 2 and 3. 1 is Gouraud shading that is hardly used anywhere anymore because it does not look correct." CreationDate="2016-05-28T08:03:34.127" UserId="38" />
  <row Id="3169" PostId="2515" Score="0" Text="So I assume from what you said that the **order** of interpolation (comes first in Gouraud Shading or comes second in the other methods) can lead to better shading results, only for the last two." CreationDate="2016-05-28T08:07:32.857" UserId="2214" />
  <row Id="3170" PostId="2515" Score="1" Text="@johnjohn Gouraud shading is a trick to compensate for the fact that hardware was not fast enough for the job. Nobody even back whan it was conceived thought it was any good. (well obviously if you use Gouraud on a mesh that has one triangle per pixel theres no difference) Since the lightning is heavily dependent on where the local normal points it means that any method that is locally per pixel calculated can have hot spots in other than corners." CreationDate="2016-05-28T08:11:42.950" UserId="38" />
  <row Id="3171" PostId="1721" Score="1" Text="What Meyer et al. 2003 did not (perhaps explicitly) mention was how to compute the curvature for border vertices. Since they have taken an angle deficit approach, the Gaussian curvature for border vertices should read $K = (\pi - \sum_j{\theta_j})/A_{mixed}$." CreationDate="2016-05-28T09:33:15.520" UserId="3444" />
  <row Id="3173" PostId="2510" Score="1" Text="Not sure if it will help you but [Clipper](http://www.angusj.com/delphi/clipper.php) uses Vatti's algorithm. The [docs](http://www.angusj.com/delphi/clipper/documentation/Docs/Overview/_Body.htm) mention: &quot;A section in 'Computer graphics and geometric modeling: implementation and algorithms' by By Max K. Agoston (Springer, 2005) discussing Vatti Polygon Clipping was also helpful in creating the initial Clipper implementation.&quot;" CreationDate="2016-05-28T13:56:46.263" UserId="141" />
  <row Id="3175" PostId="2516" Score="0" Text="Does orthogonal in this context mean using only vertical and horizontal edges? Are you looking for a way to represent more than 4 edges meeting at a single vertex?" CreationDate="2016-05-28T14:49:28.780" UserId="231" />
  <row Id="3178" PostId="2522" Score="0" Text="So what should be the correct implementation ?. If i am not wrong i used the variable as defined in this video on the algorithim.  https://www.youtube.com/watch?v=TRbwu17oAYY&#xA;&#xA;Any idea how to implement a correct Bresenham's line drawing algorithm for slope &lt; 1." CreationDate="2016-05-28T22:50:00.063" UserId="3437" />
  <row Id="3179" PostId="2522" Score="0" Text="Although the algorithm is often stated using fractions, it can be implemented in integers only, by using the horizontal and vertical measures of the line to keep track of when to move horizontally and when to move diagonally." CreationDate="2016-05-29T02:06:55.973" UserId="231" />
  <row Id="3180" PostId="2524" Score="1" Text="Thanks that was on spot. also got the reason why it was wrong. excellent answer with great explanation." CreationDate="2016-05-29T05:18:06.737" UserId="3437" />
  <row Id="3181" PostId="2522" Score="1" Text="Interesting, I'll delete the lastparagraph. Nice explanation BTW." CreationDate="2016-05-29T07:12:11.497" UserId="2479" />
  <row Id="3183" PostId="2519" Score="0" Text="Thank you for your reply. I've implemented the formula you've supplied and I got identical results as with my own (when using the macrosurface normal). So it seems it's just a different form (I got it from: http://graphicrants.blogspot.nl/2013/08/specular-brdf-reference.html)&#xA;&#xA;I was confused about the half vector because the SIGGRAPH 2015 PBS math course specifically state the geometry function dependant on the view, light and half vectors. So this is an error in the slides?" CreationDate="2016-05-29T08:26:00.767" UserId="3424" />
  <row Id="3184" PostId="2519" Score="0" Text="@Erwin, now that you provided also the formula itself, it is much clearer. Next time do it right at the beginning, it helps. Yes, both version (mine and yours) are equivalent, but neither of them uses halfway vector for computing sine or tangent function. It uses $n\cdot v$ rather than $h\cdot v$ as you did in your implementation - that seems to be the mistake. I suspect you did the same mistake with the new implementation as well." CreationDate="2016-05-29T10:01:42.137" UserId="2479" />
  <row Id="3185" PostId="2526" Score="0" Text="There are a lot of code snippets on the linked page, but none of them appear to have the character &quot;`#`&quot;. Could you show a minimal example of code that you know gives this error message?" CreationDate="2016-05-29T14:26:59.780" UserId="231" />
  <row Id="3186" PostId="2526" Score="0" Text="@trichoplax my bad. I should have explained that there is an executable provided in order for you to run it. I will update the question with a minimal example." CreationDate="2016-05-29T14:50:16.080" UserId="3452" />
  <row Id="3188" PostId="2527" Score="1" Text="Yet as you can see by yourself Chapman's unedited algorithm is given in this exact form, and I do guarantee that it could run perfectly on this but also another computer (both having ATI graphics cards). I think it could have something to do with Vulkan and a more strict set of rules, but I have not been able to verify it yet." CreationDate="2016-05-29T20:56:59.567" UserId="3452" />
  <row Id="3189" PostId="2527" Score="0" Text="@green_leaf oh yes I'm not doubting that it did work - I just don't have an explanation for why. I guess the change was a &quot;correction&quot; even though it hasn't turned out to be helpful for you..." CreationDate="2016-05-29T22:10:04.893" UserId="231" />
  <row Id="3192" PostId="2527" Score="0" Text="that's very classic with ATI" CreationDate="2016-05-30T01:04:12.653" UserId="1614" />
  <row Id="3194" PostId="2487" Score="0" Text="oh I thought he wanted to write another perfkit. which the answer would be that it's not possible unless you make your own hardware." CreationDate="2016-05-30T01:30:37.823" UserId="1614" />
  <row Id="3196" PostId="2482" Score="0" Text="russian roulette" CreationDate="2016-05-30T01:46:15.387" UserId="1614" />
  <row Id="3197" PostId="2483" Score="0" Text="I got it to work! http://i.imgur.com/m1fYRdr.png" CreationDate="2016-05-30T02:09:01.033" UserId="310" />
  <row Id="3200" PostId="2512" Score="0" Text="Say your cube is at position 0 on each axis in your volume, then it will fill the space from position 0 to position 1. So the left side of your cube will be at X=0 and the right side will be at X=1, same for the other two axes. So you use P1 and V1 for the edge end with the lower coordinate value and P2 and V2 for the end with the higher value." CreationDate="2016-05-30T07:30:24.077" UserId="1937" />
  <row Id="3201" PostId="2518" Score="0" Text="The 'rotation by area mapping' algorithm described at http://www.leptonica.com/rotation.html looks like a good place to start" CreationDate="2016-05-30T07:39:46.800" UserId="1937" />
  <row Id="3202" PostId="2465" Score="0" Text="Thanks for the feedback guys. I've finished the project I was using this for now, and wound up just using a r8ui readonly buffer texture, which worked pretty nicely :)" CreationDate="2016-05-30T08:01:59.140" UserId="1937" />
  <row Id="3205" PostId="2483" Score="0" Text="@RichieSams: Cool, but does your glass attenuate light? If it doesn't, the object should not be visible under constant illumination I'm afraid." CreationDate="2016-05-30T12:34:54.060" UserId="2479" />
  <row Id="3206" PostId="2519" Score="0" Text="I did use N dot V in my new implementation, that gave me identical results to the second image I've posted. But I'm still not clear on why the PBS course slides state that the halfway vector should be used (See: http://blog.selfshadow.com/publications/s2015-shading-course/hoffman/s2015_pbs_physics_math_slides.pdf, Slide 88)." CreationDate="2016-05-30T15:24:04.623" UserId="3424" />
  <row Id="3207" PostId="2483" Score="0" Text="Currently, the BTDF just returns the surface Albedo, (which is float3(0.95) in this render). If I set it to float3(1.0), yes, it will disappear. Implementing Beer-Lambert is my next task." CreationDate="2016-05-30T15:50:47.180" UserId="310" />
  <row Id="3209" PostId="2423" Score="1" Text="@KristofferHelander :  Converting from 14 bit capture to a 16 bit representation of the 0-1 range is easily achieved by multiplication. But most of our textures are painted, not photographed -- sometimes they are painted directly in a 16 bit format, sometimes they are painted in sRGB and then converted to 16 bit when &quot;linearized&quot; for use as a texture. There's no need for HDR for albedo textures." CreationDate="2016-05-30T17:31:00.883" UserId="1781" />
  <row Id="3211" PostId="2423" Score="1" Text="@KristofferHelander : For albedo textures, we tend to use TIFF with 16 bit integer data (what we call .tx is just TIFF format but tiled and with MIP-map multiresolution stored as multiple subimages within the TIFF file). For true HDR data, like environment captures, we use OpenEXR. Renderer output also tends to be OpenEXR." CreationDate="2016-05-30T17:34:53.267" UserId="1781" />
  <row Id="3213" PostId="2531" Score="0" Text="What i understand is that you want to make User interface that will not be used in a game. right ?" CreationDate="2016-05-30T17:43:53.450" UserId="3437" />
  <row Id="3214" PostId="2527" Score="0" Text="@v.oddou indeed now that you mention it. It seems that they do that quite often." CreationDate="2016-05-30T18:19:32.347" UserId="3452" />
  <row Id="3215" PostId="2531" Score="0" Text="Yes, My current goal is to create my own pomodoro time management application, so I want to create a simple User Interface that can be used for that purpose." CreationDate="2016-05-30T18:57:16.693" UserId="3459" />
  <row Id="3216" PostId="2531" Score="0" Text="Check the answer i posted just now" CreationDate="2016-05-30T19:55:42.357" UserId="3437" />
  <row Id="3217" PostId="2533" Score="0" Text="This is perfect, Thank you so much!" CreationDate="2016-05-30T20:57:03.377" UserId="3459" />
  <row Id="3219" PostId="2519" Score="0" Text="Do I understand it correctly that using $h\cdot v$ instead of $n\cdot v$ was THE problem? Regarding the use of halfway vector in $G_1$: In fact it is used in both of the versions I posted (I made a mistake when constructing the LaTeX formula and wrote geomeotric normal into the first one, I'll fix it soon), but the point is that the halfway vector is not used to compute the cosine value (i.e. there is no $h\cdot v$ used)." CreationDate="2016-05-30T21:28:30.040" UserId="2479" />
  <row Id="3220" PostId="2533" Score="0" Text="@ArdenRasmussen So what are you using to make UI ?" CreationDate="2016-05-30T21:49:57.927" UserId="3437" />
  <row Id="3221" PostId="2534" Score="1" Text="I am unsure what you are asking here. What you posted as quote from the book is just the definition of a line, your title sounds unrelated. Are you asking how to find point on a line of which you have an implicit form like the one you posted?" CreationDate="2016-05-30T21:59:39.150" UserId="100" />
  <row Id="3222" PostId="2534" Score="0" Text="The midpoint algorithm uses the implicit equation of the line and I am looking forward to some proofs of the three cases mentioned in my initial post." CreationDate="2016-05-30T23:51:27.607" UserId="2712" />
  <row Id="3223" PostId="2534" Score="0" Text="This question should be on Math StackExchange." CreationDate="2016-05-31T04:51:05.573" UserId="3437" />
  <row Id="3226" PostId="2538" Score="0" Text="Are you doing coding by deferring to stackexhange, perchance?" CreationDate="2016-05-31T12:38:14.947" UserId="38" />
  <row Id="3228" PostId="2516" Score="0" Text="@trichoplax Exactly, and I do appreciate any help." CreationDate="2016-05-31T12:52:09.830" UserId="537" />
  <row Id="3229" PostId="2538" Score="0" Text="I totally missed your last question ?" CreationDate="2016-05-31T12:52:12.900" UserId="2712" />
  <row Id="3231" PostId="2538" Score="0" Text="if `xMax` and `xMin` are `float`s, why is `xDist` an `int`?" CreationDate="2016-05-31T14:32:39.117" UserId="457" />
  <row Id="3232" PostId="2538" Score="0" Text="All the declared variable are in short int . The only floating point issue may arise with xDist/scanSpacingStep." CreationDate="2016-05-31T14:47:44.910" UserId="2712" />
  <row Id="3233" PostId="2540" Score="0" Text="No. Constant buffers are just a chunk of memory. The DX spec and hlsl spec define the different types." CreationDate="2016-05-31T23:04:07.060" UserId="310" />
  <row Id="3234" PostId="2542" Score="0" Text="Will you always have a fixed number of floats to store per cell? If so, would using one texture for each float work? For example, using 3 textures to store 9 floats per cell (3 per texture using the R, G, and B channels)." CreationDate="2016-06-01T00:12:22.283" UserId="231" />
  <row Id="3235" PostId="2538" Score="0" Text="I'm not quite sure what you are asking. Are xMax and xMin pixel locations? If so are both inclusive? If xMin=2 and xMax=4, does that cover 2 columns of pixels or 3?" CreationDate="2016-06-01T00:22:07.583" UserId="231" />
  <row Id="3236" PostId="2543" Score="0" Text="I love the code review part. I'd add that in addition to renaming `startHueInRadians` the poster should also change it from an array to a `struct`. None of hue, saturation, or brightness represent the same type of value, so you shouldn't treat them as such!" CreationDate="2016-06-01T05:05:30.140" UserId="3003" />
  <row Id="3237" PostId="2543" Score="0" Text="@user1118321 Sadly there is nothing like a c++ struct in java, i need to create a special class for it." CreationDate="2016-06-01T06:02:35.160" UserId="3437" />
  <row Id="3239" PostId="2538" Score="0" Text="yes , it can be associated with pixel locations. (xMin,yMin) and (xMax,yMax) are inclusive. If xMin = 2 and xMax = 4 , and if both of them are inclusive , it should cover three columns, is not it ?" CreationDate="2016-06-01T11:04:27.273" UserId="2712" />
  <row Id="3240" PostId="2544" Score="0" Text="Can show some pictures and code ?" CreationDate="2016-06-01T17:22:46.613" UserId="3437" />
  <row Id="3241" PostId="2544" Score="0" Text="@ritwiksinha Done!" CreationDate="2016-06-01T17:24:52.747" UserId="2308" />
  <row Id="3242" PostId="2544" Score="0" Text="Are you using $sine/cosine$ as the oscillating function ?" CreationDate="2016-06-01T17:30:14.887" UserId="3437" />
  <row Id="3243" PostId="2544" Score="0" Text="Yes technically. However as you can see by the value v I am going to map it differently." CreationDate="2016-06-01T17:37:01.890" UserId="2308" />
  <row Id="3244" PostId="2516" Score="2" Text="Is the graph guaranteed to be planar so that, forgetting the orthogonal edges for a moment, the graph can be drawn without edges overlapping?" CreationDate="2016-06-01T18:36:59.830" UserId="2500" />
  <row Id="3245" PostId="2542" Score="0" Text="@trichoplax Thanks for your comment! No, I won't have a fixed number of floats, unfortunately. But even if I could fix it for those purposes, that solution wouldn't works since I will have high number of floats - which would result in great memory overhead due to the high number of textures to be used in order to implement your suggestion" CreationDate="2016-06-01T20:00:51.870" UserId="3410" />
  <row Id="3246" PostId="2542" Score="0" Text="This is very relevant information - I'd recommend editing the question to include this." CreationDate="2016-06-01T21:19:55.183" UserId="231" />
  <row Id="3247" PostId="2544" Score="1" Text="If you solved your own problem, you can also post an answer for anyone who sees the same problem in future. This is [actively encouraged](http://computergraphics.stackexchange.com/help/self-answer)." CreationDate="2016-06-01T21:28:31.180" UserId="231" />
  <row Id="3248" PostId="1978" Score="0" Text="Is it correct to think of this model-matrix as a model-to-world matrix? in case I want to transform a point to the coordinate system of the model, then will I have to use the inverse of the model-matrix?" CreationDate="2016-06-02T05:24:49.527" UserId="116" />
  <row Id="3249" PostId="1978" Score="0" Text="Regarding the camera-matrix (built with cam position, lookat point and up vector), can I call it a camera-to-world matrix? Then, if I want to go from world to camera space, will I have to use the inverse of that camera-matrix?" CreationDate="2016-06-02T05:27:06.040" UserId="116" />
  <row Id="3250" PostId="2549" Score="0" Text="Yes, you are completely right, the direction vector from the camera has nothing to do with the world z-axis, my mistake. What confused me, I think it is silly but still, is [this image](http://www.scratchapixel.com/images/upload/perspective-matrix/camera2.png?) where the +z-axis enters the lens and I think you mean the +z-axis exits the lens ... what are the implications of using one or the other? This is what I cannot seem to understand" CreationDate="2016-06-02T06:02:47.743" UserId="116" />
  <row Id="3251" PostId="2549" Score="0" Text="@BRabbit27 your image is interesting, it does mention the **default** word which is important. I'm curious as to why the cam is pointing to `-z`, maybe a row matrix vs column matrix thing again ?" CreationDate="2016-06-02T06:07:12.227" UserId="1614" />
  <row Id="3252" PostId="2549" Score="0" Text="@BRabbit27 oh right, we can think of it this way: an identity camera matrix is the same as no view matrix. therefore we end up with the default basis of the API. In OpenGL we definitely see towards `-z` by default. In DirectX historically I've used LeftHanded systems I guess which made me reverse z." CreationDate="2016-06-02T06:09:42.617" UserId="1614" />
  <row Id="3253" PostId="2549" Score="0" Text="In [Scratchapixel](http://www.scratchapixel.com/index.php) the authors use row-major matrix and right-hand system. So actually when you say &quot;camera looking along&quot; it means the direction the lens points to? Maybe a mistake in the image? I think, if they say &quot;looking along -z&quot;, the direction vector is as you just said `lookat - campos` ?" CreationDate="2016-06-02T06:14:16.007" UserId="116" />
  <row Id="3254" PostId="2549" Score="0" Text="@BRabbit27 Yes that what it means, I think there is no mistake in their image. That just happens to be configurable by a `-1` in the projection matrix later on. So any image would still be right." CreationDate="2016-06-02T06:16:24.417" UserId="1614" />
  <row Id="3255" PostId="2549" Score="0" Text="Ok, now I think I am getting somewhere. So if I choose to have the direction as `camPos-lookat` to agree with the image, that's why the projection matrix has the `-1` in the (2,3) entry to reverse this, right?" CreationDate="2016-06-02T06:18:28.343" UserId="116" />
  <row Id="3256" PostId="2549" Score="0" Text="The other thing I still don't get, is how can I convince myself that the view-matrix = camera-to-world? What kind of exercise can I do in order to visualize it?" CreationDate="2016-06-02T06:22:52.727" UserId="116" />
  <row Id="3257" PostId="2549" Score="1" Text="@BRabbit27 it's world-to-camera because &quot;the view matrix changes space from world points to view points&quot;. If a point is far away, but your camera is also far away (close to the point) passing the point into the view matrix will make it close to zero. that is: a point visible in your view space. Another way to see it, is use the fact &quot;view matrix = inverse of world matrix for the camera&quot;. The wold matrix for the camera would be the matrix to move it from zero in world, to its place in world (away from origin)." CreationDate="2016-06-02T06:28:26.767" UserId="1614" />
  <row Id="3258" PostId="2516" Score="1" Text="General graph layout algorithms are anoying to program and implement. Even just finding proper ones from literature is pain. You should describe a bit more what you expect, draw a picture." CreationDate="2016-06-02T07:57:35.683" UserId="38" />
  <row Id="3259" PostId="2551" Score="3" Text="&quot;fails to compile&quot; is kinda useless without the error messages, check the `shaderInfoLog` and/or the `programInfoLog` after attempting the compile and link." CreationDate="2016-06-02T08:42:53.983" UserId="137" />
  <row Id="3260" PostId="2516" Score="0" Text="@DanielMGessel It is an electricity network and I have the real world map(coordinates) in hand, The graph is planar and maybe the edges cross each other, so its not a problem if the orthogonal edges cross each other, because in some case I believe there is no way to draw the graph without overlapping edges." CreationDate="2016-06-02T09:01:18.467" UserId="537" />
  <row Id="3261" PostId="2516" Score="0" Text="@joojaa I have GIS data of an electricity network and just like any electricity computation software(cyme, digsilent, pti,...) I just need to draw an arc-node structure in an orthogonal manner. As I described above for Daniel. I have extracted the hypernodes and their connections so the ortho graph could be drawn from sketch using these data. I will attach some picture to the question to increase the clearity, thanks for suggesting." CreationDate="2016-06-02T09:10:01.800" UserId="537" />
  <row Id="3262" PostId="2552" Score="3" Text="More annoyingly, this fails on only some drivers/cards, which makes it hard to debug in the wild. As best practice, never rely on implicit casts to float in GLSL, always specify the decimal dot." CreationDate="2016-06-02T12:33:37.630" UserId="457" />
  <row Id="3263" PostId="1978" Score="1" Text="The model-matrix is indeed a model-to-world matrix. If you can call the camera-matrix or often view-matrix a &quot;camera-to-world&quot; or &quot;world-to-camera&quot; matrix depends on how you build it i think. I aways use a approach of &quot;world-to-camera&quot; in which the matrix is composed with the coordinate system and the _negative_ position of the camera. Then multiplying by this takes a world coordinate into camera/view space. http://www.opengl-tutorial.org/beginners-tutorials/tutorial-3-matrices/ explains this in a much more detailed fashion and in the way I use the terms and matrices." CreationDate="2016-06-02T15:55:16.567" UserId="273" />
  <row Id="3264" PostId="2542" Score="1" Text="@trichoplax Thanks! I did it, in order to better clarify that particular detail" CreationDate="2016-06-03T00:20:28.447" UserId="3410" />
  <row Id="3265" PostId="2533" Score="0" Text="I am going to take a look at QT, as that uses c++ which I know best, and seems like a good multi platform utility." CreationDate="2016-06-03T03:48:22.027" UserId="3459" />
  <row Id="3266" PostId="2549" Score="0" Text="To complement the answer, check [this video](https://www.youtube.com/watch?v=mpTl003EXCY&amp;list=PL_w_qWAQZtAZhtzPI5pkAtcUVgmzdAP8g&amp;index=5)" CreationDate="2016-06-03T04:45:42.690" UserId="116" />
  <row Id="3267" PostId="2556" Score="3" Text="This is actually a very interesting question. I really hope this isn't considered off topic here." CreationDate="2016-06-03T08:25:39.840" UserId="2479" />
  <row Id="3268" PostId="2516" Score="1" Text="The vertexes already have a given position in 2D (or on a sphere), so it's more of an edge routing problem? The trivial solution is to route each edge first along the x, then along the y axis (or the other way 'round). If this is right, explaining the problems with this trivial solution might be helpful. I imagine a minimization problem by assigning penalties for the problems..." CreationDate="2016-06-03T16:54:26.387" UserId="2500" />
  <row Id="3269" PostId="2559" Score="0" Text="Thanks for your answer! Yes you are right, and I know them: they were introduced with the Compute Shaders and indeed they solve part of the problem because they allow saving values from one shader pass to the next. However, I don't think they solve the main problems I described: the array-of-arrays or array-of-lists issue" CreationDate="2016-06-03T20:05:16.150" UserId="3410" />
  <row Id="3270" PostId="2562" Score="2" Text="The things that stand out to me about the model are its symmetrical features. Not all of these are mismatched on all people, but most people have *some* asymmetry. This model has ears positioned at the same height. The wrinkles in general are symmetrical, particularly the frown lines on the left and right of the bridge of the nose. In addition to these permanent features, the expression is also very symmetrical. None of these things are impossible on a human face, but they invite closer inspection, making it more difficult to miss other subtle clues that the face is artificial." CreationDate="2016-06-04T00:38:34.417" UserId="231" />
  <row Id="3271" PostId="2562" Score="2" Text="The reason why i would not want to go there is that i know that the fact that it IS artificial makes you more sceptical and start to see things that may or may not be critical. Yes i agree its a archetypical character. But again since its not a blind test i would refrain from few of the points." CreationDate="2016-06-04T06:00:12.370" UserId="38" />
  <row Id="3273" PostId="2516" Score="0" Text="@DanielMGessel I have added some pictures and explanation, hope it clear the ambiguities." CreationDate="2016-06-04T21:41:29.730" UserId="537" />
  <row Id="3274" PostId="2516" Score="0" Text="@joojaa Please take a look at the added picture.Thanks" CreationDate="2016-06-04T21:42:15.963" UserId="537" />
  <row Id="3275" PostId="2558" Score="0" Text="Thank you, Dragonseel! I actually suspected that all went down to collision detection. Do you think AABB is enough?" CreationDate="2016-06-05T15:47:59.580" UserId="3472" />
  <row Id="3276" PostId="2558" Score="1" Text="A General BB should be fine but the axis alignment of the boxes prohibits you from calculation correct rotation of the vehicles. For the easier just up-down movment you could maybe make them work. But I think for the more general solution you need rotating bounding volumes." CreationDate="2016-06-05T23:42:31.653" UserId="273" />
  <row Id="3277" PostId="2559" Score="0" Text="Doh you have the same number of floats for all pixel? I mean just in a single frame the number could change between frames if necessary. If you have you could unpack the 2D data structure into a 1D array by index shifting. This could then be written as a buffer. If the amount of data varies you could write some raw data Ald just bytes and then an additional index structure which pixel has what amount of data... That would be just one not per pixel overhead." CreationDate="2016-06-05T23:46:16.750" UserId="273" />
  <row Id="3278" PostId="2554" Score="0" Text="I can't test this until I get home so can't fix your main problem right now, but there's a fair bit of unnecessary maths in your shader :" CreationDate="2016-06-06T06:15:01.990" UserId="1937" />
  <row Id="3279" PostId="2554" Score="2" Text="the W divide of vertexPositionHomogenous is pointless as only the projection matrix alters W ; you're normalizing lightDirection twice, once in the declaration and once in the calculation of cosTheta ; and the halfway vector calculation is cheaper as (lightDirection + viewDirection) * 0.5 .... could you post the fragment shader you're using with this vertex shader as well? Because other than that the maths look right." CreationDate="2016-06-06T06:27:01.367" UserId="1937" />
  <row Id="3280" PostId="2519" Score="0" Text="Yes, that was the problem. But my main question was: What IS the half vector used for, since it appears in the function definition. As far as I understand now it is only used in the check if H dot V is positive. Thank you for taking time to write the answers." CreationDate="2016-06-06T08:00:54.563" UserId="3424" />
  <row Id="3284" PostId="2519" Score="0" Text="That is my understanding as well. Glad to help." CreationDate="2016-06-06T09:28:04.990" UserId="2479" />
  <row Id="3288" PostId="2556" Score="0" Text="An oft-overlooked feature is how well (or poorly) the scene data was mapped to the display. It's common for high-dynamic-range scene data to be arbitrarily &quot;squashed&quot; into an sRGB range without careful mapping. This can create a lot of VERY subtle problems that cue the brain to be skeptical. A great (generic) discussion was started here: http://blender.stackexchange.com/questions/46825/render-with-a-wider-dynamic-range-in-cycles-to-produce-photorealistic-looking-im" CreationDate="2016-06-06T19:59:42.570" UserId="4494" />
  <row Id="3289" PostId="3569" Score="0" Text="This looks like a homework problem. Is this a homework problem?" CreationDate="2016-06-06T20:00:49.890" UserId="4494" />
  <row Id="3290" PostId="3569" Score="2" Text="@mHurley Question from an exam paper" CreationDate="2016-06-06T20:20:10.177" UserId="1971" />
  <row Id="3291" PostId="3569" Score="0" Text="What exactly don't you understand? You have n and you have VUP. And you have the formula for u. Just plug in the values. The x stands for the cross product which you can lookup if that makes you trouble. And the | | means length. / should be clear as division." CreationDate="2016-06-07T00:29:30.770" UserId="273" />
  <row Id="3292" PostId="3569" Score="0" Text="Should this be on Math.stackexchange ?" CreationDate="2016-06-07T03:36:12.037" UserId="3437" />
  <row Id="3293" PostId="3572" Score="0" Text="Thanks, i thought i need a translation matrix but i didn't know that i messed the order of operation." CreationDate="2016-06-07T06:32:58.537" UserId="3437" />
  <row Id="3294" PostId="2558" Score="0" Text="The collision check has to be done with all objects in the scene, correct? Or at least, the ones within a certain distance.&#xA;I wonder if calculating the vector between the center of the car and the center of the obstacle and adding it to the car's speed would suffice." CreationDate="2016-06-07T08:33:52.040" UserId="3472" />
  <row Id="3295" PostId="3569" Score="2" Text="@ritwiksinha questions can be on topic on more than one site." CreationDate="2016-06-07T08:47:23.357" UserId="231" />
  <row Id="3296" PostId="2558" Score="1" Text="The collision would have to be checked against all nearby objects that you want to be able to colide with. In general using that distance would not work. Think of a long object. It's center of mass would be far away from the object and the force you add would be much too large. What is physically correct to use is the penetration depth aka how deep is the object inside the obstacle (using the maximum depth). But I cannot say that in your special case it is impossible to find some plausible simplification. I just don't know enough about your project and goals." CreationDate="2016-06-07T08:53:46.657" UserId="273" />
  <row Id="3297" PostId="2538" Score="0" Text="Could you explain what &quot;tool-path&quot; and &quot;scanSpacingStep&quot; mean in this context?" CreationDate="2016-06-07T09:03:19.353" UserId="231" />
  <row Id="3298" PostId="2538" Score="0" Text="The tool-path is basically the path that follows a series of coordinate points over a grid cell. And the scanSpacingStep is the distance between two adjacent lines. Check the link http://imgur.com/NTtPYoB The fractal path in the image could be a tool path that is defined by series of coordinates and the in-between distance of each and every fractal line segments is the scanSpacingStep." CreationDate="2016-06-07T09:09:47.980" UserId="2712" />
  <row Id="3300" PostId="3570" Score="0" Text="How would you sample distances for non-monochromatic scattering/absorption coefficients? Randomly choose a channel, then divide by 1/3 (in the case of RGB or XYZ)?" CreationDate="2016-06-07T17:25:55.173" UserId="310" />
  <row Id="3301" PostId="3570" Score="1" Text="@RichieSams The recommendation I've seen for that case is to assign each ray a single wavelength or color channel. So you basically calculate the scattering for each channel separately. For instance, in atmospheric scattering, blue light scatters much more strongly than red and therefore needs a lot more scattering events, and the blue photons will follow much more convoluted paths than the red ones. So it makes some sense to simulate them separately—much like dispersion due to refraction. I've never really tried this myself though." CreationDate="2016-06-07T17:46:09.757" UserId="48" />
  <row Id="3302" PostId="3570" Score="0" Text="Ahh, that makes sense. Though, performance will suffer... No wonder everyone wants to estimate Monte-Carlo Participating media. Thanks for all the info!" CreationDate="2016-06-07T17:51:53.400" UserId="310" />
  <row Id="3303" PostId="2568" Score="0" Text="As to your last comment - it's not a requirement of course for CG to look like a photo, but the goal of a lot of animation is to look as life-like as possible. Maybe I picked a less-than-perfect example, but I've always been interested in how real-time rendering could be made more realistic, especially in games." CreationDate="2016-06-07T18:29:52.393" UserId="3477" />
  <row Id="3304" PostId="2568" Score="0" Text="Of course, in this case I was talking about the information provided in the question, so I was discussing a still image. For extra realistic look, I don't know what software you're using, but if it's Blender, there's a new LUT that can be replaced (not official yet) which makes Cycles rendering very close to real life filming quality, concerning material's reaction to light, lighting is a major aspect, specially when discussing photo real." CreationDate="2016-06-07T18:36:48.823" UserId="3487" />
  <row Id="3305" PostId="2568" Score="0" Text="So do you think it's often the case that while something is almost realistic, the CG is &quot;designed&quot; (if you will) with the idea that it shouldn't be picture-perfect?" CreationDate="2016-06-07T21:09:22.370" UserId="3477" />
  <row Id="3306" PostId="2568" Score="0" Text="I wouldn't use &quot;Should not&quot; or &quot;must not&quot; with anything related to an artistic matter, there is no should nor must in art, everything is subjective, what I meant is that (as I believe) a glance of non realism, even in realistic artworks, keeps a small window open to the personality of the artist, compared to stylized artworks which for this example would be no roof at all. It all depends on were you like to stand." CreationDate="2016-06-07T21:43:00.577" UserId="3487" />
  <row Id="3307" PostId="3575" Score="0" Text="If you can do something in one API, you can do it any other API, just the way of doing it will depend on the API." CreationDate="2016-06-07T21:46:02.013" UserId="3437" />
  <row Id="3309" PostId="3577" Score="0" Text="I would bet that if OSX would ever support Vulkan it will support only a small subset of it, and that would also become the Next Generation graphics api for web browsers, OpenGL is still ways simpler to use (to a certain degree of course) than Vulkan, what vulkan gain in simplicity over rendering pipeline it lose it in explicit handling of much other stuff" CreationDate="2016-06-08T08:04:14.783" UserId="4503" />
  <row Id="3310" PostId="3577" Score="1" Text="@DarioOO OpenGL immediate mode is way simpler to use than whatever-you-call-the-thing-that's-not-immediate-mode, yet it's not recommended." CreationDate="2016-06-08T08:56:08.637" UserId="2316" />
  <row Id="3312" PostId="3580" Score="0" Text="Excellent question! I've been wondering about this myself. It's not that the checkerboard pattern has a yellow tint... it's the grays that are incorrect. When I compare what I see on my laptop to my phone, where the checkerboard pattern seems to have roughly the same color as the grays, the gradient on my laptop appears distinctly more blue. Perhaps a better question would be to ask why do shades of gray look blue on most LCD computer screens." CreationDate="2016-06-08T14:11:16.993" UserId="3470" />
  <row Id="3313" PostId="3580" Score="0" Text="@Quinchilion Interesting perspective. I'm so used to how gray looks on my screen that I might not notice if there's some blue to it, though it looks really metallic to me. It might be relevant that blue is the complementary color of yellow." CreationDate="2016-06-08T14:28:41.600" UserId="4490" />
  <row Id="3314" PostId="3576" Score="2" Text="Good answer, reminded me of this: http://www.elise.com/quotes/heinlein_-_specialization_is_for_insects" CreationDate="2016-06-08T16:52:32.307" UserId="3473" />
  <row Id="3316" PostId="3586" Score="2" Text="I'm not aware of any distinction between &quot;GPU instancing&quot; and &quot;standard instancing&quot;. As far as I'm aware, there's only one kind of instancing and it is a GPU feature. Whatever this is, it's probably something specific to Unity (and perhaps a term made up by Unity marketing), and you might have to track down a Unity engineer to get an explanation of what the new feature really is." CreationDate="2016-06-09T07:50:32.520" UserId="48" />
  <row Id="3317" PostId="3586" Score="0" Text="By 'standard instancing', could they be referring to just having one copy of the mesh in system memory? That sounds rather elementary to any game engine. It also seems odd that they didn't have GPU instancing until now." CreationDate="2016-06-09T07:55:16.000" UserId="457" />
  <row Id="3318" PostId="3581" Score="0" Text="Vulkan is stable right now. Your GL 1.0 *vs* 2.0 analogy is apt. Starting with GL now would be like starting learning GL 1.0 six months after GL 2.0 comes out." CreationDate="2016-06-09T08:15:01.433" UserId="2041" />
  <row Id="3319" PostId="3585" Score="1" Text="re `new`, it looks more like c#, not c++, though OP has not explicitly specified." CreationDate="2016-06-09T08:17:30.277" UserId="457" />
  <row Id="3320" PostId="3585" Score="0" Text="Actually looks more like Java." CreationDate="2016-06-09T08:27:45.267" UserId="457" />
  <row Id="3321" PostId="3584" Score="0" Text="Are you writing a rasterizer in Java?" CreationDate="2016-06-09T08:48:18.887" UserId="3331" />
  <row Id="3322" PostId="3585" Score="0" Text="@Rotem I don't think there is any way of knowing the language except when i tell that it is Java." CreationDate="2016-06-09T09:06:21.463" UserId="3437" />
  <row Id="3323" PostId="3584" Score="0" Text="@Syntac_ Yes the language is Java." CreationDate="2016-06-09T09:06:54.213" UserId="3437" />
  <row Id="3324" PostId="3585" Score="0" Text="@ritwik In c# the `Math` methods are capitalized, e.g. `Round()`, not `round()`." CreationDate="2016-06-09T09:08:49.103" UserId="457" />
  <row Id="3325" PostId="3585" Score="0" Text="@Rotem ok that's legit." CreationDate="2016-06-09T09:09:53.553" UserId="3437" />
  <row Id="3326" PostId="3585" Score="0" Text="@Nathan reed i used the good old school maths first but not the way that you suggested. Thanks i will try your method." CreationDate="2016-06-09T09:12:20.440" UserId="3437" />
  <row Id="3327" PostId="3580" Score="1" Text="Monitors are frequently very poorly calibrated. That's the most likely cause of issues with images not looking grey.&#xA;&#xA;Further, assuming the display is meant to be sRGB, then (and this is from memory so please take with a grain of salt) the RGB values that should correspond to a B&amp;W chequer board are about 186.&#xA;&#xA;What you may find helpful is to repeat your experiment separately with each of R,G &amp; B using a Primary/Black pattern VS Black-&gt;Primary blend. The crossover points are ideally meant to be at the same position but frequently aren't." CreationDate="2016-06-09T10:34:52.397" UserId="209" />
  <row Id="3328" PostId="3586" Score="0" Text="@Rotem Those were exactly my thoughts and the origin of my question. See here in their roadmap what they say about it: https://unity3d.com/pt/unity/beta/unity5.4.0b1 And yes, the only thing I can think of is that their &quot;non-GPU&quot; instancing is a copy of mesh in RAM, which **is** quite a ppor way of doing that. As per the link I sent, it seems that now they are firstly introducing the possibility of instancing using shaders, i.e. directly in the GPU" CreationDate="2016-06-09T12:22:30.517" UserId="2061" />
  <row Id="3329" PostId="2567" Score="1" Text="What's an orthographic cuboid? &quot;Orthographic&quot; is a property of the projection, it's not a shape. Perhaps you could explain what effect you're hoping this projection will have." CreationDate="2016-06-09T13:04:08.070" UserId="2041" />
  <row Id="3330" PostId="3588" Score="0" Text="If you coded the calculation wouldn't it be easier to query the data before it gets turned into colours?" CreationDate="2016-06-09T13:14:31.240" UserId="273" />
  <row Id="3331" PostId="3576" Score="5" Text="This C++ analogy is inappropriate.  Vulkan is a new next generation API developed by the creators of OpenGL.  C++ is an established, mostly backwards-compatible competitor to C." CreationDate="2016-06-09T13:33:16.703" UserId="4531" />
  <row Id="3332" PostId="3588" Score="0" Text="I did not code it it is not an open source component I can not acces to surce code of 3d sun exposure component. But I want to convert result to data. Is it possible? Are there any component for this purpose? Or Do I have to code myself all workflows of analysis to achieve this?" CreationDate="2016-06-09T13:36:26.720" UserId="4513" />
  <row Id="3333" PostId="3576" Score="0" Text="Those specifics are irrelevant to the point of the analogy." CreationDate="2016-06-09T13:59:02.013" UserId="4494" />
  <row Id="3334" PostId="3583" Score="0" Text="&quot;*Another problem I have is with Vulkan's claims is how it claims to be better.*&quot; Vulkan does not claim to be anything. It's just an API; it cannot make claims." CreationDate="2016-06-09T15:18:32.767" UserId="2654" />
  <row Id="3335" PostId="3587" Score="0" Text="Please include a reference image with your question. Googling 'logic studio environment editor' for images gives me nothing useful." CreationDate="2016-06-09T15:20:50.717" UserId="457" />
  <row Id="3336" PostId="3581" Score="0" Text="Vulkan might be &quot;stable&quot; it doesn't mean things wont change, this is the nature of languages, which I presented 2 recent changes to languages within the last few years, so how do we know this wouldn't happen to Vulkan? You're basically saying that learning OpenGL is a waste, and that's false.  Making it seem like OpenGL is dead, and wont be used any more... Also false.  Besides, I'm not the only one who believes that Vulkan could have stability issues.  It might not, but with any new language, that's what you look for." CreationDate="2016-06-09T16:07:02.913" UserId="3420" />
  <row Id="3337" PostId="3581" Score="1" Text="Of course things will change, and the working group is already working towards new features for a next spec version (ssh, you didn't hear that from me). *Stable* means that those things will add to the spec, and the things you learn about it today will still be valid two years from now. OTOH, the things you learn about GL today are already a poor match for how GPUs work: just like learning about fixed-function pipelines in a programmable-shader world. If you read my answer, you see I don't think learning GL now is a waste. But &quot;Vulkan is too new&quot; is not a good reason to discount it." CreationDate="2016-06-09T16:13:32.113" UserId="2041" />
  <row Id="3338" PostId="3581" Score="0" Text="Libraries change with addons, but I'm talking about architecture changes, which is what happened in FX, Play!, OpenGL, and others.  How can we say in 2 years it will be the same?   The Fixed-Pipeline comment is what the issue with GL 1-2 was, so yes, why learn it if soon it could change, and we wouldn't even know?   Vulkan is too new isn't the full reason.  It's that it's subject to change, not many companies will adapt to using it right away, and how many people really want to change from OpenGL to Vulkan who are hardcore GL coders?  I think you should look at Vulkan though." CreationDate="2016-06-09T16:18:43.730" UserId="3420" />
  <row Id="3339" PostId="3590" Score="1" Text="Yup same exact result, i even went as far as calibrate the one not calibrated showing hue and poof gone." CreationDate="2016-06-09T16:30:50.033" UserId="38" />
  <row Id="3341" PostId="3581" Score="1" Text="@Lasagna: &quot;*It's that it's subject to change, not many companies will adapt to using it right away*&quot; Valve. Epic. Unity. All of them are heavily invested in making their engines work on Vulkan. CryTech and Id aren't exactly ignoring it either. So your statement is not commensurate with the facts." CreationDate="2016-06-09T17:37:22.600" UserId="2654" />
  <row Id="3342" PostId="3577" Score="1" Text="@Gavin: It should be noted that OpenGL versions 4.2 or greater aren't supported on OSX either. So if you want to use anything *recent* from OpenGL on OSX, you can't. And Apple is highly unlikely to adopt later OpenGL versions. The Apple platform is all-in on Metal, so cross-platform is a bust either way." CreationDate="2016-06-09T17:40:28.680" UserId="2654" />
  <row Id="3343" PostId="3576" Score="2" Text="Claiming specialization is inefficient or unmarketable is incredibly naive. A translator that knows all of five words in every single spoken language is useless, people will hire translators that mastered (a.k.a specialized in) a small number of languages. Now *over*-specializing *is* problematic. A translator that completely mastered one language and only knows that language is also not a useful translator. And devs (indie or otherwise) need to be careful not to spend all their time learning new tools. Eventually they need to actually make something to sell, lest they find themselves bankrupt" CreationDate="2016-06-09T18:32:26.773" UserId="4535" />
  <row Id="3344" PostId="3576" Score="0" Text="Just to be clear, I don't necessarily disagree with you in regards to OpenGL and Vulkan, This quite possibly is a case where learning both instead of just Vulkan is the better choice." CreationDate="2016-06-09T18:34:30.713" UserId="4535" />
  <row Id="3345" PostId="3576" Score="0" Text="I'm pretty sure that's what I implied by using the examples I did ;-) maybe not..." CreationDate="2016-06-09T18:38:41.940" UserId="4494" />
  <row Id="3346" PostId="3581" Score="0" Text="Just because it's being adapted into 3D Engines, doesn't mean that companies are investing their projects into it, especially projects that are mid-way.  You might get a brand new project in Vulkan, but again, are you willing to hire new employees, train new ones, and be the one to test out a new platform?  I've done testing on new platforms, and it's a PITA sometimes working around bugs, reporting bugs, etc, when you're trying to just put out a product." CreationDate="2016-06-09T19:38:40.367" UserId="3420" />
  <row Id="3347" PostId="3586" Score="1" Text="If the mesh moves and needs to be drawn in many places then one could instantiate it by reusing the draw calls and not redo mesh deformations. Used to be done in ages past ages." CreationDate="2016-06-09T20:23:37.800" UserId="38" />
  <row Id="3348" PostId="3581" Score="1" Text="@Lasagna: &quot;*Just because it's being adapted into 3D Engines, doesn't mean that companies are investing their projects into it*&quot; ... So 3D engines don't qualify as &quot;projects&quot;? Does DOTA2 count as a &quot;project&quot;? Because it's got support for Vulkan *right now*. Does Samsung's line of smartphones and tablets count as a &quot;project&quot;? Because they're looking at incorporating it into the UI. The reality is that *lots* of people are willing to &quot;be the one to test out a new platform&quot;, if that platform gets them what they need." CreationDate="2016-06-09T22:30:15.220" UserId="2654" />
  <row Id="3349" PostId="3587" Score="0" Text="https://macprovid.vo.llnwd.net/o43/hub/media/1074/8250/Arp_Env_Pic.png" CreationDate="2016-06-10T02:13:57.483" UserId="4528" />
  <row Id="3350" PostId="3587" Score="0" Text="http://media.soundonsound.com/sos/jan03/images/staudiomedia712.l.gif" CreationDate="2016-06-10T02:16:06.510" UserId="4528" />
  <row Id="3351" PostId="3587" Score="0" Text="http://www.pcrecording.com/extlinkexp.jpg" CreationDate="2016-06-10T02:16:39.340" UserId="4528" />
  <row Id="3352" PostId="3587" Score="0" Text="http://musicplayers.com/reviews/recording/2015/images/Eventide_patchbay.jpg" CreationDate="2016-06-10T02:16:58.610" UserId="4528" />
  <row Id="3353" PostId="3587" Score="0" Text="http://i40.tinypic.com/2dhwqp1.jpg" CreationDate="2016-06-10T02:17:44.490" UserId="4528" />
  <row Id="3355" PostId="3579" Score="1" Text="I somewhat disagree. Vulkan (and DX12)has proven to be very difficult to implement, for *experienced* devs. With all the power that Vulkan gives you, it's very very easy to shoot yourself in the foot. In a addition, Vulkan requires a lot more boilerplate code. For someone just learning about GPU programming, I tend to think that Vulkan will be very overwhelming." CreationDate="2016-06-10T12:09:34.540" UserId="310" />
  <row Id="3356" PostId="3593" Score="0" Text="Is there any reason why you use the old style openGL" CreationDate="2016-06-10T14:06:58.027" UserId="38" />
  <row Id="3357" PostId="3595" Score="0" Text="Looks like simple trigonometry to me, if you consider that the two triangles are mirror images of each other. The triangle (v1, v2, middle) is isosceles and you know both the length of v1-v2 and the angle defined by (v1, middle, v2), so you should be able to calculate both v1-middle and alpha." CreationDate="2016-06-10T14:45:32.137" UserId="3470" />
  <row Id="3358" PostId="3595" Score="0" Text="I don't know where middle is, though. I only have the vertices of the cube and a degree by which I want it bend." CreationDate="2016-06-10T15:00:36.320" UserId="4543" />
  <row Id="3359" PostId="3588" Score="0" Text="I have closed the [older question](http://computergraphics.stackexchange.com/questions/3582/reporting-results-of-3d-sun-exposure-analysis) as a duplicate of this one, since this one is more specific about what is required." CreationDate="2016-06-10T15:17:06.007" UserId="231" />
  <row Id="3360" PostId="3595" Score="0" Text="If you know where v1 and v2 are, you can calculate where 'middle' is from the requested angle. If you don't, can you clarify the problem? I see no cube in the picture." CreationDate="2016-06-10T15:45:36.743" UserId="3470" />
  <row Id="3361" PostId="3595" Score="0" Text="http://prntscr.com/ber3a7 that is input and result (with a 45° angle). Those are Screenshots from Cinema4D's bending tool. As you can see the 45° is the angle of the topmost edge of the cube. The bending process strectches the sides but not the 'spine' of the object. I'm trying to emulate the tool in code with a given Mesh." CreationDate="2016-06-10T15:55:16.443" UserId="4543" />
  <row Id="3362" PostId="3579" Score="1" Text="@RichieSams I don't think you meant &quot;implement&quot;. Only the GPU vendor has to implement Vulkan, and it's a lot easier than implementing OpenGL. (Believe me, I've done it!) But assuming you meant it's difficult to integrate with or program against, I've added a paragraph with some information I learned from the Vulkan WG." CreationDate="2016-06-10T16:01:46.880" UserId="2041" />
  <row Id="3363" PostId="3579" Score="0" Text="Correct. Implement is perhaps a poor word choice. I meant 'to use' in your program. But I like your edits. Well put" CreationDate="2016-06-10T16:06:08.887" UserId="310" />
  <row Id="3364" PostId="3589" Score="0" Text="I'm glad you posted this, because it makes very clear some ideas that I was hesitating to express in my answer: that learning the concepts is most important. I think where we differ is that you encourage GL because it's easier to get started, while I think that ease of getting started brings a risk of doing things in legacy ways. It's quite hard to know in GL what the &quot;modern GL idiom&quot; is, compared with programming in a legacy style. If nobody tells you to use VAOs instead of a separate draw call per primitive, it's a lot harder to unlearn that mistake later." CreationDate="2016-06-10T16:08:50.723" UserId="2041" />
  <row Id="3365" PostId="3589" Score="1" Text="@DanHulme: It's all a matter of using the right learning materials. There are plenty of such materials online that use modern idioms. Nobody should ever try learning graphics just by reading a reference manual or specification." CreationDate="2016-06-10T16:28:17.220" UserId="2654" />
  <row Id="3366" PostId="3597" Score="0" Text="how can I entegrate it to my software. I am thinking to use unity3d as a developing platform. Can I entegrate Nvidia Optix to it?" CreationDate="2016-06-10T17:16:25.420" UserId="4513" />
  <row Id="3367" PostId="3597" Score="0" Text="Depends. OptiX has a C/C++ API so if you use Unity with Cpp you should be fine. Just install CUDA and link against the OptiX library. The tricky part will be to get the Nvcc to do its thing. I use CMake for that but I don't know about Unity + CMake. Then just ist unity to visualize the output of the OptiX simulation." CreationDate="2016-06-10T17:20:06.220" UserId="273" />
  <row Id="3368" PostId="2006" Score="1" Text="When I first read about this (15-16 years ago), I wanted to try applying it to Wolfenstein 3D to see if it was a major improvement over ray casting, especially at higher resolutions and/or when using OpenGL rendering. Just using BSP trees as the SNES and Macintosh Wolf 3D and DOOM did are probably a win regardless, but I was hung up on wanting to learn this potential best method before actually implementing anything." CreationDate="2016-06-10T21:53:40.930" UserId="2608" />
  <row Id="3369" PostId="2006" Score="1" Text="I think that any and all (BSP-based, specifically, unless it's just that awesome) algorithms that meet the criteria of being suitable for solving visibility and rendering in Wolfenstein 3D / DOOM-style games, including their implementations being practical on those games' target systems, would be sufficient in answering the question, and not necessarily any algorithms that were invented or known by John Carmack." CreationDate="2016-06-10T22:03:16.923" UserId="2608" />
  <row Id="3370" PostId="3580" Score="0" Text="@SimonF 186 corresponds roughly to the middle of the bottom gradient, and that place also seems to match the intensity of the top rectangle." CreationDate="2016-06-10T23:30:52.850" UserId="4490" />
  <row Id="3371" PostId="3583" Score="0" Text="You do realise that Vulkan and GL are made by mostly the same people?" CreationDate="2016-06-11T00:03:25.313" UserId="2041" />
  <row Id="3372" PostId="3583" Score="0" Text="Yes I do. I still prefer GL" CreationDate="2016-06-11T00:04:18.247" UserId="4522" />
  <row Id="3373" PostId="3589" Score="0" Text="You make it sound really easy! Even so, I still see programmers getting started in graphics - some of them very competent in other fields - and using legacy features and learning nothing about the performance characteristics. It's really hard to get that wrong in Vulkan, because it makes expensive things *look* expensive. Anyway, we don't need to argue. I agree with your main point: learning the concepts is most important, and you can do that without Vulkan or GL." CreationDate="2016-06-11T00:08:47.103" UserId="2041" />
  <row Id="3374" PostId="3589" Score="0" Text="@DanHulme: &quot;*It's really hard to get that wrong in Vulkan*&quot; Because you're too busy getting *everything else* wrong ;) Also, it's still pretty easy to get performance wrong in Vulkan. Unnecessary synchonization. Using only the &quot;general&quot; image layout. Not taking advantage of subpasses. Frequent pipeline changes. And so forth. Not to mention, different vendors don't even agree on how best to use Vulkan." CreationDate="2016-06-11T00:23:03.147" UserId="2654" />
  <row Id="3375" PostId="3589" Score="2" Text="@DanHulme: As for how easy it is to use modern OpenGL, I [do my best](https://www.opengl.org/wiki/Main_Page) to [make it easy](http://alfonse.bitbucket.org/oldtut/). If people insist on learning from garbage sites like NeHe, or by reading random documentation, that's not something anyone can help. You can lead horses to water, but you can't make them drink." CreationDate="2016-06-11T00:25:05.303" UserId="2654" />
  <row Id="3376" PostId="2347" Score="1" Text="Right is not defined, if it works it works" CreationDate="2016-06-11T10:48:57.437" UserId="38" />
  <row Id="3377" PostId="3593" Score="0" Text="It is a computer graphics project in OpenGL and SDL." CreationDate="2016-06-11T14:58:30.990" UserId="3472" />
  <row Id="3378" PostId="3599" Score="0" Text="Thank you! I might as well try to reimplement some basic reader - but in the sample code that was given as example the calls didn't mess up things like this. I'm going to look at it again!" CreationDate="2016-06-11T15:00:05.277" UserId="3472" />
  <row Id="3379" PostId="3593" Score="0" Text="that's not a reason its a circular definition." CreationDate="2016-06-11T15:07:44.933" UserId="38" />
  <row Id="3380" PostId="3599" Score="0" Text="I see you're using the SDL surface's pixel buffer directly. You could get the debug version of the SDL library and the source, and inspect what's happening inside SDL. The core of the problem is that OpenGL expects the pixels to be laid out in memory differently than what is actually in the buffer." CreationDate="2016-06-11T16:29:33.877" UserId="2817" />
  <row Id="3381" PostId="3602" Score="1" Text="Could you also explain how this differs from standard instancing?" CreationDate="2016-06-11T16:55:58.717" UserId="231" />
  <row Id="3382" PostId="3593" Score="0" Text="Yeah, you're right, I didn't explain, sorry. I'm doing this for a computer graphics class which mainly focuses on OpenGL, SDL and many basic concepts of cg." CreationDate="2016-06-12T20:19:48.513" UserId="3472" />
  <row Id="3383" PostId="3593" Score="1" Text="Yes but why are you using obsolete and deprecated openGL calls?" CreationDate="2016-06-12T21:57:10.490" UserId="38" />
  <row Id="3384" PostId="3606" Score="0" Text="Alright, thank you for the hint to gl_vertextID. That's what I looked for. Regarding the number of blendshapes: I am writing a renderer for facial animation research and in this context the possibility to use more than 10 shapes should at least exist. Another possibility I just stumbled upon is the use of transform feedback to process blocks of shapes iteratively. Do you have a take on that? Again, thank you!" CreationDate="2016-06-13T09:52:01.777" UserId="4556" />
  <row Id="3385" PostId="3608" Score="3" Text="[Same question on Math.SE.](http://math.stackexchange.com/q/184121/50421)" CreationDate="2016-06-14T07:00:06.440" UserId="16" />
  <row Id="3386" PostId="3593" Score="0" Text="Because the class taught this &quot;dialect&quot;, mainly... Are there better ways to use OpenGL?" CreationDate="2016-06-14T07:51:27.473" UserId="3472" />
  <row Id="3387" PostId="3593" Score="1" Text="Yes  that's what [deprecated means](http://stackoverflow.com/questions/8111774/deprecated-meaning), any new code should **not** use this method if possible (here its possible). Read its not very performant [why-do-vertex-buffer-objects-improve-performance](http://computergraphics.stackexchange.com/questions/32/why-do-vertex-buffer-objects-improve-performance). Nobody should be teaching you this stuff anymore." CreationDate="2016-06-14T08:03:27.687" UserId="38" />
  <row Id="3388" PostId="3593" Score="0" Text="also read http://stackoverflow.com/questions/14300569/opengl-glbegin-glend" CreationDate="2016-06-14T08:13:06.377" UserId="38" />
  <row Id="3389" PostId="3612" Score="7" Text="Dividing normals by w is not useless, because it affects the interpolation. The values are weighed differently. Even if you normalize after the interpolation, it gives different results than if you didn't divide by w. Perspective correct interpolation for normals is the same as for any other surface attribute." CreationDate="2016-06-14T17:48:19.440" UserId="3470" />
  <row Id="3390" PostId="3613" Score="0" Text="Thanks, works great! I'm still not quite understand the purpose of the float. Is it possible to have the object's speed static?" CreationDate="2016-06-14T19:13:02.223" UserDisplayName="user4570" />
  <row Id="3391" PostId="3613" Score="0" Text="yes, the part (X.x - Y.x) is the first component of the direction vector. If you normalize it, you could multiply it with your speed and delta time. Regarding a former question that you posted on stackexchange, you should gather more information on linear algebra. See [this](http://blog.wolfire.com/2009/07/linear-algebra-for-game-developers-part-1/) for example." CreationDate="2016-06-14T20:01:49.413" UserId="4558" />
  <row Id="3392" PostId="3614" Score="1" Text="Very detailed response, thanks!" CreationDate="2016-06-14T20:50:48.673" UserDisplayName="user4570" />
  <row Id="3393" PostId="3615" Score="0" Text="Most 3d apps can do this." CreationDate="2016-06-15T09:26:46.287" UserId="38" />
  <row Id="3395" PostId="3620" Score="0" Text="Yeah i was just looking for that answer" CreationDate="2016-06-15T11:00:46.270" UserId="38" />
  <row Id="3396" PostId="3617" Score="0" Text="Normals, gotcha." CreationDate="2016-06-15T12:33:07.117" UserId="361" />
  <row Id="3397" PostId="3615" Score="0" Text="Could you add some details?If you could explain the general process in Blender (or any other app) that would be great." CreationDate="2016-06-15T13:28:24.457" UserId="4574" />
  <row Id="3398" PostId="3620" Score="1" Text="Relevant passage from the Black Book: http://www.jagregory.com/abrash-black-book/#how-do-you-fit-polygons-together" CreationDate="2016-06-15T15:36:39.593" UserId="4582" />
  <row Id="3399" PostId="3620" Score="0" Text="Very many thanks! It sorta kinda makes sense now, and that black book looks like a great resource too" CreationDate="2016-06-15T16:10:58.480" UserId="4577" />
  <row Id="3401" PostId="3627" Score="0" Text="This is going to delve into the core (proprietary) design of the gpu. Also the ALU isn't the magic component (it only does integral math) but the FPU is." CreationDate="2016-06-15T20:39:33.493" UserId="137" />
  <row Id="3402" PostId="3627" Score="1" Text="Im trying to build a simple gpu on an fpga that only joins multiple points in 3d space and diplays it on a screen. Im using fixed point operations only." CreationDate="2016-06-15T20:48:07.153" UserId="4583" />
  <row Id="3404" PostId="416" Score="0" Text="I'm afraid resource requests are off topic. Ironically I only spotted this was off topic because of the recent edit... This could potentially be edited to ask specifically for details of the benefits of this data format, but after the time since posting and the existing answers, such a significant change is probably better posted as a brand new question. Feel free to pop into [chat] if you have any queries (or anyway...)." CreationDate="2016-06-15T22:02:58.727" UserId="231" />
  <row Id="3405" PostId="3631" Score="0" Text="Will this work with perspective projection ?" CreationDate="2016-06-16T06:09:18.900" UserId="3437" />
  <row Id="3406" PostId="3615" Score="1" Text="@MichaelLitvin I think it would be off-topic if you ask &quot;How to do in a specific software&quot; on this site." CreationDate="2016-06-16T06:10:23.703" UserId="3437" />
  <row Id="3407" PostId="3631" Score="2" Text="This is a solution for a single image or frame, not the while video. To use this, the texture maps for many frames will need to be stitched somehow... This is the main problem here." CreationDate="2016-06-16T08:00:35.683" UserId="4574" />
  <row Id="3408" PostId="3615" Score="0" Text="I'm not asking about just Blender, though a suggestion for how to do this in Blender would be nice." CreationDate="2016-06-16T08:01:23.600" UserId="4574" />
  <row Id="3409" PostId="416" Score="1" Text="Fair enough. I was looking for a mininal working example anyways and solved that via [this thread](http://computergraphics.stackexchange.com/q/1934/361)." CreationDate="2016-06-16T09:53:49.033" UserId="361" />
  <row Id="3411" PostId="2346" Score="1" Text="Can you tell how you are calling the draw function ? and what functionality does Vertex class perform. I remember i once experienced the same problem when i was overwriting two VBOs. Maybe you are doing the same, I can't tell without seeing how you are drawing" CreationDate="2016-06-16T12:22:41.590" UserId="3437" />
  <row Id="3412" PostId="3631" Score="1" Text="unknown @MichaelLitwin they dont need to be stitched, just masked. Its pretty easy all you do is you calculateuseful  how much the pixel area is facing the camera tha when the facing drops under a certain treshold mask it out. Then median filter the overlap to remove noise. Then either bake to 2D or just use the projection as is. Any projection works just as long as you can track the projector location in 3D" CreationDate="2016-06-16T13:08:57.653" UserId="38" />
  <row Id="3413" PostId="405" Score="0" Text="What about using GeoGebra for graphs, 2D/3D geometric figures." CreationDate="2016-06-16T21:58:13.753" UserId="3437" />
  <row Id="3414" PostId="3599" Score="0" Text="I &quot;surprisingly&quot; solved by letting the code load a jpg rather than a png. I guess SDL likes MPEG codecs more than PNG." CreationDate="2016-06-17T11:55:14.220" UserId="3472" />
  <row Id="3416" PostId="2558" Score="0" Text="I actually solved by creating a list of points, checking continually the vehicle's position and direction with respect to the list and updating its speed and angles to match the expected result." CreationDate="2016-06-17T11:57:02.197" UserId="3472" />
  <row Id="3417" PostId="3638" Score="0" Text="`OES_element_index_uint` extension is available but it's not rendering the buffers. Strange." CreationDate="2016-06-17T13:00:36.523" UserId="361" />
  <row Id="3418" PostId="3631" Score="0" Text="I don't understand - I'm using several images for texture. These images will meet somewhere, and edge lines won't necessarily meet at exactly the same place. How do I ensure they do?" CreationDate="2016-06-17T13:08:54.047" UserId="4574" />
  <row Id="3419" PostId="3638" Score="0" Text="have you changed the index buffer to store ints?" CreationDate="2016-06-17T13:10:03.973" UserId="137" />
  <row Id="3420" PostId="3640" Score="0" Text="Hi and thank you :) sorry, it is a typo: the cycle is correct (otherwise i would be out of scope). I'm adding the necessary braces." CreationDate="2016-06-17T13:29:43.373" UserId="3472" />
  <row Id="3427" PostId="3645" Score="7" Text="17 milliseconds sounds more like the synchronisation to screen refresh rate than a actual rendering speed since that would be 1/60 (or well 16.666ms) of a second. So most likely your render is faster than that is just that displaying is restricted to one cycle. You can disable screen sync or render to a offscreen buffer for a more representative rendertime." CreationDate="2016-06-19T07:57:28.473" UserId="38" />
  <row Id="3429" PostId="3647" Score="0" Text="Nice information, thanks Julien. One more question: How could I use the depth or normal information in my fragment shader? I'm still quite new with GLSL and therefore have no idea how to include the values in the Sobel algorithm." CreationDate="2016-06-19T14:25:37.927" UserId="3377" />
  <row Id="3430" PostId="3646" Score="0" Text="I wonder if you might get the results you want by utilizing OpenGL's [edge flag](https://www.opengl.org/sdk/docs/man2/xhtml/glEdgeFlagPointer.xml) in some way. It looks like only edges between vertices with the edge flag on are drawn when in line mode. There's [a description here](http://www.songho.ca/opengl/gl_vertexarray.html). (I've not used it myself or I'd post an example.)" CreationDate="2016-06-19T21:44:11.327" UserId="3003" />
  <row Id="3431" PostId="3645" Score="2" Text="sounds indeed like a vsync, turn it off and try again or render it several times to see when you get any slowdown." CreationDate="2016-06-19T22:48:34.840" UserId="137" />
  <row Id="3432" PostId="3639" Score="0" Text="What's your `GL_TEXTURE_MATRIX` set to? Perhaps it has some scaling in it? (Also, what's the matrix mode for this code? You push the current matrix, but don't appear to be setting the mode before doing so.)" CreationDate="2016-06-19T23:54:04.283" UserId="3003" />
  <row Id="3433" PostId="3647" Score="2" Text="When you create the framebuffer for your rendering pass, you can attach more than one texture to it. If you attach a depth texture, it will be used automatically for depth. It you attach another color texture you can write into it separately (see https://www.opengl.org/sdk/docs/man/html/glDrawBuffers.xhtml), a feature called &quot;Multi Render Target&quot; (MRT)." CreationDate="2016-06-20T00:12:16.130" UserId="182" />
  <row Id="3434" PostId="3648" Score="0" Text="What was the original wording of what you read?" CreationDate="2016-06-20T01:27:20.107" UserId="231" />
  <row Id="3435" PostId="3599" Score="1" Text="PNG supports alpha, JPEG doesn't. Have you tried uploading your PNG texture as GL_RGBA, not GL_RGB? Because you haven't solved your problem, you just sidestepped it." CreationDate="2016-06-20T07:26:51.490" UserId="2817" />
  <row Id="3436" PostId="3638" Score="0" Text="Yeah, forgot about that. Now it works. Thanks." CreationDate="2016-06-20T14:14:09.760" UserId="361" />
  <row Id="3437" PostId="3650" Score="0" Text="Tried that and didn't work. I think it's not the case of Direct3D geometry, because I'm on Ubuntu - isn't Direct3D a Microsoft exclusive?" CreationDate="2016-06-20T19:38:34.540" UserId="3472" />
  <row Id="3438" PostId="3639" Score="0" Text="Before the bind code:&#xA;`glMatrixMode(GL_PROJECTION ); glLoadIdentity(); gluPerspective(70, 3.0/4, 0.2, 1000 ); glMatrixMode(GL_MODELVIEW); glLoadIdentity();`&#xA;No scaling of sort (actually there was, but when you mentioned it I removed it and it didn't change the result by much)." CreationDate="2016-06-20T19:44:46.640" UserId="3472" />
  <row Id="3439" PostId="3631" Score="1" Text="@MichaelLitvin you dont need to you can simply let them overlap." CreationDate="2016-06-20T20:02:12.593" UserId="38" />
  <row Id="3440" PostId="3655" Score="0" Text="How bilateral filtering specifically works is what I am trying to get information about. Can you elaborate on that?" CreationDate="2016-06-20T21:01:51.810" UserId="56" />
  <row Id="3441" PostId="3655" Score="0" Text="One can lend images for the explanations in the beginning from [here](http://computergraphics.stackexchange.com/questions/1862/how-does-a-computer-upscale-1024x768-resolution-to-1920x1080/1870#1870)" CreationDate="2016-06-20T21:10:43.230" UserId="38" />
  <row Id="3442" PostId="3647" Score="0" Text="Awesome explanation, thank's much :)" CreationDate="2016-06-20T21:15:50.243" UserId="3377" />
  <row Id="3443" PostId="98" Score="0" Text="you can get brutal shutdowns after the security threshold is passed. eletronics do weaken, condensers particularly. which diminishes power supply quality and have a variety of bad consequences. For ASICs electromigration is an issue, but usually after 30 years." CreationDate="2016-06-21T01:33:47.413" UserId="1614" />
  <row Id="3444" PostId="1755" Score="0" Text="From the caption text, it seems like Prusinkiewicz's own implementation." CreationDate="2016-06-21T02:37:09.173" UserId="482" />
  <row Id="3445" PostId="3647" Score="0" Text="@enne87: Happy to help. :)" CreationDate="2016-06-21T02:52:35.430" UserId="182" />
  <row Id="3446" PostId="3655" Score="0" Text="@AlanWolfe I assumed you were asking after broader context, since implementing basic bilateral filtering is highly Google-able and rather simple. As I wrote, the basic idea is to make values factor into the weight, not just distance." CreationDate="2016-06-21T03:49:44.197" UserId="523" />
  <row Id="3447" PostId="3655" Score="0" Text="Also @joolaa's answer is quite nice and may be useful as a second explanation of my overview." CreationDate="2016-06-21T03:49:58.150" UserId="523" />
  <row Id="3449" PostId="3655" Score="1" Text="Now that I have the term &quot;bilateral filter&quot; I am having more luck on Google. Thanks for that." CreationDate="2016-06-21T03:53:37.333" UserId="56" />
  <row Id="3450" PostId="1755" Score="0" Text="[ref 152](https://team.inria.fr/virtualplants/files/2013/09/treesviennot.pdf), [ref 70](http://www.geos.ed.ac.uk/homes/s0451705/horton_1945.pdf)" CreationDate="2016-06-21T05:18:29.297" UserId="482" />
  <row Id="3453" PostId="3660" Score="0" Text="the alignment of your left border of the toroidal slice with  the rectangle axis is purely coincidental ?" CreationDate="2016-06-22T01:42:17.693" UserId="1614" />
  <row Id="3454" PostId="3661" Score="0" Text="Commenting all the code related to the normal results in a correct rendered mesh with colors (or white)" CreationDate="2016-06-22T04:56:57.343" UserId="116" />
  <row Id="3455" PostId="3660" Score="1" Text="Is this a coverage calculation? Why would you choose to implement a very ineficient box filter?" CreationDate="2016-06-22T05:44:05.863" UserId="38" />
  <row Id="3456" PostId="3660" Score="0" Text="@joojaa Agree but if it makes it easier, an approx, say, Gaussian could be built from a few of these coverage calculations." CreationDate="2016-06-22T08:33:15.223" UserId="209" />
  <row Id="3463" PostId="3647" Score="0" Text="@trichoplax: thanks for the suggestion; done." CreationDate="2016-06-22T12:27:37.463" UserId="182" />
  <row Id="3464" PostId="3661" Score="1" Text="In one statement in the vertex shader you do a left multiply with the vertex data and in the other you do a right multiply. One of those is probably wrong." CreationDate="2016-06-22T14:00:45.187" UserId="137" />
  <row Id="3465" PostId="3661" Score="0" Text="Good catch but still not rendering. I am starting to think that the normals are not being passed to the shader ... somehow." CreationDate="2016-06-22T14:06:56.720" UserId="116" />
  <row Id="3466" PostId="3660" Score="1" Text="@trichoplax: points well taken!  I'll take down the question on Maths.SE - thank you for that guidance." CreationDate="2016-06-22T14:52:59.153" UserId="4625" />
  <row Id="3467" PostId="3660" Score="0" Text="@joojaa: yes, this is exactly a coverage calculation.  Does your comment mean that there's an efficient box filter to calculate this?" CreationDate="2016-06-22T15:00:48.383" UserId="4625" />
  <row Id="3468" PostId="3660" Score="2" Text="Something like that. Box filtering in general is the worst filter you could choose. Pixels are **not** really squares but point samples (read a [pixel is not a square x 3](http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf)) By using a standard function reconstruction you can do the same with a finite sampling which results in a better image than a box filter would with less work since your box filtering algorithm is so incredibly convoluted and expensive." CreationDate="2016-06-22T15:08:19.370" UserId="38" />
  <row Id="3469" PostId="3660" Score="0" Text="@joojaa: duly noted.  in fact, your comment motivated me to entirely reword  the question.  now: how do I design a sensible filter for this?  :)" CreationDate="2016-06-22T15:23:53.630" UserId="4625" />
  <row Id="3470" PostId="3660" Score="0" Text="Dont delete answer your own question to let somebody else answer with this info and then ask a new one. Its useful for others to see that people before them also made mistakes. Now that we know your scope isnt intentional we can start to answer the question at hand. But you could ass a note saying that you do not really require this sort of filtering just something thats good for the problem at hand." CreationDate="2016-06-22T15:32:55.153" UserId="38" />
  <row Id="3471" PostId="3660" Score="0" Text="True -- that's why I kept the original question un-edited." CreationDate="2016-06-22T15:42:28.927" UserId="4625" />
  <row Id="3472" PostId="3655" Score="0" Text="While this answer doesn't answer the intent of my question, it does answer the letter of the question, so I feel I ought to accept it as an answer hehe.  It's good info on upsampling, so is a very good answer.  I should have asked &quot;what is bilinear filtering&quot;, but that could be another question.  I have a lot more info about that now so am not going to be asking it, perhaps someone in the future will :P" CreationDate="2016-06-22T17:09:55.717" UserId="56" />
  <row Id="3473" PostId="3659" Score="0" Text="Keep in mind that most stencil buffers only have 8-bits allotted to them, which limits the number of unique IDs to 256. A similar strategy that allows for a much greater number of items is color picking. With it you render each object using a basic flat shader and a unique RGB color value. You then check your render target for the color under the mouse click. [Here is an example](http://www.opengl-tutorial.org/miscellaneous/clicking-on-objects/picking-with-an-opengl-hack/) (have not done more than a cursory look over the article)." CreationDate="2016-06-22T19:38:01.543" UserId="4630" />
  <row Id="3474" PostId="3659" Score="0" Text="@ssell Thank you for your comment. I have also seen that article when I started working on object selection. As I am a newbie in OpenGL, I resorted to the stencil buffer approach, which I think is pretty straightforward, and also fits my need as I have only 10 objects to pick. I have found a (buggy) workaround to this problem, I will share the details soon (by weekend I guess:P)." CreationDate="2016-06-22T19:46:20.773" UserId="3098" />
  <row Id="3475" PostId="3659" Score="0" Text="My OpenGL is rusty, but everything seems to be set up OK. Keep in mind that an index of `0` indicates your background (an area where no triangle was drawn) as that is the default stencil clear value when calling `glClear`. Do you happen to have an image of the scene you are drawing? An accompanying render of the stencil buffer would also be useful. Also keep in mind that you may need to invert your `y` value when performing the actual picking if your window coordinate system does not match your texture coordinate system (OpenGL origin is in lower-left corner, Windows origin is in upper-left)." CreationDate="2016-06-22T19:59:39.347" UserId="4630" />
  <row Id="3476" PostId="3660" Score="0" Text="very interesting article about sources pixels to consider for filtering. by Nathan http://www.reedbeta.com/blog/2014/11/15/antialiasing-to-splat-or-not/" CreationDate="2016-06-23T05:44:26.993" UserId="1614" />
  <row Id="3477" PostId="3661" Score="0" Text="I notice you are specifying your vertices as vec3s in the CPP side but use vec4s in the vertex shader. Change &quot;in vec4 position&quot; to &quot;in vec3 position&quot;, you may need to convert to vec4 with something like vec4(position, 1); . Also I would perform colour calculations in the pixel shader using a vec3 otherwise you will get lighting in your alpha channel." CreationDate="2016-06-23T05:52:34.133" UserId="3073" />
  <row Id="3482" PostId="3659" Score="0" Text="Are you actually making these triangles visible? If not, could you draw them? The fact that quads are returning a non-zero stencil value just makes me think you should check you haven't got a winding order problem with the second triangle of each pair." CreationDate="2016-06-24T09:02:46.860" UserId="209" />
  <row Id="3483" PostId="3661" Score="0" Text="I changed that, but still not able to see anything. Am I binding the index buffer correctly to the positions and normals of the vertices?" CreationDate="2016-06-24T16:30:53.587" UserId="116" />
  <row Id="3484" PostId="3672" Score="1" Text="Ill give you a hint, use cross product." CreationDate="2016-06-25T07:01:42.247" UserId="38" />
  <row Id="3487" PostId="3673" Score="0" Text="OK will try narrowing the question, thanks." CreationDate="2016-06-25T20:54:30.980" UserId="4638" />
  <row Id="3488" PostId="3671" Score="0" Text="@trichoplax ok just wanted to err on the side of caution regarding spam, will upload screenshots. re what i'm asking, questions #1 and #2 aren't specific enough?" CreationDate="2016-06-25T20:55:24.463" UserId="4638" />
  <row Id="3491" PostId="3671" Score="0" Text="Incidentally, thank you for erring on the side of caution regarding spam - this is much appreciated. I think it's quite clear this is a genuine question rather than advertising - so you need not worry." CreationDate="2016-06-25T22:21:53.440" UserId="231" />
  <row Id="3492" PostId="3671" Score="0" Text="@trichoplax ok thanks will upload within 15-20 minutes. thanks again for the help!" CreationDate="2016-06-25T22:26:03.283" UserId="4638" />
  <row Id="3493" PostId="3671" Score="0" Text="@trichoplax uploaded with screenshots from our app. thanks!" CreationDate="2016-06-25T22:37:48.127" UserId="4638" />
  <row Id="3494" PostId="3674" Score="0" Text="Could you expand on how this can be used to determine the angle of the tube? Sorry if I'm missing something obvious..." CreationDate="2016-06-26T00:09:20.400" UserId="231" />
  <row Id="3495" PostId="3674" Score="2" Text="@trichoplax you don't even need the angle. You rarely, if ever, do for setting up geometry. You can nearly always skip getting the angle explicitly because 99% of the time you will be feeding it into a sin or cos right after you get it from a asin or acos." CreationDate="2016-06-26T00:20:49.083" UserId="137" />
  <row Id="3496" PostId="3673" Score="2" Text="Looks like you are more up to speed on scene kit than I thought. Trichoplax has the answer, its about the lighting/shadowing." CreationDate="2016-06-26T02:06:46.713" UserId="4495" />
  <row Id="3499" PostId="3677" Score="2" Text="You can not be helped until you describe what you interpolate. A keyframe is just the frame where you have no interpolation. But what is in thw keyframe. The underlying mathematical model is important to the interpolation. No model no interpolation. So how do you plan to track the pixels. Vague waving of hands does not help." CreationDate="2016-06-26T09:13:08.343" UserId="38" />
  <row Id="3501" PostId="3678" Score="0" Text="[Related](http://computergraphics.stackexchange.com/questions/2381/finding-the-maximum-number-of-disconnected-fragments) (though not a duplicate)" CreationDate="2016-06-26T11:17:25.157" UserId="231" />
  <row Id="3502" PostId="3678" Score="0" Text="Does &quot;clipping&quot; in this context mean &quot;clipping against a rectangle&quot;?" CreationDate="2016-06-26T11:18:20.427" UserId="231" />
  <row Id="3503" PostId="3678" Score="0" Text="Yes, that is exactly what I mean :)" CreationDate="2016-06-26T11:57:34.617" UserId="4646" />
  <row Id="3504" PostId="3678" Score="0" Text="Sorry - I just realised you did mention a rectangle in your question..." CreationDate="2016-06-26T12:02:37.173" UserId="231" />
  <row Id="3505" PostId="2521" Score="0" Text="What is DDA (in this context)? [Digital differential analyzer](https://en.wikipedia.org/wiki/Digital_differential_analyzer_(graphics_algorithm))?" CreationDate="2016-06-26T13:23:00.840" UserId="3304" />
  <row Id="3507" PostId="3681" Score="1" Text="Normalizing is almost certainly faster than doing anything with trigonometry: a square root and a few adds and multiplies should be cheaper to evaluate than a sequence of trigonometric functions. I'm surprised to hear that normalizing is too expensive, as this should be quite fast on modern CPUs. Can you say more about the context of this problem? How have you determined that the normalization is causing a performance issue?" CreationDate="2016-06-26T19:41:12.927" UserId="48" />
  <row Id="3508" PostId="3681" Score="0" Text="The problem is their are tons of points that this needs to be done for. Anywhere from 400-1500 points have this done about every frame" CreationDate="2016-06-26T19:45:40.577" UserId="2308" />
  <row Id="3509" PostId="3681" Score="1" Text="OK. 400-1500 points is a fair number, but not _that_ many; it should only take microseconds to normalize them if it's coded efficiently in a native-compiling language like C++. What language/framework/engine/etc are you using? How are you implementing the normalization process?" CreationDate="2016-06-26T20:07:23.617" UserId="48" />
  <row Id="3510" PostId="3681" Score="0" Text="Is the radius a variable or a constant?" CreationDate="2016-06-26T20:09:11.737" UserId="231" />
  <row Id="3511" PostId="3681" Score="0" Text="I added some details about the simulation above." CreationDate="2016-06-26T20:20:08.580" UserId="2308" />
  <row Id="3512" PostId="3675" Score="1" Text="Take a look at the [SCNTechnique Class Reference...](https://developer.apple.com/library/ios/documentation/SceneKit/Reference/SCNTechnique_Class/)" CreationDate="2016-06-26T20:20:10.517" UserId="4495" />
  <row Id="3513" PostId="3681" Score="0" Text="The radius is variable. Right now I first check if it's in the circle using the squared distance. Then after that I subtract the center from the point and decide that by the sqrt of the distance squared. That is then multiplied by the radius." CreationDate="2016-06-26T20:21:56.127" UserId="2308" />
  <row Id="3514" PostId="3681" Score="0" Text="If you have existing code then it might help to edit it into the question so we can see it. There may be potential optimisations that are not obvious from the question wording." CreationDate="2016-06-26T21:37:08.937" UserId="231" />
  <row Id="3515" PostId="3681" Score="0" Text="For example, from your last comment I would suggest storing the radius squared and comparing that to the distance squared if many points are compared against the same radius, but I wouldn't have thought to mention that just from the question wording." CreationDate="2016-06-26T21:39:15.400" UserId="231" />
  <row Id="3516" PostId="3681" Score="0" Text="It would also help to know how often the point is inside the circle. If there are enough points that are outside the circle then it may be worth including a fast test that excludes points outside a bounding square." CreationDate="2016-06-26T21:40:53.640" UserId="231" />
  <row Id="3517" PostId="3677" Score="1" Text="Possible duplicate of [How to do keyframe interpolation on matlab? Any toolboxes present?](http://computergraphics.stackexchange.com/questions/3644/how-to-do-keyframe-interpolation-on-matlab-any-toolboxes-present)" CreationDate="2016-06-26T22:19:37.397" UserId="1981" />
  <row Id="3518" PostId="3681" Score="0" Text="Does your current implementation use an acceleration structure such as a grid? If not that would be your answer (for a huge speed up). Let me know and I'll write something up if that's the case." CreationDate="2016-06-26T22:27:36.783" UserId="231" />
  <row Id="3519" PostId="3682" Score="0" Text="Is that even faster then doing a quick broad phase AABB check?" CreationDate="2016-06-26T23:00:37.050" UserId="2308" />
  <row Id="3520" PostId="3681" Score="1" Text="BTW, although an acceleration structure may be helpful, I still think that even brute-force testing 1500 points against each of 10 circles (for instance) is not a huge amount of calculation, even on a mobile device. I would look into making sure it's coded efficiently, i.e. not having unnecessary function calls or pointer indirections, and possibly using SIMD for the calculations, before going to the trouble of implementing an acceleration structure." CreationDate="2016-06-26T23:04:47.543" UserId="48" />
  <row Id="3521" PostId="3682" Score="0" Text="If you do a bounding box check on each point, then that fast check is still being done on all 1000 points, for each circle. With a grid acceleration structure, if a circle only overlaps grid cells containing 30 points, none of the points outside those grid cells get checked at all, as they are not relevant." CreationDate="2016-06-26T23:10:09.330" UserId="231" />
  <row Id="3522" PostId="3682" Score="0" Text="You will see the best speed up from a grid if the points are evenly spread over the screen. If the circles force all the points into a small area of the screen then the speed up will be less significant, and then you might need to look for other optimisations." CreationDate="2016-06-26T23:12:47.687" UserId="231" />
  <row Id="3523" PostId="3682" Score="0" Text="A rough guide to whether this is the right answer is to observe the change of speed in your current implementation. Does it slow down (1) in proportion to the total number of points, or (2) in proportion to the number of points needing to be moved? (1) suggests you need an acceleration structure (such as a grid), while (2) suggests you need to optimise the movement of the points to the circumference. This answer is a best guess without seeing the code or the app running, so it may not be what you need." CreationDate="2016-06-26T23:16:39.597" UserId="231" />
  <row Id="3524" PostId="3682" Score="0" Text="Also see Nathan Reed's comment on the question. I trust Nathan's intuition far more than my own." CreationDate="2016-06-26T23:19:37.483" UserId="231" />
  <row Id="3525" PostId="3681" Score="0" Text="@NathanReed so as of now I am using a static math library of my own design that has functions like add subtract and multiply. It creates a new point struct and returns it. Would it be faster to now use the static class?" CreationDate="2016-06-26T23:30:57.657" UserId="2308" />
  <row Id="3526" PostId="3681" Score="1" Text="@J.Doe Ideally you would look at the generated code (assembly language) to see how it's coming out. Compilers can often optimize away small function calls by inlining, and can keep small structs in registers, etc. I don't develop for Swift or iOS so I can't tell you in detail about that platform, but a quick google did turn up [this article on viewing disassembly in Swift](https://medium.com/swift-programming/secret-of-swift-performance-fcc5d2a437a8#.6e33qpynu)." CreationDate="2016-06-26T23:36:51.247" UserId="48" />
  <row Id="3527" PostId="3682" Score="2" Text="Ok I tried out broad phase, and grid phase.  Turns out the best one was doing a AABB check before anything else. Reading all of your comments (thankyou so much) has helped me make sense of why. The radius is growing, and points group up quickly. Thank much!" CreationDate="2016-06-26T23:42:51.710" UserId="2308" />
  <row Id="3528" PostId="3681" Score="0" Text="@NathanReed Looking at assembly code is a bit beyond my knowledge so far, I think I will just try and replace the static methods before the final build. Thanks!" CreationDate="2016-06-26T23:43:25.710" UserId="2308" />
  <row Id="3530" PostId="3683" Score="2" Text="I don't think that is going to measure what you think it will. Not to mention fragment shaders run (at least) in a 2x2 block for each fragment." CreationDate="2016-06-27T10:00:28.337" UserId="137" />
  <row Id="3531" PostId="3683" Score="0" Text="@ratchetfreak 2x2 block per fragment even without multisampling? Why?" CreationDate="2016-06-27T10:12:39.550" UserId="4647" />
  <row Id="3532" PostId="3683" Score="3" Text="@Ruslan: fragments are treated as 2x2 blocs so the derivatives can be evaluated." CreationDate="2016-06-27T11:09:12.183" UserId="182" />
  <row Id="3533" PostId="3688" Score="0" Text="If you set the `w` component in the vertex output to 0 the triangle it is part of will not get pushed down the rasterizer. Though that may not help if the bottle neck is vertex processing." CreationDate="2016-06-27T15:20:15.583" UserId="137" />
  <row Id="3534" PostId="3688" Score="0" Text="Thanks for your answer. But if I understand you correctly I would need to render every position with every LOD and then check the distance to camera for every vertex and set w to 0 if it belongs to the wrong LOD for this position. If this is correct, I don't think that it will help because doing this test for all vertices of the highest LOD at every position leads to an insane number of tests. Even if I don't send most of this vertices to the rasterizer, I don't think that this will work, but if I don't find a better solution I will try it anyways and comment my result here." CreationDate="2016-06-27T15:32:18.177" UserId="4654" />
  <row Id="3535" PostId="3693" Score="0" Text="I didn´t see the error! Thank you so much!" CreationDate="2016-06-27T20:36:41.917" UserId="4653" />
  <row Id="3536" PostId="3693" Score="1" Text="@Maribel Common mistake. I do it all the time." CreationDate="2016-06-27T20:38:31.777" UserId="3437" />
  <row Id="3537" PostId="3697" Score="4" Text="Curves are mathematical entities they do not have centers as such. However you might be looking for bouding box center, minimum bounding box center, center of gravity of the enclosed area underlying the curve when endpoints are connected, center of gravity of closed polybeziers, curve midpoint, control point average etc." CreationDate="2016-06-27T21:41:51.703" UserId="38" />
  <row Id="3538" PostId="3694" Score="0" Text="Thanks for your answer and the extensive code sample. I'm not familiar with DX, but it pointed me in the right direction." CreationDate="2016-06-28T00:27:08.420" UserId="4654" />
  <row Id="3539" PostId="3694" Score="1" Text="And your assumption that the pcie bus is the bottleneck seems to be indeed right (on my hardware I can now render about 300k bushes and I get the same framerate for the middle and the coarse LOD, therefore the number of vertices is not the limiting factor). For anyone who has the same problem: The use of instanced arrays did the trick for me (very nice explanation here: http://learnopengl.com/#!Advanced-OpenGL/Instancing). Therefore you don't run into the limit of uniforms, which was the problem of my 2nd approach." CreationDate="2016-06-28T00:33:45.973" UserId="4654" />
  <row Id="3540" PostId="3685" Score="0" Text="Nailed it. Of course, doing anything on a single fragment is massively wasteful anyway since you will have 31 or 63 other threads idling while it completes. If i was gonna  run a test like this to get at least a ballpark figure, i'd probably draw a full-window quad at say 1024^2, load up a bunch of different textures then sample each of them in turn until the framerate dropped, making sure to actually use the results like you say." CreationDate="2016-06-28T05:51:25.017" UserId="1937" />
  <row Id="3541" PostId="3697" Score="1" Text="@joojaa there are definitely 2 natural centers I can think of, one is the `t=0.5` point. and two is the geometric midpoint regarding travel cartesian distance along the line." CreationDate="2016-06-28T06:12:10.427" UserId="1614" />
  <row Id="3542" PostId="3697" Score="1" Text="@v.oddou yes in fact there are many more thats why the question needs clarification. All vector applications that i have used (Illustrator, xara, corel, sketch, etc..) Use the local bounding box center to rotate objects, so its rare to see the other center definitions used at all." CreationDate="2016-06-28T06:44:21.133" UserId="38" />
  <row Id="3544" PostId="3698" Score="0" Text="maybe this is better as a comment..." CreationDate="2016-06-28T09:15:26.723" UserId="3437" />
  <row Id="3545" PostId="3698" Score="2" Text="Could you expend the question a bit so that its not just a link only answer," CreationDate="2016-06-28T12:37:00.660" UserId="38" />
  <row Id="3546" PostId="3699" Score="3" Text="I wouldn't call this a comment. I'd call it an excellent answer that addresses the asker's current level of knowledge, explains fully why the question is broader than expected, and opens the way for new questions." CreationDate="2016-06-28T14:00:40.993" UserId="231" />
  <row Id="3547" PostId="3699" Score="0" Text="@trichoplax though could be summarized as &quot;define center&quot; by the less polite." CreationDate="2016-06-28T15:24:46.537" UserId="137" />
  <row Id="3548" PostId="3699" Score="0" Text="@ratchetfreak I prefer answers that try to identify the knowledge gap of the asker rather than expecting them to fully understand the topic they are asking about." CreationDate="2016-06-28T17:56:16.807" UserId="231" />
  <row Id="3549" PostId="3700" Score="0" Text="Are you looking for something different from [this](http://computergraphics.stackexchange.com/questions/295/does-it-matter-whether-i-learn-opengl-or-direct3d)?" CreationDate="2016-06-28T19:23:24.583" UserId="231" />
  <row Id="3550" PostId="3700" Score="0" Text="@trichoplax Yes, I have read all of the answers before asking this question. Is this a duplicate, Or broad ?" CreationDate="2016-06-28T19:25:34.833" UserId="3437" />
  <row Id="3551" PostId="3700" Score="0" Text="I don't think this is a duplicate, but the title looks like one, so I'm just trying to think of a way to word it" CreationDate="2016-06-28T19:45:39.533" UserId="231" />
  <row Id="3553" PostId="3700" Score="2" Text="BTW, I answered [a similar question on GameDev.SE](http://gamedev.stackexchange.com/a/49038/9894) a couple years ago." CreationDate="2016-06-28T21:10:34.987" UserId="48" />
  <row Id="3554" PostId="3700" Score="1" Text="here's a good lesson of history that sorta answer your question: http://programmers.stackexchange.com/questions/60544/why-do-game-developers-prefer-windows" CreationDate="2016-06-29T01:01:08.207" UserId="4665" />
  <row Id="3555" PostId="3700" Score="0" Text="@darius especially Nicol Bolas's answer." CreationDate="2016-06-29T01:53:55.793" UserId="1614" />
  <row Id="3556" PostId="3700" Score="1" Text="@NathanReed *&quot;So, it's possible we will see a swing back toward multi-platform developers using OpenGL on Windows.&quot;* And now, 3 years later, would you say this has happened?" CreationDate="2016-06-29T04:08:32.790" UserId="457" />
  <row Id="3557" PostId="1941" Score="0" Text="This is a great idea, and probably the closest you can get to single-stepping shader code.  I wonder if running through a software renderer (Mesa?) would have similar benefits?" CreationDate="2016-06-29T06:03:38.983" UserDisplayName="user3412" />
  <row Id="3558" PostId="3700" Score="0" Text="You should check these two blogs out:&#xA;[Why you should use OpenGL and not DirectX](http://blog.wolfire.com/2010/01/Why-you-should-use-OpenGL-and-not-DirectX) and &#xA;[DirectX vs. OpenGL revisited](http://blog.wolfire.com/2010/01/DirectX-vs-OpenGL-revisited).&#xA;&#xA;They both helped me forumate an opinion on which to use. Although from a technical standpoint both are going in very similiar directions." CreationDate="2016-06-29T08:58:21.233" UserId="214" />
  <row Id="3559" PostId="1941" Score="0" Text="@racarate: I thought about that as well but did not have the time to try yet. I am no expert on mesa but I think it might be hard to debug the shader as the shader debug information has to somehow reach the debugger. Then again, maybe the folks at mesa already have an interface for that to debug mesa itself :)" CreationDate="2016-06-29T10:21:35.590" UserId="2521" />
  <row Id="3560" PostId="3700" Score="2" Text="@Rotem I don't think so, no. OGL (and now Vulkan) today is better at maintaining feature parity with D3D than it was, but AFAICT, the GPU vendors still prioritize D3D drivers for bugfixes and perf improvements, and most games/engines still use D3D by default on Windows. A notable exception is the [idTech engines](https://en.wikipedia.org/wiki/Id_Tech), which all use OGL on Windows." CreationDate="2016-06-29T18:50:46.827" UserId="48" />
  <row Id="3565" PostId="3685" Score="0" Text="Thank you for this, giving up on that quest helped me realize how stupid it was to think I could take the amount of samples I do and somehow calculate a downsample size, and radius for a gaussian blur shader. Turns out it will work much better to just have predefined combinations of varying intensity and see which ones run at the desired fps." CreationDate="2016-06-30T07:34:04.937" UserId="2308" />
  <row Id="3566" PostId="3685" Score="0" Text="Are GPU compilers more innovative then CPU ones? I mean I have heard stuff about a gpu compiler recompiling because a uniform triggers a different part of an if statement and you speak of pulling out of the loop because the calculation is the same. I haven't heard of CPU's doing that." CreationDate="2016-06-30T07:35:16.233" UserId="2308" />
  <row Id="3567" PostId="3577" Score="0" Text="Apple will never support Vulkan unless they're forced to, and the economics of it are pretty simple. By going all in on Metal, they hope to lock in mobile game developers that need more than GLES2 can offer but don't have the resources to write their games for both Vulkan and Metal. They're prepared to sacrifice Mac's tiny gaming ecosystem for this, especially if iOS devs also release on the Mac App Store." CreationDate="2016-06-30T07:52:16.723" UserId="4671" />
  <row Id="3568" PostId="3685" Score="0" Text="@J.Doe optimizers (for gpu and cpu) try to use every trick in the book however GPU optimizers know more about the program they are trying to optimize and what hardware it will be running on. The optimizations you mention are called [partial evaluation](https://en.wikipedia.org/wiki/Partial_evaluation) (with the uniforms as additional constant data) and [Loop-invariant code motion](https://en.wikipedia.org/wiki/Loop-invariant_code_motion)" CreationDate="2016-06-30T08:02:38.230" UserId="137" />
  <row Id="3569" PostId="3702" Score="0" Text="So there is no performance reasons to prefer Direct3D ?" CreationDate="2016-06-30T10:07:51.453" UserId="3437" />
  <row Id="3570" PostId="3707" Score="0" Text="No the power is 2.2 usually (read about [gamma correction](https://en.wikipedia.org/wiki/Gamma_correction))" CreationDate="2016-06-30T11:14:48.513" UserId="38" />
  <row Id="3573" PostId="3702" Score="0" Text="@ritwik sinha DirectX 11 has better support for multi threading rendering than OpenGL 4, but to my knowledge that has not resulted in any realt performance difference. On the other side, comparing Windows with DirectX 11 and Linux with OpenGL 4, the winner was Linux with OpenGL. Regarding DirectX 12 vs Vulkan, I think it is too early to know, but i don't know." CreationDate="2016-06-30T20:46:43.773" UserId="4665" />
  <row Id="3574" PostId="3688" Score="0" Text="Have you considered an imperfect solution spreading LOD update of the 10000 bushes across multiple cycles? I doubt your view changes alot anyways. And if it changes a huge amount, a little lag is usually not noticable." CreationDate="2016-06-30T20:50:29.330" UserId="3041" />
  <row Id="3575" PostId="3702" Score="0" Text="i never worked with Direct3D, so i don't understand what you mean by multi threading. As far as i know different threads are handled by programmers for different tasks. Does Direct3D does this under the hood  ? Anyway nice answer." CreationDate="2016-06-30T21:09:44.243" UserId="3437" />
  <row Id="3576" PostId="3702" Score="1" Text="@ritwik sinha an application of multi threading to real time graphics is that you may use diffrent threads to allocate resoruces in the background (texture streaming, vertex buffers), wile a main thread does the draw calls. This is also possible in OpenGL, but very very tricky. In DirectX 11 you can also make draw calls, from many threads, which should improve performance, but that's not a common practice. I think that the last release of UnrealEngine4 does it. However, with DirectX 12 and Vulkan multi threading rendering is going to be a real deal." CreationDate="2016-06-30T23:59:26.960" UserId="4665" />
  <row Id="3577" PostId="3707" Score="0" Text="So wait add the sum of your pixels with each one raised to the 2.2 power. And then take that sum by the 1/2.2 power?" CreationDate="2016-07-01T05:09:22.967" UserId="2308" />
  <row Id="3578" PostId="3707" Score="0" Text="Depends on how pedantic you want to be, a gamma correction of 2.2 certainly is close to srgb but if you really want to be pedantically correct google dor linear to srgb" CreationDate="2016-07-01T05:27:04.647" UserId="38" />
  <row Id="3580" PostId="3710" Score="0" Text="Beautifully answered.  And rendered.  Thanks." CreationDate="2016-07-01T23:24:17.753" UserId="4625" />
  <row Id="3581" PostId="3710" Score="0" Text="Use a indented block for code in future please. Nice answer" CreationDate="2016-07-02T07:14:18.663" UserId="38" />
  <row Id="3582" PostId="3709" Score="0" Text="@ritwiksinha Everything written in black is known.The central points, the radii, and the arc angle." CreationDate="2016-07-02T11:53:18.863" UserId="4620" />
  <row Id="3584" PostId="3712" Score="2" Text="This is a [duplicate question](http://computergraphics.stackexchange.com/questions/1669/why-for-perfect-reflections-a-surface-must-have-g2-continuity) Basically its because you want the reflective properties of the surface to continue the same way over the gap. Otherwise humans will interpret a sudden change as a crease" CreationDate="2016-07-02T17:40:24.233" UserId="38" />
  <row Id="3585" PostId="3712" Score="0" Text="Although this question mentions two additional properties in addition to the previous question, it is not clear what is being asked. If a term has multiple conflicting definitions, then each one is probably for a specific purpose. Without knowing your intention it is difficult to judge what you need to know." CreationDate="2016-07-02T18:30:09.230" UserId="231" />
  <row Id="3586" PostId="3712" Score="0" Text="@trichoplax actually all the 3 definitions seem to say the same thing. Curvature must not suddenly change. 1 specifies how deep your continuity must be 2 that it may not abruptly change 3 that the change must not be too fast. All of those have been discussed n the duplicate, i would agree. But yes the question is not so clear. Also note G2 curvature does not apply to curves as such for silhouettes it could be less than G2 is its not too abrupt change. However CAD deals with surfaces a curve must have same properties as a surface it models or it will destroy the surfaces continuity requirement." CreationDate="2016-07-02T18:56:37.067" UserId="38" />
  <row Id="3587" PostId="3712" Score="0" Text="@joojaa I don't have the knowledge to judge how close to being a duplicate this is - thanks for the extra information. If this is closed as a duplicate then people searching for the terms in this question will still be redirected to the previous question, so we just need to judge whether anything new would be present in answers to this new question." CreationDate="2016-07-02T19:22:00.157" UserId="231" />
  <row Id="3588" PostId="3713" Score="0" Text="I don't know if it would be practical to implement this, but would it help to have the precomputed sine values spaced unevenly, with more closely clustered values where the slope of the sine curve is steepest, and fewer values where it levels out and doesn't change as much? Would this allow higher accuracy where it is needed, without needing to store a large number of values?" CreationDate="2016-07-02T19:26:16.430" UserId="231" />
  <row Id="3589" PostId="3713" Score="0" Text="Can you just render to a texture that is 1/2 or 1/4 the size of the final target?" CreationDate="2016-07-02T21:59:49.260" UserDisplayName="user3412" />
  <row Id="3590" PostId="3713" Score="5" Text="Regarding #2, the built-in [`mod` function](http://docs.gl/sl4/mod) is what you want. You would write `mod(angle, 360.0)`." CreationDate="2016-07-03T01:10:42.147" UserId="48" />
  <row Id="3591" PostId="3713" Score="1" Text="@trichoplax brilliant idea but I don't know how you would be able to look up values on the table then. Let's say we put them in an array with some of them more concentrared. How could we find the right index?" CreationDate="2016-07-03T01:35:25.583" UserId="2308" />
  <row Id="3592" PostId="3713" Score="0" Text="@racarate actually will probably do that. Feel free to post that as the answer." CreationDate="2016-07-03T01:35:56.930" UserId="2308" />
  <row Id="3593" PostId="3712" Score="0" Text="They aren't 3 definitions. It is an unique definition; all 3 conditions must hold." CreationDate="2016-07-03T10:03:59.617" UserId="1636" />
  <row Id="3594" PostId="3688" Score="0" Text="I have not tried it, since the performace that I gained by using instanced arrays (see my comment under the accepted answer) was enough in my case. But it is something that could be combined with the use of instanced arrays if even more performace would be needed." CreationDate="2016-07-03T17:09:28.237" UserId="4654" />
  <row Id="3596" PostId="3713" Score="6" Text="How about putting your values into a 3-channel 1D texture? That way you can get sin, cos and tan out for the price of a single texture lookup. If you map 0 - 2pi angle to 0 - 1 UV and use repeat texture mode you don't even need the mod call, it will 'wrap' automatically, and you can also get linear-filtered approximations in between your stored values rather than snapping to the nearest one." CreationDate="2016-07-04T05:31:04.567" UserId="1937" />
  <row Id="3597" PostId="3712" Score="0" Text="@Valerio yes and if you read the linked post you would know why. Basically eyes structure will see up to second derivate. For obvious reasons this means inflection will be seen and uneven variation will be seen by a human because of the nature of the system." CreationDate="2016-07-04T08:18:30.957" UserId="38" />
  <row Id="3598" PostId="3713" Score="3" Text="A lot of the time you can eliminate trig functions when used for geometry by not using the angle but start and end with the sin/cos pair and use trig identities for half angles and such." CreationDate="2016-07-04T08:27:06.563" UserId="137" />
  <row Id="3599" PostId="3717" Score="0" Text="This sounds promising. Could you edit to add some detail of what this provides?" CreationDate="2016-07-04T12:24:45.830" UserId="231" />
  <row Id="3600" PostId="3711" Score="0" Text="Thank you. Looking at the solutions, I think knowing the quadrants where the circles touch is sufficient, if I also know the direction of curvature." CreationDate="2016-07-04T13:57:32.307" UserId="4620" />
  <row Id="3602" PostId="3711" Score="0" Text="Could you please elaborate on what you would do with the given data and radius $x$ to obtain the tangential points and directions?" CreationDate="2016-07-04T15:51:19.163" UserId="4620" />
  <row Id="3605" PostId="3711" Score="0" Text="@Rikki-Tikki-Tavi, What part do you not know how to compute: The other angles of a triangle  (use [law of sines](https://en.wikipedia.org/wiki/Law_of_sines)) or [vector rotation](https://en.wikipedia.org/wiki/Rotation_matrix)? If you look at the image you'll see that 2 of the solutions are in same quadrant. I would Instead use whether or not its the inside or outside tangent, or then you need to check all of them. (by the way it feels like I'm teaching you your high school trig again as i have taught you [Law of Cosines](https://en.wikipedia.org/wiki/Law_of_cosines) already)" CreationDate="2016-07-04T17:20:30.423" UserId="38" />
  <row Id="3606" PostId="3717" Score="1" Text="It would also be useful to have an overview of how each algorithm works, so that they can be compared without needing to leave the answer." CreationDate="2016-07-05T01:21:35.790" UserId="231" />
  <row Id="3607" PostId="2011" Score="0" Text="Perhaps you are missing enabling the attrin pointers? Take a look at glEnableVertexAttribArray()." CreationDate="2016-07-05T05:05:54.453" UserId="116" />
  <row Id="3608" PostId="2011" Score="0" Text="Another thing that comes to mind is trying to set the location in the shaders. The first parameter in glVertexAttribPointer is the index of the vertex attribute. Therefore, trying something in your shader like `layout(location = 0) in vec3 posición;` and `layout(location = 3) in vec4 i_color;` might help." CreationDate="2016-07-05T05:09:18.037" UserId="116" />
  <row Id="3609" PostId="3723" Score="1" Text="Just to confirm, you want the output vertices to be a subset of the input vertices? It isn't sufficient for the output vertices to be on the surface defined by the input mesh?" CreationDate="2016-07-05T09:32:38.003" UserId="231" />
  <row Id="3611" PostId="3692" Score="1" Text="Now I understand, thanks. It seems most people use projected area instead of projected solid angle in the definition for radiance. I guess it doesn't make any difference in practice. I can see the math checks out using projected solid angle, but intuitively it doesn't make much sense to me (using projected area seems like &quot;the right&quot; way to me). Can you please describe/show the reasoning for using projected solid angle visually/geometrically?" CreationDate="2016-07-05T17:32:27.700" UserId="4578" />
  <row Id="3612" PostId="3724" Score="1" Text="Many animation tools have such functions, such as mayas curve deformer. But thenagain your question is a bit vague in terms of what you want as you may in fact want to know about spline patches such as nurbs surfaces that indeed have a quite wide mathematical research behind them." CreationDate="2016-07-06T06:50:04.203" UserId="38" />
  <row Id="3614" PostId="3713" Score="0" Text="Russ has, I think, the best solution here.  If you only need sine, then a single channel half precision will work best, as it'll be important for performance that the 1d texture to fit in cache (less precision than half float will likely not reproduce the sine function very well)." CreationDate="2016-07-06T19:15:29.220" UserId="3386" />
  <row Id="3615" PostId="3729" Score="0" Text="Could you clarify what difference you want to see each frame? Do you want the total &quot;mass&quot; to stay constant, and have it spread a small distance each frame until it is evenly distributed? Do you want it to spread evenly in all directions, giving a circular spread?" CreationDate="2016-07-06T19:19:03.353" UserId="231" />
  <row Id="3616" PostId="3729" Score="0" Text="I think it would be most appropriate to have it spread outwards each frame at a constant rate, but since the initial mass for each pixel is random, all pixels will stop spreading at different times. If the &quot;mass&quot; is constant across all pixels, the effect would just be an even circular spread. I think a major issue I'm having is figuring out how to pass that mass value along." CreationDate="2016-07-06T19:22:53.910" UserId="4708" />
  <row Id="3617" PostId="3731" Score="0" Text="Thank you very much, I hadn't heard of the Kronecker delta before." CreationDate="2016-07-06T19:59:49.547" UserId="4705" />
  <row Id="3618" PostId="3713" Score="0" Text="That is brilliant! Though filling up a texture with values would be a new technique for me. I can write to a texture but I can't ensure that I'm writing exact pixels rather then say .5 pixels" CreationDate="2016-07-06T20:11:04.593" UserId="2308" />
  <row Id="3619" PostId="3713" Score="0" Text="And I don't quite understand how I would look up values on the texture for a given radian value." CreationDate="2016-07-06T20:11:33.710" UserId="2308" />
  <row Id="3620" PostId="3729" Score="0" Text="Maybe try to start with just describing the overall effect you want to give, rather than the implementation details. Do you want multiple drops of ink that all spread out independently and eventually overlap, or are the multiple pixels you mention part of modeling a single ink drop?" CreationDate="2016-07-06T22:39:12.627" UserId="231" />
  <row Id="3621" PostId="3729" Score="0" Text="Thank you all for the help. The effect I want is as if a heavy drop of ink was dropped on paper, and the ink is spreading outwards. With that it spreads to different degrees, and maybe even has some sort of capillary element." CreationDate="2016-07-06T22:45:58.847" UserId="4708" />
  <row Id="3622" PostId="3729" Score="1" Text="This is much clearer. Could you edit the question to reflect this more specific requirement? Comments aren't guaranteed to last forever." CreationDate="2016-07-06T22:50:49.060" UserId="231" />
  <row Id="3623" PostId="3735" Score="0" Text="There is a less understood trick to handle the flip of the spline direction, it happens because you use a naive local approach to the problem. But it is quite easy to find all the locations where the flip will occour than its just a matter of saying okay this segment is reversed." CreationDate="2016-07-07T09:08:06.480" UserId="38" />
  <row Id="3624" PostId="3735" Score="0" Text="Interesting. In my case the flip was never a problem though. I had to handle the case when the spline was backtracking, but that was easy to detect and then just a matter of switching a sign." CreationDate="2016-07-07T09:25:53.757" UserId="182" />
  <row Id="3625" PostId="3734" Score="0" Text="You're right! I just tried to delete those lines and the first example now behaves like the second one. At this point I think the second example has just a &quot;rotated texture&quot; issue, I'll try and let you know." CreationDate="2016-07-07T11:33:37.720" UserId="3472" />
  <row Id="3626" PostId="3734" Score="0" Text="Yep, I confirm, that did the trick. Thank you immensely!" CreationDate="2016-07-07T12:20:48.917" UserId="3472" />
  <row Id="3627" PostId="3677" Score="0" Text="@flawr yes it was me. My previous question got closed as mentioned." CreationDate="2016-07-07T13:41:23.377" UserId="4608" />
  <row Id="3628" PostId="3677" Score="0" Text="@joojaa	&#xA;I'm interpolating image frames. Literal image frame pixel values. Imagine this, there are some pixel values I select, then I call those pixel values features and decide to track those pixel values against the frames. Those pixel values must have some sort of trajectory. I want to interpolate pixel values at various points of a trajectory such that the trajectory approximates a smooth motion between the pixel values." CreationDate="2016-07-07T13:42:24.893" UserId="4608" />
  <row Id="3629" PostId="3677" Score="0" Text="@user6334139 yes but finding the trajectory is another thing and has nothing to do with interpolation. Interpolation only comes into play once you know where the trajectory is going (quite literally you have billions of options on where the pixel went). Look into optical flow tracking. https://www.youtube.com/watch?v=KoMTYnlNNnc" CreationDate="2016-07-07T13:46:03.407" UserId="38" />
  <row Id="3630" PostId="3677" Score="0" Text="@joojaa Okay fine I know the per pixel affine transform matrices between frames, the warps they perform on any single pixel in the image give rise to displacement in the pixel in a frame-time plot and end up being a trajectory. I smooth this thing somehow and have another set of matrices now describing the same thing but smoother. So I know where the trajectory is going, now what?" CreationDate="2016-07-07T13:51:40.533" UserId="4608" />
  <row Id="3631" PostId="3677" Score="1" Text="Yes time to rewrite your question. Your not looking to interpolate pixels but interpolation of matrices. Is it 4 by 4 matrix? Please [EDIT](http://computergraphics.stackexchange.com/posts/3677/edit) your question" CreationDate="2016-07-07T13:53:22.243" UserId="38" />
  <row Id="3632" PostId="3677" Score="0" Text="Matrices are only a representation of things needed to get a trajectory. The end result is still interpolation of the pixel values in the trajectory." CreationDate="2016-07-07T13:56:59.437" UserId="4608" />
  <row Id="3634" PostId="3677" Score="0" Text="Not being wiser friend, I've articulated it as it is, thats the problem statement, thats what I couldnt solve, thats why I posted here" CreationDate="2016-07-07T14:11:03.093" UserId="4608" />
  <row Id="3636" PostId="3737" Score="0" Text="Also if you use subdivision meshes then all quads makes it easier for you to anticipate how the mesh flows. Which can be important to the artist" CreationDate="2016-07-07T16:40:27.647" UserId="38" />
  <row Id="3637" PostId="3736" Score="0" Text="Well on hardware side but in software renders its actually pretty common. After all a quad representation is more compact." CreationDate="2016-07-07T16:41:19.307" UserId="38" />
  <row Id="3638" PostId="3677" Score="0" Text="Rather than trying to keep everything abstract, if you have some example data this may make it easier to understand your intention. A series of images or the location data of the points you are working with, perhaps." CreationDate="2016-07-07T19:25:02.637" UserId="231" />
  <row Id="3639" PostId="3741" Score="0" Text="How much higher resolution should I upscale it" CreationDate="2016-07-07T20:22:03.073" UserId="2308" />
  <row Id="3640" PostId="3741" Score="0" Text="Usually 4 is fine but read this first https://www.opengl.org/wiki/Multisampling you dont actually need to store those extra pixels" CreationDate="2016-07-07T20:24:11.140" UserId="38" />
  <row Id="3641" PostId="3740" Score="1" Text="Well you probably could solve the render equation for a point analytically if youd have a stupidly simple scene. And possibly you could get an analytical solution for the entire projected image if your scene is even simpler than that. But that would be useless... :)" CreationDate="2016-07-07T20:33:31.707" UserId="38" />
  <row Id="3642" PostId="3740" Score="3" Text="The scene would have to be really simple like only a single flat object. Since there is the integration about the whole sphere even if there are any two points which can see each other the rendering equation gets infinite. Each point includes the other in the integration domain. so Only a single reflector that cannot reflect on itself. then you could solve it. Then there would simply no global lighting effects so is comes down to local lighting. And that is solvable." CreationDate="2016-07-07T20:37:11.353" UserId="273" />
  <row Id="3643" PostId="3740" Score="0" Text="Yes that would be stupidly simple." CreationDate="2016-07-07T20:38:44.160" UserId="38" />
  <row Id="3644" PostId="3741" Score="0" Text="I hope that is compatible with opengl es 2.0. It seems almost nothing is. My fear is that my app uses an accumulation buffer so I never clear the buffer that is drawn on, i simply fade it then draw a new frame. Oviously a bigger FBO means slower loading time. Hopefully the extension is supported making that a non issue." CreationDate="2016-07-07T23:36:25.307" UserId="2308" />
  <row Id="3645" PostId="3738" Score="5" Text="Good question, no research effort." CreationDate="2016-07-08T05:43:47.840" UserId="2479" />
  <row Id="3646" PostId="3736" Score="0" Text="&quot;_After all a quad representation is more compact_&quot;. Surely not when compared to a triangle strip/fan." CreationDate="2016-07-08T11:05:57.873" UserId="209" />
  <row Id="3647" PostId="3742" Score="1" Text="If you google for corner pin you get a few implementations of this" CreationDate="2016-07-08T15:42:21.993" UserId="38" />
  <row Id="3649" PostId="3677" Score="0" Text="Not keeping anything abstract, I've uploaded my source. It has the problem in it" CreationDate="2016-07-09T08:08:44.407" UserId="4608" />
  <row Id="3650" PostId="3738" Score="0" Text="How much do you know about integral equations in general? Are you asking about an analytical solution to it?" CreationDate="2016-07-10T01:38:40.420" UserId="3477" />
  <row Id="3651" PostId="3740" Score="2" Text="@joojaa To my understanding, it's not that the rendering equation is impossible to solve in all cases, but rather that for any time it is solvable, it's of no practical use" CreationDate="2016-07-10T02:48:54.103" UserId="3477" />
  <row Id="3652" PostId="3740" Score="0" Text="@jaska it is solvable whenever there is no global effect in the lighting calculation. But we want to solve it because of the global lighting. So yes, whenever it is solvable it is pretty useless." CreationDate="2016-07-10T10:50:08.813" UserId="273" />
  <row Id="3655" PostId="3740" Score="3" Text="FYI the Mathjax syntax works on this StackExchange, so if you put $ signs around your identifiers they'll look all math-y." CreationDate="2016-07-11T02:08:31.183" UserId="182" />
  <row Id="3656" PostId="3597" Score="0" Text="@Ziya6161 I don't know but Unity3D may have what you need." CreationDate="2016-07-11T09:33:13.603" UserId="3437" />
  <row Id="3657" PostId="3725" Score="1" Text="Does the array length vary per-pixel?" CreationDate="2016-07-11T13:42:22.097" UserId="197" />
  <row Id="3658" PostId="3746" Score="0" Text="You need only 4 vertices for all cases. `1:-`  You could easily use nested `for loops` to draw the `case 0`. You could get 4 end points by simple maths and then draw lines and triangles between them for `case 1`, `case 2` respectively. `2:-` Or you can have the end points and then divide them up for `case 0`. I would probably do the latter." CreationDate="2016-07-11T19:08:10.910" UserId="3437" />
  <row Id="3659" PostId="3746" Score="2" Text="Your data is off. You do not account for winding rules." CreationDate="2016-07-11T20:00:48.447" UserId="38" />
  <row Id="3660" PostId="3746" Score="0" Text="@ritwiksinha Yeah the thing is I want it to be that many vertices." CreationDate="2016-07-12T07:19:59.593" UserId="2485" />
  <row Id="3661" PostId="3746" Score="0" Text="@joojaa What do you mean by &quot;data is off&quot;? I know some of the points are not in the right order I changed that, It does not fix the issue." CreationDate="2016-07-12T07:21:42.937" UserId="2485" />
  <row Id="3662" PostId="3718" Score="0" Text="@Nathan_Reed I applied most of your tips, except instead of doing MSAA which oddly enough did the trick with not to much of a hit at 2x. If I had a question about adding more highlights to to the current effect should I edit the question or make a new one?" CreationDate="2016-07-12T07:31:10.410" UserId="2308" />
  <row Id="3663" PostId="3718" Score="0" Text="@J.Doe Great, glad to hear it's working out! I'd say it's better to ask a new question about tweaking the effect if you have a specific look you're going for." CreationDate="2016-07-12T07:36:54.207" UserId="48" />
  <row Id="3664" PostId="3746" Score="0" Text="@Käsebrot Why don't you use section formula to calculate the points in between the 4 vertices, namely $(1, 1), (-1, 1), (-1, -1), (1, -1)$" CreationDate="2016-07-12T09:00:10.267" UserId="3437" />
  <row Id="3665" PostId="3725" Score="1" Text="How many bits are the integers? 32? 64?" CreationDate="2016-07-12T22:04:20.717" UserId="1781" />
  <row Id="3666" PostId="3726" Score="0" Text="Since the OP said &quot;array of integers&quot; without qualifying the number of bits, it perhaps seems safe to assume they mean a C-style 'int' (at least 32 bits). Does DDS support 32 bit integer channels?" CreationDate="2016-07-12T22:05:36.857" UserId="1781" />
  <row Id="3667" PostId="3726" Score="0" Text="Also note that since the OP did not specify that they were Windows-only, DDS may be a bit of a problem on other platforms, where few tools will properly read or write them." CreationDate="2016-07-12T22:06:42.153" UserId="1781" />
  <row Id="3668" PostId="3750" Score="0" Text="Thank you for your answer. I tried with disabling the depth test, it produces the same result. However, I would like to understand what do you mean by &quot;...drawing with a non-opaque alpha channel.&quot; For opacity I am relying on alpha channel of the texture." CreationDate="2016-07-13T05:00:10.230" UserId="3098" />
  <row Id="3669" PostId="3750" Score="0" Text="In setting the color of the texture in `transformValueToColor()`, you set the alpha value to `255 * (1 - tValue)`, which means that as long as `tValue` is not 1.0, will be somewhere between 0 and 255. An alpha value that is not 255 will produce a non-opaque (or semi-transparent) pixel - one you can see through. My point was that if the front-most geometry is semi-transparent, you must draw it after the farther away geometry or the depth test will cause the farther away geometry not to ever draw." CreationDate="2016-07-13T05:07:22.190" UserId="3003" />
  <row Id="3670" PostId="3750" Score="0" Text="I think I am drawing the furthest quad first and the nearest one last: `model = glm::translate(model, glm::vec3(0.0f, 0.0f, -index*0.003f));`. I think that should have done?" CreationDate="2016-07-13T05:25:50.370" UserId="3098" />
  <row Id="3671" PostId="3746" Score="0" Text="For doing the solid part, you're better off using something like TRIANGLE_STRIP. You will need topology information in the form of an indice buffer which tells the GPU how to interpret your list of points as a strip. Vertices will need be reused for TRIANGLE_STRIP so the indice buffer is necessary." CreationDate="2016-07-13T06:06:32.553" UserId="3073" />
  <row Id="3672" PostId="3726" Score="0" Text="According to `MSDN` DDS Format Overview:&#xA;&#xA;`Note that the DDS format supports any valid DXGI_FORMAT value...`&#xA;&#xA;so it support 32 bit integer.&#xA;But yes, DDS may be problem on other platforms." CreationDate="2016-07-13T11:27:12.340" UserId="3123" />
  <row Id="3673" PostId="3750" Score="0" Text="It depends on the other matrices, too, though. How are your view and projection matrices set?" CreationDate="2016-07-13T15:57:55.053" UserId="3003" />
  <row Id="3674" PostId="3750" Score="0" Text="I have added the details of `projection` and `view` matrix in the question above." CreationDate="2016-07-13T16:31:58.570" UserId="3098" />
  <row Id="3675" PostId="3750" Score="0" Text="I've added an edit with some additional information to my answer." CreationDate="2016-07-14T04:35:59.063" UserId="3003" />
  <row Id="3676" PostId="3725" Score="0" Text="array length vary per-pixel, 32 bit integers." CreationDate="2016-07-14T08:37:29.837" UserId="4700" />
  <row Id="3677" PostId="3714" Score="1" Text="You can use glGetAttribLocation(&quot;vposition&quot;); also to get the location without needing assignment in the shader. Although be careful with that, if your attributes are never referenced in your shader the entire attribute may be optimised away and you won't be able to get its location." CreationDate="2016-07-14T09:13:19.600" UserId="3073" />
  <row Id="3679" PostId="3755" Score="0" Text="Does that estimate the distance of the real world object?" CreationDate="2016-07-14T14:13:43.323" UserId="38" />
  <row Id="3680" PostId="3755" Score="0" Text="I'm not sure. I was trying to interpret the equation you put up. But I'll give it a try with a ruler when I get home tonight and post the results here." CreationDate="2016-07-14T15:55:02.413" UserId="3003" />
  <row Id="3681" PostId="3755" Score="0" Text="This was exactly what I wanted, a conversion system. A straight forward one from the sensor size is not good enough because cameras usually crop part of the image they receive.&#xA;Thankfully though most cameras will have something called pixel size (in microns usually) which is exactly the conversion measure needed for this!" CreationDate="2016-07-14T19:15:28.217" UserId="4735" />
  <row Id="3682" PostId="3751" Score="0" Text="For your last comment, do you mean that moving average should be used for the simulated accumulation buffer?  Aren't you missing a step where you draw a fullscreen black quad to &quot;erase&quot; the buffer lest it blow out to white?" CreationDate="2016-07-15T06:16:49.030" UserDisplayName="user3412" />
  <row Id="3684" PostId="3751" Score="1" Text="Actually, that should be taken care of by the moving average. If new pixel's value is:&#xA;&#xA;`gl_FragColor = weight * current + (1.0 - weight) * history`&#xA;&#xA;Then note that the contribution to the final image of a particular frame will simply approach zero over time by principle of diminishing weight, thus fading away.&#xA;&#xA;There is no need to clear the RT, then, just overwrite it with each ping-pong." CreationDate="2016-07-15T08:03:35.323" UserId="2817" />
  <row Id="3685" PostId="3758" Score="1" Text="You have no doubt read [this](http://stackoverflow.com/questions/540014/compute-the-area-of-intersection-between-a-circle-and-a-triangle) already? Would you be open to a montecarlo of say 16 samples?" CreationDate="2016-07-15T12:21:18.653" UserId="38" />
  <row Id="3686" PostId="3759" Score="0" Text="I betcha the size of the arc size can be computed with closest point on line that would make this very neat." CreationDate="2016-07-15T13:36:53.297" UserId="38" />
  <row Id="3687" PostId="3757" Score="1" Text="I would write an answer if I had time, but in a concise way: because of definition. Roughly speaking radiance measure OUTGOING light in a certain direction (or better: radiant flux per solid angle). Irradiance is INCOMING light from a certain direction (or better radiant flux received per unit area. BRDF is describing the ratio of *outgoing* light to *incoming* light" CreationDate="2016-07-15T21:06:23.883" UserId="100" />
  <row Id="3688" PostId="3760" Score="0" Text="Can you expand on what you mean by &quot;complex 3D objects&quot;? How are you representing the objects currently?" CreationDate="2016-07-16T05:26:07.157" UserId="48" />
  <row Id="3689" PostId="3759" Score="0" Text="I love the elegance of this approach. Does it need a special case approach when one of the vertices is inside the triangle? It looks like that could count part of the area twice." CreationDate="2016-07-16T12:25:00.307" UserId="231" />
  <row Id="3691" PostId="3751" Score="1" Text="Got it, thanks!" CreationDate="2016-07-16T17:06:33.667" UserDisplayName="user3412" />
  <row Id="3693" PostId="3760" Score="0" Text="Are you dealing only with closed surfaces with holes (such as a bagel or doughnut) or also with holes that lead to the interior of the surface (such as a sphere with a hole leading inside)?" CreationDate="2016-07-17T10:35:59.927" UserId="231" />
  <row Id="3694" PostId="3760" Score="0" Text="A given collection of points can be connected into a wide variety of different shape surfaces. Could you explain how these points are produced or how they relate to the intended surface?" CreationDate="2016-07-17T10:51:29.487" UserId="231" />
  <row Id="3697" PostId="3759" Score="0" Text="@joojaa This can indeed be computed using the sagitta, and there even is an approximate formular for this (Harris and Stocker 1998). See http://mathworld.wolfram.com/CircularSegment.html" CreationDate="2016-07-17T15:33:36.233" UserId="385" />
  <row Id="3698" PostId="3759" Score="0" Text="@trichoplax Indeed this method would fail if at least one vertex is inside the circle, and I can't guarantee it won't. Still looking for a nice solution there..." CreationDate="2016-07-17T15:34:33.203" UserId="385" />
  <row Id="3699" PostId="3766" Score="0" Text="It prserves vertical lines, that is true. But I am working wit Bézier surface (16 points in a plane) and I believe it must be doable just by moving these 16 points, without computing points on bézier curves etc. Here you can see, how Bézier surface works: http://philipandrews.org/sandbox/BezierSurface/bin/BezierSurface.swf" CreationDate="2016-07-19T15:02:23.283" UserId="4761" />
  <row Id="3700" PostId="3766" Score="0" Text="I think I found solution to that problem if I understood is correctly. You are asking how to deform bezier surface A to bezier surface B or C to D with d parameter being 0.8, is it correct?" CreationDate="2016-07-19T15:22:10.283" UserId="4524" />
  <row Id="3701" PostId="3766" Score="0" Text="Well it seems that is not the exact formula but quite close. I will ponder on this a little more. Formula is correct at least for on-the-curve-points." CreationDate="2016-07-19T15:32:48.593" UserId="4524" />
  <row Id="3702" PostId="3766" Score="0" Text="You are getting closer :) But for the second example, the Y coordinate changes, too.  As I mentioned, all points move along the lines, so it is enough to find the new position for d = 1 for each point, then I can interpolate linearly." CreationDate="2016-07-19T15:33:24.697" UserId="4761" />
  <row Id="3703" PostId="3766" Score="0" Text="I have added another picture, it may help you." CreationDate="2016-07-19T15:46:00.767" UserId="4761" />
  <row Id="3706" PostId="3767" Score="0" Text="See code above. We are not multiplying by world first. Even if you have a matrix stack it's always projection first if you were to put it in the stack. Maybe in your mind we are but in code we are not. If you were to write `a = b + c + d` no one would describe that as adding `d` first" CreationDate="2016-07-20T03:43:47.190" UserId="4766" />
  <row Id="3707" PostId="3767" Score="3" Text="Personally I would prefer more explicit naming conventions. For example:   &#xA;`worldToScreen = worldToView * viewToScreen` , or   &#xA;`screenPosition = worldToScreen * worldPosition` ..." CreationDate="2016-07-20T05:44:40.087" UserId="110" />
  <row Id="3709" PostId="3768" Score="0" Text="To add, this is usually seen when the light direction is near parallel to the surface (like a sunset scene). When the light direction is perpendicular the specular highlight becomes much more circular. I would check your normal calculation in color(..), it looks like it is producing a normal that isn't perpendicular to your plane. Something like {0,0,-1} should be correct and facing towards your {0,0,1} light, not {pos.x,pos.y,1} like it currently is" CreationDate="2016-07-20T07:01:38.030" UserId="3073" />
  <row Id="3711" PostId="3766" Score="0" Text="Oh, you basically solved it, as soon as I got time I will formulate it for you." CreationDate="2016-07-20T07:55:41.473" UserId="4524" />
  <row Id="3712" PostId="3767" Score="2" Text="Can we just blame this on the fact we multiply right-to-left while reading from left-to-right ?" CreationDate="2016-07-20T07:58:47.347" UserId="3073" />
  <row Id="3713" PostId="3766" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/42753/discussion-between-ivan-kuckir-and-cem-kalyoncu)." CreationDate="2016-07-20T09:22:11.523" UserId="4761" />
  <row Id="3714" PostId="3773" Score="0" Text="Thanks for your reply. I try this but I only have the ambiant color" CreationDate="2016-07-20T09:52:53.937" UserId="3328" />
  <row Id="3715" PostId="3773" Score="0" Text="Maybe flip the sign. I'm not sure what your intention for the normals is, but for a plane every pixel should have the same normal." CreationDate="2016-07-20T09:55:40.323" UserId="3073" />
  <row Id="3716" PostId="3773" Score="0" Text="Yes I try -1 and then 1. My purpose is to have a circular specular on the surface plane" CreationDate="2016-07-20T09:57:18.407" UserId="3328" />
  <row Id="3717" PostId="3773" Score="0" Text="The half-vector takes care of this normally, this will vary across your image. It looks like you are close to what you are intending, just something is on the wrong axis, at a guess. I encountered this problem and had similar results several times, was almost always down to incorrectly working out the eye-space/half-vector." CreationDate="2016-07-20T10:06:16.520" UserId="3073" />
  <row Id="3718" PostId="3771" Score="2" Text="Its called frustum culling. You can accelerate this with spatial querries like bspor occtrees for example." CreationDate="2016-07-20T10:13:01.630" UserId="38" />
  <row Id="3719" PostId="3773" Score="0" Text="Looking a bit more at your code example, the L vector calculation looks wrong. Should that not just be the world-space light vector?(Lum). Your lambert calculation would be something like dot( N={0,0,-1}, L={0,0,1} ) for the whole surface, which should return an intensity of 1 for the diffuse component." CreationDate="2016-07-20T10:17:42.097" UserId="3073" />
  <row Id="3721" PostId="3775" Score="1" Text="@trichoplax I removed the pic and added a GIF. Thanks." CreationDate="2016-07-20T11:30:25.487" UserId="4770" />
  <row Id="3729" PostId="3766" Score="0" Text="To your new formlas: so p(k) is always zero? You suggest moving along X the  same distance, as moving along Y, it can not work. You also suggest treating control points the same way, as corner points, which is wrong. It seems like you are just throwing at me what comes into your head. I doubt you can come up with the right formula without acutally implementing it and &quot;seeing the thing&quot;." CreationDate="2016-07-20T22:55:30.110" UserId="4761" />
  <row Id="3730" PostId="3748" Score="0" Text="If your glow effect is additive you don't need any kind of sorting." CreationDate="2016-07-21T02:01:30.597" UserId="3073" />
  <row Id="3733" PostId="3780" Score="0" Text="Normally you would use YUV semi-planar for this, which is a seperate single channel Y texture and a packed UV channel.  You could also unpack inside a fragment shader by testing if the Y texel you are reading is on a odd or even column. So say you read Y0,U for the first pixel (Assuming a 2 channel texture), you test if Y0 is on a even-column, which means you need to read the texel to right to get the missing V. For odd-columns you would read the texel to the left to get the missing U." CreationDate="2016-07-21T04:36:26.513" UserId="3073" />
  <row Id="3734" PostId="3650" Score="0" Text="Update: it actually had something to do with that, although it was a minor fault." CreationDate="2016-07-21T07:28:24.947" UserId="3472" />
  <row Id="3735" PostId="3780" Score="0" Text="Just did a little googling, you want NV12 pixel format in Microsoft Media Foundation if you want the semi-planar format. Should be the fastest format for doing colour space conversion on the GPU." CreationDate="2016-07-21T09:04:57.653" UserId="3073" />
  <row Id="3743" PostId="3775" Score="0" Text="Looks like a palette cycling effect. The image itself is static." CreationDate="2016-07-20T13:19:57.157" UserId="3073" />
  <row Id="3745" PostId="3751" Score="1" Text="Isn't this the same as Alpha blending?" CreationDate="2016-07-22T06:38:39.143" UserId="3073" />
  <row Id="3747" PostId="3751" Score="0" Text="As long as you don't need to accumulate alpha, it is so indeed, since you can put weight in the alpha channel." CreationDate="2016-07-22T12:30:30.760" UserId="2817" />
  <row Id="3748" PostId="3780" Score="0" Text="@PaulHK The camera only supports YUY2 and MJPG formats. I tried unpacking in the shader - the results were close to correct but there were vertical lines. I assume this is because I am incorrectly fetching the next pixel: `texture(tex, vec2(Texcoord.x + 1, Texcoord.y));` I'm guessing this is due to scaling." CreationDate="2016-07-22T19:57:25.430" UserId="4497" />
  <row Id="3749" PostId="3757" Score="0" Text="The short answer is: _&quot;because then it wouldn't be bidirectional&quot;_. It's been a while, but I believe my [alternate formulation](http://geometrian.com/programming/tutorials/clearer_re/index.php) of the rendering equation works out to be using a 1:1 reflectance function." CreationDate="2016-07-22T21:13:54.303" UserId="523" />
  <row Id="3754" PostId="3755" Score="0" Text="Focal length is also affected by your camera focus. so in the course of shooting the focal length varies slightly because that is how the focus is achieved, by moving the elements. This is why you use lots of measurements and a ransac on the image to estimae the actual values better. So not only is your effective value different from spec, its diffenet in each camera and in each shot. Also your ruler may not be perfectly aligned to camera and the camera.lense not being a perfect pinhole etc." CreationDate="2016-07-24T19:44:46.350" UserId="38" />
  <row Id="3756" PostId="3780" Score="0" Text="It might be simpler to deinterleave it into 2 buffers on the CPU ( Y &amp; CrCb ), then the GPU can do CSC." CreationDate="2016-07-25T02:25:10.113" UserId="3073" />
  <row Id="3758" PostId="3652" Score="0" Text="The original paper on bilateral filtering, _&quot;Bilateral Filtering for Gray and Color Images&quot;_, by Tomasi and Manduchi: https://users.cs.duke.edu/~tomasi/papers/tomasi/tomasiIccv98.pdf" CreationDate="2016-07-26T05:30:40.837" UserId="182" />
  <row Id="3759" PostId="3791" Score="1" Text="For something more interesting you can animate the origin of radial shapes too. -&gt; float d = sin(length(uv - vec2(0.5)) * 35.0) + sin(length(uv - vec2(0.2+sin(iGlobalTime),0.3)) * 45.0);" CreationDate="2016-07-26T08:21:40.353" UserId="3073" />
  <row Id="3760" PostId="3785" Score="0" Text="I am not sure I understand it correctly, your idea is to compute screen space positions via vertex shader, save it to some texture, and then read the texture and make computations on CPU or via GPGPU if possible? computing everything on GPGPU and sending it to OpenGL would be better?  Currently, I am trying to figure out the architecture and frameworks to use, not the algorithm to use, but the part with SIMD approach is interesting, thank you for that." CreationDate="2016-07-26T08:39:52.783" UserId="4765" />
  <row Id="3761" PostId="3755" Score="0" Text="Maybe you can perform several experiments on your ruler to calibrate the parameters of your camera. Or just use samples to estimate parameters directly by regression." CreationDate="2016-07-26T16:29:52.933" UserId="120" />
  <row Id="3766" PostId="3786" Score="1" Text="Yes, I've decided it's best to unpack the data on the CPU (a task that lends itself well to multi-threading), then do color conversion in a shader." CreationDate="2016-07-27T14:20:20.693" UserId="4497" />
  <row Id="3767" PostId="3785" Score="0" Text="The example alg I gave could be performed entirely within a single OpenCL kernel or OpenGL compute shader. OpenCL can directly share data with OpenGL via OpenGL/OpenCL interop, while OpenGL can share data among its shaders. (Data = buffer or texture.) The CPU need never be involved, though you may find using OpenCL on CPU faster if you have too few objects on screen to overcome bridge overhead." CreationDate="2016-07-27T19:22:14.983" UserId="58" />
  <row Id="3768" PostId="3785" Score="0" Text="Updated my answer with a graphics pipeline example." CreationDate="2016-07-27T20:24:27.957" UserId="58" />
  <row Id="3769" PostId="3795" Score="2" Text="Why you want to use GPGPU for drawing million cubes in first place?" CreationDate="2016-07-28T06:39:57.177" UserId="3123" />
  <row Id="3770" PostId="3773" Score="0" Text="I edited my post. I change the way to compute L and E vectors and I get some inverse specular lobe" CreationDate="2016-07-28T10:12:28.170" UserId="3328" />
  <row Id="3771" PostId="3795" Score="2" Text="Compute shaders are used for compute workloads, not rendering.&#xA;&#xA;The rule of thumb is that if you need rasterization (i.e. processing of triangulated geometry into pixels), you should be using the rendering pipeline; if you simply need to process a large piece of data, you should be using compute.&#xA;&#xA;I'm also interested in sound arguments for and against compute shaders and CUDA/OpenCL (with graphics API interop). One that I've heard of is better queueing of compute workloads with the compute-specific API, but I'd like to know more (i.e. how does async compute come into the picture)." CreationDate="2016-07-28T10:20:26.327" UserId="2817" />
  <row Id="3772" PostId="3795" Score="0" Text="@Derag Just trying to feed my cube fetish as fast as possible" CreationDate="2016-07-28T13:44:09.133" UserId="4820" />
  <row Id="3774" PostId="3768" Score="0" Text="It appears that the new images that have replaced the old ones show a new and unrelated problem. Would this be better posted as a new question, so that the old problem is still here for future readers? There is some [discussion on meta](http://meta.computergraphics.stackexchange.com/a/182/231) about this general point." CreationDate="2016-07-29T10:48:23.123" UserId="231" />
  <row Id="3776" PostId="3799" Score="0" Text="For reference: Previously asked 2 hours earlier [on an external site](http://www.gamedev.net/topic/680892-does-anyone-know-what-el-means-in-the-context-of-shading-equations-from-the-book-real-time-rendering/)" CreationDate="2016-07-29T13:20:42.510" UserId="231" />
  <row Id="3777" PostId="3799" Score="0" Text="I've edited the title to include MathJax to show the subscript. If anyone finds this causes a problem please contribute to the [discussion on meta](http://meta.computergraphics.stackexchange.com/questions/238/should-we-use-mathjax-latex-in-question-titles)." CreationDate="2016-07-29T13:33:51.560" UserId="231" />
  <row Id="3778" PostId="3797" Score="0" Text="It can bypass the OS / driver" CreationDate="2016-07-29T15:36:48.320" UserId="310" />
  <row Id="3779" PostId="3797" Score="0" Text="@RichieSams: Interesting; does that mean in principle someone could achieve much the same result by accessing a dedicated PCIe SSD over the system bus?  (I don't know to what extent main system PCIe traffic needs OS support for devices to do any communication, or whether they can do it more autonomously once set up)." CreationDate="2016-07-29T15:55:55.073" UserId="4824" />
  <row Id="3780" PostId="3801" Score="1" Text="Should the colors be equally spaced in terms of t? Or do you need some more general solution for other distributions also?" CreationDate="2016-07-29T23:07:52.117" UserId="273" />
  <row Id="3781" PostId="3801" Score="0" Text="Do you require the same colour value for t=0 and t=1? That is, must the colour change wrap smoothly at the end points?" CreationDate="2016-07-30T13:50:41.707" UserId="231" />
  <row Id="3782" PostId="3804" Score="0" Text="The final image appears to be using subpixel rendering (note the coloured fringes to the left and right are different colours). Do you want to use this approach (which requires knowing the pixel geometry of the particular screen you are rendering to) or do you just want the best approach for pixel rendering, ignoring subpixels (which can be displayed on any screen)?" CreationDate="2016-07-30T21:48:56.747" UserId="231" />
  <row Id="3783" PostId="3804" Score="1" Text="@trichoplax I think for right now I'd be happy with the best approach for pixel rendering, ignoring subpixels (which sounds more complicated). One step at a time. :)" CreationDate="2016-07-30T22:11:29.727" UserId="2507" />
  <row Id="3784" PostId="3804" Score="0" Text="That sounds like a good approach - I just wanted to have it stated explicitly so answerers don't get distracted into explaining the last image." CreationDate="2016-07-30T22:30:51.197" UserId="231" />
  <row Id="3785" PostId="3803" Score="0" Text="[Here is a link to the project in github](https://github.com/amdreallyfast/render_particles_2D_basic_GPU/).  It includes all necessary 3rd party libraries and header files, so it should simply build and run after downloading. &#xA;&#xA;Note: This was built in VS2015 express, but I built it such that it should not need any project-specific setup.  All linking is performed in the source file (should never be done for release code, but this is a barebones example program), so a non-VS2015 setup shouldn't be difficult." CreationDate="2016-07-30T22:38:05.777" UserId="4838" />
  <row Id="3786" PostId="3804" Score="0" Text="Can you explain what you mean by bilinear filtering in this context? I don't understand how that concept applies to rasterizing the glyph shapes. Also, when you tried &quot;setting the alpha value at the edges according to the pixel coverage of the outline&quot;, how did you accomplish that? Supersampled rasterization?" CreationDate="2016-07-30T22:39:33.053" UserId="48" />
  <row Id="3787" PostId="3804" Score="0" Text="@NathanReed Sure. For bilinear filtering, I sweep over the raw bitmap and for each pixel, look at the three pixels surrounding it (the one to the right, the one below, and the one diagonally down and to the right) and average them together to come up with the final intensity value. Pixels off the edge of the bitmap are treated as 0." CreationDate="2016-07-31T00:07:49.193" UserId="2507" />
  <row Id="3788" PostId="3804" Score="0" Text="@NathanReed For the pixel coverage, I use the intersection point and the pixel coordinate to determine how much of the pixel is &quot;covered&quot;. i.e. Let's say we're at pixel position (4, 2). The intersection at this location with the curve or line is (4.6, 2.3). `xf = 4.6 - 4`, `yf = 2.3 - 2`, so pixel coverage is then: `coverage = (xf + yf) / 2.0;` This might be a naive approach, but it was worth a try. :)" CreationDate="2016-07-31T00:12:09.983" UserId="2507" />
  <row Id="3789" PostId="3803" Score="0" Text="is uDeltaTimeSec none-zero? If you are drawing something then we know you are outputting _position correctly. So I would suspect some how the delta is 0" CreationDate="2016-07-31T10:25:02.683" UserId="3073" />
  <row Id="3790" PostId="3805" Score="0" Text="Why would scaling the billboard quad not mimic the appearance of scaling the asteroid mesh? If they're far away enough to use billboards in the first place, that should work fine, unless I'm missing something." CreationDate="2016-07-31T16:37:09.927" UserId="48" />
  <row Id="3791" PostId="3804" Score="0" Text="Every os has its own way of dealing with this. Windows uses hinting while OSX does not for example." CreationDate="2016-07-31T18:05:05.643" UserId="38" />
  <row Id="3792" PostId="3804" Score="0" Text="@joojaa Hinting has to do with adjusting the glyph outlines when scaled down to a small size to best maintain the font's characteristics while ensuring readability. Hinting doesn't have anything to do with anti-aliasing or font smoothing directly does it? Because that's handled during scan conversion." CreationDate="2016-07-31T19:36:09.697" UserId="2507" />
  <row Id="3793" PostId="3805" Score="0" Text="Are you concerned about scaling up a billboard to the point that it requires a different level of detail?" CreationDate="2016-07-31T19:56:34.817" UserId="231" />
  <row Id="3794" PostId="3806" Score="1" Text="This is great, thank you! I have looked at the source of stb_truetype, but my knowledge in computer graphics isn't thorough enough to really know what was happening on a theoretical level. I was not aware of the article he posted on it though, so that's a great help and will help clarify the process he uses, I'm sure. I'm glad to hear that maybe I was on the right track with the pixel coverage method, but I just wasn't taking into account the vertical part. :)" CreationDate="2016-08-01T03:38:26.250" UserId="2507" />
  <row Id="3795" PostId="3773" Score="0" Text="Getting close, maybe negate the half vector ?" CreationDate="2016-08-01T06:41:57.193" UserId="3073" />
  <row Id="3796" PostId="3808" Score="1" Text="Could you explain the difference in what you are looking for here, compared to your [previous question](http://computergraphics.stackexchange.com/q/2271/231)? Is this a different approach?" CreationDate="2016-08-01T13:27:21.677" UserId="231" />
  <row Id="3797" PostId="3810" Score="0" Text="Would sampling half pixels give equivalent results to generating an image at double resolution, producing lines 2 pixels wide, and then scaling the image down to the intended size?" CreationDate="2016-08-01T16:01:03.243" UserId="231" />
  <row Id="3798" PostId="3810" Score="1" Text="Couldn't I just use the sign of the dot-product (i.e. gx or gy) in any way? For example, when sampling a horizontal edge, i get -4 for gx on one side of the edge and 4 on the other side." CreationDate="2016-08-01T18:49:32.863" UserId="3377" />
  <row Id="3799" PostId="3801" Score="0" Text="@Dragonseel.. I think i can make them to be equally spaced out.. but I would definitely be interested in hearing about a more general solution as well.." CreationDate="2016-08-01T18:58:23.330" UserId="4830" />
  <row Id="3800" PostId="3801" Score="0" Text="@trichoplax.. i don't really need them to be wrapped around.. would it be different than having the same colors at the end points of the array?" CreationDate="2016-08-01T18:59:38.477" UserId="4830" />
  <row Id="3801" PostId="3801" Score="0" Text="@lokstok good point - I think that covers it so it shouldn't make a difference to the solution." CreationDate="2016-08-01T19:08:31.807" UserId="231" />
  <row Id="3803" PostId="3803" Score="0" Text="@PaulHK I am sending a non-zero value by glUniform1fv, so it should be." CreationDate="2016-08-01T23:42:00.530" UserId="4838" />
  <row Id="3804" PostId="3814" Score="0" Text="Isn't it a bit restrictive? I mean, IMO, the texture is not the best-suited data storage for passing data in general (but I am new to computer graphics and maybe I have to get used to it)." CreationDate="2016-08-02T06:28:33.867" UserId="4765" />
  <row Id="3805" PostId="3814" Score="2" Text="A texture is just a normal buffer, although in the fragment shader we use fancy sampler2D to read it with filtering/etc. You can use a compute shader to read it more in its raw form and skip the texture sampler stage." CreationDate="2016-08-02T06:30:21.073" UserId="3073" />
  <row Id="3806" PostId="3797" Score="0" Text="@timday partially but there would still be overhead as you would need to wait for your turn form the graphic contriollers" CreationDate="2016-08-02T07:45:03.430" UserId="38" />
  <row Id="3807" PostId="3794" Score="0" Text="I've made some minor edits, and edited the code to match. Hopefully this leaves the answer as you intended it but feel free to revert anything that doesn't." CreationDate="2016-08-02T12:47:41.790" UserId="231" />
  <row Id="3809" PostId="3810" Score="0" Text="Just one more question: Would morphological erosion (http://homepages.inf.ed.ac.uk/rbf/HIPR2/erode.htm) or thinnening (http://homepages.inf.ed.ac.uk/rbf/HIPR2/thin.htm) be an alternative?" CreationDate="2016-08-02T22:46:13.750" UserId="3377" />
  <row Id="3810" PostId="3810" Score="0" Text="@enne87 why not skip edge detection alltogether and use edge geometry to draw the lines?" CreationDate="2016-08-03T01:35:57.573" UserId="38" />
  <row Id="3811" PostId="2271" Score="0" Text="I'm not sure Moore's law applies to GPU, you can keep adding cores and using wider buses." CreationDate="2016-08-03T04:43:36.190" UserId="3073" />
  <row Id="3813" PostId="3808" Score="0" Text="@trichoplax, thanks for your comment. It is same. I'm trying to implement VR in Cloud rendering. But I have to decide which way. Rendering in single server would be easier than distributed parallel rendering, but there are still 2 ways to achieve that: rely on SLI driver or use equalizer. It isn't flexible to do all things in one server, in terms of resource sharing and hardware sourcing. If we can achieve distributed parallel rendering, maybe we can choose PowerVR or Mali GPU, not NVIDIA/AMD. I know concerns on network latency, which we have solutions to handle. So here I don't discuss that." CreationDate="2016-08-03T06:10:46.907" UserId="3058" />
  <row Id="3814" PostId="3810" Score="0" Text="@enne87: yes I think erosion could be a possibility, but I didn't suggest it because I'm not familiar enough with it so I don't know how well suited it is." CreationDate="2016-08-03T06:14:15.107" UserId="182" />
  <row Id="3815" PostId="3811" Score="0" Text="thanks for your answer. I have read this article before. Do you mean distributed parallel rendering will introduce much latency? I just want to lower down the rendering latency." CreationDate="2016-08-03T06:18:12.910" UserId="3058" />
  <row Id="3816" PostId="3810" Score="0" Text="Thank you both! @joojaa: I thought about it but how would I draw the outline of a sphere with geometrical data?" CreationDate="2016-08-03T08:55:50.733" UserId="3377" />
  <row Id="3817" PostId="3810" Score="0" Text="@enne87 you draw the edges on the culling treshold and hard edges slightly offset towards the camera. A sphere is in fact one of the easier shapes." CreationDate="2016-08-03T09:10:08.330" UserId="38" />
  <row Id="3818" PostId="3811" Score="0" Text="Yes. Distributed rendering has the inherent cost of integrating results from all the distributed nodes. In this case, I imagine serializing a scene, sending it out to computation nodes and then collecting parts of the resulting image (frame buffer tiles probably?). It seems unlikely to me that at current I/O speeds you can fit all of that + the rendering itself under ~10 ms. What I would expect from such a scenario is that you could probably get silky-smooth, high-performance rendering, but lagging a consistent dozen frames behind the player input, therefore somewhat defeating the purpose." CreationDate="2016-08-03T10:29:38.290" UserId="2817" />
  <row Id="3819" PostId="3810" Score="0" Text="@joojaa: Thanks for your suggestion. Just to make clear that I understand your idea: &#xA;&#xA;1. For the silhouette I render the edges of the backfaces and move the faces in screen Z forward so that only the edges of the backfacing triangles are visible, right?&#xA;&#xA;2. Sorry but I don't understand how I would draw the inner edges (creases and ridges) ?" CreationDate="2016-08-03T10:30:47.260" UserId="3377" />
  <row Id="3820" PostId="3810" Score="0" Text="@enne87 edges on the front backface boundary prefeerably. You draw them as gl lines." CreationDate="2016-08-03T11:01:01.980" UserId="38" />
  <row Id="3821" PostId="3810" Score="0" Text="Ok I'll try that, thanks for your help!" CreationDate="2016-08-03T12:58:09.517" UserId="3377" />
  <row Id="3822" PostId="3815" Score="0" Text="Yes, normal at witness point P must have same direction as vector S-P (where S is slave point, or red one on the picture above), but surface can have several such points (If it's folded, and point is inside), and we will have to choose closest from ones with &quot;good&quot; normal. Anyway thanks for answer." CreationDate="2016-08-03T13:33:50.740" UserId="2644" />
  <row Id="3823" PostId="3811" Score="0" Text="Sort-last may have this issue as what you said, but how about sort-first? Use 2d of sort-first, it seems that few interactions are introduced  between rendering worker. According to Equalizer evaluation result, it consumes little network bandwidth: http://eyescale.github.io/equalizergraphics.com/scalability/2D.html. Thanks" CreationDate="2016-08-04T00:29:23.813" UserId="3058" />
  <row Id="4823" PostId="2271" Score="0" Text="@PaulHK, the main issue is power and thermal. It's very hard to make a very big chip, especially for mobile system." CreationDate="2016-08-04T03:14:42.310" UserId="3058" />
  <row Id="4824" PostId="3811" Score="0" Text="But I never even mentioned interaction between workers. :) It's entirely possible to eliminate it altogether. By synchronisation I meant dispatching the workload to nodes and then compositing the results together. Also, &quot;little&quot; network bandwidth is a pretty imprecise and relative term – I expect it not to be &quot;little&quot; on the scale that VR applications operate in. It's quite simple, really – if you can make a round trip within 10 ms, you're fine. Experience simply tells me that you can't. You are welcome to prove me wrong. :)" CreationDate="2016-08-04T07:19:26.580" UserId="2817" />
  <row Id="4837" PostId="3808" Score="0" Text="I get the feeling what you want/need is an open minded discussion, not a definite answer." CreationDate="2016-08-05T17:13:59.430" UserId="3041" />
  <row Id="4838" PostId="3814" Score="0" Text="There's a few options. Render-to-texture is probably the most straightforward but also kind of wasteful unless you need the results of your calculations in screen-space (like in deferred rendering), since you're rasterizing geometry before you can do any calculation. There's transform feedback, where you pull the output of a vertex or geometry shader before it hits the rasterizer and write it back to a buffer instead, might be good for the use case you mentioned but I haven't tried it myself. Otherwise compute shaders are the way to go, they're not that scary once you get used to them ;-)" CreationDate="2016-08-05T17:38:24.513" UserId="1937" />
  <row Id="4839" PostId="3795" Score="0" Text="If you want to draw a million cubes, use ray-marching :)" CreationDate="2016-08-05T17:53:23.497" UserId="1937" />
  <row Id="4840" PostId="3808" Score="0" Text="Keep in mind that transfer of information between computers isn't free.  For instance, even getting data from RAM to the GPU is a pretty costly operation, let alone from another machine's RAM, even on the local network.  Transferring data between GPUs is also not free.  VR requires very low latency frames (not just high frame rate), and what you are proposing sounds like it would add quite a bit of latency and may not be practical due to that." CreationDate="2016-08-05T18:09:58.597" UserId="56" />
  <row Id="4841" PostId="3823" Score="1" Text="I'm not sure, but if you scan a document multiple times with slightly different tiny offsets, resulting in slightly different scans of the same document, you should be able to use that information to do Monte Carlo style sampling and get a better result of your scans.&#xA;&#xA;If you don't get any good answers here, another venue that is likely to be able to help you is the digital signal processing stack exchange: http://dsp.stackexchange.com/" CreationDate="2016-08-05T18:12:32.563" UserId="56" />
  <row Id="4842" PostId="3823" Score="1" Text="Could you add more detail about the characteristics of the documents being scanned? At present it's difficult to judge whether this is on topic. For example, if the documents have arbitrary sized fonts and images, the task will be different than if the documents have a single small font size with no images." CreationDate="2016-08-05T20:04:50.620" UserId="231" />
  <row Id="4843" PostId="3825" Score="0" Text="Your question header seems to ask a different question than the body of your question. So which is it? How to sample points or how many do you need to sample?" CreationDate="2016-08-06T16:13:07.167" UserId="38" />
  <row Id="4844" PostId="3760" Score="0" Text="Would using some algorithm like [marching cubes](https://en.wikipedia.org/wiki/Marching_cubes) over a density field work for you? You could populate a density field with contributions from the points and then sample from that." CreationDate="2016-08-06T22:10:58.320" UserId="174" />
  <row Id="4845" PostId="165" Score="0" Text="I personally would guess that the garish result is taking the subpixel samples further apart than the nice result, which means that there's a more dramatic difference in coverage and therefore brightness." CreationDate="2016-08-06T22:23:25.930" UserId="174" />
  <row Id="4846" PostId="3825" Score="0" Text="@joojaa Sorry for my unclear description. My question originates from this [question](http://mathematica.stackexchange.com/questions/122056/specialized-sampling-techniques-of-built-in-bsplinesurface). For example, in Mathematica, I could use `ParametricPlot3D[f[x, y], {x, 0, 1}, {y, 0, 1}]` to draw a B-spline surface." CreationDate="2016-08-07T01:51:08.737" UserId="1796" />
  <row Id="4847" PostId="1720" Score="0" Text="What software did you use in the screenshot? THX:)" CreationDate="2016-08-07T02:50:31.993" UserId="1796" />
  <row Id="4848" PostId="3827" Score="0" Text="Are you talking specifically about OpenGL? Specifically about WebGL? Is instancing available for you?" CreationDate="2016-08-07T08:06:15.780" UserId="457" />
  <row Id="4849" PostId="3823" Score="1" Text="Can you even add an example of such artifact so we can have a look at it?" CreationDate="2016-08-07T12:06:21.213" UserId="182" />
  <row Id="4850" PostId="3827" Score="1" Text="Good question, let me clarify that. I'm not particularly interested in a specific graphics API, although I do want the work done on the GPU." CreationDate="2016-08-07T14:26:07.770" UserId="174" />
  <row Id="4851" PostId="3825" Score="0" Text="As joojaa points out, the question in your title is different from the rest of your question. Are you looking to only sample uniformly, and adjust only the size of the sample, or are you also interested in sampling in different ways, that may reduce the number of points required?" CreationDate="2016-08-07T16:11:39.040" UserId="231" />
  <row Id="4852" PostId="3811" Score="0" Text="ok, thanks. let me try:)" CreationDate="2016-08-08T00:38:18.243" UserId="3058" />
  <row Id="4853" PostId="3808" Score="0" Text="Ok, get it. We still want to try. At last, we have to try multi gpus in one server. Thanks." CreationDate="2016-08-08T00:41:02.663" UserId="3058" />
  <row Id="4854" PostId="3825" Score="0" Text="@trichoplax Obviously, the *uniformly sample* is **not** a good method. My confusion is whether exist a **specialized algorithm** about sampling 3D-points and visualization for the B-spline surface. Please see this [screenshots](http://i.stack.imgur.com/BmCqn.png)" CreationDate="2016-08-08T01:15:01.013" UserId="1796" />
  <row Id="4855" PostId="3829" Score="0" Text="Could point sprites be used in place of 2x triangle vertices? The instance data could be a vec2 to define both the orientation and the size. Although you might need another float in there for rectangular sprites." CreationDate="2016-08-08T03:00:39.677" UserId="3073" />
  <row Id="4856" PostId="3829" Score="1" Text="@PaulHK Point sprites are very limited—they sometimes have a small maximum size depending on GPU, and they don't support rotation or any non-square shapes. Also, they get clipped based on their center point, so they pop out too soon when they move off the edge of the screen. I would not bother with them." CreationDate="2016-08-08T03:32:17.280" UserId="48" />
  <row Id="4857" PostId="3829" Score="0" Text="Yeah I thought they would be a little inflexible, you could rotate in the frag shader but then you need to over-project the size to fit the diagonal.." CreationDate="2016-08-08T03:51:36.223" UserId="3073" />
  <row Id="4858" PostId="3824" Score="1" Text="Your ASCII diagram is cool" CreationDate="2016-08-08T04:23:50.967" UserId="3386" />
  <row Id="4859" PostId="3831" Score="2" Text="Most software define chord height as a subdivision criteria along with angle error (which is what you describe... ) . Reason being  that usually in engineering solutions the tolerance is usually for deflection from true surface not how well the curvature is preserved. But there is many ways to skim the fish depenzing on what your needs are. Simple UV division can be bad for multipatched /  trimmed surfaces." CreationDate="2016-08-08T04:29:27.650" UserId="38" />
  <row Id="4860" PostId="3832" Score="0" Text="I've [updated the diagram](http://geometrian.com/programming/tutorials/gltextureterm/index.php); can you check it? Also, perhaps you could elaborate on the &quot;context&quot; for &quot;mipmap chain&quot;?" CreationDate="2016-08-08T05:56:02.177" UserId="523" />
  <row Id="4861" PostId="3823" Score="0" Text="The problem is that i cannot access the physical document, I only have access to the output of the scan. &#xA;Most of the documents are text or ID Card.&#xA;I cannot had an example because all of the documents that I have access are confidencial." CreationDate="2016-08-08T07:48:40.930" UserId="4866" />
  <row Id="4862" PostId="3829" Score="0" Text="I've seen it commented before that instancing can be slow if you're dealing with very small meshes (like a single pair of triangles). Can you comment on that at all?" CreationDate="2016-08-08T19:14:12.373" UserId="174" />
  <row Id="4863" PostId="3829" Score="0" Text="@porglezomp Hmm, I haven't heard that before, and in fact my experience is the opposite: instancing is a _more_ efficient way to draw small meshes, as it allows the GPU to pack verts from different instances together in each warp/wave. AFAIK, verts from different draws can't be packed together that way." CreationDate="2016-08-08T19:19:12.977" UserId="48" />
  <row Id="4864" PostId="3834" Score="0" Text="Did you try the smooth conductor?" CreationDate="2016-08-08T19:46:47.540" UserId="38" />
  <row Id="4865" PostId="3829" Score="0" Text="@NathanReed I didn't mean as opposed to individual draw calls, just in contrast with larger meshes. If you've had success using instancing on such small meshes, then it seems like it'd be worth trying since it's a really clean solution." CreationDate="2016-08-08T20:53:01.457" UserId="174" />
  <row Id="4866" PostId="3832" Score="0" Text="Yeah, so by context, i mean that the 'mipmap chain' could refer just to the image pyramid for a single layer-face, or for all faces in a layer, or for the whole thing.  Which is meant will depend on what is being discussed.&#xA;&#xA;So, for the purposes of your ASCII chart, where you have &quot;mipmap / mipmap chain&quot; grouped, I'd qualify that as the &quot;layer-face mipmap chain&quot;" CreationDate="2016-08-08T22:29:54.770" UserId="3386" />
  <row Id="4867" PostId="3835" Score="0" Text="Already tried that case unfortunately.. it's the sphere on the image you can find here: http://imgur.com/a/2unOE . Which seems quite metal-ish but not really a mirror unfortunately" CreationDate="2016-08-08T23:02:33.933" UserId="3069" />
  <row Id="4868" PostId="3835" Score="0" Text="Hm. Weird. What kind of integrator are you using to render this image? Maybe share your hole scene xml file so I can try to reproduce it?" CreationDate="2016-08-08T23:22:30.820" UserId="1930" />
  <row Id="4869" PostId="3835" Score="0" Text="I am using &lt;integrator type=&quot;path&quot;/&gt; . I downloaded a scene from the website of the Mitsuba renderer itself, implementing a simple cornell box. I just added there my simple sphere. You can find the whole XML here (https://www.dropbox.com/s/qzjuzyg3bont7od/cbox.xml?dl=0) and at the end of the file my simple added sphere.. thanks in advance!" CreationDate="2016-08-08T23:29:55.183" UserId="3069" />
  <row Id="4870" PostId="3835" Score="0" Text="Here is a rendering using your scene: https://www.dropbox.com/s/k1s825tkus2s4yk/tarta.png?dl=0&#xA;Looks good to me.&#xA;Did you render the scene? Your image looks a lot like the OpenGL preview at the start..." CreationDate="2016-08-08T23:41:32.327" UserId="1930" />
  <row Id="4871" PostId="3835" Score="0" Text="I am ashamed..you are totally right. I thought it was already the render itself, while it was just a preview. I am sorry.." CreationDate="2016-08-08T23:44:47.603" UserId="3069" />
  <row Id="4872" PostId="3835" Score="0" Text="No worries. Glad to help :)" CreationDate="2016-08-08T23:45:26.873" UserId="1930" />
  <row Id="4873" PostId="3835" Score="0" Text="thanks once mroe! One last question: I don't know if you are familiar with the GDPT algorithm that Mitsuba renders.. but I was trying to understand if it is realtime and I was basing my judgement on the preview.. thinking wrongly that it was real-time. Then you made me realise that I have actually to render the image, and the real-time thing seems going away. Do you know by any chance anything about this?" CreationDate="2016-08-08T23:47:38.850" UserId="3069" />
  <row Id="4874" PostId="3835" Score="0" Text="No sorry, I'm not familiar with GDPT at all.. didn't even know this was implemented in Mitsuba." CreationDate="2016-08-09T00:11:34.623" UserId="1930" />
  <row Id="4875" PostId="3835" Score="0" Text="no worries and thanks a lot anyway :)" CreationDate="2016-08-09T08:12:23.993" UserId="3069" />
  <row Id="4878" PostId="3841" Score="0" Text="Thanks Nathan!  I'm attempting to simulate a LIDAR scanner, so that's the reason for the fan beam and the equal angle spacing.  The particular scanner I'm modeling my simulation after has its lasers in the fan beam configuration external to the housing, but as the returns pass in they are diverted from a fan beam into parallel beams (by way of a lens).  I'm trying to find a way to maintain this model so as to make the simulation as accurate as possible.  Any thoughts as to how I could do this?" CreationDate="2016-08-10T15:19:47.513" UserId="4886" />
  <row Id="4879" PostId="3845" Score="0" Text="Here is a YouTube video also explaining that the stored value in an image is the 1/2.2 (but he simplifies it and just say that it's the square root, there is a disclaimer of that in text.)&#xA;Computer Color is Broken - MinutePhysics &#xA;[link](https://www.youtube.com/watch?v=LKnqECcg6Gw)" CreationDate="2016-08-10T15:22:55.063" UserId="2736" />
  <row Id="4880" PostId="3845" Score="0" Text="@KristofferHelander Yes, but he says it's only the camera that stores the values as square roots and only the display that squares it back. Image editing software like Photoshop then just open and save the image with no conversions, which results in artifacts shown in the video when you try to blend colors." CreationDate="2016-08-10T15:33:50.350" UserId="3470" />
  <row Id="4882" PostId="3841" Score="0" Text="@Irongrave Sorry, I don't know much about LIDAR scanners. Do you have a diagram or something of the setup with the lens you mention? I'm having a hard time visualizing it." CreationDate="2016-08-10T20:54:14.940" UserId="48" />
  <row Id="4883" PostId="3846" Score="0" Text="How do you tell Photoshop to save an image with the gamma baked in as you said above: _&quot;And so does Photoshop if you tell it so.&quot;_" CreationDate="2016-08-10T21:05:59.033" UserId="2736" />
  <row Id="4884" PostId="3841" Score="0" Text="Here's a picture from the manual:  (http://imgur.com/a/czhpt)" CreationDate="2016-08-11T01:16:47.347" UserId="4886" />
  <row Id="4886" PostId="3845" Score="0" Text="Correct me if I'm wrong on this. When I draw a gradient ramp in Photoshop that looks linear to me on the screen the values in that ramp actually are linear. But Photoshop adds a viewing gamma of 1/2.2 since the monitor adds a gamma of 2.2, thus the images pixel values are linear and they also look linear. But this 1/2.2 gamma is only a viewing gamma that Photoshop use for displaying the image, it is not baked into to image, and therefore no gamma correction to the image need to be done in the 3D software, (use gamma 1.0) since the values already are linear?" CreationDate="2016-08-11T13:31:03.027" UserId="2736" />
  <row Id="4887" PostId="3845" Score="0" Text="@KristofferHelander https://goo.gl/yqkeUP The bottom gradient represents physical brightness. If Photoshop applied a gamma of 1/2.2 for either drawing the gradient or displaying the image, you would see the bottom gradient. Instead, you see the top one, which coincidentally appears to be linear, but isn't. It also wouldn't work for viewing photos. The camera applied 1/2.2 when saving the photo, then Photoshop would apply another 1/2.2, but the display only adds gamma of 2.2 once. That wouldn't look right." CreationDate="2016-08-11T13:58:48.603" UserId="3470" />
  <row Id="4888" PostId="3845" Score="0" Text="If I fill a document in Photoshop half side red (255,0,0) and the other half with green (0,255,0) and then use a Gaussian blur on that, the resulting gradient becomes brownish rather then yellow in the middle as shown in that video I posted. Why is that? If the pixels in the image are linear then why does the math get screwed up? I don't understand how a displacement map painted in Photoshop is linear and good to go for rendering but at the same time the blurring of pixels gets screwed up with the wrong gamma and colors become to dark and brownish?" CreationDate="2016-08-11T14:19:24.253" UserId="2736" />
  <row Id="4889" PostId="3845" Score="0" Text="@KristofferHelander It's a confusing subject, I understand. As I was trying to say, Photoshop does no conversions. So if you load an image that is stored with gamma, it stays with gamma applied and because of that blurring is wrong. If you load a displacement map that is treated as linear data, it is loaded as linear and saved as linear because no conversions are applied by Photoshop by default. There is no special treatment based on the color model. The only difference between the two is that the displacement map is displayed wrong - the display still applies a gamma of 2.2 to it." CreationDate="2016-08-11T16:11:49.957" UserId="3470" />
  <row Id="4890" PostId="3845" Score="0" Text="If I create a gradient, let say 256 pixels wide, and I make sure those pixels are linear going from 0, 1, 2, 3, ... to 255. Then I have created a linear ramp, and if I use that as a displacement in 3D without any gamma correction in the 3D software I get a linear displacement effect. But does that same gradient look linear on the screen? And if I wanted to use that same gradient as a diffuse texture, I would have to apply the gamma correction on it, right? So when piping the map into Displacement you keep the pixels as is, but for Diffuse you have to linearize the texture by gamma correcting?" CreationDate="2016-08-12T09:58:51.210" UserId="2736" />
  <row Id="4891" PostId="3845" Score="0" Text="You got it. The gradient will only look linear on the screen if you apply gamma correction to it. That, however, makes the gradient itself non-linear in terms of the raw values and thus unsuitable for use as a displacement map." CreationDate="2016-08-12T12:07:05.883" UserId="3470" />
  <row Id="4892" PostId="3850" Score="0" Text="You could use a 16 bit image format, but then paint dot net wont cut it. Though quite many other editors would work fine." CreationDate="2016-08-13T08:19:17.027" UserId="38" />
  <row Id="4893" PostId="3850" Score="0" Text="@joojaa and how well do GPUs cope with 16 bit textures? This game needs to run on mobile too." CreationDate="2016-08-13T09:25:06.283" UserId="2437" />
  <row Id="4896" PostId="3850" Score="0" Text="What are you trying to accomplish with a 16-bit or 32-bit alpha channel? The human eye can barely distinguish 64 levels of any given color, so it's unlikely that more bits for your alpha channel will be noticeable." CreationDate="2016-08-14T01:11:24.507" UserId="3003" />
  <row Id="4897" PostId="3850" Score="0" Text="To answer your question above, modern desktop and laptop GPUs should have no problem using 16-bit half float textures. Mobile will be another story. Some may support it but most probably won't at this point in time." CreationDate="2016-08-14T01:12:08.880" UserId="3003" />
  <row Id="4898" PostId="3830" Score="0" Text="Supersampling would work albeit not very practical for limiting platforms." CreationDate="2016-08-14T10:47:02.380" UserId="3041" />
  <row Id="4905" PostId="3846" Score="0" Text="This doesn't explain the &quot;why&quot; at all. It just reiterates the &quot;what&quot;." CreationDate="2016-08-15T03:05:57.087" UserId="3003" />
  <row Id="4906" PostId="3845" Score="0" Text="This answer is factually incorrect. You tell Photoshop in what color space to manipulate data, and many images have information about what colorspace they are stored in. Photoshop will convert from the image's color space to its working space (which you can change), and can save color space info in the images it creates. If there is no color space info stored in the image, it makes a decision about which color space it assumes. But it is always doing this unless you specifically set it not to, and that is not the default." CreationDate="2016-08-15T03:09:19.413" UserId="3003" />
  <row Id="4907" PostId="3823" Score="0" Text="When you say &quot;black and white&quot; do you literally mean 1-bit per pixel images where each pixel is either fully black or fully white? Or do you actually mean grayscale (where there are shades between fully black and fully white)? What type of compression is used?" CreationDate="2016-08-15T03:18:59.837" UserId="3003" />
  <row Id="4909" PostId="3849" Score="1" Text="Is this in 2D or 3D? In 2D what do you do if both offsets intersect?" CreationDate="2016-08-15T07:02:52.420" UserId="38" />
  <row Id="4910" PostId="3846" Score="0" Text="@user1118321: I believe it does. I added emphasis in an attempt to make it clearer." CreationDate="2016-08-15T07:42:23.793" UserId="182" />
  <row Id="4911" PostId="3846" Score="0" Text="Well, true but it still does not really explain why. I admit I have the same problem i just mention the lack of benefit or harm done by such compression. There is no need but the why is that the data of images is peculiar due to human senses whereas our spatial reasoning is more linear in nature (as far as the 3D app is concerned)" CreationDate="2016-08-15T07:58:38.783" UserId="38" />
  <row Id="4912" PostId="3844" Score="0" Text="This is especially true for a normal map were you expect vector lengths to be 1." CreationDate="2016-08-15T09:01:25.493" UserId="3073" />
  <row Id="4915" PostId="3859" Score="1" Text="Yup, that's the one. Thanks. :-) I've been doing indexed Marching Cubes on pretty large volumes and was curious about how they were being rendered, as vertex locality is pretty bad between slices. Makes sense to cache the vertices somewhere (I guess just in standard L1 / 2 cache?). My meshes are probably out-of order enough to be re-processing a lot of vertices, so I'll look into those algorithms." CreationDate="2016-08-16T09:06:03.783" UserId="1937" />
  <row Id="4916" PostId="3829" Score="1" Text="In my own experiments drawing sprites, rendering one mesh with 6k vertices (ie one mesh containing all sprites) was faster than rendering a mesh with 6 vertices and 1k instances. Note that in my instancing I did not use glVertexAttribDivisor but instead read from a buffer object. Using glVertexAttribDivisor may be faster since the gpu could push the data to the shader, while reading a buffer object means the shader has to pull its data. But I didn't benchmark this, so dunno if it matters. I still like instancing, because its a simple clean solution." CreationDate="2016-08-16T10:17:13.393" UserId="4925" />
  <row Id="4917" PostId="3861" Score="0" Text="IS taking the real star position data and pojecting it on your map considered realistic enough?" CreationDate="2016-08-16T12:51:54.893" UserId="38" />
  <row Id="4918" PostId="3861" Score="1" Text="You can find data here: http://www.astronexus.com/hyg" CreationDate="2016-08-16T12:57:55.317" UserId="38" />
  <row Id="4919" PostId="3862" Score="1" Text="Have you called `glGetError()` to see if there are any errors?" CreationDate="2016-08-16T14:40:11.073" UserId="67" />
  <row Id="4920" PostId="3862" Score="0" Text="Gives me 1281 - Invalid Value" CreationDate="2016-08-16T14:56:28.573" UserId="4543" />
  <row Id="4921" PostId="3862" Score="0" Text="So I get GL_INVALID_ENUM on the first run of the render loop after glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_IndexBuffer);&#xA;glBindBuffer(GL_ARRAY_BUFFER, m_VertexBuffer); and GL_INVALID_VALUE after the first shaderProgram.activate() and everytime thereafter." CreationDate="2016-08-16T15:06:22.370" UserId="4543" />
  <row Id="4923" PostId="3861" Score="0" Text="Maybe calculate random 3D positions for the stars and then project them onto a sphere. Their brightness depends on the distance from the sphere center." CreationDate="2016-08-16T19:06:09.997" UserId="2670" />
  <row Id="4924" PostId="3861" Score="4" Text="Maybe you could add a screenshot of your current results versus what you're looking for (e.g. a photo of a real starry sky that you'd like to imitate)? Otherwise, we're just guessing what &quot;unnatural-looking&quot; means here." CreationDate="2016-08-16T19:52:34.583" UserId="48" />
  <row Id="4925" PostId="3864" Score="0" Text="I don't know if any renderers implement something like this, but one thing that springs to mind is this would only be useful when objects have a single solid color, or just a few discrete colors. It wouldn't scale when textures are used, for instance (although I suppose you could apply a global RGB tint to the texture with this method)." CreationDate="2016-08-17T05:43:20.467" UserId="48" />
  <row Id="4926" PostId="3864" Score="0" Text="@NathanReed no thats one of the first things people made these things do." CreationDate="2016-08-17T09:55:34.360" UserId="38" />
  <row Id="4927" PostId="3865" Score="0" Text="Thanks! Do you know where I can read more about this? Maybe the documentation page of a freely available renderer?" CreationDate="2016-08-17T16:52:14.343" UserId="4933" />
  <row Id="4928" PostId="3865" Score="1" Text="@DaanMichiels Pixar used to have a video showing their relight and continious render capabilities go search for renderman" CreationDate="2016-08-17T17:11:28.703" UserId="38" />
  <row Id="4929" PostId="3863" Score="2" Text="As it stands this is more of a comment than an answer. Could you expand to explain the bug and how it affected your code?" CreationDate="2016-08-17T21:53:29.477" UserId="231" />
  <row Id="4930" PostId="3861" Score="0" Text="Maybe you could look into poisson sampling, you can define some density function and transform that into distributed points of fairly convincing random distribution. Check here http://devmag.org.za/2009/05/03/poisson-disk-sampling/" CreationDate="2016-08-18T06:07:11.843" UserId="3073" />
  <row Id="4931" PostId="3871" Score="2" Text="Not sure that is strictly true for diffuse, although I may be wrong. Low diffuse does not imply high specular, what about strongly absorbing materials like carbon or charcoal ?" CreationDate="2016-08-19T09:31:07.670" UserId="3073" />
  <row Id="4932" PostId="3867" Score="0" Text="Usually to avoid getting uniform patterns everywhere, if you are using equally spaced uniform samples then you will get moire pattern effects as some pixels hit and miss features at a regular interval. Ideally some kind of importance sampling should be used to bias sampling directions." CreationDate="2016-08-19T09:33:22.333" UserId="3073" />
  <row Id="4933" PostId="3872" Score="0" Text="Yes but mostly it can be summed as this for a unknown problem, ramdom is the best strategy. since contribution  is unknown..." CreationDate="2016-08-19T13:10:24.670" UserId="38" />
  <row Id="4934" PostId="3872" Score="0" Text="@joojaa: oh, I realize you and I have read the question differently. Yes, Monte Carlo is a strategy that works when the analytical solution is unknown (although a math person would probably have a much more precise way of putting it)." CreationDate="2016-08-19T14:34:15.447" UserId="182" />
  <row Id="4935" PostId="3872" Score="0" Text="Said differently: even if the solution is unknown, Monte Carlo is proven to converge toward it." CreationDate="2016-08-19T14:35:59.760" UserId="182" />
  <row Id="4937" PostId="3871" Score="0" Text="@PaulHK. charcoal have low albedo because low proportion of its incident light is reflected away from a surface." CreationDate="2016-08-20T00:36:40.890" UserId="4946" />
  <row Id="4938" PostId="3877" Score="0" Text="Ah.  And blue noise prevents the clumping by having less low frequency data in the sampling pattern." CreationDate="2016-08-20T12:35:43.813" UserId="56" />
  <row Id="4941" PostId="3861" Score="0" Text="Is the starfield to be a fixed background? That is, do your spaceships move slowly enough that the stars do not need to move?" CreationDate="2016-08-20T13:00:50.700" UserId="231" />
  <row Id="4942" PostId="3878" Score="0" Text="I think the Signal Processing SE (http://dsp.stackexchange.com/) would be more suited for this type of question." CreationDate="2016-08-20T14:54:45.953" UserId="182" />
  <row Id="4945" PostId="3878" Score="2" Text="I like your idea of reversing the darkened band so the inpainting only needs to be applied to the text and small face. The darkening is likely to be a fairly simple function such as halving the colour values of each pixel. You could experiment with different values to see if there's an obvious fit." CreationDate="2016-08-20T17:17:36.523" UserId="231" />
  <row Id="4946" PostId="3878" Score="0" Text="Bear in mind that the darkening may have been applied to linear or gamma corrected pixel values, so that's another thing to experiment with." CreationDate="2016-08-20T17:18:26.420" UserId="231" />
  <row Id="4947" PostId="3878" Score="0" Text="@trichoplax Thanks! Alright I'll try that then. Also, is there any way to reach you by email? I'd love some guidance and I can't find anyone" CreationDate="2016-08-20T17:22:44.603" UserId="4954" />
  <row Id="4948" PostId="3878" Score="0" Text="There was an [inpainting programming competition](http://codegolf.stackexchange.com/questions/70483/patch-the-image) over on Programming Puzzles and Code Golf Stack Exchange, which may give you some inspiration and different approaches to try. The top approach is quite in depth, and there are also a number of different simpler ideas lower down." CreationDate="2016-08-20T17:30:37.880" UserId="231" />
  <row Id="4949" PostId="3878" Score="1" Text="If you start work on your own approach and run into a problem, you can ask another question here, explaining what you have tried so far. If you have any doubts over what you can ask about, you can contact me in [chat] using @trichoplax" CreationDate="2016-08-20T17:32:07.867" UserId="231" />
  <row Id="4950" PostId="3878" Score="0" Text="@tricoplax: but I should go for the impainting approach only after I've gotten rid of the darkened strips yes?" CreationDate="2016-08-20T17:33:42.150" UserId="4954" />
  <row Id="4951" PostId="3879" Score="0" Text="Ok, I'll add one." CreationDate="2016-08-20T18:14:20.377" UserId="4956" />
  <row Id="4953" PostId="3879" Score="0" Text="Yes, I'd need something that could work with different widths, I'l change the picture once again, because it's slightly different than what I achieved in d3.js." CreationDate="2016-08-20T19:10:43.440" UserId="4956" />
  <row Id="4954" PostId="3881" Score="1" Text="Can you be more specific about what you do and don't get? Right now we're just guessing about what kind of explanation would help you." CreationDate="2016-08-20T19:25:07.627" UserId="48" />
  <row Id="4955" PostId="3879" Score="0" Text="I notice in your updated second image there are some that overlap. For example, the mid grey strip near the top of the circle overlaps the green strip that starts next to it (even though both the grey end points are inside the range of the green end points). If that is permitted in the solution then it will be easier to solve." CreationDate="2016-08-20T19:31:47.617" UserId="231" />
  <row Id="4956" PostId="3879" Score="0" Text="If you have existing code in d3.js that already gives the desired result, it would be helpful to see it." CreationDate="2016-08-20T19:32:24.837" UserId="231" />
  <row Id="4957" PostId="3879" Score="0" Text="I think I'll have to live with some of the curves overlapping. Yes, I can paste here d3.js code, but it does not deal with drawing the chords. It's already in the library, I think that it's here in ribbon function: https://github.com/d3/d3-chord/blob/master/src/ribbon.js." CreationDate="2016-08-20T19:39:38.397" UserId="4956" />
  <row Id="4958" PostId="3881" Score="0" Text="Well, it is an old computer graphics exam question, perhaps I should have started with this and I decided to work on it a bit to prepare myself for future examinations." CreationDate="2016-08-20T19:58:11.477" UserId="4959" />
  <row Id="4959" PostId="3881" Score="0" Text="The question goes as stated &quot;How does an analogue image is sampled and then stored in a ture-colour frame buffer (e.g. in a camera) and how do you prevent noise when you do so?&quot;" CreationDate="2016-08-20T19:58:58.737" UserId="4959" />
  <row Id="4960" PostId="3883" Score="0" Text="Thanks. Just managed to get it working (option 2). Transformation while raymarching to sample and inverse transformation while injecting. But I'm still raymarching to worldpos as usual, maybe i can be done simpler - iterating like you said but it would still require a loop." CreationDate="2016-08-21T11:45:11.353" UserId="4958" />
  <row Id="4961" PostId="3883" Score="0" Text="I mean not using transformations but iterate through z." CreationDate="2016-08-21T11:56:41.673" UserId="4958" />
  <row Id="4962" PostId="3890" Score="0" Text="Would it help to replace the edge texture with one that uses pure black and several levels of alpha, rather than using several levels of grey?" CreationDate="2016-08-21T23:11:00.450" UserId="231" />
  <row Id="4963" PostId="3890" Score="0" Text="@trichoplax: I'd say yes, since I could use the glsl mix funtion. But I'm not sure how I'd have to change my fxaa-shader to use alpha instead of grey values. I added the fxaa shader code abvoe." CreationDate="2016-08-21T23:23:23.957" UserId="3377" />
  <row Id="4964" PostId="3890" Score="0" Text="I haven't tested this so I'll leave it to someone else to post an answer if they are confident it works, but I'd imagine it working something like this: For each pixel in the texture, change the alpha value to the opposite of the grey value, and set the colour to black (0, 0, 0). Black pixels become fully opaque, white pixels become fully transparent, so the final colour can simply be set to black for all pixels, since the ones you want to appear white will be fully transparent preventing the black from showing up." CreationDate="2016-08-21T23:28:40.397" UserId="231" />
  <row Id="4965" PostId="3833" Score="0" Text="@Julien Guertault, Thank you for refining the format. I don't know how to do that:)" CreationDate="2016-08-22T00:34:50.647" UserId="3058" />
  <row Id="4967" PostId="3891" Score="1" Text="Could you [edit] to expand on exactly what you require, and what you have tried? Is there a particular approach to clustered lights that you want to implement? What effect are you looking to achieve?" CreationDate="2016-08-22T01:11:42.393" UserId="231" />
  <row Id="4968" PostId="3884" Score="1" Text="This isn't enough info to be a full answer obviously but the scattering function of the surface (aka BSDF) is what describes how light acts on a surface.  The rendering equation includes the BSDF and shows how this stuff all fits together.  I recommend checking out graphicscodex.com. It is 13 chapters long, where each chapter is only a few pages, but after you read through it (shouldn't take long) you'll have a deep understanding of this stuff.  A deeper read would be anything on &quot;physically based rendering&quot; specifically the book at pbrt.com." CreationDate="2016-08-22T01:25:20.833" UserId="56" />
  <row Id="4969" PostId="3884" Score="0" Text="Apologies, pbrt.org is the deeper read I mean." CreationDate="2016-08-22T01:37:08.940" UserId="56" />
  <row Id="4970" PostId="3887" Score="0" Text="It does look weird. Could you put some of your actual code for us to see? Pseudo code shows your intent, which looks decent, but the problem might be in some subtle issue of the math or a typo (:" CreationDate="2016-08-22T01:59:10.037" UserId="56" />
  <row Id="4971" PostId="3871" Score="0" Text="Ah yes I get what you mean now" CreationDate="2016-08-22T02:11:59.467" UserId="3073" />
  <row Id="4972" PostId="3833" Score="0" Text="At your service. :) Here's an overview if you're interested: http://stackoverflow.com/editing-help" CreationDate="2016-08-22T02:16:58.200" UserId="182" />
  <row Id="4973" PostId="3890" Score="2" Text="Something like: gl_FragColor = mix(colourRGB, vec3(0), 1 - edgeRGB.r); Would blend in your edge outlines as black" CreationDate="2016-08-22T02:18:47.630" UserId="3073" />
  <row Id="4974" PostId="3890" Score="2" Text="I would suggest something very similar to @PaulHK's snippet too: `combinedColor = mix(sceneColor, edgeColor, edgeOpacity * (1. - edgeRGB.r));`." CreationDate="2016-08-22T03:17:30.913" UserId="182" />
  <row Id="4975" PostId="3881" Score="1" Text="The question is a bit ambiguous, are we talking about the A-to-D process that goes on in the sensor? Or are we talking about the bayer conversion camera sensor arrays need to apply to convert sensor data into RGB ?" CreationDate="2016-08-22T04:19:16.473" UserId="3073" />
  <row Id="4976" PostId="3729" Score="0" Text="Are you wanting something like diffuse-reaction? There is a great example of this on shadertoy (https://www.shadertoy.com/view/4dcGW2)" CreationDate="2016-08-22T04:22:07.523" UserId="3073" />
  <row Id="4977" PostId="3887" Score="0" Text="@AlanWolfe updated my question, added some real code." CreationDate="2016-08-22T04:25:42.377" UserId="4966" />
  <row Id="4978" PostId="3892" Score="0" Text="Done it. It looks like the problem is really with hamma. But I still has a question. There are to results of my ray tracing. In the first one I" CreationDate="2016-08-22T04:27:47.463" UserId="4966" />
  <row Id="4979" PostId="3892" Score="0" Text="Sorry for kind of spamming. Didn't use to sending commentary with Enter button. There is continuation. In the left one I make at first gamma to linear conversion and then multiplying it on the max(0, scalar multiplication of light and normal) and after that summing results in pixel. And convert results into gamma space again. At the right one I convert colour into linear space only after multiplying it on on the max(0, scalar multiplication of light and normal). I don't quite get the reason why  should I multiply in the gamma space. ![img](http://i.imgur.com/7WRCFb0.png)" CreationDate="2016-08-22T04:40:27.343" UserId="4966" />
  <row Id="4980" PostId="3892" Score="0" Text="The most important thing is the output is converted to gamma (meaning your light accumulation is linear). Because your inputs are constant colours it doesn't affect shading as much as the output conversion." CreationDate="2016-08-22T04:50:26.380" UserId="3073" />
  <row Id="4981" PostId="3892" Score="0" Text="I'd be interested to see your new image output if you can post an update" CreationDate="2016-08-22T04:51:01.107" UserId="3073" />
  <row Id="4982" PostId="3892" Score="0" Text="Don't forget, you picked the orange/green colours by eye (presumably) using a gamma-space monitor, so likely these are the correct/intended colours." CreationDate="2016-08-22T04:54:22.533" UserId="3073" />
  <row Id="4983" PostId="3892" Score="0" Text="I understand it. Don't have any complaints about color. Just about dark parts at the left part of the second image in the updated post. The right one is good without them. But why should I make transformation to the linear space after multiplication." CreationDate="2016-08-22T05:05:02.593" UserId="4966" />
  <row Id="4984" PostId="3892" Score="0" Text="I think the left image is the correct one looking at the function, although you may need to adjust your light colours to something a bit darker." CreationDate="2016-08-22T07:43:07.343" UserId="3073" />
  <row Id="4985" PostId="3890" Score="0" Text="Thank you both, but just to make clear: How would I retrieve the edgeOpacity?" CreationDate="2016-08-22T08:32:19.067" UserId="3377" />
  <row Id="4987" PostId="3890" Score="0" Text="@enne87 edge opacity in this case is edge color (so 1 - color) because edge is black" CreationDate="2016-08-22T11:44:48.440" UserId="4503" />
  <row Id="4988" PostId="3890" Score="0" Text="@enne87: `edgeOpacity` is meant to be a parameter between 0 (no edges at all) and 1 (opaque edges) that you choose to your liking." CreationDate="2016-08-22T14:08:39.733" UserId="182" />
  <row Id="4989" PostId="3891" Score="3" Text="WebGL doesn't have compute shaders and I'm not sure if clustered lighting would be fast enough without them." CreationDate="2016-08-22T15:02:55.103" UserId="67" />
  <row Id="4990" PostId="3893" Score="0" Text="Do the buildings have any properties that would help distinguish them? Are they disconnected items, or is the city a single connected mesh with the buildings rising from a plane?" CreationDate="2016-08-22T15:35:21.550" UserId="231" />
  <row Id="4991" PostId="3893" Score="0" Text="yes absolutely second one a single connected mesh" CreationDate="2016-08-22T17:21:08.150" UserId="4513" />
  <row Id="4992" PostId="3893" Score="1" Text="It's worth editing to include that in your description." CreationDate="2016-08-22T17:55:02.320" UserId="231" />
  <row Id="4993" PostId="3878" Score="0" Text="Is this your original image? Or did you just dump a jpeg compression on it?" CreationDate="2016-08-22T18:12:12.430" UserId="38" />
  <row Id="4994" PostId="3878" Score="1" Text="@trichoplax it is a simple function. If it were for the horrible compression artifacts in he image Id have deleted the line by now, [here is the test](http://i.stack.imgur.com/cDE1A.png) again i wouldn't bother to do this with these kinds of jepg qualities. The color cast could be easily deleted if the jpeg artifacts wouldn't have ruined the area." CreationDate="2016-08-22T18:29:49.930" UserId="38" />
  <row Id="4995" PostId="3890" Score="0" Text="I see, thank you guys very much :)" CreationDate="2016-08-22T20:41:27.677" UserId="3377" />
  <row Id="4996" PostId="3878" Score="0" Text="@jooja Actually the original image is of better quality. This one o downloaded off facebook so its compressed. How exactly did you transform the image? I mean remove the darkened part? What's the function?" CreationDate="2016-08-22T21:59:17.777" UserId="4954" />
  <row Id="4997" PostId="3892" Score="0" Text="Could your colors be overflowing?" CreationDate="2016-08-23T02:44:07.993" UserId="56" />
  <row Id="4998" PostId="3892" Score="0" Text="Nope, I clamp to 1.0f every component when it becomes more than 1.0f. I thought about this case that's why I specially check it in my tests." CreationDate="2016-08-23T05:18:39.280" UserId="4966" />
  <row Id="4999" PostId="3893" Score="4" Text="Then it really depends on the buildings. Are all your buildings just cubes? Or at least convex? And is the ground plane really just a plane or has it terrain?" CreationDate="2016-08-23T10:32:44.027" UserId="273" />
  <row Id="5000" PostId="3878" Score="0" Text="@joojaa What was the technique you used to remove the darkened pixels?" CreationDate="2016-08-23T19:17:57.980" UserId="4954" />
  <row Id="5001" PostId="3897" Score="1" Text="If R2 is a vector, it _can't_ be equal to NL which is a scalar." CreationDate="2016-08-24T14:51:07.393" UserId="209" />
  <row Id="5005" PostId="3901" Score="1" Text="Hi and welcome to the site. Can you edit the question and explain a bit more about the context, what the algorithm is and what you're trying to accomplish? Currently I can't make heads or tails of it. :)" CreationDate="2016-08-25T02:10:45.730" UserId="48" />
  <row Id="5007" PostId="3898" Score="0" Text="The original code with its location param set to 3 for the colour attribs looks like the source of the problem." CreationDate="2016-08-25T07:45:46.513" UserId="3073" />
  <row Id="5012" PostId="3904" Score="0" Text="Updated question for clarification" CreationDate="2016-08-25T13:16:41.583" UserId="3331" />
  <row Id="5014" PostId="3903" Score="0" Text="Right. Basically I wrote the module instead of the vector. Thank you" CreationDate="2016-08-25T20:41:08.780" UserId="4981" />
  <row Id="5015" PostId="3907" Score="0" Text="I clarified a lot of things and I really appreciate your response. It remains a doubt, you correct me if I'm wrong or confirm:     if  **t&lt;0** the intersection point (plan and line) is surely behind the origin of light source but it can happen that the object is still in front but surely the object isn't crossed from the ray (because the intersection is behind) and not being illuminated  can be eliminated.          I'm a beginner in CG, forgive me if I say stupid things" CreationDate="2016-08-25T23:50:55.727" UserId="4981" />
  <row Id="5016" PostId="3907" Score="1" Text="@Umbert That's right. A plane and line (or ray) have at most a single point of intersection, and if that point is behind the ray origin, then no object in the plane can intersect the ray." CreationDate="2016-08-26T00:08:03.360" UserId="48" />
  <row Id="5017" PostId="3910" Score="0" Text="&quot;the normal direction of the arc is not always tangential to the spine&quot;. I'm not quite sure what this means in 3d. Is it equivalent to &quot;the plane in which the arc lies is not always perpendicular to the closed curve&quot;?" CreationDate="2016-08-26T12:17:13.977" UserId="231" />
  <row Id="5018" PostId="3910" Score="0" Text="Is &quot;spine&quot; a specific term here, or should this say &quot;[spline](https://en.wikipedia.org/wiki/Spline_(mathematics))&quot;?" CreationDate="2016-08-26T12:20:01.677" UserId="231" />
  <row Id="5019" PostId="3905" Score="0" Text="I'd be interested to see an explanation of how your solution works, and it might be useful for future visitors too..." CreationDate="2016-08-26T12:38:52.143" UserId="231" />
  <row Id="5020" PostId="3894" Score="0" Text="I have questions. **1.** In the &quot;putting it all together&quot; part you have a condition on whether an object is opaque or transparent. How do I determine this? Do I just add an `opacity` (or `transparency`) boolean value to my materials? **2.** If `ks` is for point lights and `reflectivity` for reflected rays, how do I deduct one from the other as you suggest? There seems to be an equation taking refractive indices and calculating ks from that. (I'm still having a hard time really understand your complete answer. Bear with me, please. Thank you. :)" CreationDate="2016-08-26T13:33:37.437" UserId="4962" />
  <row Id="5021" PostId="3909" Score="0" Text="Unity can export to WebGL." CreationDate="2016-08-26T14:34:29.973" UserId="67" />
  <row Id="5023" PostId="3911" Score="1" Text="Seems to me that $|AC|$ is just shorthand for length of vector A to C which is $|C-A|$ or $|A-C|$ which is your second question. If so then this does work. For you to uderstand why you would need to draw the images of the vectors but im not able to do that on mobile phone." CreationDate="2016-08-27T05:49:59.210" UserId="38" />
  <row Id="5024" PostId="3905" Score="1" Text="I've added a simple explanation..." CreationDate="2016-08-27T06:38:11.690" UserId="4956" />
  <row Id="5025" PostId="3894" Score="1" Text="@kleinfreund 1. Right, I'm suggesting a simplified material model where everything is either opaque (diffuse) or translucent, so you could use a boolean flag for this. 2. I'm saying `ks` and `reflectivity` are equal—literally the same value." CreationDate="2016-08-27T07:20:57.477" UserId="48" />
  <row Id="5026" PostId="3914" Score="0" Text="I was thinking about the mirror row and it is possible that we are seeing a slightly malformed version of the original question. In which case it could be correct." CreationDate="2016-08-27T08:46:44.857" UserId="38" />
  <row Id="5027" PostId="3914" Score="0" Text="@joojaa Why do you think the question is a malformed version?" CreationDate="2016-08-27T10:06:08.440" UserId="4993" />
  <row Id="5028" PostId="3914" Score="0" Text="@Nathan, Thanks alot. Although I'm left with several questions:&#xA;1. Can you clarify &quot;You could fix the answer either way.. fully orthonormalizing u and v.&quot;) ? How do I fully orthonormalize u and v? That is also a question regarding bullet #3 of your answer. Would leaving $u$ as it is and setting $v=u\times|B-A|$ satisfies the requirement? If not, can you please show me explicitly?&#xA;2. So for a point I can use anything, even $(1,0,0),(0,1,0),(1,0,0)$?&#xA;3. Regarding the normalization with $|AC|$, so is it correct? or it should be $|C-A|$? Because I'm not sure the first one is a shorthand." CreationDate="2016-08-27T10:16:20.233" UserId="4993" />
  <row Id="5029" PostId="3914" Score="1" Text="@jiang because its a trivial error and depends on exact wording of the problem it is eady to construct a sentence where the 2 solutions would ony differ by a two letter preposition. $|AC|$ is perfectly normal shorthand in literature." CreationDate="2016-08-27T11:15:51.780" UserId="38" />
  <row Id="5030" PostId="3914" Score="1" Text="@Jjang For orthonormalization, look at the [Gram–Schmidt process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process). Briefly, you'd start with $v = B - A$, then set $v' = v - (u \cdot v)u$. This subtracts off the component of $v$ that's parallel to $u$, leaving only the perpendicular component. Then you normalize $v'$. And joojaa is right, $|AC|$ is common shorthand for the distance between $A$ and $C$." CreationDate="2016-08-27T17:45:28.297" UserId="48" />
  <row Id="5031" PostId="3914" Score="0" Text="Ok, and just to clarify, $u=\dfrac{B-A}{|AB|}$, $v'=u-((C-A)*u)(C-A))$ and then $v=\dfrac{v'}{|v|}$, $w=u\times v$? I'm just verifying this because I've never seen this way before (the way you created $v'$)." CreationDate="2016-08-27T19:03:18.123" UserId="4993" />
  <row Id="5032" PostId="3910" Score="0" Text="Indeed, it is equivalent. The &quot;spine&quot; is not necessarily a spline, it can be an array of points connected with straight lines. Some software calls the spine curve trajectory." CreationDate="2016-08-27T19:22:42.767" UserId="4992" />
  <row Id="5034" PostId="3914" Score="0" Text="By the way, I've just seen this solution (to finding $u,v,w$), what do you say? $u=\dfrac{AB}{|AB|}$, $v=u\times w$, $w=\dfrac{AB\times AC}{|AB\times AC|}$? Is it correct? So there are 2 questions here: 1. Was I right in the previous answer? 2. Is the answer given here correct?" CreationDate="2016-08-27T21:41:38.830" UserId="4993" />
  <row Id="5035" PostId="3909" Score="0" Text="Why are JavaScript-based physics engines &quot;mostly suitable for 2D use cases&quot;? Also you said &quot;Keep in mind that Game Engines may be a viable way for 2D applications too and can make your life a whole lot easier&quot;, which is why I'm making http://github.com/infamous/infamous (renaming it soon and adding docs/demos). It's a 3D API that currently renders to DOM (CSS3D with my DOMMatrix polyfill), and I'd like to add WebGL soon. Here's a couple small demos: http://trusktr.io (hover on the left edge for the 3D menu) and http://mightydevs.com (letters moving into formation, needs word wrapping)." CreationDate="2016-08-28T00:53:17.450" UserId="4991" />
  <row Id="5036" PostId="3905" Score="0" Text="Can you post a working example? (one way to do it could be with http://jsfiddle.net)" CreationDate="2016-08-28T01:00:11.153" UserId="4991" />
  <row Id="5037" PostId="3914" Score="0" Text="@Jjang I added more to the answer that will hopefully clarify it. Yes, your second approach is fine too and gives the same results as the Gram–Schmidt process I outlined." CreationDate="2016-08-28T01:22:23.257" UserId="48" />
  <row Id="5039" PostId="3920" Score="0" Text="Excellent answer, thanks! I just didn't get the last part: using a `half` wouldn't incur in the very same problems that you mentioned before, due to the lack of precision? I though that would be particularly important at the GPU, due to the many color calculations that are frequently done in shaders" CreationDate="2016-08-28T05:09:13.383" UserId="3383" />
  <row Id="5040" PostId="3920" Score="1" Text="@AndrewSteer A `half` is 16 bits, including 11 effective bits of mantissa precision, so while not as precise as `float`, it's still sufficient for most color operations. (Maybe not _quite_ sufficient if you're outputting to a high-gamut HDR display; I'm not sure.)" CreationDate="2016-08-28T05:10:51.280" UserId="48" />
  <row Id="5041" PostId="3914" Score="0" Text="Perfect thanks!" CreationDate="2016-08-28T05:20:45.253" UserId="4993" />
  <row Id="5042" PostId="3922" Score="2" Text="Well you could live without the matrices but it would be hideously complex and wasteful. Many people who did not understand matrices attempt this regularily in stackoverflow but either end up reimplementing matrices without knowing it or ending with huge problems." CreationDate="2016-08-28T06:55:39.897" UserId="38" />
  <row Id="5043" PostId="3909" Score="0" Text="@trusktr Because, or at least that's what I thought, there is no JS 3D physics engine that comes close to PhysX in terms of features. Then again I just found out that someone [ported Bullet to JS](https://github.com/kripken/ammo.js). YMMV." CreationDate="2016-08-28T15:47:28.807" UserId="385" />
  <row Id="5044" PostId="3922" Score="0" Text="I heard of an alternative algebra before that does not use linear transformations, but some other theory. I can't remember what it is or any of the keywords so I don't know what to search for, so I am hoping someone might mention it here. The things you mentioned all seem to build on top of linear algebra, but I believe there are other techniques to build from." CreationDate="2016-08-28T18:49:26.360" UserId="4991" />
  <row Id="5045" PostId="3922" Score="0" Text="@tru There are additional algebras such as [geometric algebra](https://en.wikipedia.org/wiki/Geometric_algebra) that supplement linear algebra with additional concepts, but I'm not aware of anything that _replaces_ linear algebra and fulfills the same purposes." CreationDate="2016-08-28T19:03:25.800" UserId="48" />
  <row Id="5046" PostId="3922" Score="0" Text="@NathanReed Thanks for the link! That might possibly be what it was. I really dislike all that symbolic mathematical jargon though. I'd much love to see it expressed with code, so I'll search for that. Thanks!" CreationDate="2016-08-28T20:01:05.637" UserId="4991" />
  <row Id="5048" PostId="187" Score="0" Text="@ratchetfreak but not all uses of 3D formats end up on the screen." CreationDate="2016-08-28T20:59:45.067" UserId="38" />
  <row Id="5049" PostId="3927" Score="0" Text="Can you please give a full technical answer with the vectors exact calculations and normalizations? I'm finding it difficult to follow the logic. 1. Why is it the same as rotation about an axis? With rotation around axis, you HAVE to use the axis as one of the 3 orthonormal basis you create (here you don't use the normal N).&#xA;2. Why do you take a &quot;vector thats not parallel to normal&quot; and not the normal?" CreationDate="2016-08-28T21:44:06.077" UserId="4993" />
  <row Id="5050" PostId="3927" Score="0" Text="I'd expect something like: $w=\dfrac{n}{|n|}$, $v=\dfrac{n\times (0,0,1)^T}{|n\times (0,0,1)^T|}$, $u=v\times w$" CreationDate="2016-08-28T21:46:30.633" UserId="4993" />
  <row Id="5051" PostId="1541" Score="0" Text="While this is a good answer I don't think it answers the OPs question and kind of suggests that homogenous coords are used because the hardware is optimised for it, rather homogenous coords are more useful and hardware was eventually developed around that. Another argument for vec4s is they are 128bit aligned which makes it more efficient to read on wide memory buses (GPUs)" CreationDate="2016-08-29T01:53:56.173" UserId="3073" />
  <row Id="5052" PostId="3922" Score="0" Text="Monte carlo / Path tracing should be added to your list. Great list btw." CreationDate="2016-08-29T02:49:51.747" UserId="3073" />
  <row Id="5053" PostId="3768" Score="1" Text="Yeah, my comments make no sense now the question has been updated." CreationDate="2016-08-29T02:53:07.970" UserId="3073" />
  <row Id="5054" PostId="3922" Score="1" Text="@PaulHK I mentioned that under raytracing" CreationDate="2016-08-29T03:17:59.397" UserId="48" />
  <row Id="5055" PostId="3922" Score="0" Text="So you did, more coffee needed." CreationDate="2016-08-29T03:29:29.080" UserId="3073" />
  <row Id="5056" PostId="3927" Score="0" Text="@jiang you need to stop thinking in formulas and drawing the vectors. Once you do it gets easier to understand the only reason you do not understand is because you look at the formulas. Ys there was a paste error in the procedure." CreationDate="2016-08-29T05:15:19.470" UserId="38" />
  <row Id="5057" PostId="3927" Score="0" Text="But note you do not have to orthonormalize the matrix, you do not have to normalize the vectors, you could use the sizing in the reflection vectors. You dont need the vectors to be calculated this way. Your secodary directions could easily be  n+(0,1,0) and n+(0,0,1) but its just easier if you have a rotation matrix. But as i said draw the matrix vector on a piece of paper or screen @jiang then you will never need to remember the formulas as it becomes evident once you draw enough images." CreationDate="2016-08-29T05:30:53.837" UserId="38" />
  <row Id="5058" PostId="3917" Score="0" Text="So those values needs to be computed on a case basis? and are not real constants?" CreationDate="2016-08-29T07:42:11.683" UserId="4503" />
  <row Id="5059" PostId="3917" Score="0" Text="ok, seems Those are the actual constants used in every existing shader." CreationDate="2016-08-29T08:20:58.427" UserId="4503" />
  <row Id="5060" PostId="3923" Score="0" Text="One of the reasons I want to do this outside of cad softwares is that this feature is not that simple to create with them. a) On your image above the shape of the cross-section is changing, but the plane of the sketch is perpendicular to the local trajectory. b) The surface can self-intersect, what is often a problem" CreationDate="2016-08-29T09:51:16.847" UserId="4992" />
  <row Id="5061" PostId="3923" Score="0" Text="@Tawhiri a) No the cross section is always the same its just oriented to the sweep path. The plane pf the sketch is not allways perpendicular it is changing with the funclion 60cos(4t)+110. b) is hard to avoid if you put arbitrary functions in place. So the function either selfintersects or does not. But yes that can be counteracted by making a rail sweep along and not thinking in terms of across." CreationDate="2016-08-29T10:05:06.743" UserId="38" />
  <row Id="5062" PostId="3917" Score="0" Text="Yes, for diffuse lighting. For other forms of lighting you need to compute other constants." CreationDate="2016-08-29T11:39:15.993" UserId="5002" />
  <row Id="5063" PostId="3923" Score="0" Text="@Tawhiri oh and what CAD are you using? What your proposing is actually very simple in Creo which is what i use mostly. Admittedly the tool used is not one that many users know how to use but still very easy to do." CreationDate="2016-08-29T12:27:22.437" UserId="38" />
  <row Id="5064" PostId="3923" Score="0" Text="Right now Catia V5R24 and Creo 2.0 is available. What is your method? I have a cuple of years experience with both, but I cant figure it out how to vary the angle of the sketch plane." CreationDate="2016-08-29T13:06:38.953" UserId="4992" />
  <row Id="5065" PostId="3923" Score="0" Text="When you use sweep you  must enable variable sections then inside the sketch add a relation to your dimension. In this relation you can write a function along curve by using the variable `trajpar` which is from 0 to 1 along the swept curve. Likewise you can make curves out fo equations with the **datum (additional dropdown) -&gt; curve -&gt; curve from equation** command. Also if you want to adjust the top down trajectory direction then  you can define an additional curve for the sweep. Its all there but very few people ever use these tools." CreationDate="2016-08-29T13:13:11.567" UserId="38" />
  <row Id="5066" PostId="3923" Score="0" Text="@Tawhiri is this what your after? http://i.stack.imgur.com/bNJgi.png" CreationDate="2016-08-29T13:27:29.973" UserId="38" />
  <row Id="5067" PostId="3923" Score="0" Text="Yes it is. Actually I've tried this before, but the feature keeps crashing with &quot;design intent is unclear&quot;, so I assumed this is not the proper way to go. Probably the problem is that there is a corner with a small radius in the path. And the mathematical derivation of the second curve is quite challenging. Anyway, I will keep trying, thank you so much for the help!" CreationDate="2016-08-29T13:54:52.177" UserId="4992" />
  <row Id="5068" PostId="3923" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/44627/discussion-between-joojaa-and-tawhiri)." CreationDate="2016-08-29T14:04:27.227" UserId="38" />
  <row Id="5069" PostId="1576" Score="1" Text="These are sometimes referred to as &quot;explicit normals&quot; (3ds max and maya terminology)." CreationDate="2016-08-29T14:06:57.273" UserId="5005" />
  <row Id="5070" PostId="3848" Score="0" Text="Thank you for your input - wow! :) I'll have a look at this when I get more spare time. I'll get back to you, thanks!" CreationDate="2016-08-29T14:31:50.973" UserId="2736" />
  <row Id="5071" PostId="3910" Score="1" Text="Even tough i have explained how to do this in comments does not mean that this is a forum for telling you how to operate your CAD application." CreationDate="2016-08-29T16:27:33.680" UserId="38" />
  <row Id="5072" PostId="3930" Score="0" Text="Thanks, but that does not work for me. Here's the problem. Let's say you are 5 seconds into the countdown (percentCompleted = 0.5 or 50%). Then &quot;Update Now&quot; is pressed and `totalTime` is changed to 1. Now, when onUpdate is fired, percent complete will be 5 or 500%!! and there is no more animation as we have just flown way past 100%&#xA;&#xA;Or am I missing something?" CreationDate="2016-08-29T20:30:06.777" UserId="5003" />
  <row Id="5073" PostId="3930" Score="2" Text="no because at that point percent completed would remain at 0.5 % (the variables outside functions are the state kept across frames) and the next update would add a different delta based on the new totaltime as if the total time was always the new value." CreationDate="2016-08-29T20:41:48.473" UserId="137" />
  <row Id="5074" PostId="3910" Score="0" Text="Obviously not. I guess I got the answer, though not the one I was hoping for. Again, thanks for the help." CreationDate="2016-08-30T04:05:03.443" UserId="4992" />
  <row Id="5075" PostId="3930" Score="0" Text="Ah, my mistake. I just didn't read it correctly the first time.  Very good, just what I needed." CreationDate="2016-08-30T14:03:25.317" UserId="5003" />
  <row Id="5076" PostId="3894" Score="0" Text="I see. A `isTransparent` boolean attribute was added to our materials and I'm starting to replace the use of `reflectivity` with `ks`. The fresnel factor based on the light vector is calculated just as the one that is based on the view vector, correct? Instead of `fresnel_reflectance(v, n, rI1, rI2)` where `rI` are the refractive indices, I would do `fresnel_reflectance(l, n, rI1, rI2)` with the first parameter being the light vector instead of the view vector assuming the function is implemented correctly? (based on [this paper, page 6](http://stanford.io/1zco8xk))" CreationDate="2016-08-30T15:25:04.603" UserId="4962" />
  <row Id="5077" PostId="3894" Score="0" Text="Ah! For opaque objects which I don't have a *real* refractive index for, I can determine that by using `ks` a.k.a. `reflectivity` as the solution for `fresnel_reflectance(l, n, rI1, rI2)` to get `rI1`. As for your section *For reflecting the rest of the scene* I would just use `ks` instead of calculating the Fresnel factor based on the view vector?" CreationDate="2016-08-30T16:17:38.717" UserId="4962" />
  <row Id="5078" PostId="3894" Score="0" Text="Sorry for the confusion. I my last comment, instead of *For reflecting the rest of the scene* I meant *For each point light* and also the question concerns the light vector, not the view vector." CreationDate="2016-08-30T16:35:07.997" UserId="4962" />
  <row Id="5079" PostId="3894" Score="0" Text="I will ask my questions in more detail in a [chat room](http://chat.stackexchange.com/rooms/44693/re-controlling-reflection-and-refraction-with-material-properties-in-ray-tracing) I created. If you want, you can join me there and help me out. This is not the right place for the comments." CreationDate="2016-08-30T17:48:46.767" UserId="4962" />
  <row Id="5080" PostId="3932" Score="2" Text="Yes but due to numerical instability it will eventually skew. I should look up my lecture notes, havent revisited this after I stopped teaching." CreationDate="2016-08-31T05:40:06.783" UserId="38" />
  <row Id="5081" PostId="3910" Score="0" Text="Tawhiri well your really trying to substitute a somewhat easy CAD problem with a horribly complicated programming problem. So we are talking 10 hours of getting intimate with advanced CAD components versus 1000 hours of calculus, programming and model fitting experience! I could do this for you but i doubt you can afford my services. Polygons are easy by comparasion." CreationDate="2016-08-31T07:30:34.683" UserId="38" />
  <row Id="5082" PostId="1759" Score="2" Text="As [pointed out on meta](http://meta.computergraphics.stackexchange.com/questions/239/can-we-do-something-about-this-good-question-left-unanswered) this question is deserving of a good answer. I don't have one myself, but I'm happy to give some of my reputation to someone who does." CreationDate="2016-08-31T13:05:18.403" UserId="231" />
  <row Id="5083" PostId="3932" Score="0" Text="Will it skew for any type of transformation, or only ones that are too large?" CreationDate="2016-08-31T14:42:41.197" UserId="4705" />
  <row Id="5084" PostId="3931" Score="0" Text="Have you tried setting stride to 0in your glVertexAttribPointer calls? Having skim read your code it looks mostly ok :)" CreationDate="2016-09-01T06:23:03.643" UserId="3073" />
  <row Id="5085" PostId="3931" Score="0" Text="That worked for some reason. Unfortunately the rectangle only fills half the screen nomatter the scale factor." CreationDate="2016-09-01T06:37:00.323" UserId="2308" />
  <row Id="5086" PostId="3931" Score="0" Text="Your stride should be 0, usually that is only used when you have interleaved buffers (Pos&amp;Tex in the same VBO). For your half-screen thing I would need to see your orthographic matrix." CreationDate="2016-09-01T06:46:17.393" UserId="3073" />
  <row Id="5087" PostId="3931" Score="0" Text="@PaulHK Added to post, I use apples function for that and then put it into a float array. The width and height values are the same ones passed into the initialization of the ripple simulation." CreationDate="2016-09-01T06:48:31.220" UserId="2308" />
  <row Id="5088" PostId="3931" Score="0" Text="Can't see anywhere which would cause only half the screen to be visible, it could be a bad glViewPort setup ?" CreationDate="2016-09-01T06:56:53.743" UserId="3073" />
  <row Id="5089" PostId="3931" Score="0" Text="@paulhk I think it is that the triangles don't exist or aren't being drawn after the half way point. Sometimes it will have like triangles jetting out at the edge of the rectangle" CreationDate="2016-09-01T06:58:53.817" UserId="2308" />
  <row Id="5090" PostId="3931" Score="0" Text="@PaulHK I added a picture" CreationDate="2016-09-01T07:07:29.923" UserId="2308" />
  <row Id="5091" PostId="3931" Score="0" Text="I would suspect something is wrong with your indice buffer. It looks like there is a giant single triangle spanning your whole image (i can see a slanting near horizontal edge along the top amongst the saw tooth pattern). Your indice count for a grid of triangle strips should be ((W*2) + 1) * (H-1) * 2 indices. You only need 1 degenerate vertice per triangle strip emitted. Maybe enabling wireframe rendering will better help visualise this." CreationDate="2016-09-01T07:17:42.433" UserId="3073" />
  <row Id="5092" PostId="3934" Score="0" Text="Your code causes the same issue however it is significantly more efficent so thanks!, I have updated the post with some new diagrams that might help." CreationDate="2016-09-01T08:44:53.087" UserId="2308" />
  <row Id="5093" PostId="3934" Score="0" Text="It kind of looks like its being drawn as a triangle fan, but not quite. Is it possible you can make a small grid (say 9 points, so rSimXXX = 3) and then manually inspect/dump the index buffer ?" CreationDate="2016-09-01T08:59:55.597" UserId="3073" />
  <row Id="5094" PostId="3934" Score="0" Text="Sure, i added a section to the bottom of the post. Let me know if you need/want more data." CreationDate="2016-09-01T09:40:23.603" UserId="2308" />
  <row Id="5095" PostId="3934" Score="0" Text="Ok, things are starting to make sense, it looks like you have 0's interleaved for every other element, which explains all the spiky triangles going to one corner. I suspect your Indicie(..) objects are 32bits, but in the glDrawElements call you tell opengl the index buffer is 16bits (unsigned shorts). You can either change the glDrawElements to accept 32bit integers (change GL_UNSIGNED_SHORT -&gt; GL_UNSIGNED_INT) or change your Indicie object to be 16bit (I guess int-&gt;short but I am not familiar with swift)." CreationDate="2016-09-01T09:42:10.400" UserId="3073" />
  <row Id="5096" PostId="3934" Score="0" Text="I also editted my example above to remove the redundant end-of-row if(...)" CreationDate="2016-09-01T09:44:17.437" UserId="3073" />
  <row Id="5097" PostId="3934" Score="0" Text="OK so if I use glshorts then my draw call should have Glenum(gl_unsigned_short) however if I am using Ints  (32 I think) then it's glenum (gl_unsigned_int)?" CreationDate="2016-09-01T09:50:38.590" UserId="2308" />
  <row Id="5098" PostId="3934" Score="0" Text="Yes, I'm almost certain from your last image this is the cause of the problem. If you interpret a buffer full of 32bit ints as 16bit shorts, you will read the upper/lower halves of the 32bit indexes per 16bit index, which in your case will be index 0 every odd index." CreationDate="2016-09-01T09:57:20.047" UserId="3073" />
  <row Id="5099" PostId="3934" Score="0" Text="Yes you are most certainly right! It is working quite well now! If you could please add the bit about matching up the indicie data type with the gldraw call then I will award you the bounty when SO allows me to. Thankyou so much!" CreationDate="2016-09-01T09:58:45.507" UserId="2308" />
  <row Id="5100" PostId="3934" Score="0" Text="Brilliant, happy to help" CreationDate="2016-09-01T10:02:37.163" UserId="3073" />
  <row Id="5101" PostId="1759" Score="0" Text="@trichoplax the problem is that its done in matlab." CreationDate="2016-09-01T15:24:03.160" UserId="38" />
  <row Id="5102" PostId="1759" Score="0" Text="@joojaa ah good point. If no matlab experts step in during the bounty period, I'll consider learning [Octave](https://en.wikipedia.org/wiki/GNU_Octave) to see if that's close enough to find a solution." CreationDate="2016-09-01T20:17:27.367" UserId="231" />
  <row Id="5103" PostId="3932" Score="2" Text="A transformation matrix only captures the end result of a transformation, not &quot;how it got there&quot;. The matrix for 180° rotation in one direction, the matrix for 180° rotation in the other and the matrix for flipping the grid, that is scaling by (-1, -1), all look the same." CreationDate="2016-09-01T21:08:45.287" UserId="3470" />
  <row Id="5104" PostId="1759" Score="0" Text="I understand what the matlab code is supposed to do. I don't understand some of the used terms. &quot;in camera coordinates&quot; means what? The camera is supposed to be at $(-1,1,5)$ and the plot does not have this position nor do the axis suggest the image is taken from that position." CreationDate="2016-09-02T08:28:57.793" UserId="273" />
  <row Id="5105" PostId="1759" Score="1" Text="It's not very clear to me what the first image is supposed to mean. The second one is from the camera's point of view, and after a back of the envelope estimation I think it looks correct." CreationDate="2016-09-02T09:58:40.027" UserId="182" />
  <row Id="5106" PostId="3938" Score="0" Text="You can not reliably work with polygon data in CAD applications. Why dont you use VBA to make the n discrete points directly in creo sketch and then sweep that? Note this is not what you asked. PS why dont you share a dummy formula and i can add instructions int the chat how to properly do this shape. PS this is not within scope of this forum." CreationDate="2016-09-02T10:28:15.520" UserId="38" />
  <row Id="5107" PostId="3934" Score="0" Text="@J.Doe it's entirely your choice, but if you avoid awarding the bounty until nearer the end of the bounty period, then your question will continue to be promoted on the front page, which means more people will see it. This may mean more voting on both your question and on PaulHK's answer..." CreationDate="2016-09-02T12:35:13.583" UserId="231" />
  <row Id="5108" PostId="3934" Score="0" Text="I've just been waiting for the 24 hours before bounty to expire. But now that you say that I'll hold off a bit so that perhaps he can get more votes." CreationDate="2016-09-02T16:34:35.657" UserId="2308" />
  <row Id="5109" PostId="3768" Score="0" Text="I did not figure out how to resolve it." CreationDate="2016-09-02T19:59:30.123" UserId="3328" />
  <row Id="5110" PostId="3939" Score="1" Text="Can you expand the question a bit? Clarify what you mean by &quot;not shared by any other neighbour&quot;, and discuss how your current algorithm works? Looking at your other question it sounds like you have two clouds of points and you want to put them into 1:1 correspondence, so each point in cloud A is matched to exactly one in cloud B and vice versa. And you want the matched pairs of points to be close to each other. Is that right?" CreationDate="2016-09-02T21:53:15.507" UserId="48" />
  <row Id="5111" PostId="3944" Score="0" Text="Thanks Aces, but I'm still a bit confused. You multiply each fragment by the view Matrix after the usual normal mapping, pass the x and y to the deferred shader, and then multiply by inverse the view Matrix during the deferred shader?" CreationDate="2016-09-03T03:37:28.513" UserId="5026" />
  <row Id="5112" PostId="3943" Score="0" Text="To clarify, is the dot product you are referring to &quot;len&quot;?" CreationDate="2016-09-03T03:41:28.150" UserId="2707" />
  <row Id="5113" PostId="3943" Score="0" Text="Yes, I have used dot(delta, delta) before in place of dist * dist. I just did the dot product that way to simplify things." CreationDate="2016-09-03T03:43:57.580" UserId="2308" />
  <row Id="5115" PostId="3943" Score="0" Text="What is the average speed of the particle? Is it normal to be the same size as that blue diagram at the bottom? I'm just trying to get a good idea of the use cases." CreationDate="2016-09-03T04:02:03.603" UserId="2707" />
  <row Id="5116" PostId="3944" Score="0" Text="Yes, you can do that, but you can also just calculate the vertex normals through the vertex shader and apply the normal map from there. That way, you don't need to multiply each fragment by the View matrix." CreationDate="2016-09-03T04:05:47.103" UserId="2707" />
  <row Id="5117" PostId="3943" Score="0" Text="The simulation has many modes. Sometimes they move with herd mentality meaning they might all be longer or they all might be shorter. Sometimes they are vary individualistic with some being long and others short. I have thought about it quite a bit and thought the simulation the particles will be shorter and longer equally.  So unfortunately we can optimize due to use cases." CreationDate="2016-09-03T04:07:48.937" UserId="2308" />
  <row Id="5118" PostId="3943" Score="1" Text="@J.Doe: The only way you could avoid the dot product would be... by using a conditional branch around it. Which could wind up being slower than just doing the dot product. And let's face facts: a 2D vector dot-product is *trivially* fast on GPUs these days." CreationDate="2016-09-04T03:49:50.160" UserId="2654" />
  <row Id="5119" PostId="3939" Score="0" Text="Yes, please check edit." CreationDate="2016-09-04T10:13:16.240" UserId="5021" />
  <row Id="5120" PostId="312" Score="0" Text="note that the subscript is not a 0 (zero), but an O (oh).  it reads like... &quot;Light out at the out angle equations light emitted towards the out angle plus ... &quot;.  o and i are complements, meaning out and in respectively (:" CreationDate="2016-09-04T21:57:01.943" UserId="56" />
  <row Id="5121" PostId="3939" Score="0" Text="The link you give to the code on Code Review.SE appears to be a question asking the same thing. Are you looking for the same thing from Computer Graphics.SE (a review of how to optimise for speed) or are you asking whether there is a different algorithm that gives different but acceptable results?" CreationDate="2016-09-04T22:13:32.713" UserId="231" />
  <row Id="5122" PostId="3939" Score="0" Text="Do you just need a 1 to 1 pairing of points from set A with points from set B, or do you require the pairing that has the shortest possible total length?" CreationDate="2016-09-04T22:21:45.593" UserId="231" />
  <row Id="5125" PostId="3943" Score="1" Text="This shader looks like it should be super fast already, can't think of any further optimizations really. If / else branches will only really hurt you in a shader if you have a fair bit of code in each branch, here you're not even doing any arithmetic in the branches, just returning values, so the performance hit should be negligible. You could always replace it with a ternary operator, as in gl_FragColor = (condition) ? true_value : false_value. This compiles to a conditional move rather than a branch, so might get you a small speed boost." CreationDate="2016-09-05T03:14:05.163" UserId="1937" />
  <row Id="5126" PostId="3939" Score="1" Text="@trichoplax: I'm looking for another algorithm to make it faster. I need 1:1 pairing with shortest possible distance. My code is already working but it's too slow." CreationDate="2016-09-05T04:59:54.327" UserId="5021" />
  <row Id="5127" PostId="3951" Score="0" Text="Thanks for the answer. I tried blurring the output but as you say I do just get a blurred edge which seems hard to get back without bringing the noise back in. &quot;applying an edge to a color buffer&quot; - think outline around geometry in games. I will try a bilateral blur instead of a Gaussian. Going to accept this as answer for now." CreationDate="2016-09-05T10:50:39.893" UserId="3331" />
  <row Id="5128" PostId="3952" Score="0" Text="You don't project it.  You clip the polygon it against a front clipping plane (or more generally against the view frustum) and then project those results.&#xA;&#xA;I had a quick search to try to find a good page explaining the details but nothing really gave a simple/clear explanation. I could suggest a textbook if that'd help." CreationDate="2016-09-05T12:18:31.030" UserId="209" />
  <row Id="5129" PostId="3952" Score="0" Text="@SimonF I would appreciate a textbook suggestion. Is the 'clip' approach then suitable for rendering? Moving around in a model a made, it does not look unnatural." CreationDate="2016-09-05T12:25:04.627" UserId="5043" />
  <row Id="5130" PostId="3952" Score="0" Text="Actually it is possible to avoid near clipping by using homogeneous rendering but that has some other drawbacks.&#xA;Possible texts _include_  Watt &amp; Watt's &quot;Advanced Animation and Rendering Techniques&quot;  Foley, Van Dam et al's &quot;Computer Graphics: Principles and Practice&quot; - It's old but clipping hasn't changed.  &#xA;&#xA;I'm surprised nothing simple/clear shows up in my web searches (for example, [Wikipedia's](https://en.wikipedia.org/wiki/Clipping_(computer_graphics)#Clipping_in_3D_graphics) entry doesn't seem to contain much), but you could try searching for Weiler-Atherton or Cohen–Sutherland." CreationDate="2016-09-05T12:41:38.633" UserId="209" />
  <row Id="5131" PostId="3951" Score="1" Text="Oh, I see. I would have answered differently if I knew what you were trying to achieve, as what I said was mainly for pixel shading on colors (I am more a computer vision person than graphics...). Have you seen this related question? http://answers.unity3d.com/questions/60155/is-there-a-shader-to-only-add-an-outline.html (First answer has a link to a code example)" CreationDate="2016-09-05T17:21:48.097" UserId="5039" />
  <row Id="5132" PostId="3960" Score="0" Text="Were did you get the original equations from? I'm curious about the final part : return NL*(Spec + max(Diffuse, AmbientColor))*lightModifier;  Is there a reason to multiply by N.L? The Diffuse and Spec calculations already include this term." CreationDate="2016-09-06T08:21:17.597" UserId="3073" />
  <row Id="5133" PostId="3960" Score="0" Text="I think this might well be a point-lighting issue actually. This problem comes up in Unity, when using the standard shader, setting smoothness to 100% causes highlights to disappear completely. This is physically accurate, since the lights are being modelled as a single point, but obviously not what you want in a game." CreationDate="2016-09-06T08:36:06.157" UserId="1937" />
  <row Id="5134" PostId="3960" Score="0" Text="@PaulHK Sorry, that was an oversight, but removing it didn't help. I reviewed multiple sources and found my favorite equations, including Unreal's paper, Frostbite's paper, and a couple tutorials, one by codinglabs, and one by trentreed." CreationDate="2016-09-06T09:07:31.357" UserId="5026" />
  <row Id="5135" PostId="3960" Score="0" Text="@russ But isn't there a way around this? Point Lights still need to show...something. EDIT: Aside from giving it a size." CreationDate="2016-09-06T09:11:56.123" UserId="5026" />
  <row Id="5136" PostId="3932" Score="0" Text="The first thought that comes to my mind is you could factor the matrix into a Möbius transformation and then lerp the quantities. https://en.wikipedia.org/wiki/M%C3%B6bius_transformation#Decomposition_and_elementary_properties" CreationDate="2016-09-06T09:22:29.000" UserId="2831" />
  <row Id="5137" PostId="3960" Score="0" Text="@russ - Would using swapping to 1-Smoothness^4 and 0.16*Specular^2 help? This was suggested in the Frostbite papers, but it might be specific to them. I've tried it, and it just lowers the reflections on the nonmetals a bit, plus widens the reflection on the metals. I'm not sure if this is physically accurate. Also, I retract what I said, without NL, it's bright all over." CreationDate="2016-09-06T09:45:08.723" UserId="5026" />
  <row Id="5138" PostId="3928" Score="0" Text="Same thing different way on xforms: converting a complex number to matrix M(2,R) as above is an isomorphism.  Addition-&gt; addition, product-&gt;product, conjugate-&gt;transpose.  All complex ops can be expressed in terms of these. https://en.wikipedia.org/wiki/2_%C3%97_2_real_matrices" CreationDate="2016-09-06T09:46:46.523" UserId="2831" />
  <row Id="5139" PostId="3925" Score="1" Text="The problem with quaternions is the literature.  Most standard operations are complex numbers wearing a mustache." CreationDate="2016-09-06T09:49:49.813" UserId="2831" />
  <row Id="5140" PostId="3925" Score="0" Text="@MBReynolds well that is to be expected, having 3 separate imaginary axes certainly has that effect. But then multiplicative transforns have their share of problems." CreationDate="2016-09-06T10:27:45.700" UserId="38" />
  <row Id="5141" PostId="3925" Score="0" Text="Anything specific you thinking of?" CreationDate="2016-09-06T11:49:43.297" UserId="2831" />
  <row Id="5142" PostId="3932" Score="0" Text="I hadn't heard of the Möbius transformation before, thank you!" CreationDate="2016-09-06T14:19:59.963" UserId="4705" />
  <row Id="5143" PostId="3960" Score="0" Text="@Karim, I'm not that knowledgeable about PBR, just at the stage of digging into Unity's shaders and trying to understand them myself. But vanishing highlights at full smoothness are not 'wrong' from a physical perspective, just an artifact of using infinitely small light sources. Maybe someone on the forum will have a workaround for it but not me unfortunately. Don't worry too much about being physically 'accurate' though, it's still realtime CG after all. Cook-Torrance and the likes are not accurate, just plausible." CreationDate="2016-09-06T17:32:23.470" UserId="1937" />
  <row Id="5144" PostId="3867" Score="0" Text="I recently read that both stratified sampling and low discrepency sequences (like halton, sobol, etc) are a middle ground between random sampling (which creates noise) and uniform sampling (which creates aliasing)." CreationDate="2016-09-06T21:04:58.757" UserId="56" />
  <row Id="5147" PostId="3954" Score="0" Text="Just so you know, [self answers are very welcome here](http://computergraphics.stackexchange.com/help/self-answer), whether you know the answer already or find it after posting the question." CreationDate="2016-09-07T10:32:24.703" UserId="231" />
  <row Id="5148" PostId="3964" Score="0" Text="When you say &quot;gradients&quot;, do you mean visible discontinuities in the colour? I'm assuming you don't want flat colour, but a smooth gradient with no bands visible?" CreationDate="2016-09-07T20:53:47.633" UserId="231" />
  <row Id="5149" PostId="3964" Score="0" Text="That is exactly what I want." CreationDate="2016-09-07T20:59:10.497" UserId="3377" />
  <row Id="5151" PostId="3965" Score="0" Text="Nice trick. I noticed you're adding .5 to pixel coordinates in the Shadertoy, is this to hit pixel centres in the texture rather than lerped samples?" CreationDate="2016-09-08T06:23:01.810" UserId="1937" />
  <row Id="5152" PostId="3965" Score="2" Text="@russ Hmm, I actually copy-pasted that bit from another Shadertoy. :D On a second look, I think that .5 is actually not needed; `fragCoord` should be on pixel centers already." CreationDate="2016-09-08T06:28:14.263" UserId="48" />
  <row Id="5153" PostId="3965" Score="0" Text="Nice approach, thanks!" CreationDate="2016-09-08T13:28:26.687" UserId="3377" />
  <row Id="5155" PostId="144" Score="0" Text="Funny, and on Intel Haswell GPU ARBfp's `DP3` instruction appears implemented as 3 multiplies and 2 additions, see `INTEL_DEBUG=fs` output from Intel Linux driver: http://paste.ubuntu.com/23150494/ . Not sure whether it's just poor driver or the HW really doesn't have special vector mul instructions." CreationDate="2016-09-08T15:14:00.117" UserId="4647" />
  <row Id="5159" PostId="144" Score="0" Text="@Ruslan Very likely that hardware just doesn't have special vector mul instructions. More accurately, though, they _do_, but they are vectorized across the SIMD width of the architecture (the lanes), not vectorized across the vec3/vec4 dimension." CreationDate="2016-09-08T18:17:01.540" UserId="196" />
  <row Id="5160" PostId="3967" Score="0" Text="http://gizmodo.com/17-amazing-renders-that-youll-swear-are-photographs-1399998593" CreationDate="2016-09-09T03:37:42.483" UserId="38" />
  <row Id="5161" PostId="3968" Score="1" Text="One of the property of all these images which betrays them is that they show ideally clean surfaces. In real world the surfaces are rarely so clean, even in jewelry shops." CreationDate="2016-09-09T09:41:06.030" UserId="4647" />
  <row Id="5162" PostId="3970" Score="1" Text="Your monitor probably needs calibrating/adjusting if it looks like it's linear. To get a good range of colour brightness without obvious banding for the (non-linear) human visual system when you only have a very limited, 8 bits per channel, you need to use a non-linear image encoding as sRGB.  Typically monitors should support this but it's not uncommon for them to be poorly calibrated especially if someone's been fiddling with the contrast/brightness controls.  Your monitor might need adjusting. To summarise: render in linear with *more* than 8bpc (e.g 12 or half-float) and map to sRGB." CreationDate="2016-09-09T11:21:56.950" UserId="209" />
  <row Id="5163" PostId="3971" Score="0" Text="Could you explain what you mean by mass in this context? Does it depend on the colour values of the hexagonal cells? Are you trying to find the centre of the light pixels/dark pixels/a particular specified colour? This is likely to affect the answer so it's worth giving as much detail as you have." CreationDate="2016-09-09T11:50:24.430" UserId="231" />
  <row Id="5164" PostId="3970" Score="0" Text="@SimonF I don't think so. See [the linked question](http://computergraphics.stackexchange.com/q/1587/4647), it says that what I observe is as expected — superlinear change of brightness with input data. Also my monitor shows [this image](https://en.wikipedia.org/wiki/SRGB#/media/File:Srgbnonlinearity.png) as said in its description, suggesting that it's calibrated satisfactorily." CreationDate="2016-09-09T12:18:15.037" UserId="4647" />
  <row Id="5165" PostId="3970" Score="1" Text="Sorry, caffeine deficiency!&#xA;RE _&quot; When you're going to present this on the screen, should the values be transformed as Clinear→CsRGB as given here&quot;_   It's that one, (not the reverse). What you should expect is that (and this is from memory so I may be off by a few values) approx &quot;128/255&quot; in linear should map to about &quot;187/255&quot; in sRGB.   Since the eye can perceive smaller deltas in brightness when the colours are darker, you want more displayable shades in the darker end of 'the palette' than at the brighter end. Also, at the start of the linked Q, subst sRGB for RGB to make correct." CreationDate="2016-09-09T13:25:44.833" UserId="209" />
  <row Id="5166" PostId="3971" Score="0" Text="Good question. Yes colour value is good enough. Actually I'll be working with B&amp;W, so its really the cog of the ink." CreationDate="2016-09-09T14:32:52.820" UserId="5087" />
  <row Id="5167" PostId="3970" Score="0" Text="@SimonF thanks, I seem to have realized where I had a mistake. Made an answer to my own question now." CreationDate="2016-09-09T14:34:44.117" UserId="4647" />
  <row Id="5168" PostId="3972" Score="2" Text="[WebGL Path Tracing demo](http://madebyevan.com/webgl-path-tracing/) is also not very fast converging for diffuse reflectors. So yours is at least not the only one with such a property." CreationDate="2016-09-09T14:41:11.993" UserId="4647" />
  <row Id="5169" PostId="3965" Score="3" Text="For anyone interested, [here](http://www.anisopteragames.com/how-to-fix-color-banding-with-dithering/) is also a well described article how to implement dithering to avoid color banding. Happy coding!" CreationDate="2016-09-09T14:38:59.200" UserId="3377" />
  <row Id="5170" PostId="3973" Score="2" Text="Shouldn't the angle be `atan(c/a)`? (Or better `atan2(c, a)`, if your language supports that.) Also it's worth noting this assumes the matrix doesn't include shear. If it does, you could try extracting that part and interpolating it separately as well." CreationDate="2016-09-09T17:45:22.420" UserId="48" />
  <row Id="5171" PostId="3964" Score="0" Text="Are you passing lightDir is in viewspace in this case?" CreationDate="2016-09-10T00:49:21.630" UserId="5095" />
  <row Id="5172" PostId="3977" Score="0" Text="Could you clarify what you mean? It may help to include your motivation for wanting this effect, and to use simpler example images using only 2 or 3 objects." CreationDate="2016-09-10T05:33:32.203" UserId="231" />
  <row Id="5173" PostId="3964" Score="0" Text="Yes, I do lighting in view-space." CreationDate="2016-09-10T10:03:47.850" UserId="3377" />
  <row Id="5175" PostId="3973" Score="1" Text="In a pure rotation matrix, _c_ = sin(_t_), and both _a_ and _d_ equal cos(_t_). So maybe both your answers work?" CreationDate="2016-09-10T15:53:10.723" UserId="4705" />
  <row Id="5176" PostId="3980" Score="0" Text="I'm so sorry, I'm an idiot. I thought it was there.." CreationDate="2016-09-10T16:47:19.610" UserId="5087" />
  <row Id="5179" PostId="3980" Score="0" Text="OK. What's special about the areas removed is one of: There is a straight edged border (at least one). There is a border that is the arc of a circle - an interior or exterior border. The items left have irregular borders, with no straight edges and no arcs." CreationDate="2016-09-10T17:59:10.450" UserId="5087" />
  <row Id="5180" PostId="3973" Score="1" Text="Thanks @NathanReed, for correcting the angle! And indeed, this does not take shear into account. I'm used to working in 3D were we can usually get away with ignoring shear, but in 2D transforms I guess shear is used much more frequently." CreationDate="2016-09-10T18:16:20.873" UserId="5088" />
  <row Id="5181" PostId="3973" Score="0" Text="@Vermillion: indeed, for a pure rotation matrix they are the same, but for a scaled matrix the scale is different per column (so if you want to use element d in stead, you need to calculate scale first and do something like atan((sx / sy) * c / d)." CreationDate="2016-09-10T18:16:27.990" UserId="5088" />
  <row Id="5183" PostId="3980" Score="0" Text="OK, thank you, that's a fair comment. Would it help the straight edge, or arc, will be at least x pixels or cm long? This will be the case, what I'm wanting to keep will never have a straight border, or a proper arc, more than .5 cm long. Does that make it easier?" CreationDate="2016-09-10T19:10:11.877" UserId="5087" />
  <row Id="5185" PostId="3980" Score="0" Text="I am searching medical  images that have things like clips that I need to eliminate, so I can concentrate on the anatomy - our anatomy doesn't have straight lines, or, at the level I'm working at, arcs." CreationDate="2016-09-10T20:51:10.810" UserId="5087" />
  <row Id="5186" PostId="3980" Score="0" Text="Ah I see - you have a practical real world problem rather than a computer graphics problem. This sounds like it will be off topic. Image processing is on topic here, but this sounds like it would need a sophisticated computer vision/machine learning approach, which goes outside our scope." CreationDate="2016-09-10T22:23:32.250" UserId="231" />
  <row Id="5188" PostId="3980" Score="0" Text="It's difficult to judge what is simple image processing and what is far more complex. [This xkcd](http://xkcd.com/1425/) sums it up well." CreationDate="2016-09-10T22:26:05.573" UserId="231" />
  <row Id="5189" PostId="3980" Score="0" Text="LOL! Yes, xkcd nails it (as it does so many things) brilliantly. I do understand. I'll look for a good representative image." CreationDate="2016-09-11T01:54:08.837" UserId="5087" />
  <row Id="5190" PostId="3980" Score="0" Text="I've added a representative image - it doesn't show any clips, but the straight-edged areas it show are probably better, if I could remove them, then a rectangle, or perfect circle / ellipse, should be easy." CreationDate="2016-09-11T02:09:53.470" UserId="5087" />
  <row Id="5191" PostId="3980" Score="0" Text="It's worth reading about the [XY problem](http://meta.stackexchange.com/a/66378/258647)." CreationDate="2016-09-11T15:24:44.723" UserId="231" />
  <row Id="5192" PostId="3980" Score="2" Text="I suspect that the overall problem of finding dense masses will require a sophisticated machine learning approach, which may render the initial removal of smooth edges redundant." CreationDate="2016-09-11T15:25:50.683" UserId="231" />
  <row Id="5193" PostId="3980" Score="0" Text="To summarise, I suspect that the only part of the solution that will be on topic here may turn out to be unnecessary. It may be worth having a look at [Data Science Stack Exchange](http://datascience.stackexchange.com/help/on-topic), where machine learning questions are on topic." CreationDate="2016-09-11T15:38:40.540" UserId="231" />
  <row Id="5194" PostId="3982" Score="2" Text="For sparks, try a Google for &quot;particle systems&quot;" CreationDate="2016-09-11T15:55:13.157" UserId="56" />
  <row Id="5195" PostId="3988" Score="0" Text="Altough it could be used to other things" CreationDate="2016-09-11T20:09:51.170" UserId="38" />
  <row Id="5196" PostId="3973" Score="0" Text="That makes sense, thank you both." CreationDate="2016-09-11T20:30:50.507" UserId="4705" />
  <row Id="5198" PostId="3990" Score="1" Text="Optimized for what? Memory use, speed etc. ? How often does your data update  and so on." CreationDate="2016-09-12T07:32:48.470" UserId="38" />
  <row Id="5199" PostId="3990" Score="0" Text="Optimized for speed" CreationDate="2016-09-12T07:36:12.413" UserId="5089" />
  <row Id="5204" PostId="3995" Score="0" Text="If you don't get good answers, graphicscodex.com talks about these things." CreationDate="2016-09-12T15:17:37.053" UserId="56" />
  <row Id="5205" PostId="3996" Score="2" Text="This also works if you keep the same depth buffer for the two render passes without clearing it between them." CreationDate="2016-09-12T16:52:09.167" UserId="182" />
  <row Id="5206" PostId="3997" Score="0" Text="Awesome, thanks much!" CreationDate="2016-09-12T17:08:05.570" UserId="3377" />
  <row Id="5207" PostId="3998" Score="1" Text="The title refers to `Image2D` and the code and body refer to `image3D`. Does one of these need to be edited? Which one are you asking about?" CreationDate="2016-09-12T18:16:56.613" UserId="231" />
  <row Id="5208" PostId="3959" Score="0" Text="Hi @Nathan, thank you for your detailed answer. I know this is a huge topic (and a big subject of study). The biggest problem is that the documentation online is fragmented, and it is difficult to find a good approch. My project is this http://goo.gl/Fgo21x: a BRDF viewer (inspired by WDAS viewer) to show the most common empirical and physically based BRDF models and that supports the colours calculation using Spectral data -  tristimulus values. This is an educational project to study OpenGL." CreationDate="2016-09-12T23:50:28.343" UserId="2237" />
  <row Id="5209" PostId="3959" Score="0" Text="I think that a good first approach could be to use the common extension you mentioned: SH or small cubemap + environmental cube map (for reflection and refraction). What do you think about it? (I'm developing this application after work during my sleepless nights :)). Thank you again for the collection of sources you linked above (I have a lot of material to study now)." CreationDate="2016-09-12T23:53:34.133" UserId="2237" />
  <row Id="5210" PostId="4004" Score="2" Text="The normal way to draw a static background if to draw a static textured full-screen quad." CreationDate="2016-09-13T13:25:42.010" UserId="137" />
  <row Id="5211" PostId="3997" Score="0" Text="Just to make sure: If I had a double between two floats, would the two floats also be automatically expanded?" CreationDate="2016-09-13T14:00:53.110" UserId="3377" />
  <row Id="5212" PostId="3997" Score="0" Text="@enne87 depends on the surrounding values and the precise rules of the padding algorithm." CreationDate="2016-09-13T14:05:52.753" UserId="137" />
  <row Id="5213" PostId="4003" Score="2" Text="Just to add some more details about how the right answer can be situational, if your application is CPU bound, that means your GPU is doing less work and is waiting on your CPU between frames.  In this case, it would be less expensive to do it on the GPU since the CPU is already working the entire time, but the GPU isn't.  If you are GPU bound, the reverse is true.  Also, passing tangent and bitangent increases memory usage. If you are memory bound, calculating in the shader can be a good move." CreationDate="2016-09-13T14:17:57.563" UserId="56" />
  <row Id="5214" PostId="3997" Score="0" Text="Aha, I thought this is standardized in every OpenGL implementation. Thanks anyway." CreationDate="2016-09-13T18:04:52.723" UserId="3377" />
  <row Id="5215" PostId="3997" Score="1" Text="Well this layout is by the C# marshaller which is independent of OpenGL" CreationDate="2016-09-13T18:13:54.533" UserId="137" />
  <row Id="5216" PostId="3959" Score="0" Text="@FabrizioDuroni Yep! For a BRDF viewer, a simple directional ambient plus an environmental cubemap should be great." CreationDate="2016-09-13T20:29:39.210" UserId="48" />
  <row Id="5217" PostId="4000" Score="0" Text="Is there a typo in the third bullet point? I'm guessing it should say &quot;they do _not_ need to be part of framebuffer color attachments to write into them&quot;?" CreationDate="2016-09-14T00:46:37.490" UserId="48" />
  <row Id="5218" PostId="4005" Score="0" Text="[Relevant](http://computergraphics.stackexchange.com/questions/2482/choosing-reflection-or-refraction-in-path-tracing)" CreationDate="2016-09-14T03:59:45.393" UserId="231" />
  <row Id="5219" PostId="4004" Score="0" Text="@ratchetfreak please make that comment an answer, so we can vote it up :)" CreationDate="2016-09-14T06:08:37.170" UserId="5088" />
  <row Id="5220" PostId="4000" Score="0" Text="Yes you are right. Hmm strange, I meant the opposite - edited." CreationDate="2016-09-14T09:05:20.380" UserId="4958" />
  <row Id="5222" PostId="4008" Score="0" Text="Assuming this is from an exercise question, does the question specify whether the height of the cylinder is the same as the distance between the specified points?" CreationDate="2016-09-14T12:24:32.390" UserId="231" />
  <row Id="5223" PostId="4008" Score="0" Text="We need to calculate the height based on the distance between the two points." CreationDate="2016-09-14T12:26:11.993" UserId="5121" />
  <row Id="5224" PostId="4008" Score="0" Text="And the result is wrong because it doesn't join the two points. I drawn the points(as small spheres) and the cylinder but the cylinder is attached only the point I have translated it to." CreationDate="2016-09-14T12:32:31.017" UserId="5121" />
  <row Id="5226" PostId="4005" Score="0" Text="The proportions of light that refract and reflect are dependent on incident angle. Are you looking to take this into account? For example, the [Fresnel equations](https://en.wikipedia.org/wiki/Fresnel_equations) or [Schlick's approximation](https://en.wikipedia.org/wiki/Schlick%27s_approximation) if you want to ignore polarisation." CreationDate="2016-09-14T13:25:26.517" UserId="231" />
  <row Id="5227" PostId="3979" Score="0" Text="I don't quite get what you mean. If you compute a value per vertex it will get interpolated over the surrounding triangles. Do you want each triangle to get a different stencil value, computed from its vertex normals somehow? In any case, outputting to the stencil buffer from a shader is not widely supported (though it can be done with [this extension](https://www.opengl.org/registry/specs/ARB/shader_stencil_export.txt)) , so you probably want to use a regular offscreen render target instead." CreationDate="2016-09-14T16:09:22.993" UserId="48" />
  <row Id="5228" PostId="4009" Score="2" Text="As an addendum: The answer to this other question has some sample code for russian roulette. http://computergraphics.stackexchange.com/questions/2316/is-russian-roulette-really-the-answer/2325#2325" CreationDate="2016-09-14T19:23:27.573" UserId="310" />
  <row Id="5229" PostId="4008" Score="0" Text="A picture would definitely be helpful, you should also note that the cross product of two parallel vectors leads to the zero vector, which can happen if the vector p points in the same direction as the vector n. Are the 2 points in 3D lying on the xy plane as well?" CreationDate="2016-09-14T21:28:10.253" UserId="2351" />
  <row Id="5230" PostId="4007" Score="0" Text="I tried, but no result..." CreationDate="2016-09-15T04:19:29.677" UserId="5117" />
  <row Id="5231" PostId="4004" Score="0" Text="Without access to the code, it is difficult to guess what might have gone wrong. The link does not work for me (perhaps it requires you to be logged into your storage account). Would you be happy to paste the code here? If so you can [edit] and the question can be reopened. Until then I'm putting the question on hold so that there will be no further answers until it is clarified, to avoid further confusion." CreationDate="2016-09-15T08:58:45.180" UserId="231" />
  <row Id="5233" PostId="3823" Score="0" Text="I can't find a really good solution but the closer I could get was to use Aforge library to help me detect blobs, not really a good solution but works. &#xA;&#xA;For the project where I work we change the operation so now people decide if the document has quality or not. &#xA;&#xA;For compressing I divide TIFF in pages then convert to JPEG reduce the quality and then add it to a pdf file, sometimes achieving around 80% less space occupied." CreationDate="2016-09-15T09:10:03.727" UserId="4866" />
  <row Id="5234" PostId="4004" Score="0" Text="@trichoplax what do you think about github or mail, becouse the code is long and i think that if you look the all code will be easier what i wrong. Thank you so much!" CreationDate="2016-09-15T13:31:34.940" UserId="5117" />
  <row Id="5235" PostId="4004" Score="0" Text="I recommend separating out just the part of the code that places the background, and seeing if it works on its own. If it doesn't, you can post just that part knowing that it contains a problem. If it does work, add in the simplest extra code that causes the problem, then post that slightly longer code knowing that it contains a problem." CreationDate="2016-09-15T18:13:48.977" UserId="231" />
  <row Id="5236" PostId="4013" Score="0" Text="I'm finding some answers here: http://groups.csail.mit.edu/graphics/classes/6.837/F04/lectures/14_Sampling-used6.pdf" CreationDate="2016-09-16T18:04:09.997" UserId="56" />
  <row Id="5237" PostId="4015" Score="0" Text="Thanks for the great link! the last link on that page seems to dive right into this.  In practical terms, do you happen to know if doing anything above a box blur really gives much visual difference? Or would it give any boost to convergence?" CreationDate="2016-09-16T18:47:58.780" UserId="56" />
  <row Id="5238" PostId="4015" Score="2" Text="Its the best primer to 3D graphics rendering concepts that i have ever seen (Tough its not so much about physical based or tracing stuff but anyway). A sinc filtered image is much sharper than the box filter gives a very blurred look and feel. Would it converge faster, I doubt it." CreationDate="2016-09-16T19:02:08.410" UserId="38" />
  <row Id="5239" PostId="4012" Score="0" Text="you can't do 3d translation with a 3x3 matrix, you need a 4x4.  The rest I totally agree with :P" CreationDate="2016-09-16T20:48:07.340" UserId="56" />
  <row Id="5240" PostId="4012" Score="1" Text="Sorry if it was unclear. I was trying to say that you can do translation of 2D objects. I'll update to make that more clear." CreationDate="2016-09-16T20:49:38.973" UserId="3003" />
  <row Id="5241" PostId="4007" Score="0" Text="Or you simply put the square behind everything and render it same way as everything else." CreationDate="2016-09-16T21:08:18.380" UserId="38" />
  <row Id="5244" PostId="4016" Score="0" Text="Could we see an image of how it looks?  You said it looks the same with tube lights vs not so if you don't feel like posting both images that's ok, but an image alone might shed some light on things (:" CreationDate="2016-09-16T23:54:53.233" UserId="56" />
  <row Id="5245" PostId="4016" Score="0" Text="cant post images as i need 10 rep to do more then 2 links" CreationDate="2016-09-17T00:11:49.867" UserId="5136" />
  <row Id="5246" PostId="4016" Score="0" Text="Annoying. Someone please upvote this question more :p" CreationDate="2016-09-17T00:18:24.220" UserId="56" />
  <row Id="5247" PostId="4016" Score="0" Text="Could try this [http://imgur.com/a/IxhTd](http://imgur.com/a/IxhTd)" CreationDate="2016-09-17T00:21:00.593" UserId="5136" />
  <row Id="5248" PostId="4016" Score="0" Text="I added the link.  For whatever it's worth, is the top or bottom image supposed to be the tube light?" CreationDate="2016-09-17T00:23:04.243" UserId="56" />
  <row Id="5249" PostId="4016" Score="0" Text="yea the botton one is tube lights, the sizes are currently set through defines as its easier to debug. the values are  #define LightRad 0.4&#xA;    #define LightLengh 0.5" CreationDate="2016-09-17T00:24:22.033" UserId="5136" />
  <row Id="5253" PostId="4016" Score="0" Text="I've edited to clarify which paper in the link is the Epic one - please edit if I've misinterpreted your intention. You should be able to edit in the link to the website you mentioned now too, with your increased reputation." CreationDate="2016-09-17T08:32:52.263" UserId="231" />
  <row Id="5254" PostId="4016" Score="0" Text="thanks for cleaning it up a little, i changed the image to the tube lighting one as i felt it was more relevant :)" CreationDate="2016-09-17T08:48:28.493" UserId="5136" />
  <row Id="5255" PostId="4004" Score="0" Text="I did that. Do you have any ideas? @trichoplax" CreationDate="2016-09-17T10:18:16.717" UserId="5117" />
  <row Id="5256" PostId="4007" Score="0" Text="@joojaa Can you look my code and say me where i am wrong. I tried everything, but maybe there ware mistakes in my code." CreationDate="2016-09-17T12:11:56.597" UserId="5117" />
  <row Id="5257" PostId="4007" Score="0" Text="@I.To I made an update, tl;dr you draw, clear and draw again move the first draw to after the clear." CreationDate="2016-09-17T12:31:16.757" UserId="137" />
  <row Id="5259" PostId="4019" Score="0" Text="Your second question isn't something anyone else can answer. It's like asking, &quot;I want to plot 'sin(ωx);', what should 'ω' be?&quot; It depends on what you're trying to accomplish. You can set it to 1 to see what it looks like, and then try other values. Or, if you have a specific use-case in mind, explain that, and then maybe we can help you." CreationDate="2016-09-17T16:28:08.563" UserId="3003" />
  <row Id="5260" PostId="4019" Score="0" Text="@user1118321, Thank you very much for your response. OK. here is my use-case:  ...  ...  I need to implement Gabor Filter using both spatial and frequency domain equations. In the link ...  http://www.cs.utah.edu/~arul/report/node13.html  ..., you can see that there are several equations. (14) is the equation of Gabor Filter in spatial domain. (15) is the equation of Gabor Filter in frequency domain. Hence, my question." CreationDate="2016-09-17T16:41:35.307" UserId="464" />
  <row Id="5261" PostId="4020" Score="0" Text="Just a beginner question, so sorry in advance. Is what you explained equivalent to taking real component as x-axis and imaginary as y-axis ? if not, then can we also do that to represent a complex function ?" CreationDate="2016-09-17T17:47:46.303" UserId="3437" />
  <row Id="5262" PostId="4020" Score="0" Text="@A---B The input to the function is the position on screen, with x = real, y = imaginary. The output is the color, with red = real, green = imaginary." CreationDate="2016-09-17T20:52:00.397" UserId="48" />
  <row Id="5263" PostId="4016" Score="0" Text="Added a little more detail on multiplying in the light position into L01, it creates a closer to tube look but is not right as the tube rotates." CreationDate="2016-09-17T21:46:18.580" UserId="5136" />
  <row Id="5265" PostId="3982" Score="0" Text="As for the second part of your question, I would start with Processing, or maybe OpenFrameworks if you know some C++ already. These both use OpenGL under the hood but don't force you to write 100 lines of boilerplate just to get a window open..." CreationDate="2016-09-18T05:37:32.630" UserId="1937" />
  <row Id="5267" PostId="3841" Score="1" Text="It turned out that I had incorrectly believed that physical LIDAR units had some algorithmic way of &quot;straightening&quot; out the curves, which is not the case.  Your post helped me reach a final conclusion, so thank you, and I selected it as an answer as you were correct." CreationDate="2016-09-19T01:51:17.410" UserId="4886" />
  <row Id="5268" PostId="4012" Score="0" Text="You can use 3x3 matrices for 3D animation though if the bones are connected at their end points and do not exhibit scaling (e.g., for human-shaped skeletons where all you need is the rotation matrix and bone length), and this can significantly reduce memory/bus-bandwidth." CreationDate="2016-09-19T03:20:21.600" UserId="2707" />
  <row Id="5269" PostId="4024" Score="3" Text="Almost right, but you should be multiplying the x and y weights together, not adding them. You'd get 0.81, 0.09, 0.09, 0.01." CreationDate="2016-09-19T03:47:51.207" UserId="48" />
  <row Id="5270" PostId="4024" Score="0" Text="@NathanReed Good catch!" CreationDate="2016-09-19T06:01:47.247" UserId="2707" />
  <row Id="5271" PostId="4024" Score="0" Text="So, I just sort of stretch my new image over original and then for each new pixel I compute coordinates of nearest pixels in original image and then compute their weights and blend them. Am I right?" CreationDate="2016-09-19T07:04:54.837" UserId="5148" />
  <row Id="5272" PostId="4024" Score="0" Text="@justanothercoder While an ok answer. It does nothing for dispelling the myth that pixels are squares. You are dealing with point samples (that are not squares). And that resampling is in fact consisting of 2 distinct phases rolled into one, making the datafield continious and then sampling said continious function." CreationDate="2016-09-19T14:57:51.080" UserId="38" />
  <row Id="5274" PostId="4024" Score="0" Text="@justanothercoder Yes, I posted a new diagram that hopefully makes more sense. You normalize each image between 0 and 1 so that you can directly compare them." CreationDate="2016-09-20T01:14:01.923" UserId="2707" />
  <row Id="5275" PostId="4024" Score="0" Text="@joojaa I see your point, do the new diagrams look better? It's kind of hard to make a diagram that shows both what paint program show and what is going on with the math." CreationDate="2016-09-20T01:15:13.793" UserId="2707" />
  <row Id="5277" PostId="4024" Score="0" Text="Thanks, now answer makes even more sense! You're very helpful :)" CreationDate="2016-09-20T04:33:34.597" UserId="5148" />
  <row Id="5278" PostId="4018" Score="0" Text="Excellent answer, as always." CreationDate="2016-09-20T10:38:59.547" UserId="2479" />
  <row Id="5282" PostId="4025" Score="0" Text="Nice looking results!" CreationDate="2016-09-20T16:37:24.520" UserId="56" />
  <row Id="5283" PostId="4028" Score="0" Text="thanks, still both Diffuse and Specular should be colored" CreationDate="2016-09-20T17:15:14.033" UserId="2069" />
  <row Id="5288" PostId="4030" Score="0" Text="Since comments were cleared theres nolonger a link." CreationDate="2016-09-22T18:42:19.030" UserId="38" />
  <row Id="5289" PostId="4029" Score="0" Text="[Related](http://computergraphics.stackexchange.com/questions/2310/could-we-dispense-the-near-clipping-plane)" CreationDate="2016-09-22T22:15:20.343" UserId="231" />
  <row Id="5290" PostId="4030" Score="0" Text="@joojaa even after the comment was auto-deleted, the linked question still shows in the &quot;linked&quot; section at the top right of this page. Since that's easy to miss I've added a comment with the link too." CreationDate="2016-09-22T22:16:47.903" UserId="231" />
  <row Id="5291" PostId="4030" Score="0" Text="I still don't know how I can deal with internal register overflow problem. For example, we adopt a world window whose x coordinate from 0~1023, that's a 10 digits binary. While scanning, a line whose x coordinates exceeds the 1023 will have more than 10 digits, then the extra part will be ignored,  right? So this line will be display in 0~1023 area. How can I deal with this ?" CreationDate="2016-09-23T04:00:45.753" UserId="5105" />
  <row Id="5292" PostId="4030" Score="0" Text="@zfb Just as an aside, typically  HW rasterisers will have, say, an additional 4 to 8 bits of sub-pixel precision to allow smoother animation.  As for raster scanning, you'd only start/end on pixels inside the window. Out of curiosity are you writing a software rasteriser?" CreationDate="2016-09-23T08:11:59.063" UserId="209" />
  <row Id="5293" PostId="4030" Score="0" Text="I am still studying this, no creation..." CreationDate="2016-09-23T13:20:25.837" UserId="5105" />
  <row Id="5294" PostId="4033" Score="0" Text="I wonder if it's the texture read(s?) that are slow.  If you don't use the value that results from the texture read, it probably isn't even doing the texture read.  You could try doing dummer math with those values to see if it's still slow.  like maybe return shadowValue + shadowTexZ;" CreationDate="2016-09-23T23:22:27.310" UserId="56" />
  <row Id="5295" PostId="4033" Score="0" Text="I also think the compiler may be optimizing out some parts of your code if you comment this line. Please check the lower level generated code to be sure. Moreover, you should check the step function documentation for optimizing out this particular branch." CreationDate="2016-09-24T04:42:17.613" UserId="4768" />
  <row Id="5296" PostId="4032" Score="0" Text="I'll give this a go asap and return with the results" CreationDate="2016-09-24T11:36:58.767" UserId="5171" />
  <row Id="5298" PostId="4034" Score="2" Text="Could you share you reasons for settling on these approaches? The logic behind it or profiling evidence perhaps?" CreationDate="2016-09-25T11:40:28.387" UserId="231" />
  <row Id="5299" PostId="4032" Score="0" Text="It worked Thanks a tonn man, it worked exactly I wanted it to" CreationDate="2016-09-25T12:20:40.937" UserId="5171" />
  <row Id="5300" PostId="4032" Score="0" Text="see the result here BEFORE https://i.gyazo.com/a314d1f7a0509788c272f83279c5077b.png   and AFTER https://i.gyazo.com/b879db7294d82a72b86cc4eaf0132a13.png" CreationDate="2016-09-25T12:23:22.877" UserId="5171" />
  <row Id="5301" PostId="4034" Score="0" Text="The first one to me seems like the most straightforward approach to obtain the desired result and the second one is something the GPU potentially might be better suited for than comparisons and type casting. Maybe I'm completely wrong, but it's worth a try at least." CreationDate="2016-09-25T14:30:10.763" UserId="2811" />
  <row Id="5302" PostId="4032" Score="0" Text="@Allahjane: glad to hear it worked fine. :)" CreationDate="2016-09-25T16:07:43.850" UserId="182" />
  <row Id="5303" PostId="4034" Score="2" Text="These seem like points worth expanding your answer with." CreationDate="2016-09-25T18:55:57.403" UserId="231" />
  <row Id="5305" PostId="4037" Score="2" Text="Thanks for your guides. for generate a cube map with 32x32x6, we only use 32x32x6 texels from 64x64x6 ( i mean we only compute 32x32x6 texels), so why we have to multiply 32x32x6 to 64x64x6 again ? (i think my question would be answered if i understand why diffuse equation combines all texels of the environment maps. and also i can't understand the relation of diffuse equation and cube map). since i have learned computer graphics by myself, some stupid questions arise when i'm facing to bigger problems. sorry about that :D" CreationDate="2016-09-26T08:56:04.340" UserId="5182" />
  <row Id="5307" PostId="4021" Score="0" Text="This is for deferred lighting pipeline, right? Are you using tiled implementation to reduce bandwidth? Also, have you considered tiled deferred shading or tiled forward shading instead?" CreationDate="2016-09-26T14:50:33.917" UserId="1952" />
  <row Id="5311" PostId="2472" Score="0" Text="Are you implementing the original Phong IS or modified (cos weighted) Phong IS? The multiplier (pow+1)/(pow+2) indicates the latter but you seem to be missing the cos weighting and it should be (pow+2)/(pow+1). Didn't check if the rotation of the IS to reflection vector is correct but I assume so." CreationDate="2016-09-27T13:32:36.983" UserId="1952" />
  <row Id="5312" PostId="4045" Score="0" Text="Thank you, that was helpful. I will be checking out the conversions. I am using OpenGL." CreationDate="2016-09-28T03:24:27.023" UserId="5183" />
  <row Id="5313" PostId="4045" Score="0" Text="For OpenGL, converting to and from YCbCr should just be a matrix multiply of your color value, so just a single instruction in a glsl shader." CreationDate="2016-09-28T03:40:33.747" UserId="3003" />
  <row Id="5314" PostId="4042" Score="2" Text="Also just for your information you are learning [legacy OpenGL](https://www.opengl.org/wiki/Legacy_OpenGL) which is obsolete since 2008. Since your spending effort in learning atleast go and learn relevant OpenGL. No you do not nedd matrices but the alternative is more painful." CreationDate="2016-09-28T14:48:12.493" UserId="38" />
  <row Id="5316" PostId="4049" Score="3" Text="You could look into stencil buffers. You can draw the masking triangle into the stencil buffer only. Then the rest of the circle can be drawn using the stencil buffer as a mask." CreationDate="2016-09-29T03:23:48.760" UserId="3073" />
  <row Id="5318" PostId="4042" Score="0" Text="I didn't know that but anyway the legacy OpenGL is a manadatory course for me. But I will most certainly learn the new one." CreationDate="2016-09-29T06:03:09.253" UserId="5193" />
  <row Id="5319" PostId="4042" Score="0" Text="Sounds weird, there are lot of devices that will never support legacy openGL. Like your mobile phone, and even desktops will see a gradual phasing out of legacy OpenGL. So no new code should be made with legacy openGL for any reason, including teaching." CreationDate="2016-09-29T06:26:45.230" UserId="38" />
  <row Id="5320" PostId="4042" Score="0" Text="Like I said I've been learning opengl for a few weeks now and since it is a one year course I definately believe I will get to opengl4.5  later . And It is not weird at all because University education also  focus on teaching older stuff along with the newer stuff." CreationDate="2016-09-29T07:09:23.670" UserId="5193" />
  <row Id="5321" PostId="4042" Score="1" Text="No, it's definitely not ok. There is no concepts that the old way does that the new way does not do. Theres absolutely no need for you to learn how to send data and use fixed pipeline programming, it does not even change all that much. In fact if you were not using the fixed pipeline you wouldn't have this question. All your doing is learning an API that nolonger should be used, i mean ist fine if you learned how to blit colors on a c64 also in the course bit other than that no benefit, other than perhaps knowing the difference." CreationDate="2016-09-29T07:21:06.540" UserId="38" />
  <row Id="5322" PostId="4042" Score="0" Text="you know what I'm gonna leave you to  argue that with the education board." CreationDate="2016-09-29T07:26:51.037" UserId="5193" />
  <row Id="5323" PostId="4042" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/46057/discussion-between-joojaa-and-neolardo-va-dinci)." CreationDate="2016-09-29T07:27:03.957" UserId="38" />
  <row Id="5326" PostId="4051" Score="0" Text="Great! Do you have any advice on how to begin changing the level of detail?" CreationDate="2016-09-29T16:17:22.883" UserId="4708" />
  <row Id="5327" PostId="4043" Score="0" Text="Can you give some more details? Are you rendering a 3d scene? What is happening right now to cause your luminance not to be preserved?  Are you just trying to render without lighting? It might also be useful info to know what you are using to render: OpenGL, DirectX, other?" CreationDate="2016-09-29T21:12:19.173" UserId="56" />
  <row Id="5328" PostId="4051" Score="0" Text="@fluffological_studies Yeah, you do it by mixing multiple octaves of noise together. I added something to the answer about that." CreationDate="2016-09-30T01:43:10.010" UserId="48" />
  <row Id="5329" PostId="4051" Score="1" Text="You can animate the FBM noise also by scrolling the octave textures without the need for extra texture dimension." CreationDate="2016-09-30T04:27:03.880" UserId="1952" />
  <row Id="5330" PostId="4054" Score="0" Text="Are you shearing the left and right halves of the [trapezoid](https://en.wikipedia.org/wiki/Isosceles_trapezoid) independently? Could you explain the underlying problem you are trying to solve?" CreationDate="2016-09-30T11:17:14.117" UserId="231" />
  <row Id="5331" PostId="4054" Score="0" Text="@trichoplax    yes independently, but I am not convinced of the correctness of the exposed solution.      The underlyng problem is transform the view frustum , a truncated pyramid, in a parallepiped  (in 3-D) but isn't important for now." CreationDate="2016-09-30T12:11:01.070" UserId="4981" />
  <row Id="5332" PostId="4054" Score="0" Text="I'd recommend describing the background of what you're trying to do because it can affect which solution is most useful. For example, if your objective is to draw a parallelepiped, a simple answer may be &quot;start with a cube instead of a truncated pyramid&quot;. If your objective is to convert a 3D texture then the use of a shear may be essential to the end result you require. In other cases there may be an easier approach. Without knowing the reasons behind it, it's hard to know what simplifying assumptions can be made." CreationDate="2016-09-30T12:36:22.783" UserId="231" />
  <row Id="5333" PostId="4054" Score="0" Text="thing And this is one transformation when you have perspective divide but not possible with one 3x3 matrix without one." CreationDate="2016-09-30T13:30:39.337" UserId="38" />
  <row Id="5334" PostId="4053" Score="0" Text="I made some edit" CreationDate="2016-09-30T14:11:04.027" UserId="4958" />
  <row Id="5335" PostId="4043" Score="0" Text="Rendering a 3D scene with OpenGL, ambient lighting only. I wanted blue to be as perceptually bright (if that's the correct term) as green." CreationDate="2016-10-01T01:26:21.590" UserId="5183" />
  <row Id="5336" PostId="4043" Score="0" Text="I likely need to apply a conversion as the answer below suggests." CreationDate="2016-10-01T01:29:10.133" UserId="5183" />
  <row Id="5338" PostId="4026" Score="0" Text="What are the walls that don't seem correct? How do they look? How do their N and T look?" CreationDate="2016-10-01T02:39:50.120" UserId="182" />
  <row Id="5339" PostId="392" Score="1" Text="It might also be worth mentioning that rigid body transforms are a subset of affine transforms, and affine transforms are a subset of perspective transforms." CreationDate="2016-10-02T04:29:48.287" UserId="3003" />
  <row Id="5340" PostId="4059" Score="0" Text="What is it doing instead? Is it rotating along a different axis?" CreationDate="2016-10-02T11:35:54.200" UserId="182" />
  <row Id="5341" PostId="4062" Score="1" Text="&quot;Why is a graphics processing unit not at universal processing unit&quot;, is that you question?" CreationDate="2016-10-02T14:02:44.700" UserId="3041" />
  <row Id="5342" PostId="4062" Score="1" Text="@Andreas No, my question is as it's stated in the post. Why are rasterizers still a hardware part, when they could be done in software (in fact they already could be done with OpenCL, or compute shaders). Question is why it is not common... maybe it's simply performance, I don't know, that's why I'm asking..." CreationDate="2016-10-02T14:09:54.537" UserId="1764" />
  <row Id="5344" PostId="4058" Score="2" Text="This isn't really the type of question anyone else can answer. It's going to depend entirely on how your fluid sim and rendering are written and the hardware it's running on. Maybe you should put together a prototype and profile it. That could give you data to push you in one direction or the other." CreationDate="2016-10-02T14:52:09.190" UserId="3003" />
  <row Id="5345" PostId="4058" Score="1" Text="I would suggest to provide more information to make this question specific. For example, how do you simulate your fluid? Do you use a particle based method such as SPH? Do you use a grid base method(the benchmark you mentioned is a grid based method)? How is your fluid coupled with other objects? All of these will impact the stability, convergence rate, and performance." CreationDate="2016-10-02T15:57:11.243" UserId="120" />
  <row Id="5346" PostId="4058" Score="0" Text="Well you did answer my question. The fact that you can't give me a (specific) answer tells me that there, in fact, are scenarios where GPU powered sim is viable and/or desirable. Look at the bold text." CreationDate="2016-10-02T18:10:29.233" UserId="5215" />
  <row Id="5347" PostId="4063" Score="0" Text="I dont think you need to store the subpixel samples if box filttering is enough then you can just do running averages." CreationDate="2016-10-02T19:41:53.920" UserId="38" />
  <row Id="5348" PostId="4063" Score="2" Text="I suspect texture sampling and cache are probably also areas where hardware implementations allows performance otherwise impossible to achieve with software implementations." CreationDate="2016-10-03T02:37:43.943" UserId="182" />
  <row Id="5349" PostId="4064" Score="1" Text="Is the dot notation correct or even common for matrix multiplication? I've only seen it as a cross ($A \times B$) or omitted ($AB$)." CreationDate="2016-10-03T02:42:41.747" UserId="182" />
  <row Id="5350" PostId="4058" Score="3" Text="_What's the use of rendering the graphics at 60FPS if fluid simulation runs at 10 steps per second,_ Even if the simulation is slower, 60fps gives a better interactivity (think of rotating the scene). You could also for example interpolate between simulation steps." CreationDate="2016-10-03T02:47:14.500" UserId="182" />
  <row Id="5351" PostId="4064" Score="0" Text="Oops, yeah I think you're right. I just get used to using dots for multiplication. :)" CreationDate="2016-10-03T03:03:46.910" UserId="2707" />
  <row Id="5352" PostId="4065" Score="0" Text="Do you want to use specific features that are only available using a given graphics card? Or is it sufficient simply to know how fast the code can run? This will determine whether you need to determine which graphics card (if any) is being used, or whether you can use an approach that starts up in the lowest quality setting and detects frame rate, using that to decide whether to increase the quality." CreationDate="2016-10-03T07:10:47.163" UserId="231" />
  <row Id="5353" PostId="4061" Score="0" Text="Do you know if this solves the problem?" CreationDate="2016-10-03T07:17:19.373" UserId="231" />
  <row Id="5354" PostId="4060" Score="1" Text="[Relevant](http://computergraphics.stackexchange.com/questions/3964/opengl-specular-shading-gradient-banding-issues) - even 24 bit colour (16M colours) is insufficient to ensure smooth gradients. The human eye cannot distinguish this many colours, but it can spot the boundary between two adjacent colours." CreationDate="2016-10-03T07:22:13.860" UserId="231" />
  <row Id="5355" PostId="4065" Score="0" Text="I think it would be sufficient to just know how fast it can run" CreationDate="2016-10-03T08:03:04.393" UserId="5220" />
  <row Id="5357" PostId="4063" Score="3" Text="@aces One other item worth mention is power. Dedicated hardware will do the same job typically with a fraction of the power budget and, with power throttling already an issue, especially on mobile devices, going fully programmable would make it vastly worse." CreationDate="2016-10-03T12:02:19.677" UserId="209" />
  <row Id="5358" PostId="4062" Score="0" Text="You can bypass the rasterization and implement your own rasterizer with compute units on modern GPU's and I in fact know people doing this for specific purposes." CreationDate="2016-10-03T13:18:26.700" UserId="1952" />
  <row Id="5359" PostId="4067" Score="0" Text="Do you by chance have a screenshot of what you are trying to achieve? (:" CreationDate="2016-10-03T14:05:19.983" UserId="56" />
  <row Id="5360" PostId="4067" Score="0" Text="Unfortunately not but here is a little diagram. It's easy to do a simple directional light `dot(worldNormal, lightDir)` but how to limit it." CreationDate="2016-10-03T14:11:13.550" UserId="2372" />
  <row Id="5361" PostId="4061" Score="0" Text="I believe the problem was not rotating along its own axis so doing it this way should solve it. Maybe there are other solutions" CreationDate="2016-10-03T15:06:21.257" UserId="4958" />
  <row Id="5362" PostId="4067" Score="3" Text="This isn't a full answer but essentially you want to test if a point is in the oriented box defined by the light. You may want to soften the edges as it gets farther away from the light source but that is more challenging." CreationDate="2016-10-03T15:27:39.650" UserId="56" />
  <row Id="5363" PostId="4060" Score="1" Text="@trichoplax good find. I´ll rephrase that part of the question." CreationDate="2016-10-03T16:18:54.840" UserId="3041" />
  <row Id="5364" PostId="4060" Score="1" Text="I think you have a error in assuming that dithering does not work on macro scales it does all you need to do is fool the visual systems sharpening and noise cancellation algorithm. Which with perfect dithering may be spectacularily big. Dithering works well into the range where you can see individual pixels at 8bits per channel color depth." CreationDate="2016-10-03T17:38:00.740" UserId="38" />
  <row Id="5366" PostId="4068" Score="0" Text="I used to write a Mathematica script to infer the projection matrix(here: http://blog.linearconstraints.net/2016/02/03/inferring-projective-mappings.html) except that 8 vertices are used to solve a linear system at that time. I think 6 non coplanar vertices should be adequate. I will try later." CreationDate="2016-10-03T20:22:21.220" UserId="120" />
  <row Id="5367" PostId="4063" Score="0" Text="@JulienGuertault Fair point, but I would think this is mostly applicable to MSAA. There test results seem to indicate that this isn't a huge issue when everything fits on on-chip memory (although this could have some performance impact regardless)." CreationDate="2016-10-04T03:47:06.850" UserId="2707" />
  <row Id="5368" PostId="4063" Score="0" Text="@SimonF I think that's a great point as well. I included it in the answer." CreationDate="2016-10-04T03:49:46.283" UserId="2707" />
  <row Id="5371" PostId="4071" Score="0" Text="Perhaps even using pixel doubling. By the way I get a consistent 60 fps no matter what." CreationDate="2016-10-04T13:18:19.993" UserId="38" />
  <row Id="5372" PostId="4071" Score="0" Text="Sounds like I need a newer computer..." CreationDate="2016-10-04T14:06:26.470" UserId="231" />
  <row Id="5373" PostId="4074" Score="0" Text="Got it, thanks, very confusing indeed but good article. Just one last thing, does this matrix pre-multiply $\vec{v'} = M\vec{v}$ or post-multiply $\vec{v'} = \vec{v}M$ ? Is just that I always confuse which is first, translation or rotation haha" CreationDate="2016-10-04T14:11:43.963" UserId="116" />
  <row Id="5375" PostId="4074" Score="1" Text="@BRabbit27 If $\vec{v}$ is a column vector it is always $\vec{v}'=\mathbf{M}\vec{v}$. There is no $\vec{v}\mathbf{M}$." CreationDate="2016-10-04T15:20:10.320" UserId="120" />
  <row Id="5376" PostId="4060" Score="0" Text="@joojaa I´m not sure what you are implying... Asuming &quot;macro scale&quot; is &quot;large enough pixels to be seen with naked eye&quot;, then yes dithering may still fool the brain to blend them together. If you take an RGB8 image and convert it to RGB1 that would require crazy pixel density and an awesome dithering pattern! At some point it would work though. My question is how to derive the point at which it would work. If you or anyone else could try clarify or rewrite the post please do so. Because I don´t see where we disagree :-)" CreationDate="2016-10-04T16:58:09.980" UserId="3041" />
  <row Id="5377" PostId="4060" Score="0" Text="@Andreas i am implying that applying perfect dithering technique may in fact be infinitely good, we dont know. The brain might with right fooling react very favorably even in the 1 bit case. In fact we need nowehere near the typical 300 dpi to make color look smooth just that trying to do the other things we are trying to do is in fact the limiting factor." CreationDate="2016-10-04T17:04:29.480" UserId="38" />
  <row Id="5378" PostId="4060" Score="0" Text="@joojaa Hm. Maybe I´m using &quot;perfect&quot; wrong. What I mean is a &quot;psedo-random pattern which is NOT repeated&quot;. In the post linked by tricho you CAN see some traces of banding if you try really hard because of the dithering pattern being repeated. This becomes even clearer if the gradient is aligned with the display." CreationDate="2016-10-04T17:15:16.083" UserId="3041" />
  <row Id="5379" PostId="4060" Score="0" Text="Nevermind last comment. Got that mixed up with another image, sorry! My point was a small repeating pattern may look bad." CreationDate="2016-10-04T17:26:50.783" UserId="3041" />
  <row Id="5380" PostId="4077" Score="1" Text="Thanks, this will help me to work out how and what to implement and will give me some foundation to work from. Also, the last paragraph is a good advice!" CreationDate="2016-10-04T18:31:23.817" UserId="5215" />
  <row Id="5381" PostId="4071" Score="0" Text="I usually get 30-60 on my macbook pro depending on how large the window is (full screen on a WQHD monitor lowers it a lot). (edit: accidentally hit enter too soon) I was hoping you could go more into some specific techniques to maximize performance. Are my initial ideas for lower quality on track?" CreationDate="2016-10-04T19:41:06.807" UserId="5220" />
  <row Id="5382" PostId="4078" Score="0" Text="Simplest solution would be to render the polygon and map the screen coordinates of the polygon to your desired texture coordinates in fragment shader" CreationDate="2016-10-05T03:09:20.103" UserId="1952" />
  <row Id="5383" PostId="4076" Score="0" Text="You means, DX12 only has one pipeline, but it optimized comprise process of PSO with multi-thread and multi-cores. Vulkan has 2 pipelines, reside on different threads, each thread has its own commands managements, but only 2 threads ?" CreationDate="2016-10-05T03:11:17.507" UserId="5222" />
  <row Id="5384" PostId="4060" Score="0" Text="You mean stochastic?. But if so are elements still asumed to be in a grid." CreationDate="2016-10-05T03:58:23.247" UserId="38" />
  <row Id="5385" PostId="4078" Score="0" Text="sure if there was only one texture. How would this work with more than one texture like I specified?" CreationDate="2016-10-05T08:23:08.357" UserId="5093" />
  <row Id="5386" PostId="4078" Score="0" Text="You just put the textures into a texture array or atlas" CreationDate="2016-10-05T11:14:38.860" UserId="1952" />
  <row Id="5387" PostId="2059" Score="0" Text="For reference, here is the source for a low discrepancy sample (blue noise) generator https://github.com/bartwronski/BlueNoiseGenerator" CreationDate="2016-10-05T11:15:57.443" UserId="2463" />
  <row Id="5388" PostId="4080" Score="1" Text="That is coming from the man himself! I'm not sure if it'll be helpful, but here's a raytraced implementation in shadertoy:&#xA;https://www.shadertoy.com/view/4lV3zV" CreationDate="2016-10-05T14:07:22.863" UserId="56" />
  <row Id="5389" PostId="4049" Score="0" Text="How are the components drawn? Are they images or generated by a frag shader? Is the triangle drawn as a single triangle, or as part of the outer circle? I don't think it's possible to make a useful suggestion without knowing how all the components relate in terms of draw calls." CreationDate="2016-10-05T15:26:35.230" UserId="2041" />
  <row Id="5390" PostId="4085" Score="0" Text="Please see the screenshot. The issue is not regarding the scaling. I already scaled cylinders to match the length between the spheres." CreationDate="2016-10-05T15:48:17.403" UserId="5121" />
  <row Id="5391" PostId="4049" Score="0" Text="I tried doing an offscreen fbo that leveraged the fact that I'm using additive blending but that cut fps in half.  I am still working on figuring out stencil buffer." CreationDate="2016-10-05T15:59:35.110" UserId="2308" />
  <row Id="5392" PostId="4049" Score="0" Text="So the outer ring with a triangle is a GLQUAD and the inner circle is another GLQuad. The outer one never gets transformed while the inner one does get rotated." CreationDate="2016-10-05T16:00:37.247" UserId="2308" />
  <row Id="5396" PostId="2429" Score="0" Text="I'd really like to absorb this answer but I'm getting stuck on the bit about approximating the solid angle as an area of circle of radius $r_\text{light}/r$ using similar triangles. I don't get the denominator, and I can't see how similar triangles come into play. Any help appreciated." CreationDate="2016-10-05T21:22:31.023" UserId="2941" />
  <row Id="5397" PostId="4079" Score="2" Text="The projection matrix transforms points into **clip space**, which is a 2x2x2 cube centered at the origin, **not** what you're calling &quot;screen space&quot;. Homogeneous coordinates means you have a `w` in addition to your spatial coordinates(x,y and possibly z). It has nothing to do with normalization. You never divide by Z - always W. The point of W is that you can perspective divide and  keep depth (again, in clip space), and nicely lets you add translation into your matrix rather than having a separate step. You're mixing up a lot of different concepts." CreationDate="2016-10-05T22:23:09.067" UserId="5240" />
  <row Id="5398" PostId="4058" Score="0" Text="What @JulienGuertault said. There's a difference between a  physically correct simulation, and something that's good for gameplay. Also, this isn't really a graphics question. Have you tried gamedev.stackexchange?" CreationDate="2016-10-05T22:38:06.607" UserId="5240" />
  <row Id="5401" PostId="4087" Score="0" Text="TL;DR - the Z-buffer holds the depth of the closest polygon to the camera. At the end of the frame, all other things being equal, the closest poly is the closest poly, so the depth buffer value will be the same regardless of draw order." CreationDate="2016-10-05T22:51:14.313" UserId="5240" />
  <row Id="5402" PostId="2429" Score="1" Text="@PeteUK it comes about because you're projecting a light source that's a distance $r$ away onto the unit sphere. (The solid angle is equivalent to area on a unit sphere.) So, distances get divided by $r$. If that doesn't help, I can draw a diagram." CreationDate="2016-10-05T22:57:07.367" UserId="48" />
  <row Id="5403" PostId="4062" Score="0" Text="Rasterizers convert vector polys into a bunch of pixels that we can light. When we no longer have pixels, or stop using vector geometry, we will no longer  need rasterizers. No matter  what the rest of your pipeline looks like, at the end of the day (or  frame), you need pixels. The rasterizer just tells us which pixels we are concerned about for a given triangle. All of that is programmable - if you want different output from the rasterizer, send different triangles its way. Or just draw everything to a render texture in a compute shader and blit it to the screen with a view-aligned quad." CreationDate="2016-10-05T22:59:17.863" UserId="5240" />
  <row Id="5404" PostId="4059" Score="0" Text="I wanted to read your question but that animated gif is too distracting." CreationDate="2016-10-05T23:07:14.267" UserId="5240" />
  <row Id="5405" PostId="3593" Score="0" Text="I teach an OpenGL class about once per year. We **start** with 4.3 core and shaders. Despite claims the fixed pipeline being better for teaching, my (undergrad) students picked it up pretty quickly. And learned a useful skill." CreationDate="2016-10-05T23:12:46.950" UserId="5240" />
  <row Id="5406" PostId="4031" Score="0" Text="n = (v1 - v0) x (v2 - v0), where v0, v1, and v2 are the vertices of the (triangle) face in question. The order is importan.  Normalize it if you really need to. (and `x` is cross product)" CreationDate="2016-10-05T23:17:56.073" UserId="5240" />
  <row Id="5407" PostId="4077" Score="0" Text="@DavidLively I just rephrased that paragraph. &gt; The SIMT and SIMD models have nothing to do with bandwidth / &quot;transfer latency.&quot;. I actually intended to describe how SIMT hides the memory access latency, not the transfer overhead. I think this is a major difference between SIMT and SIMD. I hope it is more clear now. Thank you for pointing out these issues." CreationDate="2016-10-06T00:04:26.267" UserId="120" />
  <row Id="5408" PostId="4088" Score="0" Text="I wonder if his teacher was referring to non determinism of the floating point operations? Not sure if that actually happens on the GPU but I could see it possibly happening. A pretty obscure thing though, so maybe not likely what was being talked about?" CreationDate="2016-10-06T02:00:27.687" UserId="56" />
  <row Id="5409" PostId="4088" Score="0" Text="@AlanWolfe That could happen with a floating-point render target, I suppose. Not sure if it's an issue for integer formats (even with blending)." CreationDate="2016-10-06T02:35:30.260" UserId="48" />
  <row Id="5411" PostId="1707" Score="0" Text="The problem with either method (set constant buffer GPU address with SetGraphicsRootConstantBufferView() or using secondary lookups to the descriptors in the root table) is that if there are not enough GPU addresses available to render all objects then the application developer must implement a strategy to re-use constant buffer memory. The application may have to close and submit the current command list (Close() then Reset()) which means you have to duplicate the state (PSO and other state) into a new command list. How should an application use constant buffer memory in a ring buffer strateg" CreationDate="2016-10-06T00:59:46.477" UserDisplayName="user5241" />
  <row Id="5412" PostId="1707" Score="0" Text="Looks like SE cut off the post in the conversion to comment. It continues with &quot;...y? In the past drivers would notice no-overwrite was set or rename the memory if the previous memory was busy. How do you implement these strategies in DX12?&quot;" CreationDate="2016-10-06T08:08:03.640" UserId="16" />
  <row Id="5413" PostId="2429" Score="0" Text="I get it now, thanks very much." CreationDate="2016-10-06T09:45:18.660" UserId="2941" />
  <row Id="5414" PostId="4087" Score="0" Text="At the end of the render the frame buffer may contain different values but, provided you haven't played with the Z test mode, the Z buffer should be consistent." CreationDate="2016-10-06T11:27:56.957" UserId="209" />
  <row Id="5415" PostId="4077" Score="0" Text="@TheyBusyTypist Much better. Thanks for updating. :)" CreationDate="2016-10-06T14:55:52.160" UserId="5240" />
  <row Id="5416" PostId="4060" Score="0" Text="@joojaa Sure that´s one way of putting it." CreationDate="2016-10-06T17:24:52.130" UserId="3041" />
  <row Id="5417" PostId="3959" Score="0" Text="Maybe this falls into one of your categories, but isn't old fashioned render-to-all-faces-of-a-cubemap technically a fully realtime technique?  Also, isn't it possible to augment basic ambient with a environment cubemap for diffuse reflections as well?" CreationDate="2016-10-06T23:34:19.767" UserDisplayName="user3412" />
  <row Id="5418" PostId="4100" Score="0" Text="Exactly I mean this geometric modeling" CreationDate="2016-10-07T14:30:54.867" UserId="5253" />
  <row Id="5419" PostId="4100" Score="0" Text="Good to hear! BTW I would also look at procedural content generation techniques!" CreationDate="2016-10-07T14:32:40.087" UserId="56" />
  <row Id="5420" PostId="4100" Score="0" Text="Unfortunately I am not so familiar with the problems that graphics professionals involve with,I must search more! Thanks so much !" CreationDate="2016-10-07T15:09:38.957" UserId="5253" />
  <row Id="5421" PostId="4102" Score="0" Text="Thanks for your answer. I'll move on to writing the path tracing bit :) You suggested tracing paths of length X will produce an unbiased image but my book says when looking at techniques to prevent paths growing too long: &quot;A first technique is cutting off the recursive evaluations after a fixed number [...] but important light transport might have been ignored. Thus, the image will be biased&quot;. Are you suggesting to set X fairly high (e.g. 10) to ensure important long paths are considered?" CreationDate="2016-10-08T08:49:35.463" UserId="2941" />
  <row Id="5422" PostId="4102" Score="1" Text="Yes, you should set X to a fairly large number or cut the length adaptively until the path weight factor drops below a certain threshold" CreationDate="2016-10-08T11:37:33.657" UserId="1952" />
  <row Id="5423" PostId="4101" Score="0" Text="Here's a nice resource about basic path tracing: http://blog.demofox.org/2016/09/21/path-tracing-getting-started-with-diffuse-and-emissive/" CreationDate="2016-10-09T02:01:01.330" UserId="56" />
  <row Id="5424" PostId="4107" Score="0" Text="And what about light.diffuse (diffuse light color?)" CreationDate="2016-10-09T09:56:08.623" UserId="4958" />
  <row Id="5425" PostId="4107" Score="0" Text="Oh I forgot to add in that, I've updated my answer with it." CreationDate="2016-10-09T10:05:21.773" UserId="5256" />
  <row Id="5426" PostId="4101" Score="0" Text="@AlanWolfe Thank you. I actually read through that yesterday :) and have found some of your posts on CGSE helpful too. Have downloaded the PTBlogPost1 code and will look through it when I've finished with SmallPt." CreationDate="2016-10-09T12:55:07.580" UserId="2941" />
  <row Id="5427" PostId="4101" Score="0" Text="Awesome! I'm really glad to hear that (:" CreationDate="2016-10-09T13:50:08.667" UserId="56" />
  <row Id="5428" PostId="4109" Score="0" Text="I'm confused by why there are vertical lines at all. Is it deliberate that the texture is aligned with the image plane instead of the triangle? What happens when you rotate the texture (mapping)?" CreationDate="2016-10-10T08:59:29.987" UserId="2041" />
  <row Id="5429" PostId="4109" Score="0" Text="@DanHulme It's actually aligned with the triangle, you just can't see it in the screenshot. I tried to change the uv mapping but no difference.Weird thing is though, if I rotate the triangle 90° it's perfect. I think this might have something to do with the triangle itself. The problems occur if one side of the triangle is aligned with the x-axis. When aligned with the y-axis it works..." CreationDate="2016-10-10T12:44:07.617" UserId="4571" />
  <row Id="5431" PostId="1579" Score="0" Text="Here's a good introduction to monte carlo path tracing, which does what you describe - it uses monte carlo integration to try and solve the rendering equation.  It makes for very nice results, but takes a long time to render with the naive implementation.  It's still a very active are of research.&#xA;http://blog.demofox.org/2016/09/21/path-tracing-getting-started-with-diffuse-and-emissive/" CreationDate="2016-10-10T15:53:05.530" UserId="56" />
  <row Id="5433" PostId="4110" Score="0" Text="Current answers are avoiding your question why 0.999 is used instead of 1. Besides it making no real visual difference, I suspect that this number is chosen to avoid floating point errors which could result in an energy gain. Could anyone more familiar with IEEE 754 comment on that?" CreationDate="2016-10-11T08:27:53.623" UserId="385" />
  <row Id="5434" PostId="4091" Score="0" Text="Is 1/(1/w) a typo or something I've misunderstood?" CreationDate="2016-10-11T09:03:41.303" UserId="231" />
  <row Id="5436" PostId="4091" Score="1" Text="Not a typo. You interpolate 1/w linearly over the screen and calculate 1/(1/w) per pixel. Then multiply linearly interpolated u/w and v/w with the per-pixel 1/(1/w) value. Just a trivial optimization of turning divs to muls to avoid multiple divisions per pixel." CreationDate="2016-10-11T12:41:19.703" UserId="1952" />
  <row Id="5437" PostId="4117" Score="0" Text="Isn't there still a downside if not all the waves want to take the same path?  Won't it do both paths for the wave? I was also wondering what you meant in the last paragraph about newer cards doing double the logic? Thanks for the answer and info (:" CreationDate="2016-10-11T14:34:50.643" UserId="56" />
  <row Id="5438" PostId="4114" Score="0" Text="I'm thinking how the concept of a specular colour makes sense when talking about perfect mirrors? Are you saying it's possible for all incoming light to be reflected at the interface and have it's colour (frequency) changed? Would the angle in incidence = angle of reflection? What physical process would bring about the change of colour? Sorry if I sound like a kid that asks more questions after just been given an answer." CreationDate="2016-10-11T15:14:28.313" UserId="2941" />
  <row Id="5439" PostId="4114" Score="0" Text="It's a bit more complicated. To try and fit the explanation in this comment: when light hits a surface, some will enter it (refraction), some will not and bounce (reflection). Whether it enters or not can depend on the light wavelength (hence the graphs in @JarkkoL's answer) and that's what gives gold its yellow appearance." CreationDate="2016-10-11T15:27:29.477" UserId="182" />
  <row Id="5440" PostId="4114" Score="0" Text="In case that wasn't clear, I was referring to the copper and aluminum triple graphs: they represent the reflectance for red, green and blue wavelengths." CreationDate="2016-10-11T15:34:50.497" UserId="182" />
  <row Id="5441" PostId="4107" Score="0" Text="Shouldn't it be `color = diffuse + ambient` ?" CreationDate="2016-10-11T18:34:46.690" UserId="5254" />
  <row Id="5442" PostId="4114" Score="0" Text="Thanks for your replies. Regarding the light hitting the surface that doesn't bounce and gets refracted: we're not talking about perfect mirror (or ideal specular surface) anymore, right? Is this refraction and re-emission called a specular highlight (i.e. a bright spot)? Back to the contrived ideal specular surface where **all** light gets reflected: is there a possibility for the incident and reflected light to have different colours?" CreationDate="2016-10-11T18:52:47.273" UserId="2941" />
  <row Id="5443" PostId="4118" Score="0" Text="Yeah, that's kind of what I expected. But it has to work somehow..." CreationDate="2016-10-11T19:12:49.050" UserId="4571" />
  <row Id="5444" PostId="4118" Score="0" Text="I already have bilinear filtering but I like to have both. This gives it sort of a retro look" CreationDate="2016-10-11T19:13:44.920" UserId="4571" />
  <row Id="5445" PostId="4109" Score="0" Text="The triangle edge jaggies in the top-right corner (against black background) strike me odd though. Why some pixels appear bigger than others? They should all be the same size." CreationDate="2016-10-11T20:37:13.647" UserId="1952" />
  <row Id="5446" PostId="4109" Score="0" Text="@JarkkoL that's because I scaled the picture in Photoshop in order to make the issue more visible :)" CreationDate="2016-10-11T20:46:03.460" UserId="4571" />
  <row Id="5447" PostId="4109" Score="0" Text="I thought you might have (: You should scale it properly to make it less confusing." CreationDate="2016-10-11T20:47:57.477" UserId="1952" />
  <row Id="5448" PostId="4109" Score="0" Text="Would scaling to a power of 2 multiple of the original size ensure all the scaled up pixels are consistent sizes? (To make the diagram more relevant for people answering.)" CreationDate="2016-10-11T21:20:21.493" UserId="231" />
  <row Id="5449" PostId="4119" Score="0" Text="No,no!   A is  the world reference system (the coordinate system, Oxyz) and B is the reference system of the camera (Ouvn).   Maybe I had not specified.   In light of these explanations it is correct what I said in my original post?" CreationDate="2016-10-11T21:45:02.143" UserId="4981" />
  <row Id="5450" PostId="4119" Score="1" Text="@Umbert why would world be anything other than identity? I mean lack of identity world would mean there is something more fundamental than world in the scene. So the identity basis is present no matter what even if its below your world meaning world is a object space of somekind. There is always a explicit assumption that something identity is at the lowest level of a transformation chain." CreationDate="2016-10-12T03:39:58.227" UserId="38" />
  <row Id="5451" PostId="4117" Score="0" Text="Ignore the last paragraph, I'll edit my answer. I thought you were attempting to do something else and I made a bad assumption. If the whole warp doesn't go through the same path, then you're right, then you have to go through the &quot;if&quot; and the &quot;else&quot; block even for newer cards." CreationDate="2016-10-12T04:09:04.017" UserId="2707" />
  <row Id="5452" PostId="3959" Score="0" Text="@racarate Sorry it took me awhile to respond, but yes, you're right! I think I meant to mention that, but forgot. :) Anyway, I added it. (I did mention using a cubemap for diffuse, up in the first bullet point.)" CreationDate="2016-10-12T04:54:00.447" UserId="48" />
  <row Id="5453" PostId="4118" Score="0" Text="@Leon2806 see update re not doing per-pixel increments." CreationDate="2016-10-12T08:14:38.900" UserId="209" />
  <row Id="5454" PostId="4121" Score="0" Text="It depends how much control you have over the image. e.g. making all your images 1 pixel large would have a huge benefit, but that probably isn't useful to you." CreationDate="2016-10-12T10:33:13.833" UserId="2041" />
  <row Id="5455" PostId="4121" Score="0" Text="@DanHulme I'm thinking about quality-loss optimization of existing PNG images. The questions is: how to process such images. First idea - pixelize with unicolor blocks. Second idea - decrease palette to have possibly big unicolor shapes. Or maybe something else. I don't know how PNG compression works and what change will result with best effect." CreationDate="2016-10-12T10:50:36.910" UserId="5285" />
  <row Id="5456" PostId="4121" Score="0" Text="PNG is not stored in blocks, so no that is useful but not optimal" CreationDate="2016-10-12T10:52:19.673" UserId="38" />
  <row Id="5457" PostId="4119" Score="0" Text="@Umbert It's important to say coordinate system relatively to what. &quot;World&quot; is usually considered global &quot;root&quot; frame joojaa explained in his answer. If $A$ is transformation to your &quot;world&quot; then where is it transformation from? Global frame? The answer I gave still holds if both $A$ and $B$ are defined relatively to the same frame, just use different names (world=global frame, object=world)." CreationDate="2016-10-12T12:25:14.963" UserId="1952" />
  <row Id="5458" PostId="4118" Score="1" Text="I'm not sure if I completely  understand what you mean. Should I multiply the step value by x? because that gives me the exact same result." CreationDate="2016-10-12T12:44:28.677" UserId="4571" />
  <row Id="5459" PostId="4109" Score="0" Text="What's strange though is that some of the jaggies in the image are 7 pixel wide while some are 3 or 4 pixel wide. I would expect 1 pixel deviation in the image if using non-integer upscaling, but that doesn't explain 7px vs 3/4px. Anyway, you should just upscale the image by some integer multiply (e.g. 800% in Photoshop)." CreationDate="2016-10-12T13:05:21.903" UserId="1952" />
  <row Id="5460" PostId="4118" Score="0" Text="In real mathematics it does, but not in the world of floating point.  I believe the appropriate quote is _&quot;Floating point numbers are like piles of sand; every time you move them around, you lose a little sand and pick up a little dirt.”_ ( — Brian Kernighan and P. J. Plauger).  Every time you do an operation (e.g an add or a mul) you'll typically lose up to 1/2 a ULP of accuracy.  Replacing a sequence &quot;+C+C+C...+C&quot;  with +N*C will thus improve your sand to dirt ratio :-)" CreationDate="2016-10-12T13:17:32.900" UserId="209" />
  <row Id="5461" PostId="4118" Score="0" Text="Thanks for the explanation, now it makes sense to me :)   Unfortunately it does not change anything. But I think another problem regarding accuracy might be that I'm calculating a new step value for each scanline, which is unnecessary because it should be constant across the whole triangle. That's not only more expensive but also gives me slightly different results every time which causes the visual artifacts." CreationDate="2016-10-12T14:19:22.677" UserId="4571" />
  <row Id="5462" PostId="4124" Score="0" Text="This sounds interesting. Could you expand on the basic idea and explain why it works?" CreationDate="2016-10-12T16:04:54.783" UserId="231" />
  <row Id="5463" PostId="4124" Score="0" Text="It's quite clever. I will study it more." CreationDate="2016-10-12T17:03:38.600" UserId="4958" />
  <row Id="5465" PostId="4107" Score="0" Text="Yeah, you should not be multiply ambient, if it is 0 you will end up with 0 diffuse. It should be diffuse + ambient" CreationDate="2016-10-13T03:36:54.063" UserId="3073" />
  <row Id="5466" PostId="4126" Score="1" Text="Beautifully explained, I kinda related the fact to the perspective division but wasn't sure, now I am, thanks !" CreationDate="2016-10-13T04:40:09.687" UserId="116" />
  <row Id="5468" PostId="4122" Score="0" Text="My question is simple. I must aplly the view transfrormation. I supposed that the matrix for pass from Object space to world  has already been applied.Thus I'm in world coordinates (Oxyz).I must pass in camera coordinates (Ouvn).(change of coordinates)Then I overlap the 3 axes (x,y,z) on 3 axes (u,v,n) with a transformation namely Rx*Ry*T. Later for map a point P of the coordinate system of the world (Oxyz) to the Ouvn I do (T^-1)*(Ry^-1)*(Rx^-1)*P . My doubt is: is right the transformation Rx*Ry*T for ovelaps the 2 axes?Or the right transformation is  T*Rx*Ry and then (Ry^-1)*(Rx^-1)*(T^-1)P?" CreationDate="2016-10-13T10:38:06.540" UserId="4981" />
  <row Id="5470" PostId="4129" Score="1" Text="I think IneQuation means the gaussian filter is a box filter in the sense that it applies to n*n pixels. It's then worth separating it since you go from n*n to 2*n (muliplications per pixel)" CreationDate="2016-10-13T14:20:43.133" UserId="5254" />
  <row Id="5472" PostId="4129" Score="0" Text="Given the code linked in the answer, this is the first one." CreationDate="2016-10-13T19:15:05.960" UserId="5254" />
  <row Id="5473" PostId="4047" Score="0" Text="Welcome to ComputerGraphics.SE! It's normally a good idea to make answers on Stack Exchange self contained (e.g. in case links go down and also in general so that people don't have to follow links to be able to tell if an answer is useful to them). You might want to improve your answer by including a short summary of the contents of the paper." CreationDate="2016-10-13T21:22:11.790" UserId="16" />
  <row Id="5474" PostId="4129" Score="1" Text="You are correct, @trichoplax, I meant a square neighbourhood. Fixing. :)" CreationDate="2016-10-14T06:50:51.587" UserId="2817" />
  <row Id="5475" PostId="4132" Score="1" Text="I suspect that, like myself, many of us don't have Illustrator, so you'll have to be more specific in your description. Are you just clicking to define the end points of the each curve segment and then moving the end point(s) of the tangent associated with an end point?" CreationDate="2016-10-14T08:19:50.713" UserId="209" />
  <row Id="5476" PostId="4132" Score="0" Text="It's really hard to explain what I mean but I think if you watch this video from **2:11** to **3:00** you'll understand exactly what I'm trying to say. https://youtu.be/0B_IQK7hMo0?t=2m8s" CreationDate="2016-10-14T09:53:53.187" UserId="5256" />
  <row Id="5477" PostId="4132" Score="0" Text="How your program interacts with your software depends on your GUI framework. Surely your windowing library has some tools for implementing mouse clicks. That in essesnce is not a graphics problem altough drawing pick buffers might be." CreationDate="2016-10-14T10:16:32.163" UserId="38" />
  <row Id="5478" PostId="4132" Score="0" Text="Detecting Mouse Clicks isn't an issue but if you watch this video you'll understand what I'm trying to say  youtu.be/0B_IQK7hMo0?t=2m8s watch from **2:11** to around **3:00**. If you notice the technique in which he draws, when he still holds his mouse down after the click he can tune the curve just by moving the mouse. That is basically what I am trying to implement..." CreationDate="2016-10-14T10:23:40.980" UserId="5256" />
  <row Id="5479" PostId="4132" Score="0" Text="I might be wrong, but I think you basically just add a control point at each extremity of your &quot;pen tool&quot;" CreationDate="2016-10-14T10:32:23.877" UserId="5254" />
  <row Id="5480" PostId="4132" Score="0" Text="Could you explain this with some code or pseudo code? I'm struggling to understand." CreationDate="2016-10-14T13:16:54.640" UserId="5256" />
  <row Id="5481" PostId="4134" Score="0" Text="This answer, this answer is perfection. Thanks." CreationDate="2016-10-14T13:53:41.843" UserId="5256" />
  <row Id="5482" PostId="4134" Score="0" Text="So to clarify I create the Hermite spline and then the next spline will use the previous splines tangent as the start tangent + it will also use the previous point as the start point." CreationDate="2016-10-14T14:02:15.427" UserId="5256" />
  <row Id="5483" PostId="4134" Score="0" Text="Yes, exactly. Just the opposite sign (direction) for the start tangent just like in the AI pen tool" CreationDate="2016-10-14T14:10:55.183" UserId="1952" />
  <row Id="5484" PostId="4132" Score="0" Text="Its just drawing out the tangent of the next spline segment the previous segment just inherits the inverse vector magnitude tangent. A bit like jarkkoL's answer but no need to do hermite. Yes i use illustrator all the time." CreationDate="2016-10-14T14:50:38.847" UserId="38" />
  <row Id="5485" PostId="4129" Score="2" Text="I think he's not asking about the blurred home screen icons, but about the colored &quot;clouds&quot; that slowly fade in and out. See the cloudy green/cyan area in the mid-bottom of the screenshot, or watch the linked video." CreationDate="2016-10-14T15:38:24.240" UserId="48" />
  <row Id="5486" PostId="4129" Score="0" Text="Huh, okay, I didn't notice it before you pointed it out to me. Anyway, I'd try splatting some random, slowly moving, colourful particles to a small texture, up-sampling (while Gaussian-blurring) to screen size and additive-blending them onto the screen. Should I edit the answer or add a new one?" CreationDate="2016-10-14T16:40:50.653" UserId="2817" />
  <row Id="5487" PostId="4136" Score="0" Text="Is there a tangible D3D11 book you advice (like the red, blue, orange books for OpenGL)? Or is it just better to stick to Microsoft's webpages?" CreationDate="2016-10-15T11:15:50.843" UserId="2287" />
  <row Id="5488" PostId="4136" Score="1" Text="@Matthias I learned from the online API docs and other online resources. Maybe someone else can recommend a good book; I'm sure there are some out there." CreationDate="2016-10-15T14:01:46.413" UserId="48" />
  <row Id="5489" PostId="4138" Score="0" Text="This answer corrected my way of thinking of PBR. Your entire answer was very helpful. So to summarize, in the near future graphics programming will shift towards improving the current techniques towards a physically based approach in order to improve visual quality. Also are there any new techniques that are being widely adopted? What I mean is how all games adopted shadow mapping, ambient occlusion, specular &amp; diffuse lighting." CreationDate="2016-10-15T15:37:49.083" UserId="5256" />
  <row Id="5490" PostId="4138" Score="1" Text="The shift towards PBR is a continuous process that keeps gaining a foothold as more programmers and srtists (don't forget artists!) keeps gaining better understanding of it. And more research is done in the domain and better HW enable more of the real-time rendering algorihms to be implemented in a physically based way" CreationDate="2016-10-16T04:35:25.473" UserId="1952" />
  <row Id="5491" PostId="4138" Score="1" Text="Not sure how widely adobted these techniques are, but GGX Is quite dominant BRDF at least. I imagine split-sum approximation for environment lighting is pretty popular too. Various area lighting approximations are gaining popularity and use of proper photometric units to define light illuminance. Also techniques for PBR texture acquisition is of interest to many. To name a few" CreationDate="2016-10-16T04:55:02.093" UserId="1952" />
  <row Id="5495" PostId="4141" Score="0" Text="Thanks for the answer! I guess I'll comeback to Physically Based Area lights in a while since I'm only 15 and all this Math is confusing the hell out of me. Hopefully as time passes there will be more implementation details. and I should be able to understand all of the jargon. I've also read both of those publications you suggested but I've not understood much of it correctly." CreationDate="2016-10-16T16:02:48.033" UserId="5256" />
  <row Id="5496" PostId="4142" Score="1" Text="A static or semistatic background is easy to create in 3D all you need is some reliable tracker points and you are set to go." CreationDate="2016-10-16T20:01:03.903" UserId="38" />
  <row Id="5497" PostId="4145" Score="0" Text="There might also be some ways to optimize the Snell's law calculations, any advice are also welcome on that." CreationDate="2016-10-17T15:50:07.530" UserId="2372" />
  <row Id="5498" PostId="4141" Score="3" Text="A bit of a tangent but great job on pursuing this at 15 years old.  Reading papers might seem confusing now and have a lot of complex math, but each one you read gets a little bit easier.  Keep at it and you'll be way ahead of the game by the time you hit college and/or the workforce." CreationDate="2016-10-17T17:36:53.067" UserId="56" />
  <row Id="5500" PostId="4142" Score="0" Text="...so long as parallax is minimal ;-)" CreationDate="2016-10-17T19:23:21.980" UserId="4494" />
  <row Id="5501" PostId="4143" Score="0" Text="Interesting... I had no idea that motion controlled cameras were actually a common solution. That seems like such a complicated setup... but I guess that goes to show how easy they make it look, when it's done well ;-)" CreationDate="2016-10-17T19:24:31.967" UserId="4494" />
  <row Id="5502" PostId="4147" Score="3" Text="Keeping all the data on the GPU and swapping in/out buffers as you're doing should be fine. It's called &quot;ping-ponging&quot; and is a very common technique in graphics. I can't spot the bug in your code, but your compute shader is quite complicated. I'd advise commenting out everything except a simple write to the buffer and see you can make it work that way. Then gradually build it back up again, testing each piece as you go. That should help narrow down the issue." CreationDate="2016-10-18T01:33:25.707" UserId="48" />
  <row Id="5503" PostId="4147" Score="0" Text="Thanks for looking at it! I'll give it some more investigation. I figured it was a swapping issue since if I print out how many pixels are 0x1 after each iteration, it goes something like (made up numbers) 1000, 900, 860, 890, 860, 890, 860, 890. However, if I just run one part of the switch case over and over, it will decrease monotonically until it can't anymore. Glad to know I'm not crazy about ping-ponging, though! :)" CreationDate="2016-10-18T02:21:33.393" UserId="5316" />
  <row Id="5504" PostId="4146" Score="0" Text="What is your use case? Are you working on a game? Photo-realistic rendering for cinema? CAD? That makes a huge difference to whether people will notice or not." CreationDate="2016-10-18T02:29:45.093" UserId="3003" />
  <row Id="5505" PostId="4142" Score="0" Text="Well no actually for the 3D tracker to work paralax has to be quite high. PS motion controlled camera = Multiaxis robot. Which is something even kids can do these days." CreationDate="2016-10-18T02:44:38.623" UserId="38" />
  <row Id="5506" PostId="4146" Score="0" Text="This is for a game but do people anyway notice it in a CAD Visualization or in Cinema Rendering?" CreationDate="2016-10-18T05:33:41.820" UserId="5256" />
  <row Id="5507" PostId="4147" Score="0" Text="For information: [Cross posted from Stack Overflow](http://stackoverflow.com/questions/40054643/reusing-bindbufferbase-and-opengl-compute-shader)" CreationDate="2016-10-18T11:35:14.093" UserId="231" />
  <row Id="5508" PostId="4146" Score="3" Text="Certainly in cinema rendering it would be noticeable, especially if it was mixed with live action footage. For CAD, it would probably be desired for renderings shown to, say, an architectural client, but probably not as necessary when doing the modeling, I would think. Most games I've played have not had the greatest shadows. I usually notice, but don't know how much a typical person would. Anyway, I just wanted to clarify the use case because it can make a difference between whether you need realtime or not and what quality vs. time tradeoffs you might be willing to make." CreationDate="2016-10-18T16:19:55.693" UserId="3003" />
  <row Id="5510" PostId="4150" Score="1" Text="By talking about traversing edges, it sounds like you don't simply want a bitmap of the silhouette, but some vector description. Is that right? What kind of output are you trying to generate?" CreationDate="2016-10-19T14:22:11.967" UserId="2041" />
  <row Id="5511" PostId="4150" Score="0" Text="Actually, all kinds of description is OK. For example, a sequence of edges, or start with one point then a path. And my confusion is that is there any method through which I can get the description of silhouette without project the model into a 2-D plane?" CreationDate="2016-10-19T16:37:08.633" UserId="5105" />
  <row Id="5512" PostId="4150" Score="0" Text="If you just use GL to render it into a framebuffer with a fragment shader that outputs a constant colour, you'll get a bitmap of the silhouette that's correct regardless of concavity. It's so easy that I don't think I've correctly understood what you're trying to do, which is why I'm trying to clarify." CreationDate="2016-10-19T16:48:49.693" UserId="2041" />
  <row Id="5515" PostId="4150" Score="0" Text="well, I think rendering it into a frambuffer is same as project the model on a 2D plane right? And I want algorithms Or method other than projection  to find silhouette for concave model." CreationDate="2016-10-20T03:22:46.603" UserId="5105" />
  <row Id="5516" PostId="4151" Score="0" Text="I'm voting to close this question as off-topic because it's about representing floating-point numbers; nothing here is specific to graphics." CreationDate="2016-10-20T11:40:46.117" UserId="2041" />
  <row Id="5517" PostId="4151" Score="2" Text="@DanHulme There seem to be quite a lot of questions on this SE that are &quot;how does X work&quot; with respect to GPUs and opengl/dx. I happen to be only indirectly using the results for drawing things, but that doesn't affect whether the question is useful to other people. Closing feels like splitting hairs to me, and the line I'm over is not at all clear to new users. You're going to be spending all your mod time closing questions if they all have to be sufficiently abstract computer science tasks and no concrete &quot;make GPU do X&quot; questions." CreationDate="2016-10-20T13:38:50.750" UserId="5328" />
  <row Id="5518" PostId="4152" Score="0" Text="Use clipping, good idea. Thank you very much." CreationDate="2016-10-20T16:07:56.697" UserId="5105" />
  <row Id="5519" PostId="4151" Score="1" Text="We're still quite a new site, so we don't yet have a lot of consistency in closing questions. This one in particular feels to me like a better fit for SO than for us, because it's &quot;how do I do this specific thing with this library&quot; - about WebGL as a programming environment rather than as a graphics platform. But I'm only one close-voter, so we'll see how the community thinks as a whole. Either way, thank you for helping us to think about our scope." CreationDate="2016-10-20T17:22:25.103" UserId="2041" />
  <row Id="5520" PostId="4151" Score="0" Text="On the subject of the question itself, do you get the same problem just from calling the two functions, with no texture? Can you be sure the errors aren't being introduced by some texture filtering?" CreationDate="2016-10-20T17:25:02.990" UserId="2041" />
  <row Id="5521" PostId="4151" Score="0" Text="@DanHulme Not sure what you mean by &quot;just calling them&quot;. I do have [Javascript equivalents of the two functions](https://github.com/Strilanc/Quirk/blob/master/src/webgl/ShaderCoders.js#L313). The javascript variants work on all the given test values, though that was easier since the intermediate calculations are done with 64 bit precision. I use the JS methods as a comparison tool when debugging the shaders. The shaders do give the right answers on my laptop... just not on tablets and phone." CreationDate="2016-10-20T17:56:50.407" UserId="5328" />
  <row Id="5522" PostId="4151" Score="4" Text="I personally feel this is pretty on topic.  The heart of real time computer graphics is often &quot;how can i get my data to the GPU in an efficient and accurate way&quot; and getting a float to a shader is right in that vein.  This comes up all the time when doing shadertoy things for example, and likely the best person to answer would be one of those folks who have solved it there, on shadertoy or in demoscene type code, who would be here IMO." CreationDate="2016-10-20T17:58:25.827" UserId="56" />
  <row Id="5523" PostId="4153" Score="0" Text="I'm going to go out on a limb and say that it's because the edges are infinitely thin (which follows from the fact that the flat surfaces on each side of the cube are infinitely thin), therefore they have zero surface area, therefore they don't need normals." CreationDate="2016-10-20T20:55:30.627" UserId="2817" />
  <row Id="5524" PostId="4146" Score="0" Text="_&quot;Is it worth it?&quot;_ entirely depends on *your use case*. You can (and should) look at what is done in products with a similar use case and set of constraints as yours, but in the end only you can make that decision." CreationDate="2016-10-21T03:09:10.190" UserId="182" />
  <row Id="5525" PostId="4153" Score="1" Text="Could you detail what makes you think you cannot use lambertian shading on a cube?" CreationDate="2016-10-21T06:04:38.600" UserId="182" />
  <row Id="5526" PostId="4153" Score="0" Text="@ Julien Guertault    I mean that for a mathematical cube there aren't normals on the edge points. The formula of Lambertian shading needs a surface with normals on every point. I know that software applications use some tricks, but with this view pratical computer graphics is not an approximation of theoric computer graphics." CreationDate="2016-10-21T11:56:52.870" UserId="1636" />
  <row Id="5528" PostId="4161" Score="0" Text="How would you blend material parameters like `Metalness`, `Roughness`?" CreationDate="2016-10-22T12:54:27.683" UserId="5256" />
  <row Id="5529" PostId="4161" Score="0" Text="You could just linearly blend them. Also one option is to use height threshold to just &quot;switch&quot; between material layers, e.g. if you want things like sand between rocks because linear blend would look unnatural" CreationDate="2016-10-22T13:05:31.997" UserId="1952" />
  <row Id="5533" PostId="4138" Score="0" Text="Are there any good resources other than siggraph to track advances in real time rendering?" CreationDate="2016-10-23T14:04:36.053" UserId="5256" />
  <row Id="5534" PostId="4158" Score="1" Text="One of the answers in http://computergraphics.stackexchange.com/questions/2161/modern-screen-space-ambient-occlusion-techniques links to a useful paper: http://frederikaalund.com/a-comparative-study-of-screen-space-ambient-occlusion-methods/ It goes through a few different AO techniques in some detail." CreationDate="2016-10-22T16:04:58.150" UserId="554" />
  <row Id="5535" PostId="4165" Score="0" Text="Why the down vote? :P" CreationDate="2016-10-23T15:22:22.593" UserId="56" />
  <row Id="5536" PostId="4165" Score="0" Text="It's not true that spherical harmonics are not good at representing fine details. Like the Fourier transform, they can reconstruct the original signal exactly if you keep all the frequencies. It's just that they make it easy to save space by throwing away high frequencies if you don't need them." CreationDate="2016-10-23T15:32:19.540" UserId="2041" />
  <row Id="5537" PostId="4165" Score="0" Text="And sorry, I thought after casting the vote it was harsh to downvote for a single dubious claim in an otherwise helpful answer, but my vote was locked in by the time I had second thoughts." CreationDate="2016-10-23T15:33:10.210" UserId="2041" />
  <row Id="5538" PostId="4165" Score="0" Text="Ah. Yeah it's possible, in the same way that it's possible to represent any data set with a polynomial, but in practice, both are bad choices for needing close fits to many data points.  With a polynomial you need N terms of an N order function to exactly fit N data points for instance, which makes it a worse choice than just an array since it's calculation, not lookup, to get a data point out.  Similarly, in practical terms, spherical harmonics are a bad choice for spherical data with high frequency content that you want to preserve. It's not a good choice in those situations." CreationDate="2016-10-23T15:52:05.053" UserId="56" />
  <row Id="5539" PostId="4165" Score="0" Text="That's true if you want to be able to reconstruct individual samples, but not all uses of Fourier require that - similarly for not all uses of SH. If you're going to do a convolution, it's much cheaper to do that in the frequency domain before transforming back to samples. Would you mind me proposing an edit to make this clearer in your answer, after I've finished my own answer?" CreationDate="2016-10-23T15:59:41.780" UserId="2041" />
  <row Id="5540" PostId="4165" Score="0" Text="Good point about convolution! Sure, go for it." CreationDate="2016-10-23T16:06:30.587" UserId="56" />
  <row Id="5541" PostId="4166" Score="0" Text="Great answer this really clarified everything! So basically Light Probes are a easy way to calculate the lighting on moving characters which prevent us from re calculating the GI for the entire scene. Spherical Harmonics on the other hand are used to filter out the high frequencies. (Correct me if I'm wrong, I'm just trying to see if I have the right understanding)." CreationDate="2016-10-23T16:19:44.507" UserId="5256" />
  <row Id="5543" PostId="4166" Score="0" Text="&quot;if you use the same number of sine waves as there were samples (pixels) in the original image, you can reconstruct the image exactly&quot;, actually not true. For example square wave requires infinite number of frequencies for exact representation" CreationDate="2016-10-23T19:50:30.880" UserId="1952" />
  <row Id="5545" PostId="4167" Score="1" Text="Could you show what you have tried, and explain what you currently understand?" CreationDate="2016-10-23T22:03:19.600" UserId="231" />
  <row Id="5546" PostId="4155" Score="0" Text="Are you able to include your code in the question? I recommend removing most of the code until you stop getting the error message, so you can then provide just enough code to cause the error message. This will help narrow down the source of the problem and is likely to make it easier to answer your question." CreationDate="2016-10-23T22:09:37.427" UserId="231" />
  <row Id="5547" PostId="4166" Score="0" Text="@JarkkoL Sure, a true square wave does. But if you've discretized it by sampling, then you only need the same number of frequencies to make the error less than the sampling error. It's a handy outcome of Nyquist's theorem (that the highest frequency present in the sampled signal is half the sample rate)." CreationDate="2016-10-23T23:06:43.267" UserId="2041" />
  <row Id="5548" PostId="4155" Score="0" Text="The problem is not my Code, it used to work fine with OS X El Capitan. The Error I get has something todo within the underlaying window management of OS X Sierra. I have contacted Apple already and they are working on resolving the issue" CreationDate="2016-10-24T06:58:24.050" UserId="5334" />
  <row Id="5549" PostId="4168" Score="0" Text="I'm not sure there are that many true 24bit displays. I suspect many will probably pad each pixel out to 4 bytes, thus becoming 3145728, i.e. 3MB." CreationDate="2016-10-24T07:19:39.417" UserId="209" />
  <row Id="5550" PostId="4155" Score="0" Text="If there's some aspect of your code that worked with the old version but doesn't work with the new version, then we would need to see your code to narrow down what needs to change to work with the new version. If the problem is a bug in closed source software provided by a company, then that would make this question off topic here as we have no way of fixing the problem. If you think there's a chance it is the first, providing a [minimal working example](http://meta.computergraphics.stackexchange.com/questions/126/should-we-require-minimal-working-examples-mwes) will allow us to investigate." CreationDate="2016-10-24T12:55:40.133" UserId="231" />
  <row Id="5552" PostId="4168" Score="1" Text="XGA is a specific standard from a while back, when they didn't have the memory to pad to 32bit. Check it out here:&#xA;https://en.wikipedia.org/wiki/Graphics_display_resolution#XGA_.281024x768.29" CreationDate="2016-10-24T14:04:49.003" UserId="56" />
  <row Id="5553" PostId="4166" Score="0" Text="Ah yes, that's true of course. I suppose you need half the frequency but complex (vs real) frequency domain results. Or use DCT &amp; real domain with twice the frequency of DFT." CreationDate="2016-10-24T14:28:11.923" UserId="1952" />
  <row Id="5555" PostId="4153" Score="0" Text="Ah ok. So, pixels are infinitely small sample points on a grid.  The edge of a cube is infinitely thin - a line segment.  Because of this, we don't often see edges in rendering. When we do, they show up as holes in the cube, z fighting at the edge and similar. In practice though, you aren't ever shading the edge, just the faces." CreationDate="2016-10-25T01:06:25.817" UserId="56" />
  <row Id="5559" PostId="4138" Score="1" Text="@ArjanSingh I think one way to find this out, is to look at the references section at the end of any SIGGRAPH article, to see where the references were published.  Those might indicate some other good resources." CreationDate="2016-10-25T14:05:36.290" UserId="5356" />
  <row Id="5560" PostId="4172" Score="0" Text="Don't know the details but in PenTile subpixels are shared between framebuffer pixels so you can't control them individually. e.g. could be that if you have 3x3 PenTile subpixels, they map to 2x2 framebuffer pixels and the intensity of subpixel is determined by the overlap &amp; framebuffer RGB values. This is purely a guess though" CreationDate="2016-10-25T14:23:50.283" UserId="1952" />
  <row Id="5563" PostId="4172" Score="0" Text="@JarkkoL That's the point of my question. I know the subpixels are shared, but I need to know what function determines the intensity of each subpixel, so I can predict what results I'll get from certain framebuffer values." CreationDate="2016-10-25T15:44:46.733" UserId="2041" />
  <row Id="5567" PostId="4172" Score="0" Text="Your question indicated otherwise thus the clarification. If you know the native resolution vs subpixel reso, it might give some idea about the transform" CreationDate="2016-10-25T17:23:26.737" UserId="1952" />
  <row Id="5568" PostId="4178" Score="2" Text="If the input photo is only looking in one direction then this will be impossible without distortion. You can either use a collection of photos covering all the different directions, or you can have a distorted result, which will also have a discontinuity at the left and right edges." CreationDate="2016-10-25T19:26:04.347" UserId="231" />
  <row Id="5569" PostId="4178" Score="0" Text="If you want this picture to be shown constantly then, use the UI image provided by unity." CreationDate="2016-10-25T20:11:02.463" UserId="3437" />
  <row Id="5570" PostId="4179" Score="0" Text="Including your initial ideas may help people to tailor the explanation and highlight any misconceptions." CreationDate="2016-10-25T22:14:08.147" UserId="231" />
  <row Id="5571" PostId="4180" Score="1" Text="You wouldn't just get a weird seam if you tried to use this image as an equirectangular map: it would also get increasingly distorted towards the poles. The stars would get squished together in one axis, so they'd look like lines." CreationDate="2016-10-26T08:07:44.140" UserId="2041" />
  <row Id="5573" PostId="4181" Score="0" Text="Sorry, maybe I was not clear in the question. Of course I know what you're explaining in your answer. I wanted to know why are we multiplying by $M^{−1}$ in one case and by $M$ in another. In which case do we use $M$ and in which case do we use $M^{−1}$?" CreationDate="2016-10-26T11:13:08.850" UserId="4718" />
  <row Id="5574" PostId="4181" Score="0" Text="Because $M$ transforms from one space to another (e.g. object$\rightarrow$world space), inverse of the matrix defines the opposite transformation (e.g. world$\rightarrow$object space)" CreationDate="2016-10-26T12:55:32.787" UserId="1952" />
  <row Id="5575" PostId="4181" Score="0" Text="Ok, so let me try to be more specific: why $M$ (and not $M^{-1}$) transforms from object to world space?" CreationDate="2016-10-26T12:58:07.683" UserId="4718" />
  <row Id="5576" PostId="4181" Score="0" Text="It's natural to define how objects are oriented in the world rather than how world is oriented in objects. Mathematically nothing prevents you from defining $M$ as world$\rightarrow$object transform and use $M^{-1}$ to define object$\rightarrow$world transform though, it's just unnatural (:" CreationDate="2016-10-26T13:51:01.993" UserId="1952" />
  <row Id="5577" PostId="4180" Score="0" Text="That's true about the distortion. However, I think there would be a weird seam because the purple band wouldn't meet itself. It would just suddenly stop with a hard edge and restart higher up with another hard edge." CreationDate="2016-10-26T14:32:17.553" UserId="3003" />
  <row Id="5578" PostId="4180" Score="1" Text="Yes, you'd get a seam as well. But you already mentioned a way to reduce that problem, so I thought I'd better caution the OP that even after fixing the seam they'll still have distortion." CreationDate="2016-10-26T16:26:56.160" UserId="2041" />
  <row Id="5579" PostId="4181" Score="0" Text="@nbro It might help to think of $M^{-1}$ as undoing whatever $M$ does. If you apply $M$ and then $M^{-1}$ you get back to where you started. So $M$ takes you from object to world, and $M^{-1}$ takes you from world to object. It doesn't matter what you call the matrix. Whatever you did to get from object to world, you have to do the opposite to get back from world to object." CreationDate="2016-10-26T17:16:30.310" UserId="231" />
  <row Id="5580" PostId="4183" Score="0" Text="Thanks for the answer!  Interestingly, I have had some limited success at this, thanks to some ideas from someone on twitter.  I'm experimenting more to come up with some final info &amp; a blog post, and will post an answer here with the details.  It's not a general solve, but it is still a bit interesting.  Maybe extensible.  Also had a friend mention he has been able to do this with other limited success by faking modulus mathematically, like using a badlimited saw wave.  I'll share some shadertoy links in the next comment for the curious." CreationDate="2016-10-26T17:30:58.593" UserId="56" />
  <row Id="5581" PostId="4183" Score="1" Text="Shadertoy from sebbi showing his idea: https://www.shadertoy.com/view/lly3Rc&#xA;Me exploring and formalizing some things: https://www.shadertoy.com/view/MlK3zt&#xA;Limited success for ray vs infinite layers of concentric circles: https://www.shadertoy.com/view/4tyGDK&#xA;I'm currently working on ray vs infinitely repeating pillars. Looking promising so far.&#xA;In the end these things are interesting but not in general very useful.  Maybe they can be extended. ::shrug::" CreationDate="2016-10-26T17:34:08.840" UserId="56" />
  <row Id="5582" PostId="4181" Score="0" Text="@JarkkoL well, its not nesseserily so unnatural if you build out of basic operations. Rotate is just reverse angle, translate is just reverse angle, scale is inverse number, skew is reverse. Forming the matrix from a vector will be weird at first. But the concept would be like moving the world to move object, which is really really odd and unnatural." CreationDate="2016-10-26T17:57:25.637" UserId="38" />
  <row Id="5583" PostId="4184" Score="0" Text="This is probably a a dumb question, but do you mean uv, instead of uwv? Are there 3 components to your texture coordinate? And if so, why is it uwv, not uvw?" CreationDate="2016-10-26T18:24:31.963" UserId="56" />
  <row Id="5584" PostId="4184" Score="1" Text="@AlanWolfe The titlebar in the screenshot says &quot;Edit UVWs&quot; so the questioner probably means that." CreationDate="2016-10-26T18:45:16.803" UserId="2041" />
  <row Id="5585" PostId="4183" Score="0" Text="Regarding there needing to be an inverse operation for modulus, there is the form: i = 3N where N is in Z, as a reverse of i%3 = 0.  If we had a solution of that form, we could plug in a value for N, presumably 0?" CreationDate="2016-10-26T19:46:23.730" UserId="56" />
  <row Id="5586" PostId="4184" Score="0" Text="@AlanWolfe I have a lack of experience in 3D graphics, so I called it according to 3DSMax modifier name &quot;Unwrap UVW&quot;. Renamed to UVs to avoid misunderstanding." CreationDate="2016-10-26T21:04:43.367" UserId="5363" />
  <row Id="5587" PostId="4184" Score="0" Text="Instead of using multiple UV's for different texture types (albedo, normal, etc.), you should use single UV and multiple textures for different types." CreationDate="2016-10-26T21:55:38.840" UserId="1952" />
  <row Id="5588" PostId="4185" Score="0" Text="Can you clarify what you mean by a &quot;spherical Fourier transform&quot;? I googled, but didn't turn up anything that sounds like it would match this question." CreationDate="2016-10-27T04:24:36.350" UserId="48" />
  <row Id="5589" PostId="4185" Score="0" Text="I'm thinking like taking a unit sphere, breaking it into the two angles that parameterize it, so you have a 2d function, then using 2d DFT on that." CreationDate="2016-10-27T04:30:17.043" UserId="56" />
  <row Id="5590" PostId="4185" Score="0" Text="Related: http://math.stackexchange.com/questions/17479/how-to-perform-a-fourier-transform-in-spherical-coordinates" CreationDate="2016-10-27T05:39:55.723" UserId="56" />
  <row Id="5591" PostId="4184" Score="0" Text="@JarkkoL, texture of greater size is needed in case of single UVs. For example, my model should have bump on the top part and image on the rest part - then single UVs would be divided into two parts: for bump and for image. Each texture map will use only a half of avalable space (image with &quot;sun&quot; illustrates a cube UVs with 6 different texture maps for each side)." CreationDate="2016-10-27T07:16:42.600" UserId="5363" />
  <row Id="5593" PostId="4151" Score="0" Text="@Craig Gidney example 1 is incorrect but the fixed version works well with values between 0 and 1 (i use a similar version to convert a 24 bit depth buffer value into rgb8 texture with no precision loss)." CreationDate="2016-10-27T12:57:41.497" UserId="5364" />
  <row Id="5594" PostId="4184" Score="0" Text="In general models have all types of textures used for all the parts with matching UV space thus single UV channel is fine. E.g. you would allocate both normal &amp; albedo textures for the top part of your model. Furthermore you should try to have constant pixel density for a good UV map. You are talking about very specific UV space optimization but this isn't the way models are UV mapped in general, IME." CreationDate="2016-10-27T13:37:59.750" UserId="1952" />
  <row Id="5599" PostId="4151" Score="0" Text="@Raxvan How did you &quot;fix&quot; it? Floats get more precise as you travel to 0, but the linked example-1 method is a 32-bit fixed-point thing. There's nearly a billion representable values in the range from 2^-126 to 2^-32, and they all get totally trashed by that kind of approach." CreationDate="2016-10-27T14:51:57.663" UserId="5328" />
  <row Id="5602" PostId="4151" Score="0" Text="@Craig Gidney take the example 1 as it is and replace **all** 256.0 values with 255.0. just remember that that method works for values between 0 and 1 actually ever 1 just [0..1)" CreationDate="2016-10-27T15:40:17.413" UserId="5364" />
  <row Id="5603" PostId="4151" Score="0" Text="@Raxvan Oh. You should replace some of the 256s, but not all of them. And it does round the many values under 2^-32 into 0." CreationDate="2016-10-27T15:42:42.603" UserId="5328" />
  <row Id="5604" PostId="4186" Score="0" Text="Would there be much point in mapping the sphere differently to compensate for the poles problem? Maybe something taking the vertical angle as a percent and squaring it, to pull the sample points towards the equator non linearly?" CreationDate="2016-10-27T15:49:31.127" UserId="56" />
  <row Id="5605" PostId="4186" Score="1" Text="The other reason why SH is handy is it's a rotationally invariant representation. Rotating an SH basis function (by any axis and angle) transforms it to a linear combination of other SH basis functions of the same order. So, you can rotate an SH-represented function by [applying an appropriate matrix to the SH coefficients directly](http://www.filmicworlds.com/2014/07/02/simple-and-fast-spherical-harmonic-rotation/). You don't get this if you do a 2D Fourier transform in lat-long parameter space." CreationDate="2016-10-27T17:49:50.580" UserId="48" />
  <row Id="5606" PostId="4192" Score="2" Text="Very good answer, especially for the pointing to the Wikipedia arcticle that I could not reach in google searches." CreationDate="2016-10-27T20:34:58.403" UserId="5367" />
  <row Id="5608" PostId="4186" Score="2" Text="@NathanReed Yes, I was thinking about that, but neglected to include it in the answer. I've updated it now with some more thoughts about that. Thanks for mentioning it." CreationDate="2016-10-27T22:51:03.530" UserId="2041" />
  <row Id="5611" PostId="4192" Score="1" Text="I would add that aliasing is a general phenomenon that can be found in contexts other than (screen) &quot;spatial aliasing&quot; or &quot;temporal aliasing&quot;: http://en.wikipedia.org/wiki/Aliasing" CreationDate="2016-10-28T06:27:13.267" UserId="110" />
  <row Id="5612" PostId="4194" Score="0" Text="errrr I assign range from [-2,+2] to [0, 800] and [-1.5, 1.5] to [0, 600] and not the other way around. I thought this was clear from my question." CreationDate="2016-10-28T07:50:09.040" UserId="5371" />
  <row Id="5613" PostId="4194" Score="1" Text="@quantum231 unless you get a coordinate in the [0, 600][0, 800] space and need to know what it was in the [-1.5, 1.5][-2, 2] space to sample it." CreationDate="2016-10-28T08:28:04.170" UserId="137" />
  <row Id="5614" PostId="4194" Score="2" Text="Yeah, but that's not correct. You need to do the inverse to evaluate the Mandelbrot equation for each pixel in your image exactly once. I should have probably emphasized that in my answer." CreationDate="2016-10-28T08:34:57.753" UserId="1952" />
  <row Id="5616" PostId="4184" Score="0" Text="You may want to split this up into two separate questions because they are different use cases (might help with getting answers). For your first question though, you say you want to tile a texture (presumably to save space). What do you mean by this? Are talking about overlapping the four triangles, or something totally different?" CreationDate="2016-10-29T03:50:42.547" UserId="2707" />
  <row Id="5617" PostId="4177" Score="0" Text="What are the benefits of using this method over the simpler Image based Lighting?" CreationDate="2016-10-29T11:45:29.620" UserId="5256" />
  <row Id="5618" PostId="4177" Score="0" Text="You can have better quality low-frequency lighting with similar storage and performance requirements than say using cubemaps." CreationDate="2016-10-29T14:36:46.703" UserId="1952" />
  <row Id="5619" PostId="4197" Score="0" Text="Does `glGetError()` return any errors during the process?" CreationDate="2016-10-29T15:45:29.803" UserId="3003" />
  <row Id="5620" PostId="4196" Score="0" Text="Radiosity is a way to compute global illumination under the assumption that all surfaces are perfectly diffuse (Lambertian)." CreationDate="2016-10-29T17:24:47.147" UserId="106" />
  <row Id="5621" PostId="4197" Score="0" Text="No, it returns 0 after each step." CreationDate="2016-10-29T18:27:31.640" UserId="4958" />
  <row Id="5622" PostId="4198" Score="0" Text="Do most engines like UE4, Unity, Frostbite etc use this as a global illumination technique alongside other methods? I know UE4 has Lightmass and Unity uses Enlighten, are they basically radiosity 'engines'? What other ways can you bake light or calculate indirect lighting in real time?" CreationDate="2016-10-30T05:49:09.223" UserId="5256" />
  <row Id="5623" PostId="4198" Score="2" Text="All real-time engines combine some kind of indirect illumination approximation (radiosity based or something else) with other techniques for more complete rendering equation. Your question about different techniques to approximate indirect illumination warrants its own question though as it's too broad of a question to answer in comments." CreationDate="2016-10-30T16:38:57.687" UserId="1952" />
  <row Id="5624" PostId="4201" Score="0" Text="How would that work? gl_FragCoord are the screen UVs. If the mesh occupies part of the screen it's not going to be textured correctly from these." CreationDate="2016-10-31T09:07:49.493" UserId="3331" />
  <row Id="5625" PostId="4201" Score="0" Text="@aces Yes I agree, I don't really get how this could work." CreationDate="2016-10-31T09:48:04.310" UserId="2372" />
  <row Id="5626" PostId="4200" Score="0" Text="Could you explain further the end result. Are you after decal projection? A picture of the expected result would help." CreationDate="2016-10-31T10:16:53.860" UserId="3331" />
  <row Id="5627" PostId="4200" Score="0" Text="This is still for eye rendering :) post updated." CreationDate="2016-10-31T10:47:44.343" UserId="2372" />
  <row Id="5628" PostId="4200" Score="0" Text="@MaT based on the images i don't think a normal map is enough , you also need a height map. you need to compute the direction vector on the uv space and then offset it based on surface &quot;depth&quot;" CreationDate="2016-10-31T12:35:33.907" UserId="5364" />
  <row Id="5629" PostId="4200" Score="0" Text="My question seems to be really badly formulated :P&#xA;The Normal screenshot shows the eyes without any rotation therefore the occlusion map is well applied but as soon as you rotate the eye, the occlusion classical behaviour breaks the effect.&#xA;That's why I am trying apply the occlusion map not only in object space to avoid the effect of the eye rotation, but I don't see how." CreationDate="2016-10-31T12:40:35.797" UserId="2372" />
  <row Id="5632" PostId="4201" Score="0" Text="@Syntac_ The picture makes the intent clearer, but it certainly can be textured correctly with this if you save the rendering of the pre-rotated mesh onto a texture and used screenspace coordinates to look it up. See my edit for a more detailed explanation." CreationDate="2016-11-01T02:58:26.640" UserId="2707" />
  <row Id="5633" PostId="4076" Score="0" Text="Vulkan is using system-on-a-thread ? Were as DX12 is fully multi-threaded?" CreationDate="2016-11-01T04:03:00.227" UserId="3073" />
  <row Id="5634" PostId="4207" Score="0" Text="Thanks for hints. I corrected glTexImage3D function as you pointed but the problem seem to persist. What I noticed is that when I setup texture as RGBA32F, RGBA, GL_FLOAT and then bind it as RGBA32I image and use iimage in shader it works. But if I setup texture as RGBA32I, RGBA_INTEGER, GL_INT is is empty. So it is something with texture setup still. But it *is* correct and glGetError returns 0... I updated my nvidia drivers but this is not driver fault." CreationDate="2016-11-01T11:40:31.747" UserId="4958" />
  <row Id="5635" PostId="4208" Score="0" Text="I would suggest using uniforms for most of these parameters, such as transformation matrices. You can use uniform buffers to quickly switch uniforms when rendering. Also, the vao just stores the attribute bindings and not the vbo itself." CreationDate="2016-11-01T15:04:41.280" UserId="4768" />
  <row Id="5637" PostId="4076" Score="0" Text="Am not sure if DX12 is using one pipeline, might be worth looking at the documentation for DX12 for the specifics." CreationDate="2016-11-01T19:48:27.887" UserId="4829" />
  <row Id="5638" PostId="4076" Score="1" Text="@PaulHK, according to the DX12 article, that would be case. Looks, like VUlkan is using multiple threads as well: https://www.khronos.org/registry/vulkan/specs/1.0/xhtml/vkspec.html#fundamentals-threadingbehavior" CreationDate="2016-11-01T19:52:59.323" UserId="4829" />
  <row Id="5639" PostId="4212" Score="0" Text="http://stackoverflow.com/questions/1313259/tiling-simplex-noise" CreationDate="2016-10-27T08:26:31.723" UserDisplayName="Christophe Roussy" />
  <row Id="5640" PostId="4213" Score="0" Text="Thanks for your response! Two questions: 1) The wiki article uses `R' G' B'` in the equation which corespond to &quot;gamma-compressed&quot; values. So wouldn't give me this wrong numbers because I'm using linear RGB? 2) Could you explain why you've chosen this formula from the article? I'm confused why this does apply to my case and not the one with `0.2126 ...`" CreationDate="2016-11-01T20:58:45.803" UserId="5351" />
  <row Id="5641" PostId="4213" Score="0" Text="@PeteParly It's a matter of choice. I tend to use this formula for calculating the brightness of colors in linear space, because it gives subjectively more pleasing results than the rather aggressive Rec. 709 formula, at least on my monitor. As for gamma-compressed colors, the wiki article mentions the same formula is arbitrarily used for both gamma and linear color spaces, despite it giving different results." CreationDate="2016-11-01T21:41:27.770" UserId="3470" />
  <row Id="5642" PostId="4213" Score="0" Text="@PeteParly The best thing you can do is to try out multiple formulas and choose the one that works for you. I'm not aware of any particular standard, besides the Lab color space, which tries to more closely approximate human vision: https://en.wikipedia.org/wiki/Lab_color_space&#xA;&#xA;Using it is a bit more involved, though. To get just the gray color, you'd have to convert your RGB color to Lab, then set the a and b components to 0, effectively removing chrominance, and then convert it back to the RGB space of your choice." CreationDate="2016-11-01T21:44:07.637" UserId="3470" />
  <row Id="5643" PostId="4213" Score="0" Text="I think this &quot;The best thing you can do is to try out multiple formulas and choose the one that works for you.&quot; answers a bit of my confusion in my head :) I'll try that and accept your answer" CreationDate="2016-11-01T21:48:25.713" UserId="5351" />
  <row Id="5645" PostId="4214" Score="0" Text="Are you looking for a hash function that gives an *identical* hash for images similar enough to be different sized versions of the same original, or one that gives more *similar* results for more similar inputs?" CreationDate="2016-11-02T01:46:27.097" UserId="231" />
  <row Id="5646" PostId="4212" Score="2" Text="From Wikipedia: &quot;Simplex noise has no noticeable directional artifacts (is visually isotropic), though noise generated for different dimensions are visually distinct (e.g. 2D noise has a different look than slices of 3D noise, and it looks increasingly worse for higher dimensions[citation needed]).&quot;, so sounds like what you got" CreationDate="2016-11-02T01:50:10.323" UserId="1952" />
  <row Id="5647" PostId="4214" Score="0" Text="I don't think this is answerable without clarifying the purpose of the hash function, so I've put it on hold until it is clarified and then it can be edited and reopened." CreationDate="2016-11-02T01:57:49.660" UserId="231" />
  <row Id="5648" PostId="4214" Score="0" Text="Sounds like OP wants a robust way to compare if two images are the same even if they have gone through resize or lossy compression, by generating a hash key from it. Sounds a bit something OpenCV might help with" CreationDate="2016-11-02T02:02:17.523" UserId="1952" />
  <row Id="5649" PostId="4214" Score="0" Text="Similar question with &quot;Feature Matching&quot; answer: http://stackoverflow.com/questions/11541154/checking-images-for-similarity-with-opencv" CreationDate="2016-11-02T02:17:13.103" UserId="1952" />
  <row Id="5651" PostId="4198" Score="0" Text="Could you recommend any good tutorials on the topic?" CreationDate="2016-11-02T08:23:22.180" UserId="5256" />
  <row Id="5653" PostId="4217" Score="0" Text="Thanks! And you are right, the OpenGL-Class has a static member `static GLuint NumberAddedAttributes` which is increased every time an attribute is added. And if each VAO really has its own attribute space, this would mean that i only have to make this member non-static and everything should work. But when i tried this yesterday, i ran into some problems. But now since I read your answer I'll check it again." CreationDate="2016-11-02T09:41:51.020" UserId="5399" />
  <row Id="5654" PostId="4217" Score="0" Text="I would like to extent the question to SSBO (shader storage buffer object): I also use them and create the shader preamble automatically. I guess in case of SSBO binding points the counter has to be global for all shader?" CreationDate="2016-11-02T10:04:27.940" UserId="5399" />
  <row Id="5655" PostId="4215" Score="0" Text="Lighting a scene and final rendering the lit triangles are orthogonal." CreationDate="2016-11-02T12:04:34.873" UserId="137" />
  <row Id="5656" PostId="4213" Score="0" Text="@PeteParly you can do better if you have a colorimeter, than you can make a icc profile and be more exact for your monitor" CreationDate="2016-11-02T17:02:03.867" UserId="38" />
  <row Id="5657" PostId="4214" Score="0" Text="@trichoplax yes I am looking for a hash function that produces an 'identical' digest regardless (or at least to a good degree) of the size, orientation and skewness." CreationDate="2016-11-02T18:27:25.123" UserId="5401" />
  <row Id="5658" PostId="4214" Score="0" Text="@picolo it sounds like you would prefer a slightly changed image to show as a 100% match (identical hash), but you would also be happy with a high percentage match (similar hash) if there isn't a way to make it 100%. If that's the case, you can edit the question to make it clear what you would find acceptable and it can be reopened." CreationDate="2016-11-02T21:15:34.340" UserId="231" />
  <row Id="5659" PostId="4212" Score="2" Text="Also from wikipedia &quot;For higher dimensions, n-spheres around n-simplex corners are not densely enough packed, reducing the support of the function and making it zero in large portions of space.&quot;" CreationDate="2016-11-02T23:05:05.443" UserId="56" />
  <row Id="5660" PostId="4212" Score="1" Text="You might want to also insure that all dim are returning full range values..some implementation do not." CreationDate="2016-11-03T08:56:42.733" UserId="2831" />
  <row Id="5661" PostId="26" Score="0" Text="The patent is narrowly defined and includes the bit-twiddling permutation method.  SEE claim 1: https://www.google.com/patents/US6867776" CreationDate="2016-11-03T09:07:59.743" UserId="2831" />
  <row Id="5662" PostId="4215" Score="1" Text="Like, at a right angle to each other? Dude that is deep." CreationDate="2016-11-03T09:14:44.753" UserId="5403" />
  <row Id="5663" PostId="4215" Score="0" Text="No I mean that once you have the lighting information you can simply leave out the ceiling geometry in the final render of the lit scene." CreationDate="2016-11-03T09:20:12.143" UserId="137" />
  <row Id="5667" PostId="4227" Score="0" Text="Will an implementation clamp the resolved value into the bit depth of the source before writing to the destination? OR, can I assume the precision is &quot;enough&quot; throughout the resolve to not lose any information?" CreationDate="2016-11-03T21:00:38.740" UserId="3041" />
  <row Id="5668" PostId="4226" Score="0" Text="Current GPUs only support up to 8 samples per pixel, BTW, so your scenario with 32 samples in 4-bit color unfortunately isn't possible using hardware multisampling." CreationDate="2016-11-03T21:45:00.907" UserId="48" />
  <row Id="5669" PostId="4226" Score="1" Text="@NathanReed Provide a reference of your claim. Yesterday i queried an Nvidia Quadro 4000 of possible framebuffer configurations and up to 64 samples was provided." CreationDate="2016-11-03T21:46:04.597" UserId="3041" />
  <row Id="5670" PostId="4226" Score="1" Text="@NathanReed: Not true at all. There are [many implementations that support a sample count greater than 8](http://opengl.gpuinfo.org/gl_stats_caps_single.php?listreportsbycap=GL_MAX_COLOR_TEXTURE_SAMPLES). Indeed, pretty much every GPU that supports GL 4.x supports more than 8 samples. Not because 4.x requires it, but simply because they can. Even many of Intel's GPUs support 16. The only exceptions seem to be AMD GPUs that don't use their GCN core." CreationDate="2016-11-03T22:35:04.340" UserId="2654" />
  <row Id="5671" PostId="4222" Score="0" Text="Would you know of any online tutorials that do this?" CreationDate="2016-11-04T05:56:49.953" UserId="5256" />
  <row Id="5672" PostId="4222" Score="1" Text="@ArjanSingh The UE4 source code has examples of how to do this exact method." CreationDate="2016-11-04T08:31:54.987" UserId="3331" />
  <row Id="5673" PostId="4222" Score="1" Text="It's a bit more complicated in UE4 because it also generates lens flares, and the shader system requires some engine know-how to understand, but yeah, `AddBloom()` [here](http://tinyurl.com/hswdnqg) is a good starting point. However, for a super-simple technique that involves an optional &quot;lens dirt&quot; effect, you could check out the post-processing library I wrote back at university: [the CPU side](http://tinyurl.com/zoclj9k) (lines 222-241) and the [bright pass](http://tinyurl.com/zjt3lgh) and [compositing](http://tinyurl.com/jdjdwrw) shaders. Apologies for TinyURL, comment was too long." CreationDate="2016-11-04T09:09:22.000" UserId="2817" />
  <row Id="5674" PostId="4222" Score="0" Text="Also, [this blogpost](https://extremeistan.wordpress.com/2014/09/24/physically-based-camera-rendering/) is a pretty cool resource for camera effects such as this. It links to [Intel's tutorial](https://software.intel.com/en-us/articles/compute-shader-hdr-and-bloom) on the subject, with compute shaders in mind, for instance." CreationDate="2016-11-04T09:20:08.297" UserId="2817" />
  <row Id="5677" PostId="4220" Score="0" Text="Are you trying to group all photos taken from the same location together, or to identify the location a photo was taken from, or to arrange the photos in a grid according to where they were taken from? Is it just their relative arrangement that is important, or also their absolute location?" CreationDate="2016-11-04T16:21:38.627" UserId="231" />
  <row Id="5678" PostId="4220" Score="0" Text="Are these &quot;360 photos&quot; 360 degree panoramic photographs, looking horizontally in all directions? Or are they spherical photographs, looking in all directions, including up and down as well as horizontally?" CreationDate="2016-11-04T16:24:11.263" UserId="231" />
  <row Id="5679" PostId="4220" Score="0" Text="I have put the question on hold until it can be clarified. Once it has been edited to make clear what is provided and what is required, it can be reopened if it is on topic." CreationDate="2016-11-04T16:26:37.400" UserId="231" />
  <row Id="5680" PostId="4227" Score="0" Text="@Andreas: I would not suggest attempting to use multisampling to make precise calculations of anything. You should use it for its intended purpose: making pictures. If you need guaranteed precision of some calculation, multisampling is the wrong tool to employ; it has too many implementation-dependent variables." CreationDate="2016-11-04T16:57:26.603" UserId="2654" />
  <row Id="5682" PostId="4227" Score="0" Text="For the record my (current) mission is trying out order independent translucency and specifically smoke by altering the number of samples covered and their depth. I probably need many samples, at least 8. To make that work I need some guarantees of how OpenGL resolves the multisample surface. And I´d prefer it to be hardware accelerated. I´ll be back with more questions if I screw up." CreationDate="2016-11-05T11:25:48.593" UserId="3041" />
  <row Id="5684" PostId="4230" Score="4" Text="Some diagrams or screenshots of what you mean might be helpful. When you say the two new points are a threshold apart, does that mean they aren't evenly spaced along the edge? I don't quite get how that helps with temporal stability." CreationDate="2016-11-05T15:29:02.347" UserId="48" />
  <row Id="5686" PostId="1518" Score="0" Text="`GL_SHADER_STORAGE_BARRIER_BIT` Wrong barrier. The barrier you provide states how you will *use* the memory. Not how the shader wrote to it. You are doing a pixel transfer, so you need to use `GL_TEXTURE_UPDATE_BARRIER_BIT`" CreationDate="2016-11-05T17:57:19.030" UserId="2654" />
  <row Id="5690" PostId="4234" Score="1" Text="Great answer as always" CreationDate="2016-11-06T06:59:40.213" UserId="5256" />
  <row Id="5691" PostId="4236" Score="0" Text="Maybe there are another stackexhange places, where this question would be more appropriate to ask?" CreationDate="2016-11-06T08:51:54.730" UserId="5430" />
  <row Id="5692" PostId="4235" Score="1" Text="So after everything (sky, fog, lighting, shading etc) goes physically based, what comes next? Will most people just try to improve current approximations? Where do you think rendering will head to in the coming years?" CreationDate="2016-11-06T10:08:26.243" UserId="5256" />
  <row Id="5693" PostId="4236" Score="0" Text="This is on topic here. The only thing I would suggest is explaining what you have already tried, and your current understanding of why this is incorrect. Sometimes describing what is wrong can give a clue about what is causing it. Describing what you have already tried is required on any Stack Exchange site though - it isn't a reason to move the question." CreationDate="2016-11-06T10:39:58.827" UserId="231" />
  <row Id="5695" PostId="4236" Score="0" Text="Are you using floating point numbers?" CreationDate="2016-11-07T01:00:29.867" UserId="2500" />
  <row Id="5696" PostId="4236" Score="0" Text="@DanielMGessel Yes." CreationDate="2016-11-07T02:04:36.943" UserId="5430" />
  <row Id="5697" PostId="4226" Score="0" Text="Ahh, I stand corrected. Somehow missed when GPUs crossed the 8-sample threshold. That was the limit for a long time, IIRC." CreationDate="2016-11-07T04:07:02.297" UserId="48" />
  <row Id="5698" PostId="4233" Score="0" Text="Yeah, it was a bit hasty answer, added some edits" CreationDate="2016-11-07T04:42:08.213" UserId="1952" />
  <row Id="5699" PostId="4235" Score="1" Text="For fun of it, I calculated that we would need ~250,000x the GPU performance to reach today's movie quality in real-time (30ms vs 2h per frame). This may sound a lot but if we continue with ~1.5x perf increase per year, we could get there in ~30 years (: It's obviously more complicated than that (mem bandwidth, algorithm improvements, etc.) but gives some idea in what kind of timeframe we might get into full real-time path tracing (:" CreationDate="2016-11-07T04:51:04.797" UserId="1952" />
  <row Id="5700" PostId="4236" Score="0" Text="If switching from 32 bit floats to 64 bit floats makes the errors rarer, that's an strong indication that you're running into rounding problems - I haven't done much computational geometry in a while, but my recollection was that exact results were often critical." CreationDate="2016-11-07T06:18:25.163" UserId="2500" />
  <row Id="5701" PostId="4236" Score="0" Text="Just adding to @DanielMGessel 's comment, many years ago I wrote a general polygon triangulation algorithm (i.e. it handled self-intersections) and until I replaced the float representations with exact rational maths it was a nightmare trying to track down/fix bugs." CreationDate="2016-11-07T10:00:09.693" UserId="209" />
  <row Id="5702" PostId="4233" Score="0" Text="You've given so many detailed answers recently that I guessed you had more you could say on this one. +1" CreationDate="2016-11-07T11:16:34.237" UserId="231" />
  <row Id="5703" PostId="4240" Score="0" Text="FPGAs have some internal memory, not much. I shall give thought to your description." CreationDate="2016-11-08T08:46:48.323" UserId="5371" />
  <row Id="5704" PostId="4239" Score="0" Text="Since you are not doing blending of the lines, you should be able to just write to the FB (instead of read-write). That should simplify things quite a bit I believe." CreationDate="2016-11-08T13:42:04.727" UserId="1952" />
  <row Id="5705" PostId="4239" Score="0" Text="what do you mean by fb?" CreationDate="2016-11-08T21:15:29.760" UserId="5371" />
  <row Id="5706" PostId="4239" Score="0" Text="framebuffer...." CreationDate="2016-11-08T21:17:17.283" UserId="1952" />
  <row Id="5709" PostId="4220" Score="0" Text="@trichoplax Yes. All the photos were taken in the same room. I want to arrange them in their absolute location. These are spherical photos." CreationDate="2016-11-09T23:20:41.153" UserId="5412" />
  <row Id="5710" PostId="4230" Score="0" Text="It sounds a little bit like ROAM, but hard to tell...&#xA;https://www.youtube.com/watch?v=PPjWW8uPp3o" CreationDate="2016-11-09T23:21:40.360" UserId="56" />
  <row Id="5711" PostId="4248" Score="2" Text="Can I just suggest you look at &quot;supersampling&quot; https://en.wikipedia.org/wiki/Supersampling and perhaps also  https://en.wikipedia.org/wiki/Distributed_ray_tracing?" CreationDate="2016-11-10T08:52:01.940" UserId="209" />
  <row Id="5712" PostId="4248" Score="2" Text="I can also recommend reading this chapter of PBRT http://pbrt.org/chapters/pbrt_chapter7.pdf and reading this paper http://lgdv.cs.fau.de/get/785 (which explains a different technique than the one implemented in pbrt)." CreationDate="2016-11-10T09:05:57.517" UserId="4655" />
  <row Id="5713" PostId="4248" Score="1" Text="`foreach pixel : p{acc = 0; foreach subsample : s { acc+=sample_scene(s);} store(p, acc);}`" CreationDate="2016-11-10T09:36:25.710" UserId="137" />
  <row Id="5714" PostId="4251" Score="0" Text="So basically I render an image to a large size and when saving it to an image, downscale it to a lower size? That seems quite simple :)! Is this the super sampling method?" CreationDate="2016-11-10T14:27:47.367" UserId="5256" />
  <row Id="5715" PostId="4251" Score="1" Text="@Arjan Singh yes it's https://en.wikipedia.org/wiki/Supersampling , but this is the slowest of them all , raytracing allows you to easily do adaptive supersampling, which can perform a lot better" CreationDate="2016-11-10T14:54:05.530" UserId="5364" />
  <row Id="5716" PostId="4253" Score="0" Text="Great Answer! What would be the benefits of using this method opposed to the method @Raxvan used? Will I get the same results by rendering to a large size and then downscaling to a smaller size?" CreationDate="2016-11-10T15:28:52.937" UserId="5256" />
  <row Id="5717" PostId="4253" Score="0" Text="Fundamentally, with ray tracing you just don't need to render a bigger image then scale it down. That means you have a lot more flexibility: you can have a lot of samples, you can vary the number of samples depending on the region, and simply, you don't have to add the rescale step." CreationDate="2016-11-10T15:36:18.933" UserId="182" />
  <row Id="5718" PostId="4253" Score="2" Text="On the topic of jittering, this turns out to be a fairly complex topic. Here is a great paper analyzing the state-of-the-art a few years back http://graphics.pixar.com/library/MultiJitteredSampling/paper.pdf" CreationDate="2016-11-10T15:44:10.223" UserId="2463" />
  <row Id="5719" PostId="4220" Score="0" Text="If you're not sure how to edit the wording to make clear what you want, feel free to drop into [chat]." CreationDate="2016-11-10T17:16:36.807" UserId="231" />
  <row Id="5720" PostId="4255" Score="0" Text="What do you mean by &quot;downscaling&quot;, in this regard?" CreationDate="2016-11-10T20:12:03.710" UserId="2654" />
  <row Id="5721" PostId="4255" Score="1" Text="Maybe I used the wrong term, but I meant the following: my monitor has a native 1920x1080 resolution, the game is offering me (by default) to render 40% more pixels than my monitor can actually display, so in the end, the image will be &quot;downscaled&quot; to the actual size my monitor is actually able to display." CreationDate="2016-11-10T20:15:04.110" UserId="5458" />
  <row Id="5722" PostId="4241" Score="0" Text="&quot;*since you must also handle synchronization with multiple command buffers*&quot; What synchronization, in particular, are you referring to?" CreationDate="2016-11-10T20:21:47.943" UserId="2654" />
  <row Id="5723" PostId="4252" Score="0" Text="I would also mention that LDR is probably more suited to eye-side ray-tracing as you don't need to accumulate a lot of small parts that often." CreationDate="2016-11-11T01:53:34.100" UserId="5462" />
  <row Id="5724" PostId="4241" Score="0" Text="I edited the question to be more specific. Basically, there are two types of synchronization that I'm concerned about, one with the framebuffers, and one with command buffer submission/frequency." CreationDate="2016-11-11T02:50:51.243" UserId="2707" />
  <row Id="5725" PostId="4257" Score="0" Text="I really dont think this is what he's talking about. What he is saying is that the default resolution of the game is higher than his monitor's actual resolution." CreationDate="2016-11-11T06:07:54.683" UserId="56" />
  <row Id="5726" PostId="4259" Score="1" Text="Physically Based Rendering: From Theory to Implementation has loads of code in it. It explains a lot line by line. You also have the pbrt implementation on Github too. It's a great book." CreationDate="2016-11-11T08:58:18.880" UserId="3331" />
  <row Id="5728" PostId="4259" Score="0" Text="I've only been able to read sample chapters but based on what you've said I might buy this book." CreationDate="2016-11-11T10:35:52.217" UserId="5256" />
  <row Id="5729" PostId="4259" Score="0" Text="PBRT is written with a _[literate programming](https://en.wikipedia.org/wiki/Literate_programming)_ approach, which means that the book not only explains the whole theory but also the entire implementation almost line by line.&#xA;&#xA;&#xA;It is an amazing book and well worth a read if you're interested in the topic. Make sure to get the newly released 3rd edition ;)" CreationDate="2016-11-11T11:08:33.657" UserId="1930" />
  <row Id="5730" PostId="4257" Score="1" Text="Yes, and that is precisely how supersampling is implemented. You render to a higher resolution render target and then downscale to the screen size, and capture sub-pixel detail in the process." CreationDate="2016-11-11T11:42:05.843" UserId="2817" />
  <row Id="5732" PostId="4257" Score="0" Text="Can you provide evidence that any game actually consciously makes the choice to have a larger than native resolution as a way of doing anti aliasing? The heuristics for auto detection are already complex enough without this added complexity, I'd be really surprised." CreationDate="2016-11-11T13:23:48.050" UserId="56" />
  <row Id="5733" PostId="4259" Score="0" Text="@tizian As a beginner who has never written a single ray tracer before should I buy it? I plan to do the scratchapixel tutorials online but once I'm done with that should I buy the book?" CreationDate="2016-11-11T13:30:00.680" UserId="5256" />
  <row Id="5734" PostId="4260" Score="1" Text="This book is very great. It's written in a very pleasant way and doesn't scare with advanced maths very early on." CreationDate="2016-11-11T13:56:27.457" UserId="13" />
  <row Id="5735" PostId="4257" Score="1" Text="Here you go, Battlefield 1: https://www.reddit.com/r/battlefield_one/comments/50b4m1/psa_42_resolution_scale_is_your_native_res/&#xA;&#xA;As for the &quot;heuristics&quot;, you'd be suprised. There are practically none. No game I've ever shipped had anything more complex than a simplistic, synthetic benchmark on the first run, and/or custom configurations per device IDs determined via trial-and-error QA testing on a wide range of hardware." CreationDate="2016-11-11T14:56:45.320" UserId="2817" />
  <row Id="5736" PostId="4259" Score="0" Text="@ArjanSingh PBRT might be better suited if you already have some prior knowledge about ray tracing and want to make the jump to state-of-the-art physically-based rendering.&#xA;&#xA;I heard many great things about Peter Shirley's &quot;minibook&quot; series mentioned in another answer here already; this might be a good place to start instead." CreationDate="2016-11-11T15:10:21.233" UserId="1930" />
  <row Id="5737" PostId="4257" Score="2" Text="that trial and error is what i was talking about, but i guess this doesn't add complexity in that case since it doesnt really add extra testing.  Interesting stuff.  I was definitely wrong about what I said!" CreationDate="2016-11-11T15:13:23.833" UserId="56" />
  <row Id="5740" PostId="4259" Score="1" Text="@ArjanSingh although book recommendation questions are off topic here on main, you could ask about it in [chat]." CreationDate="2016-11-12T09:40:24.407" UserId="231" />
  <row Id="5741" PostId="4233" Score="0" Text="&quot;With your current adaptive tessellation strategy you'll end up splitting flat surfaces, which unnecessarily increases your triangle count.&quot; - It´s exactly the opposite. I guess you (and many others) don´t know what a polar coordinate system is and how it warps cartesian 3D-models depending on its projected location. I do agree with you that the threshold can be more elaborate than length on display but deriving it becomes difficult. And besides my target GPU does not have tesselation capability. I´ll update my answer with more info on later occation." CreationDate="2016-11-12T10:01:46.587" UserId="3041" />
  <row Id="5742" PostId="4230" Score="0" Text="@NathanReed Added further explanations and pictures." CreationDate="2016-11-12T10:55:49.923" UserId="3041" />
  <row Id="5744" PostId="4230" Score="0" Text="@AlanWolfe I´d say the algorithm is in the same ball park with a few differences. Good find :-)" CreationDate="2016-11-12T11:02:34.960" UserId="3041" />
  <row Id="5745" PostId="4264" Score="0" Text="First of all, thanks for answering. Actually I tried to interpolate with the background color, but for some reason the results didn't seem to be correct. I will try again and I will make you know." CreationDate="2016-11-12T23:34:28.770" UserId="4718" />
  <row Id="5746" PostId="4264" Score="0" Text="See my edit in my question. Basically I think interpolation between the background color and green is being done correctly, but maybe that's not what I need to do..." CreationDate="2016-11-12T23:53:30.073" UserId="4718" />
  <row Id="5747" PostId="4264" Score="0" Text="@nbro Based on your edit, you're not doing interpolation with the background color correctly. You can't use `I = (int)255*I0` and add/subtract `I` from the colors in that way; it doesn't work when the initial color values are anything other than 0 or 255. Sorry if I wasn't clear enough about that. You need to actually implement the interpolation formula, which would involve scaling the colors by `I0` and `1-I0` and then adding them together." CreationDate="2016-11-13T00:08:35.863" UserId="48" />
  <row Id="5748" PostId="4264" Score="0" Text="Now I'm doing something like `(1 - I0) * background_color + I0 * green` and it seems to interpolate correctly between green and the background color, but it isn't smooth as it should be (second picture in my question). Also for some reason the line does not do the same movement as before, i.e. it doesn't enter the sphere anymore, but moves always in front of it.. see my edit that follows to see the output." CreationDate="2016-11-13T00:34:12.130" UserId="4718" />
  <row Id="5749" PostId="4264" Score="0" Text="@nbro Looks to me like you have the interpolation weights backward—try swapping the `I0` and `1 - I0`. About the occlusion: do you still have the z-buffer check? In your original code you had `if ( (zBuffer[x+y*w] &lt; 0) || z &lt; zBuffer[x+y*w] )` which looks like should be responsible for hiding the sections of the line that are behind the sphere." CreationDate="2016-11-13T00:39:05.127" UserId="48" />
  <row Id="5750" PostId="4264" Score="0" Text="Actually I also swaped `y` and `nextY` (when setting the pixels) and now the output is more smooth (the problem of **not** entering the sphere still persists)..." CreationDate="2016-11-13T00:39:49.020" UserId="4718" />
  <row Id="5751" PostId="4264" Score="0" Text="I still have the `z-buffer` check as before... I heard about the inverse of `z` which can be used (somewhere), but I didn't understand why and how.." CreationDate="2016-11-13T00:44:28.040" UserId="4718" />
  <row Id="5752" PostId="4264" Score="0" Text="For some reason the occlusion problem was solved...thanks a lot for all the help and time!" CreationDate="2016-11-13T01:29:45.770" UserId="4718" />
  <row Id="5753" PostId="4265" Score="1" Text="Is this rasterization happening on CPU or GPU? Also I'm curious if there's a reason you need it to be faster than polygons as it might help shape an answer. Thanks!" CreationDate="2016-11-13T15:56:04.997" UserId="56" />
  <row Id="5754" PostId="4265" Score="0" Text="GPU, OpenCL. Also why shouldn't it be faster than polygons? Why should I be looking for an algorithm, that's slower than polygons? :)" CreationDate="2016-11-13T17:13:38.957" UserId="5476" />
  <row Id="5755" PostId="4265" Score="2" Text="Faster vs just using polygons hehe.  As in... Do you have an actual usage case where you need an algorithm faster than triangles? If so, what are the details of that usage case. If it's just curiosity though, that's fine too." CreationDate="2016-11-13T17:22:36.353" UserId="56" />
  <row Id="5756" PostId="4265" Score="0" Text="Im trying to rewrite my voxel octree raycaster to a work without raycasting..if you know, what i mean. I dont want to use polygons since it seems stupid, rendering 12 individual triangles for the sake of rendering a cube, that by itself is only defined by 4 numbers and has itself some limitation of course, like rotation being impossible to be applied directly to the cube, which on the other hand is something i dont need for my renderer." CreationDate="2016-11-13T17:44:16.733" UserId="5476" />
  <row Id="5759" PostId="4271" Score="0" Text="That's quite a clarified explanation for that somewhat tricky problem." CreationDate="2016-11-14T17:44:24.810" UserId="4958" />
  <row Id="5760" PostId="4270" Score="0" Text="I'm looking for the inverse of that. but cant do it that way.. need to do it as an affine transform. I'm setting up the coordinate system apriori with translate() and scale commands applied to the http://docs.oracle.com/javafx/2/api/javafx/scene/canvas/GraphicsContext.html associated with the Canvas object which takes up the whole window and whose coordinate (0,0) is the upper-left hand of the window... currently im using something like this&#xA;&#xA;&gt;     gc.restore();&#xA;&gt;     gc.translate( 0, height / 2 );&#xA;&gt;     gc.scale( width / xrange, height / yrange )&#xA;&gt;     gc.translate( -minx, 0 );" CreationDate="2016-11-15T05:13:56.523" UserId="5478" />
  <row Id="5762" PostId="4272" Score="0" Text="i may be seriously oversimplifying this but what difference would mapping it as a 2d shape make? with your horizontal cylinder flattened, could the hight (or width with a vertical cylinder?) be exactly the same as the degree? do you need a specific unit of measurement i.e. mm? in which case you need the circumference of the cylinder (or any measure from which the circumference can be calculated) which would then be the hight once flattened. do these rectangles move or grow from the initial position? i am not sure what you are asking for." CreationDate="2016-11-14T23:21:19.023" UserId="5486" />
  <row Id="5763" PostId="4272" Score="0" Text="@Ryan I need to map the  rectangle on the pipe surface to  2D plane" CreationDate="2016-11-15T11:48:13.640" UserId="5482" />
  <row Id="5764" PostId="4270" Score="0" Text="Usually you don't want the inverse to map each pixel on the screen exactly once to your function. Here's similar question to yours: http://computergraphics.stackexchange.com/questions/4193/what-is-the-correct-order-of-transformations-scale-rotate-and-translate-and-why/4194#4194&#xA;Would have voted for close, but this is a more generic answer." CreationDate="2016-11-15T13:30:43.350" UserId="1952" />
  <row Id="5765" PostId="4207" Score="0" Text="It was also part of the problem so I'll accept this answer." CreationDate="2016-11-15T17:25:51.340" UserId="4958" />
  <row Id="5766" PostId="4270" Score="0" Text="Thank you. I'm actually doing the opposite of that, I'm looping thru each pixel x,y and mapping that to precisely one point in the complex plane to evaluate something like the Mandelbrot set. Also, the mouse click handler returns point clicked in pixels, and I need to map that onto the complex plane as well. I realize each &quot;pixel&quot; is actually a rectangle on the complex plane but the resolution is small enough that just sampling the center of this rectangle is fine for display purposes." CreationDate="2016-11-15T22:04:06.230" UserId="5478" />
  <row Id="5767" PostId="4277" Score="0" Text="I can't give you a deeper answer unfortunately (someone else surely will!) but the 2 comes from integrating the BRDF over the hemisphere.&#xA;You didn't ask, but you may also be wondering where the $\pi$ went.  It's assumed that your color values already include the adjustment for $\pi$.  You can read more about that here: https://seblagarde.wordpress.com/2012/01/08/pi-or-not-to-pi-in-game-lighting-equation/" CreationDate="2016-11-15T22:38:57.493" UserId="56" />
  <row Id="5768" PostId="4277" Score="1" Text="Thanks but why is the BRDF being integrated over the hemisphere at this stage of the algorithm when we're dealing with a single incident direction?" CreationDate="2016-11-15T22:43:12.273" UserId="2941" />
  <row Id="5769" PostId="4278" Score="0" Text="Ok, thanks for trying to clarify all of these! But I would to know in particular why they are giving us the points in global coordinates and how does it affect my calculations of the pixels that are set." CreationDate="2016-11-16T00:10:51.863" UserId="4718" />
  <row Id="5770" PostId="4278" Score="1" Text="I've expanded my explanation to show how it relates to your problem. I suspect there are parts you might not be required to do yet, as the problem doesn't specify a view matrix (or camera position and orientation). So it might not be a 1:1 mapping of your problem, but hopefully will get you going in the right direction." CreationDate="2016-11-16T00:56:00.087" UserId="3003" />
  <row Id="5771" PostId="4265" Score="0" Text="it feels like maybe you could translate the 3d points and project them into 2d (think: x/z, y/z types of transforms), and then you could use some &quot;convex hull&quot; finding algorithm on the points to come up with a 2d poly, instead of starting with triangles.  The fact that it's symmetric (since it's a cube) likely means you can do less work than transforming all of it's 8 points, or even rasterizing the whole thing.  You can likely rasterize half or a quarter, and mirror it or something." CreationDate="2016-11-16T01:07:35.150" UserId="56" />
  <row Id="5772" PostId="3978" Score="1" Text="+1 for giving me a simple/understandable introduction to importance sampling." CreationDate="2016-11-16T09:37:45.923" UserId="2941" />
  <row Id="5773" PostId="1687" Score="0" Text="Requests for software recommendations are off topic here, but you may be interested in [softwarerecs.se]." CreationDate="2016-11-16T14:38:49.280" UserId="231" />
  <row Id="5774" PostId="4280" Score="0" Text="Nice one, thanks. Trying to connect this to what my book has for MC estimator for the light going to the viewer: $\left \langle L_r(x\rightarrow\Theta) \right \rangle = \frac{1}{N}\sum_{1}^N\frac{L(x\leftarrow\Psi_i)f_r(x,\Theta\leftrightarrow \Psi_i)\cos(\Psi_i,N_x)}{p(\Psi_i)}$. The division is by the pdf which for non-cosine-weighted hemisphere sampling is $p(\Psi_i) = \frac{1}{2\pi}$. Dividing by a fraction is multiplication by reciprocal, and constant coefficient $2\pi$ gets pulled out from the sum. Does that make sense?" CreationDate="2016-11-16T14:53:17.753" UserId="2941" />
  <row Id="5775" PostId="4280" Score="0" Text="Although I've now confused myself again because that Wikipedia article says &quot;in the naive case above, there is no particular sampling scheme, so the PDF turns out to be 1&quot;. But isn't the pdf $\frac{1}{2\pi}$?" CreationDate="2016-11-16T15:24:45.793" UserId="2941" />
  <row Id="5776" PostId="4282" Score="2" Text="Can you clarify what the problems with floating-point precision are? I don't see anything in this method that would cause precision problems, unless the scale is really extreme. Also, worth noting that this method may fail if the matrix was composed from a sequence of matrices that includes both non-uniform scales and rotations. The $\mathbf{R}$ matrix will turn out not to be a rotation in that case, but will include some shear." CreationDate="2016-11-16T16:11:22.850" UserId="48" />
  <row Id="5777" PostId="4280" Score="1" Text="Yes, pdf is $\frac{1}{2\pi}$ for uniform sampling over the hemisphere." CreationDate="2016-11-16T18:05:27.777" UserId="1952" />
  <row Id="5778" PostId="4282" Score="2" Text="All floating point numbers have intrinsic (bounded) error. Any time you perform operations, and particularly addition or subtraction, you compound the error, increasing the magnitude of the bounds.  Hidden in the decomposition algorithm are many addition operations (both in the matrix multiplication and the scale magnitude calculation) and a square root (in the scale).  Further decomposition will introduce further error." CreationDate="2016-11-16T19:14:17.193" UserId="5498" />
  <row Id="5779" PostId="4282" Score="1" Text="@Timbo There isn't any full matrix multiplication here though, just multiplying the columns of the matrix by the inverse scales. And a vector magnitude involves adding all positive quantities, so there's no catastrophic cancellation there; it doesn't produce much relative error, AFAICT. Anyway, the author clarified that they're talking about further decomposing the rotation matrix into Euler angles or suchlike, which makes more sense." CreationDate="2016-11-16T20:31:34.843" UserId="48" />
  <row Id="5780" PostId="4283" Score="0" Text="Only the yaw is inverted, the pitch works as expected." CreationDate="2016-11-16T20:41:15.820" UserId="5488" />
  <row Id="5781" PostId="4283" Score="0" Text="Are you saying that the rotation doesn't happen to the counterclockwise direction with increasing angles in your coordinate system?" CreationDate="2016-11-16T20:55:11.100" UserId="1952" />
  <row Id="5782" PostId="4283" Score="0" Text="It is counterclockwise on the Y axis (the yaw), i.e. increasing the angle (or looking right) causes the fixed object the camera is looking at to go right, but clockwise on the X axis (the pitch), increasing the angle (looking up) causes the object to go down as expected." CreationDate="2016-11-16T21:09:44.730" UserId="5488" />
  <row Id="5783" PostId="4239" Score="0" Text="It sounds like that you are saying you read a 1 byte value from the frame buffer, and that part is slow / cumbersome?  If so, why are you doing the read from frame buffer, instead of a &quot;blind write&quot;? If that makes sense.  If that is what's going on, a similar thing happens in real GPUs when doing alpha blending.  Alpha blending needs a read of the pixel value already there to mix with the new pixel value, so is slower than a non alpha write, which is just a write, without a read." CreationDate="2016-11-16T22:34:25.883" UserId="56" />
  <row Id="5784" PostId="4283" Score="0" Text="What's the coordinate system you are using (x=right, etc)?" CreationDate="2016-11-17T22:18:57.007" UserId="1952" />
  <row Id="5785" PostId="4283" Score="0" Text="Right handed system with +x is right, +y is up" CreationDate="2016-11-17T22:45:38.080" UserId="5488" />
  <row Id="5786" PostId="4283" Score="0" Text="Ok, so in that case both rotate to the same direction about their respective rotation axes (x- and y-axis for pitch &amp; yaw respectively) and you get the behaviour you defined, as expected. If YOU expect different rotation direction, then you just have to transpose the matrix like you said, or negate the angle." CreationDate="2016-11-17T23:28:39.883" UserId="1952" />
  <row Id="5787" PostId="1721" Score="0" Text="@teodron Might you have any insights on mean curvature for border vertices? Can such a thing be defined?" CreationDate="2016-11-18T12:00:45.223" UserId="3294" />
  <row Id="5788" PostId="4284" Score="0" Text="Not really, actually. I we have already treated global and local coordinates systems, but my question was more related to the fact to why, for example, as you're saying, would I need to convert the coordinates to global coordinates, if they were given as local coordinates. I mean, what's the relevance of the coordinate system in this case/problem given in my question." CreationDate="2016-11-18T12:35:10.697" UserId="4718" />
  <row Id="5789" PostId="1721" Score="0" Text="@Museful I am a bit worried about the mean curvature not being negative, no matter the surface type. If the Laplacian-like operator is defined on a border vertex, then it's a matter of evaluating the same expression including only the triangles that make up the faces of the surface incident at $v_i$. There are more recent papers on discrete curvatures, however.." CreationDate="2016-11-18T12:50:42.583" UserId="3444" />
  <row Id="5790" PostId="4284" Score="0" Text="You need to convert from local to global coordinate system to define objects position and orientation in the world. In local coordinate system objects are defined relatively to their pivot point (e.g. object center), so that you can move and rotate the object about that point in the world, by defining the local$\rightarrow$global transformation. The &quot;global coordinates&quot; is there to disambiguate the coordinate system in question and that you don't need to worry about this transformation. What's confusing though is that there's no disambiguation of the camera coordinate system." CreationDate="2016-11-18T14:21:28.443" UserId="1952" />
  <row Id="5791" PostId="4285" Score="0" Text="A transform like this is not a linear transform!" CreationDate="2016-11-18T15:13:30.600" UserId="38" />
  <row Id="5792" PostId="4283" Score="0" Text="Negating the yaw won't make the matrix consistent with the definition of the view matrix (i.e. the inverse of the camera world matrix). It would mean that `Ry Rpᵀ T⁻¹` while it should have been `Ryᵀ Rpᵀ T⁻¹`." CreationDate="2016-11-18T15:52:35.830" UserId="5488" />
  <row Id="5793" PostId="4283" Score="0" Text="If you negate the yaw angle, you don't need to transpose the yaw matrix. Personally I think it's better to negate the angle." CreationDate="2016-11-18T16:02:54.773" UserId="1952" />
  <row Id="5795" PostId="4287" Score="0" Text="If I define the transformation as a function of height, how can I then express it as a matrix ? or does this mean that I cannot keep it as the traditional matrix form and there should be input inside my rotation matrix? and if so, can I use such matrix in OpenGL to achieve the same results?&#xA;Thank you" CreationDate="2016-11-18T17:45:23.680" UserId="5495" />
  <row Id="5796" PostId="4287" Score="1" Text="You can't express it as a constant matrix, because that's able to do only linear transformations (rotate, translate, skew, scale). Instead you would have to create the matrix for every vertex you transform based on the y-coordinate of the vertex." CreationDate="2016-11-18T18:12:43.130" UserId="1952" />
  <row Id="5799" PostId="4293" Score="1" Text="Can you link to the slides? It may help to answer the question better if we can put it in context." CreationDate="2016-11-19T16:43:50.810" UserId="48" />
  <row Id="5800" PostId="4293" Score="0" Text="@NathanReed Probably not... but if something is unclear, I can try to give you more information.." CreationDate="2016-11-19T16:51:46.587" UserId="4718" />
  <row Id="5801" PostId="4293" Score="1" Text="Well, I'm not sure we can really answer the question as it's not clear from the slide how &quot;screen coordinates&quot; are defined relative to &quot;projected points&quot;. There are various different coordinate systems that might be used; I don't know how the slide author has chosen to set things up." CreationDate="2016-11-19T16:54:05.323" UserId="48" />
  <row Id="5802" PostId="4293" Score="0" Text="@NathanReed Indeed the slides are very simplified and in my opinion far from being easily understandable.. I can tell you we're in the context of the _z-buffer_ and _rasterization_.. We have treated global and local coordinate systems.." CreationDate="2016-11-19T16:59:51.577" UserId="4718" />
  <row Id="5804" PostId="4291" Score="2" Text="Normals are not transformed with the same transformation matrix as positions. You need to calculate inverse of the transpose of the 3x3 sub-matrix to properly transform normals for transformations with non-uniform scaling and/or skewing." CreationDate="2016-11-19T21:37:13.017" UserId="1952" />
  <row Id="5805" PostId="4291" Score="0" Text="@JarkkoL yeah that is true, you are right with that. It is best to not use the same matrix, but depending on the implementation, it is done. Most of the times people do not care about the skewing of the normals that much, because they either do not use non-uniform scaling or scaling at all. That part about transforming positions and normals was more about that it could be useful to use one container." CreationDate="2016-11-19T21:54:07.537" UserId="4908" />
  <row Id="5806" PostId="4288" Score="2" Text="This doesn't have anything to do with your question but you seem like you probably would be interested to hear about it.  You should check out cosine weighted sampling if you haven't heard of it.  Instead of multiplying by cos theta, you make your rays be distributed based on cos theta. The easiest way to do this is choose random point on a disk (x/z), and then make y such that it's a normalized vector. You end up sampling more meaningful directions and get faster convergence." CreationDate="2016-11-19T22:08:04.640" UserId="56" />
  <row Id="5807" PostId="4292" Score="0" Text="This answer would be more complete if you noted that, in actual [homogeneous coordinates](https://en.wikipedia.org/wiki/Homogeneous_coordinates), $(wx, wy, wz, w)$ for any $w \ne 0$ is also a valid representation of the point $(x, y, z)$. When converting from ordinary 3D coordinates to 4D projective coordinates, it's convenient to choose $w = 1$, but allowing other values of $w$ in the inverse conversion lets us represent [perspective transformations](https://en.wikipedia.org/wiki/Perspective_transform) using 4D matrix multiplication, too." CreationDate="2016-11-20T07:24:49.697" UserId="525" />
  <row Id="5808" PostId="4286" Score="0" Text="@trichoplax yes but my reputation was not high enough to put more than 2 link so I changed it and then forgot to change my sentence sorry =/ I will fix it" CreationDate="2016-11-20T17:50:45.323" UserId="5506" />
  <row Id="5810" PostId="4289" Score="0" Text="My CPU load reach rarely more than 30% when I'm bellow 20FPS so I am GPU bound. For my glMultiDraw* problem for now I use matrices to move my objects (the UpdateTransformUniform() call) is this possible to link glMultiDrawElements to a buffer of uniform or something like this?  For my buffers, my final buffers are 4 1920x1080 (I'm using deffered rendering) and my bounces are 4 64x64 (will be replaced to 1 64x64 soon). For the algorythm: should I loop over my bounces in every mesh using instances rendering or loop glMultiDraw my mesh for every bounces." CreationDate="2016-11-20T18:36:59.713" UserId="5506" />
  <row Id="5813" PostId="4289" Score="0" Text="Actually, I think this is what you would want if you're using uniform transformation matrices: glDrawElementsInstancedBaseVertexBaseInstance (OpenGL 4.2). Here's more info about it: http://www.gamedev.net/topic/682969-minimizing-draw-calls-and-passing-transformmaterial-data-to-a-shader-in-opengl-3x/" CreationDate="2016-11-21T05:31:32.107" UserId="2707" />
  <row Id="5814" PostId="4289" Score="0" Text="I ask a similar question to the once you are asking here: http://computergraphics.stackexchange.com/questions/4241/instanced-stereo-rendering-vs-multiple-command-buffers" CreationDate="2016-11-21T05:41:44.900" UserId="2707" />
  <row Id="5815" PostId="4289" Score="0" Text="Either way, it's not going to make a huge difference if your performance limited by the GPU. With a 64x64 grid, you are going to be doing a lot of overdrawing, even for a deferred renderer, so you should start optimizing there. But in terms of your followup question, I would go through that slideshow I linked and see if that can help you in your situation (extending it to a grid rather than two screens)." CreationDate="2016-11-21T05:42:22.317" UserId="2707" />
  <row Id="5816" PostId="4288" Score="0" Text="It's very difficult to understand what you try to say. Can you clarify your question? Are you talking about progressive path tracing and importance sampling?" CreationDate="2016-11-21T14:44:38.000" UserId="1952" />
  <row Id="5821" PostId="4194" Score="0" Text="Hi, did you derive that equation?" CreationDate="2016-11-22T14:11:39.677" UserId="5371" />
  <row Id="5823" PostId="4286" Score="0" Text="I (finally) fixed it" CreationDate="2016-11-22T17:47:01.777" UserId="5506" />
  <row Id="5839" PostId="4295" Score="0" Text="Normals don't work because they're not vectors.  Don't know of a good intro to the concept though." CreationDate="2016-11-24T13:14:12.800" UserId="2831" />
  <row Id="5840" PostId="4309" Score="0" Text="Thank you for your answer and your interesting insights. Ropes++ is just way to traverse a kd tree without having to maintain a stack, so I have not paid attention to ray coherency so far." CreationDate="2016-11-24T14:48:18.440" UserId="385" />
  <row Id="5841" PostId="4309" Score="0" Text="Yeah, once you try to increase the ray coherency, it will likely mess up any earlier GPR optimizations you have done. I prefer to defer GPR optimizations to a later stage, though you still need to be aware of it earlier (e.g. to avoid algorithms with deep nested loops that tend to increase the pressure, etc.)" CreationDate="2016-11-24T14:56:36.483" UserId="1952" />
  <row Id="5842" PostId="4289" Score="0" Text="Okay I'm almost good, last question: it looks like the stereo instanced rendering seems to use 1 render target split in 2 but I have multiple FBO. I cannot find a way to use instances across multiple FBO, is this possible, if yes how ?" CreationDate="2016-11-25T20:09:15.917" UserId="5506" />
  <row Id="5844" PostId="4315" Score="0" Text="When i copy/paste that into shadertoy I don't get anything like the output you are seeing, and just have a small green diamond.  Did you post the right code?" CreationDate="2016-11-26T22:04:13.613" UserId="56" />
  <row Id="5845" PostId="4315" Score="0" Text="Correct the shader is just a triangle. It should be glowing. Those are lines I marked in to express where I am seeing edges in the gradient. They are not apart of the shader code." CreationDate="2016-11-26T22:31:22.193" UserId="2308" />
  <row Id="5846" PostId="4316" Score="0" Text="Just updated the code so you can see what I see now" CreationDate="2016-11-27T03:04:38.090" UserId="2308" />
  <row Id="5847" PostId="4316" Score="0" Text="Ah ok. The negative was a red herring.  Answer updated." CreationDate="2016-11-27T05:26:56.953" UserId="56" />
  <row Id="5848" PostId="4320" Score="4" Text="It's an extremely high-resolution normal map, with many texels per screen pixel, and the paper introduces a special technique to calculate highlights from all normals in a screen pixel at once. Can you be more specific about what you understand already and where you're getting lost? It's hard to help otherwise." CreationDate="2016-11-27T16:01:01.193" UserId="48" />
  <row Id="5852" PostId="4289" Score="0" Text="Take this with a grain of salt because I haven't implemented it before (and you should probably post a separate question about this specific technique as I'm sure some have and can give better advice). From my understanding, the idea is to scale each instance's NDCs down depending on the side you are rendering and then set the clip planes in a shader. This should be able to be generalized to your approach (instead of two sides, have a grid). You need to pass a region number (side) to each instance." CreationDate="2016-11-27T22:35:53.753" UserId="2707" />
  <row Id="5853" PostId="4272" Score="1" Text="I think it's not clear *which* surface. Can this be imagined as &quot;projecting&quot; the given rectangles on the (3D!) pipe, then &quot;cutting open&quot; the pipe and &quot;unwrapping&quot; it, to obtain the 2D pipe surface? (And: Why is the width of the rectangle given in degrees?)" CreationDate="2016-11-27T23:41:05.467" UserId="1649" />
  <row Id="5855" PostId="4321" Score="4" Text="Have you taken a look at [this](https://en.wikipedia.org/wiki/Color_difference)? RGB isn't a great color space for doing comparisons related to human perception." CreationDate="2016-11-28T02:37:02.827" UserId="2707" />
  <row Id="5856" PostId="4321" Score="0" Text="Good info thanks! I was looking at cielab but that article says that isn't the best.  I'm doing work with RGB source data unfortunately so have to figure out how to convert from RGB to something better, but the challenge seems to be that RGB is device dependant, while eg cielab are not.  Fortunately, a lesser approximation is good enough for my needs, if device independence isn't really feasible with RGB source data." CreationDate="2016-11-28T02:48:28.220" UserId="56" />
  <row Id="5857" PostId="4320" Score="1" Text="That explanation was quite helpful, where I am getting lost is the part about 4D Gaussians &amp; Position Normal Distributions. What are they &amp; what do they do? In the paper they show the benefits of their method as opposed to just a simple normal map." CreationDate="2016-11-28T05:13:50.033" UserId="5256" />
  <row Id="5858" PostId="4321" Score="0" Text="Check out Bruce Lindbloom's site, especiall the various DeltaE metrics: http://www.brucelindbloom.com/" CreationDate="2016-11-28T12:53:42.593" UserId="385" />
  <row Id="5859" PostId="4326" Score="3" Text="Welcome to Computer Graphics SE! In general, link-only answers are strongly discouraged on SE, because they might become useless should those links ever go down. Please include a short summary of their content, so that people can still figure out what exactly you're actually suggesting without having to rely on the links." CreationDate="2016-11-28T13:15:20.570" UserId="16" />
  <row Id="5860" PostId="4282" Score="0" Text="Thanks – great answer. Follow-up: to get the original matrix back, I am assuming we need to follow a certain order of operations, starting from the identity matrix. Would this order be TRS?" CreationDate="2016-11-28T15:45:37.537" UserId="5493" />
  <row Id="5861" PostId="4272" Score="0" Text="@Marco13 Yeah. rectangles are on a 3 D pipe" CreationDate="2016-11-28T20:46:02.093" UserId="5482" />
  <row Id="5862" PostId="4272" Score="0" Text="I would guess the width is in degrees just to give the same unit of measurement as height.  I'm betting you calculate it as if it were the height, then use that value for the width." CreationDate="2016-11-28T23:22:52.157" UserId="56" />
  <row Id="5865" PostId="4325" Score="0" Text="Once you get into this, you're going to have to come to terms with homogeneous coordinates. [Here](http://www.tomdalling.com/blog/modern-opengl/explaining-homogenous-coordinates-and-projective-geometry/) is a nice introduction to projection." CreationDate="2016-11-29T13:30:15.670" UserId="5574" />
  <row Id="5867" PostId="4247" Score="1" Text="[OpenEXR](http://openexr.com/index.html) is widely used in industry, with everything you'd expect: HDR, 32-bit float and 'half' float channel support, and support for color management, which is critical when you're using profiles to make sure the colors on your monitor match up with rushes you're viewing on, say, a projector. A lot of what it offers might not be useful to you, but the software is free, and there are plenty of minimal examples." CreationDate="2016-11-29T13:47:18.257" UserId="5574" />
  <row Id="5871" PostId="1983" Score="0" Text="Q&amp;As like these are always difficult for moderators. Apart from the asking for sources - which is kind of begging the question - this is a gem. I wouldn't mind seeing the 'question' protected', until there's something like a S.O.'s documentation implemented. Maybe that's not a bad place for it - under some 3D tags?" CreationDate="2016-11-29T21:57:53.133" UserId="5574" />
  <row Id="5874" PostId="1816" Score="0" Text="See also http://stackoverflow.com/questions/34342038/how-to-triangulate-polygons-in-boost/40892650#40892650 for code" CreationDate="2016-11-30T15:54:23.753" UserId="5582" />
  <row Id="5875" PostId="4335" Score="0" Text="It also seems like maybe the size of the spheres inside the cube correspond to the number of pixels of that color in the image. So it's a 3D histogram, basically. Is the pie wedge maybe the CIE XYZ &quot;shark fin&quot;?" CreationDate="2016-11-30T21:11:18.790" UserId="3003" />
  <row Id="5876" PostId="4335" Score="0" Text="@user1118321 I can't tell from the image whether the size of the circles inside the cube correspond to sphere size (with some unspecified meaning) or just to proximity to the camera (giving an idea of 3 dimensional position in the cube)." CreationDate="2016-11-30T21:32:36.887" UserId="231" />
  <row Id="5877" PostId="4338" Score="0" Text="It's funny, I joined this stack site because of you :) (i read your tags after you answered my UDP question on gamedev). Cheers!" CreationDate="2016-11-30T23:34:36.953" UserId="5586" />
  <row Id="5878" PostId="4338" Score="0" Text="Small world!  There's a great graphics community on twitter too.  Check out who i'm following (@Atrix256) if you want to get into that.  BTW, now I'm wondering what sort of remote control robot application needs to do OpenCL path tracing.  It sounds like you run an interesting business (:" CreationDate="2016-11-30T23:36:25.700" UserId="56" />
  <row Id="5879" PostId="4338" Score="1" Text="That is just the tip of the iceberg... I am writing the path tracer to render images for training a machine learning algo to make a SLAM for robots. Project is not officially launched yet, but site is online to cath some early SEO: http://www.octomy.org/" CreationDate="2016-11-30T23:39:35.780" UserId="5586" />
  <row Id="5880" PostId="4324" Score="0" Text="The only thing I can think of is to mipmap the data you are ray marching through.  Try it at a lower mip to find a collision, then try a higher mip to find a more detailed answer." CreationDate="2016-12-01T04:49:55.060" UserId="56" />
  <row Id="5881" PostId="4339" Score="0" Text="Thanks for the reply joojaa! I understand that relative size of the spacing of knots is irrelevant for the NURBS curve. But I need a normalized knot span for computational purpose in undocumented legacy application. Currently I am facing issues when the knot span is not normalized. Does normalizing the knot span have no effect on the control points of the NURB surface/curve? If so, I can simply normalize the knot span between 0 and 1 using standard normalizing formula without worrying about its effects on control point list. I wish to know if it is safe to do so." CreationDate="2016-12-01T09:18:12.977" UserId="5584" />
  <row Id="5882" PostId="4339" Score="0" Text="@Vaibhav as long as you scale and offset by a uniform amount yes sure you can normalize thats what example number 2 does." CreationDate="2016-12-01T12:36:15.947" UserId="38" />
  <row Id="5883" PostId="4337" Score="1" Text="Vertex attribute interpolation is quite easy, you can simply use each component as the weight for the sum of attributes. I.e. for vertex colours C1, C2 and C3 and barycentric coordinate B, fragment color is F = C1 * B.x + C2 * B.y + C3 * B.z. That is, as long as you either compute the barycentrics in world space, or you perspective-correct them: https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation/perspective-correct-interpolation-vertex-attributes" CreationDate="2016-12-01T13:11:46.313" UserId="2817" />
  <row Id="5885" PostId="4335" Score="1" Text="The white spheres seem to be the quantized colors in the 3D histogram. There are total of 24 of them in the image, i.e. the number of quantized colors." CreationDate="2016-12-01T14:19:26.590" UserId="1952" />
  <row Id="5886" PostId="4337" Score="0" Text="Why use a compute shader? You could simply render to a render target as usual, grab the data off the GPU and save it to an image file of your choice. &#xA;&#xA;While possible to do this on the compute shader, you're essentially forcing yourself to perform much of the work that the GPU already knows how to do. Unless you really want to work with compute shaders, I'd advise using the prior technique I mentioned." CreationDate="2016-12-02T02:10:37.197" UserId="5594" />
  <row Id="5887" PostId="4341" Score="0" Text="Just checking... this is ray tracing right, not path tracing?" CreationDate="2016-12-02T17:54:20.873" UserId="56" />
  <row Id="5888" PostId="4341" Score="0" Text="I meant to say Path Tracing, will change the title." CreationDate="2016-12-02T18:17:52.507" UserId="5256" />
  <row Id="5891" PostId="4342" Score="0" Text="Great Answer this really clarified everything, although I'm going off topic a little would you know of any good resources on how to implement depth of field?" CreationDate="2016-12-03T06:08:12.217" UserId="5256" />
  <row Id="5892" PostId="4342" Score="1" Text="No sorry, but I am interested too. Ask another question IMO!" CreationDate="2016-12-03T06:09:40.617" UserId="56" />
  <row Id="5893" PostId="4344" Score="0" Text="[Related question on depth of field](http://computergraphics.stackexchange.com/questions/66/how-is-depth-of-field-implemented) (not a duplicate as that is focused on approximations rather than path tracing, but gives some insight)." CreationDate="2016-12-03T10:26:58.263" UserId="231" />
  <row Id="5894" PostId="4344" Score="1" Text="[Related question on lens model for path tracing](http://computergraphics.stackexchange.com/questions/246/how-to-build-a-decent-lens-camera-objective-model-for-path-tracing). Near to being a duplicate, so I recommend you make the question more specific by explaining what you already understand, so we can see where any confusion is being introduced." CreationDate="2016-12-03T10:36:30.130" UserId="231" />
  <row Id="5896" PostId="4345" Score="2" Text="The only thing I know of that sounds remotely related to your question is this paper from SIGGRAPH 2013: https://igl.ethz.ch/projects/make-it-stand/make-it-stand-siggraph-2013-prevost-et-al.pdf" CreationDate="2016-12-03T12:49:24.687" UserId="2817" />
  <row Id="5898" PostId="4344" Score="0" Text="I understand how to calculate the focal point, focal distance and the secondary ray. But I'm confused on how to put this all together to create the 'blur' that is needed. I haven't found much online so I was hoping that someone here could explain that with some code." CreationDate="2016-12-03T14:28:30.350" UserId="5256" />
  <row Id="5899" PostId="4346" Score="0" Text="I have heard about a term 'jitter matrix' in a few DOF tutorials. I have no clue what it's for and how it comes to play in Depth of Field. Could you explain what a jitter matrix is and if I need it here?" CreationDate="2016-12-03T15:55:17.323" UserId="5256" />
  <row Id="5900" PostId="4253" Score="0" Text="The code sample above uses a 4 Sample MSAA, if I wanted to do 8x MSAA what would the matrix look like then? What would I need to change in the jitter matrix shown above?" CreationDate="2016-12-03T15:59:46.453" UserId="5256" />
  <row Id="5901" PostId="4345" Score="0" Text="Sorry, I know, could not get the title into one word. However, I believe both are the same problem." CreationDate="2016-12-03T16:13:14.220" UserId="5603" />
  <row Id="5902" PostId="4345" Score="0" Text="@IneQuation: Thanks, that looks good" CreationDate="2016-12-03T16:19:00.197" UserId="5603" />
  <row Id="5903" PostId="4346" Score="1" Text="For DOF, you just need to be able to randomly choose points on the disc. If you just choose these uniformly randomly then the method will work. If instead you use a more advanced method of choosing random points then you may be able to get the same quality image with fewer points (speeding up the process of rendering an image). &quot;Jitter matrix&quot; is a way of choosing points with a different random distribution, and you don't need it in order to make depth of field work - it's just a refinement you could add later. It's also applicable to any other situation where you need random points." CreationDate="2016-12-03T16:31:45.933" UserId="231" />
  <row Id="5904" PostId="4346" Score="1" Text="I've deliberately kept to just what is essential to make depth of field work, to avoid cluttering the answer with surplus information. You could ask a separate question if you run into any difficulties with different random distributions." CreationDate="2016-12-03T16:32:54.447" UserId="231" />
  <row Id="5905" PostId="4346" Score="1" Text="Do you think you could explain the secondary ray?" CreationDate="2016-12-03T16:54:58.993" UserId="56" />
  <row Id="5910" PostId="4347" Score="0" Text="I reposted a related question a while back that may also be relevant here: [Are there common materials that aren't represented well by RGB?](http://computergraphics.stackexchange.com/questions/203/are-there-common-materials-that-arent-represented-well-by-rgb)" CreationDate="2016-12-03T23:48:02.100" UserId="231" />
  <row Id="5911" PostId="4345" Score="0" Text="I've edited to hopefully make the title and body match. Please revert anything which changes your intention." CreationDate="2016-12-04T00:02:28.307" UserId="231" />
  <row Id="5912" PostId="4346" Score="0" Text="@AlanWolfe since the question author mentions already understanding secondary rays, I've tried to just focus on the specific aspect required. Perhaps secondary rays would make a separate question?" CreationDate="2016-12-04T00:11:31.627" UserId="231" />
  <row Id="5913" PostId="4346" Score="0" Text="Yeah your question answers the body of the question but not the title. As someone asking the question as listed in the title, your answer doesn't help me. I wonder if we ought to edit the question title or something..." CreationDate="2016-12-04T00:25:54.193" UserId="56" />
  <row Id="5915" PostId="4346" Score="1" Text="@AlanWolfe That's a good point. The reason the question was asked is because general descriptions of depth of field are too much information, and just that one narrow topic needed to be addressed alone. I've edited the title to try and better describe the question. The title is always going to be a short approximation, but search engines consider the body too, so it should still be found." CreationDate="2016-12-04T00:57:42.320" UserId="231" />
  <row Id="5917" PostId="4349" Score="1" Text="There's bsdf which is the full sphere instead of just the positive hemisphere that you get with brdf.  That makes it include refraction instead of just reflection.  That doesn't handle subsurface scattering though.  There's also &quot;participating media&quot; to think about which is stuff like fog in the air.  Also, you may have different reflection/refraction properties for different wavelengths of light.  How much of this stuff do you care about? (:" CreationDate="2016-12-04T02:47:51.550" UserId="56" />
  <row Id="5918" PostId="4347" Score="1" Text="The answers to this are probably useful too.  http://computergraphics.stackexchange.com/q/4321/56.  TL:DR - CIELAB was made to be a measurement of color for human perception.  Unfortunately different displays display things differently so you can't easily convert from RGB to this.  There have also been advancements to CIELAB over the years to make it closer to how color is actually percieved by humans. Not sure if helpful info, but I hope so!" CreationDate="2016-12-04T02:54:34.530" UserId="56" />
  <row Id="5919" PostId="4349" Score="0" Text="Im just curious:)" CreationDate="2016-12-04T03:15:46.920" UserId="5586" />
  <row Id="5921" PostId="4345" Score="0" Text="By using a principal axis rotation, and some shifts, I got what I wanted. Only works because of the special form of a mandibula-scan." CreationDate="2016-12-04T11:08:48.363" UserId="5603" />
  <row Id="5922" PostId="4349" Score="1" Text="A BSDF (a complete scattering function, including BRDF (reflection part), BTDF (transmission part), and BSSRDF (subsurface part)) as part of the rendering equation, should be &quot;the best&quot; model, just with a couple of caveats. First, there is not one BSDF model but many, each with different tradeoffs. So it really comes down to which BSDF/BRDF you use. Second, they often assume &quot;particle optics&quot; and skip the wave properties of light. That's a drawback that means that you can't model some phenomena. (Polarization, and &quot;CD diffraction&quot;, e.g.)" CreationDate="2016-12-04T21:31:22.893" UserId="4650" />
  <row Id="5923" PostId="4272" Score="0" Text="You already have a 2D parametrisation, don't you? One of the dimensions is the longitudal axis (in mm?) and the other is the circumferential axis (in degrees). The only problem I see is when you have rectangles wrapping around the 0/360-degree boundary. One workaround for that would be to duplicate each rectangle (or just the ones on the boundary) so that you get one copy on each side of the boundary. Does that help? And if you need to have a distance, and not an angle, for the circular dimension, that is readily available as l = r*a*pi/180, where r is the radius and a is the angle in degrees." CreationDate="2016-12-05T07:08:12.003" UserId="4650" />
  <row Id="5924" PostId="4008" Score="0" Text="How are the cylinders points defined?" CreationDate="2016-12-05T07:43:39.707" UserId="38" />
  <row Id="5925" PostId="4272" Score="0" Text="@Supernormal Please convert the comment as an answer. I could award you the bounty." CreationDate="2016-12-05T08:53:13.470" UserId="5482" />
  <row Id="5926" PostId="4352" Score="0" Text="Well, I said &quot;possible&quot;, not &quot;imaginary&quot; ;-). But thanks for a good answer!" CreationDate="2016-12-06T01:28:21.200" UserId="5586" />
  <row Id="5928" PostId="4363" Score="0" Text="If you want to do a rotation, R, around an arbitrary point, X, then the matrix is simply formed by&#xA; Translate(-X)*RotateAroundOrigin(R)*Translate(X)" CreationDate="2016-12-07T12:12:41.387" UserId="209" />
  <row Id="5929" PostId="4363" Score="0" Text="@SimonF could you elaborate a little more on your suggestion? Like I mentioned, still in the learning process." CreationDate="2016-12-07T12:14:27.473" UserId="2485" />
  <row Id="5930" PostId="4363" Score="0" Text="Sorry.... hit return by accident while typing the reply" CreationDate="2016-12-07T12:15:06.797" UserId="209" />
  <row Id="5931" PostId="4363" Score="0" Text="@SimonF So this arbitrary point X in my case would be the center of my texture. How could I determine it?" CreationDate="2016-12-07T12:40:13.513" UserId="2485" />
  <row Id="5932" PostId="4363" Score="0" Text="But if you are placing blades of grass, don't you know where they are being placed?" CreationDate="2016-12-07T16:23:33.583" UserId="209" />
  <row Id="5933" PostId="4363" Score="0" Text="@SimonF I do but is there a way to determine the textures width/height in world-space?" CreationDate="2016-12-07T16:25:38.527" UserId="2485" />
  <row Id="5934" PostId="4365" Score="0" Text="Thank you for your answer, this solution works very well for me." CreationDate="2016-12-07T17:06:49.510" UserId="3233" />
  <row Id="5935" PostId="4358" Score="0" Text="You're idea of using one dimension at time is a pretty good one! I like it!" CreationDate="2016-12-07T17:08:09.090" UserId="3003" />
  <row Id="5936" PostId="4363" Score="0" Text="I'm not sure what this is supposed to do model = glm::translate(model, texturePositions[0]); since texturePositions is filled with 0 vectors. But you are translating before rotating." CreationDate="2016-12-07T19:22:36.620" UserId="5636" />
  <row Id="5937" PostId="4366" Score="0" Text="You mean the noise effect?" CreationDate="2016-12-07T19:28:47.937" UserId="4958" />
  <row Id="5938" PostId="4366" Score="0" Text="@narthex I think so. How do I get rid of it?" CreationDate="2016-12-07T19:44:13.763" UserId="5640" />
  <row Id="5939" PostId="4352" Score="0" Text="Is there a name for $l(\theta)$? Or do you just say el theta?" CreationDate="2016-12-08T06:50:33.383" UserId="5636" />
  <row Id="5940" PostId="4363" Score="0" Text="@MatthewWoo as far as I am concerned it is filled with two vec3-objects, which are both set to (0,0,0). Am I worng?" CreationDate="2016-12-08T08:25:23.700" UserId="2485" />
  <row Id="5941" PostId="4352" Score="1" Text="@MatthewWoo It's a name I came up with on the spot, it's actually the irradiance from DRDF with the extra parameter of wavelength." CreationDate="2016-12-08T09:44:42.080" UserId="137" />
  <row Id="5942" PostId="4366" Score="0" Text="anything stopping you from just splitting the object?" CreationDate="2016-12-08T13:08:12.650" UserId="137" />
  <row Id="5943" PostId="4369" Score="3" Text="This is an active area of research so there are a lot of different ways to do it, but as some quick info, from what I've seen, it's commonly both a normal map for small details, as well as vertex transformation for larger details.  Gerstner waves are likely to be interesting to you, and this link probably as well:&#xA;http://http.developer.nvidia.com/GPUGems/gpugems_ch01.html" CreationDate="2016-12-08T18:46:56.440" UserId="56" />
  <row Id="5944" PostId="4370" Score="0" Text="Could you mention any other requirements? Do you need the fastest / most memory efficient approach, or the simplest one to implement to get on with prototyping?" CreationDate="2016-12-08T20:02:59.513" UserId="231" />
  <row Id="5945" PostId="4370" Score="2" Text="Maybe this question on stackoverflow helps: [link](http://stackoverflow.com/questions/838761/robust-algorithm-for-surface-reconstruction-from-3d-point-cloud)" CreationDate="2016-12-08T22:52:19.647" UserId="5632" />
  <row Id="5946" PostId="1518" Score="0" Text="Are you sure? According to [the docs](http://docs.gl/gl4/glMemoryBarrier), `GL_TEXTURE_UPDATE_BARRIER_BIT` is used when synchronizing calls to `glTexImage`, and has nothing to do with the memory used in storage buffers. I think you meant `GL_PIXEL_BUFFER_BARRIER_BIT`?" CreationDate="2016-12-09T04:00:06.630" UserId="197" />
  <row Id="5947" PostId="4370" Score="1" Text="Have you tried using [MeshLab](http://meshlab.sourceforge.net/)? It's one of its use cases by design." CreationDate="2016-12-09T09:23:47.947" UserId="2817" />
  <row Id="5950" PostId="4370" Score="1" Text="Performance doesn't matter. I think understanding the concepts and accuracy is more important. Sorry if the question is too broad, but I thought that any experiences from experts, how to dive in and which concept is worth it, would be helpful for me and potential future visitors @trichoplax" CreationDate="2016-12-09T12:12:47.310" UserId="18" />
  <row Id="5951" PostId="4370" Score="0" Text="Nice find. Thanks @wolle" CreationDate="2016-12-09T12:13:13.480" UserId="18" />
  <row Id="5959" PostId="4375" Score="1" Text="The problem is actually different, it's not that I'm missing an intersection it's that the shading normals are creating rays that are valid for the shading normal, but not for the surface geometry normal." CreationDate="2016-12-10T18:07:07.110" UserId="5653" />
  <row Id="5960" PostId="4375" Score="0" Text="You mean that some of the rays created are not on the hemisphere of the geometry normal? Like if you would create a dot product with the ray direction and the geometry normal, it ends up being negative? As far as I know this is just ignored. Don't know what kind of artifact you actually get when you have that. Could you maybe show some pictures of the artifacts?" CreationDate="2016-12-10T19:04:28.773" UserId="4908" />
  <row Id="5963" PostId="4374" Score="0" Text="How often and how much are they in the opposite direction? It could be a math precision error if they are only occasionally inside, by not very much. If they are often the wrong way, it could be that your tangent and bitangent vectors or math is wrong.  If it's almost always wrong, it could be due to mixing left and right handedness maybe." CreationDate="2016-12-12T04:26:19.290" UserId="56" />
  <row Id="5964" PostId="4155" Score="0" Text="I have the same issue with a PDF view when using full screen. To what I found else on the net this looks like a Sierra bug :-(" CreationDate="2016-12-12T13:03:18.890" UserId="5660" />
  <row Id="5966" PostId="4377" Score="0" Text="Are you able to share your actual code (as opposed to pseudo-code)? That way we can test it rather than just reason about it. Incidentally, that's a lot of semicolons for pseudocode - it almost looks as if it would compile..." CreationDate="2016-12-12T23:11:11.320" UserId="231" />
  <row Id="5967" PostId="4374" Score="0" Text="Are you able to share your code?" CreationDate="2016-12-12T23:13:09.203" UserId="231" />
  <row Id="5968" PostId="4377" Score="1" Text="Thanks for your answer @trichoplax. Concerning the screenshot it might be awkward as the whole thing is wrong because as you move the cylinder, you can notice that the penumbra doesn't behaves well.&#xA;I would be glad to share the code because I am losing all my hair trying to make true PCSS... I just need to clean the code a bit :)" CreationDate="2016-12-13T00:20:37.173" UserId="2372" />
  <row Id="5969" PostId="4377" Score="1" Text="Will post some source soon. Hope somebody can help about that." CreationDate="2016-12-13T00:22:03.370" UserId="2372" />
  <row Id="5970" PostId="4378" Score="2" Text="At first glance, the edge artifacts are somewhat reminiscent of alpha-test edges on a low-res texture. This is the outline rendered from the SDF, correct? Can you show the SDF itself? If that looks right, then I'd check if the texture is set up correctly (interpolation mode etc) and look for math bugs in the final rendering shader." CreationDate="2016-12-13T01:44:54.327" UserId="48" />
  <row Id="5972" PostId="4377" Score="0" Text="I meant &quot;the right seems to be correct and the left not&quot;. So, no error in the description." CreationDate="2016-12-13T09:19:24.510" UserId="2372" />
  <row Id="5973" PostId="4378" Score="0" Text="Well, the generated sdf looks like this (in text form): http://pastebin.com/5GY71Y15" CreationDate="2016-12-13T14:49:37.900" UserId="5661" />
  <row Id="5974" PostId="4378" Score="0" Text="And the interpolated image for the sdf looks like this: http://imgur.com/G7pAlLT" CreationDate="2016-12-13T14:59:31.720" UserId="5661" />
  <row Id="5975" PostId="4381" Score="1" Text="Possibly helpful link if you aren't dead set on b-spline.  http://blog.demofox.org/2015/08/09/cubic-hermite-rectangles/" CreationDate="2016-12-13T16:25:09.393" UserId="56" />
  <row Id="5976" PostId="4381" Score="0" Text="@AlanWolfe Thanks for the link, that looks interesting but I am looking for a smoothing effect that is gradual, i.e., Runge's phenomenon is not acceptable" CreationDate="2016-12-13T16:35:42.360" UserId="5633" />
  <row Id="5977" PostId="4380" Score="0" Text="Thank you! So these C constants are just being replaced by those integer values that to me looked like memory adresses which is wrong, but what are those integers actually representing?" CreationDate="2016-12-13T18:06:19.237" UserId="5664" />
  <row Id="5978" PostId="4380" Score="0" Text="@mbl They don't represent anything, they're just arbitrarily assigned values, defined in the OpenGL spec so that applications and GL drivers can communicate with each other." CreationDate="2016-12-13T18:14:11.060" UserId="48" />
  <row Id="5979" PostId="4380" Score="1" Text="@Dan, btw, what do you mean about &quot;single-element structs&quot;? Nowadays I would expect to use enums in C/C++ for this." CreationDate="2016-12-13T18:15:30.123" UserId="48" />
  <row Id="5980" PostId="4380" Score="1" Text="@NathanReed Enums are fine in C++ but in C they're implicitly convertible from integers and other enums, so they only provide the names for constants, not the type-safety. Nowadays I guess you can rely on compilers to warn you at appropriate times, even in C." CreationDate="2016-12-13T18:47:59.790" UserId="2041" />
  <row Id="5981" PostId="4385" Score="1" Text="Hi Jay.  The stack exchange sites are question and answer sites.  While it can happen as a side effect, they are not meant for making connections or for having open ended conversations.  I would recomend asking a specific question about something you are having trouble understanding, and then if you get an answer, use the site's chat feature to connect with the individual.  Voting to close this question as is, since it's off topic, but please feel free to modify it into a specific question about the technique and it will be re-opened!" CreationDate="2016-12-13T22:51:10.100" UserId="56" />
  <row Id="5982" PostId="4377" Score="0" Text="I've posted a new answer to avoid making a huuuuge question." CreationDate="2016-12-14T14:07:39.877" UserId="2372" />
  <row Id="5984" PostId="4377" Score="1" Text="Alright, I'll move it to the question." CreationDate="2016-12-14T14:40:38.620" UserId="2372" />
  <row Id="5985" PostId="4377" Score="1" Text="Incidentally, it's very well written. I was ready to upvote the answer if it had a solution at the end." CreationDate="2016-12-14T14:41:49.160" UserId="231" />
  <row Id="5986" PostId="4377" Score="1" Text="@trichoplax Thanks I think that it's better to give as much informations as possible especially with this kind of topics. It also might help others.&#xA;If you know somebody that might be able to help don't hesitate ;)" CreationDate="2016-12-14T14:45:13.880" UserId="2372" />
  <row Id="5987" PostId="4385" Score="1" Text="Thank you I shall revise as follows. What is the reason of SHRINKAGE after Laplacian Smoothing on noisy surface meshes. But this requires someone who is expert on the techniques, right?" CreationDate="2016-12-14T18:28:08.710" UserId="5670" />
  <row Id="5990" PostId="3699" Score="0" Text="I'm looking for a way to get the DISTANCE MIDPOINT for my quad and cubic bezier curves. I was using t value 0.5 and as you said I just realized it's problematic. What I'm trying to do now with the bezier calculation is moving balls at even speed. But using t value makes it so hard.. I think I need to find proper t values depending on the arc length. Any resource or a small hint for me, please?" CreationDate="2016-12-14T20:59:39.803" UserId="5674" />
  <row Id="5991" PostId="3699" Score="2" Text="Unfortunately no sinple general closed form solution exists for beziers but a good resource that covers numerically doing this and more can be found [Here](https://pomax.github.io/bezierinfo/) tough i read today at work a even better resource should link to it but i havent got the link on my phone. But perhaps this is enough @Jenix" CreationDate="2016-12-14T21:06:55.993" UserId="38" />
  <row Id="5993" PostId="4380" Score="0" Text="@DanHulme: C is not a type-safe language, so that's to be expected." CreationDate="2016-12-14T21:16:31.887" UserId="2654" />
  <row Id="5995" PostId="4377" Score="0" Text="Alright I think I got something by using the distance between the receiver and the blocker. Now I need to apply it correctly." CreationDate="2016-12-15T19:28:18.150" UserId="2372" />
  <row Id="5996" PostId="4381" Score="1" Text="@AlanWolfe I was still stuck and I checked out the article again and it actually gave me awesome results! Thanks so much for the tip. Post an answer if you want and I will accept it" CreationDate="2016-12-15T20:17:04.853" UserId="5633" />
  <row Id="5997" PostId="4381" Score="0" Text="I'm glad it helped, but I've never used the tesselator before, so am not able to make a decent answer.  If you write one up, i'd upvote it (;" CreationDate="2016-12-15T21:42:11.090" UserId="56" />
  <row Id="6000" PostId="4392" Score="0" Text="Thanks, I'll try that.  Yes, I'm kinda aware that such condition exists when using this operations. But I thought that it is just `or` (addition) operation so these operation will combine result in the same value no matter the order. just like 2+3+4 == 4+3+2." CreationDate="2016-12-16T10:45:56.117" UserId="4958" />
  <row Id="6002" PostId="4392" Score="0" Text="In settings bits as occupied positions sense." CreationDate="2016-12-16T13:47:12.690" UserId="4958" />
  <row Id="6005" PostId="4395" Score="0" Text="I've have seen this other formulation for the Cook-Torrance BRDF, where the equation is multiplied by $\frac{1}{4}$ instead of $\frac{1}{\pi}$. However, in the end, the effect of this modification is very small because we would be substituting 2, present in the final equation, by 1.57 ($= \frac{\pi}{2}$). I have made a test here (just in case...), and indeed the problem persisted." CreationDate="2016-12-16T16:40:44.180" UserId="5681" />
  <row Id="6006" PostId="4392" Score="3" Text="@narthex Yeah, it's a commutative operation, but that's not the issue—the problem is, the reads and writes from different threads can be interleaved. For example: thread 1 reads the initial value into a local variable, thread 2 reads the same initial value into its local variable, thread 1 writes a modified value, then thread 2 writes its own modified value that doesn't include the bits that thread 1 had set. So thread 1's contribution ends up getting lost." CreationDate="2016-12-16T18:13:22.347" UserId="48" />
  <row Id="6007" PostId="4395" Score="0" Text="@Capagot A factor of $1/\pi$ is sometimes incorporated into light source intensities (by convention) and left out of BRDFs; [see also this question](http://computergraphics.stackexchange.com/questions/3946/correct-specular-term-of-the-cook-torrance-torrance-sparrow-model). But that's more common in real-time rendering than in path tracing. Also you say your Lambertian tests match Mitsuba perfectly, so it seems less likely that this is the issue...still it might be worth looking into." CreationDate="2016-12-16T18:26:21.593" UserId="48" />
  <row Id="6008" PostId="4395" Score="0" Text="@Capagot I think you're missing a $\frac{1}{\pi}$ in your distribution function $D$. The paper I linked to includes that factor in the Beckmann distribution, which you use, so having $\frac{1}{4}$ in $f_r$ and $\frac{1}{\pi}$ in $D$ should do the trick." CreationDate="2016-12-16T21:21:21.497" UserId="5632" />
  <row Id="6011" PostId="4395" Score="0" Text="@NathanReed I've read the article about embedding $\pi$ into the color. However, for the reason that you've mentioned, I was convinced that that was not the problem." CreationDate="2016-12-16T22:00:28.830" UserId="5681" />
  <row Id="6012" PostId="4395" Score="0" Text="@wolle Exactly! Actually, I had already taken a quick look at the paper that you've mentioned, but I didn't notice that! I've just altered my implementation to account for the $\frac{1}{\pi}$ in $D$ and $\frac{1}{4}$ in $fr$, and everything now works like a charm! I will include an update to the question with the answer! Thank you!" CreationDate="2016-12-16T22:00:33.740" UserId="5681" />
  <row Id="6013" PostId="4395" Score="0" Text="Since comments are not guaranteed to stay around forever, it's worth updating the answer to show both the places that needed to be changed." CreationDate="2016-12-16T22:13:29.653" UserId="231" />
  <row Id="6015" PostId="4399" Score="0" Text="Thanks very much for the answers! To get the results I'm looking for, it might be appropriate to switch to shadow volumes. The codeflow link provided under &quot;interpolation&quot; is very useful - thanks!" CreationDate="2016-12-17T13:43:44.380" UserId="2500" />
  <row Id="6026" PostId="4400" Score="0" Text="Thanks Nathan ! Is this just a new call or does it also include some performance boost. I mean what does this call imply compared to the D3D9 version ?" CreationDate="2016-12-19T07:13:29.160" UserId="2372" />
  <row Id="6034" PostId="4400" Score="1" Text="@MaT It implies you're using D3D10+. ;) It's literally the same operation in hardware; they just redesigned the API and called it a different name." CreationDate="2016-12-19T14:51:52.943" UserId="48" />
  <row Id="6056" PostId="4401" Score="0" Text="Part of [this comment](http://computergraphics.stackexchange.com/questions/4393/handling-projective-aliasing-in-shadow-mapping#comment6015_4399) is thanking you for your answer, so I just thought I'd let you know since you won't get a notification from there." CreationDate="2016-12-20T20:51:56.627" UserId="231" />
  <row Id="6057" PostId="4401" Score="0" Text="Yes, I saw it but thanks." CreationDate="2016-12-20T21:19:53.050" UserId="4958" />
  <row Id="6060" PostId="4333" Score="0" Text="Most Color Quantization algorithms do clustering in the RGB cube (splitting the space of colors in big significant chunks). Try to search for NeuQuant, Xiaolin Wu quantization algorithms for more details." CreationDate="2016-12-21T16:02:43.457" UserId="110" />
  <row Id="6065" PostId="4410" Score="0" Text="Thanks for the answer. So this can only be fixed in the shader? Or is there something I am doing wrong with my actual light pov camera? because basically I am taking my normal cameras position and moving the light pov camera to that position every frame instead of just having a static light pov camera for the entire scene high up somewhere." CreationDate="2016-12-22T11:47:40.470" UserId="5709" />
  <row Id="6066" PostId="4409" Score="1" Text="Do you have `GL_CULL_FACE` disabled when rendering to shadow map?" CreationDate="2016-12-22T15:19:37.290" UserId="4958" />
  <row Id="6067" PostId="4410" Score="0" Text="Shouldn't the light POV camera be in the same position as the light?" CreationDate="2016-12-22T16:00:40.433" UserId="3003" />
  <row Id="6068" PostId="4409" Score="0" Text="I tried with it both on and off, it only changes the shadow slightly under the models feet." CreationDate="2016-12-22T19:34:24.083" UserId="5709" />
  <row Id="6070" PostId="4410" Score="3" Text="@Kachinsky what user1118321 means, I think, is that you're applying the shadow to a surface which is not lit (the light comes from behind). It's what it looks like to me anyway. You must think of the shadow map as something which alters the intensity of a specific light, not something you can just slap onto the object at the end of your shader. If there's no light, there can be no shadow." CreationDate="2016-12-23T15:41:40.463" UserId="5717" />
  <row Id="6071" PostId="163" Score="0" Text="I would recommend this article from Scratchapixel for an in-depth explanation: https://www.scratchapixel.com/lessons/advanced-rendering/ray-tracing-distance-fields" CreationDate="2016-12-24T12:54:46.943" UserId="1608" />
  <row Id="6072" PostId="4413" Score="1" Text="First step in any GL debugging problem: have you tried setting up [KHR_debug](http://renderingpipeline.com/2013/09/opengl-debugging-with-khr_debug/), and/or sprinkling your code with [`glGetError`](http://docs.gl/gl4/glGetError) calls, to see if the driver reports any issues?" CreationDate="2016-12-25T18:58:35.793" UserId="48" />
  <row Id="6073" PostId="4413" Score="0" Text="Thanks for commenting. Yes i am using debug output extension and it didnt report any problem. I have also checked each iteration of vertex shader with RenderDoc. Everything is updating according to their vertex divisor settings besides texture coordinates. This is weird." CreationDate="2016-12-25T19:02:17.280" UserId="2096" />
  <row Id="6074" PostId="4413" Score="1" Text="Hmm, what _are_ you getting for the texture coordinates in RenderDoc? When you say they're not updating correctly, what are they doing instead?" CreationDate="2016-12-25T19:14:58.093" UserId="48" />
  <row Id="6075" PostId="4413" Score="0" Text="I mean its showing texture coordinates of the first instance for all instances. I checked all the data before sending it to the gpu memory.  I will put the whole code and check it one more time later." CreationDate="2016-12-25T19:24:37.480" UserId="2096" />
  <row Id="6078" PostId="4415" Score="1" Text="Worth noting also that he could get the behavior he wants using a texture buffer or SSBO, and manually indexing it with `gl_InstanceID * vertex_count + gl_VertexID` in the shader." CreationDate="2016-12-25T23:40:06.227" UserId="48" />
  <row Id="6079" PostId="4415" Score="0" Text="@NathanReed: Heh heh heh, no you can't. Instance arrays and `gl_InstanceID` have different interactions with [base instance rendering](https://www.khronos.org/opengl/wiki/Vertex_Rendering#Instancing). And by &quot;different interactions&quot;, I mean `gl_InstanceID` *completely ignores* the base instance. Now, if this were *Vulkan*, `gl_InstanceIndex` could do that. But if you want to do this in GL, you [have to use an extension](https://www.opengl.org/registry/specs/ARB/shader_draw_parameters.txt)... for some reason." CreationDate="2016-12-26T01:29:12.170" UserId="2654" />
  <row Id="6080" PostId="4417" Score="1" Text="You might want to say what platform you're on, as that will likely change the answer." CreationDate="2016-12-26T06:14:47.743" UserId="3003" />
  <row Id="6081" PostId="4417" Score="0" Text="I edited my question. Thank you." CreationDate="2016-12-26T07:53:53.180" UserId="2096" />
  <row Id="6082" PostId="4417" Score="1" Text="There's a list on the OpenGL page... https://www.khronos.org/opengl/wiki/Debugging_Tools" CreationDate="2016-12-26T08:04:31.393" UserId="1937" />
  <row Id="6083" PostId="4417" Score="0" Text="Not sure about the quality of the various tools though, generally it's not up to the standard of the DirectX tools that ship with Visual Studio. I tried gDebugger on a project last year but it didn't work with compute shaders at all..." CreationDate="2016-12-26T08:06:14.427" UserId="1937" />
  <row Id="6084" PostId="4417" Score="0" Text="That link helped. Thanks. Did you try CodeXL?" CreationDate="2016-12-26T08:10:16.383" UserId="2096" />
  <row Id="6086" PostId="4417" Score="0" Text="Questions about debugging GLSL are on topic. Questions like [this](http://computergraphics.stackexchange.com/questions/96/how-can-i-debug-glsl-shaders) and [this](http://computergraphics.stackexchange.com/questions/23/how-can-i-debug-what-is-being-rendered-to-a-frame-buffer-object-in-opengl) are fine. However, requests for off site resources (such as a debugging tool) are off topic here. You can ask for recommendations over on Software Recommendations but please read their [on topic](http://softwarerecs.stackexchange.com/help/on-topic) page first." CreationDate="2016-12-26T11:36:28.700" UserId="231" />
  <row Id="6087" PostId="4418" Score="3" Text="Fascinating question, though I don't know that we have the expertise on this site to answer it. :) I'm not aware of any displays that work on principles other than additively combining a set of primaries, which as you mentioned only gives a convex polygonal subset of the CIE space. The liquid crystal tunable filter sounds like amazing tech, but I'd guess that it's not in consumer displays just due to some set of engineering tradeoffs vs standard LCDs (like cost, brightness, field of view, switching speed, and suchlike)." CreationDate="2016-12-27T00:35:56.803" UserId="48" />
  <row Id="6088" PostId="4415" Score="2" Text="OK, but &quot;no you can't&quot; is a bit too pessimistic—that's only a problem if you're using nonzero base instances to begin with; and if you are, you could also store the base instance in a uniform and account for it in the shader. :)" CreationDate="2016-12-27T00:40:04.973" UserId="48" />
  <row Id="6092" PostId="4418" Score="2" Text="One issue I see with this, what content would you play through such a display if it existed? All currently existing digital media is encoded as RGB (or YCrCb, which gets converted to the same thing). So to take advantage of this new colour gamut, graphics programmers would have to completely change the way they write colour code, and CCD manufacturers would also have to create new sensors that capture the full range of colours." CreationDate="2016-12-27T09:54:27.003" UserId="1937" />
  <row Id="6093" PostId="4420" Score="0" Text="Thank you for your answer, but don't LCD color filters filter light out too? In fact, the larger the color range a display has, the more narrow the transmittance spectrum of the filter will be, meaning that there will be large losses even for white. With tunable color filters, though, there would be large losses only for saturated colors (which matches what objects do in reality; saturated objects absorb more of the illuminant), but pure white would be created by letting all wavelengths pass through (a &lt; 385 nm, b, c &gt; 745 nm), meaning less losses than LCDs." CreationDate="2016-12-27T13:30:59.843" UserId="5724" />
  <row Id="6094" PostId="4420" Score="0" Text="Also, if we pick a backlight whose spectrum is the same as the LEDs that light up the room, we could produce all the colors that can possibly be found in objects of that room. I do agree that minaturization would be difficult (although it would probably be easier than other recent display technologies such as OLEDs since Lyot filters use liquid crystals to be tuned)." CreationDate="2016-12-27T13:41:43.123" UserId="5724" />
  <row Id="6095" PostId="4420" Score="0" Text="@d9584 yes but the LCD has the luxury of having only 3 bands to contend to so it can survive with a light that has less band than full spectrum and it can pass things in wider band." CreationDate="2016-12-27T14:04:51.913" UserId="38" />
  <row Id="6096" PostId="4418" Score="1" Text="@russ There's not much content for HDR displays yet either, and they are slowly being adapted. I think that if such a display produced images that look significantly better than what regular RGB displays can do, it would have a chance to eventually see widespread use. That's another big if, though." CreationDate="2016-12-27T14:34:29.260" UserId="3470" />
  <row Id="6097" PostId="4213" Score="0" Text="This is copy pasted in terms of weights and incorrect. What is missed in those three weight values is the fact that for any given colour space the weights are different. The weights are determined by the given primary lamp's luminance position, or Y. The values given 0.299, 0.587, and 0.114 are archaic and incorrect. For any given set of encoded values representing sRGB / 709 lights, the correct values are 0.2126, 0.7152, and 0.0722. Do *not* use the weights posted in the answer. It is *not* subjective." CreationDate="2016-12-27T16:20:18.453" UserId="5556" />
  <row Id="6098" PostId="4418" Score="0" Text="@Quinchilion yes but most devices are capable of capturing higher dynamic range but not color outside the tristimulus triangle. Besides HDR is just upwards in the chart so and can easily be reranged accetably. Sure no problem dont display color you dont measure." CreationDate="2016-12-27T16:28:35.083" UserId="38" />
  <row Id="6101" PostId="4423" Score="0" Text="...but the system I'm describing is not a tri-light system. Instead of using a normal color filter, it uses a filter that can be adjusted using liquid crystals to allow only a certain wavelength range to pass." CreationDate="2016-12-27T19:00:54.370" UserId="5724" />
  <row Id="6102" PostId="4423" Score="0" Text="That's sort of what we already have. The point is, given that the CIE model is based on a standard observer, it is impossible with any given tristimulus system and the visible light range." CreationDate="2016-12-27T19:07:38.450" UserId="5556" />
  <row Id="6103" PostId="4423" Score="2" Text="No, it isn't. Currently the liquid crystals just change the opacity of  the color filters without changing their actual colors. What I'm talking about is using three filters that change the transmittance range of the color filter (like in this image: https://www.rp-photonics.com/img/edge_filter.png). This would mean that the three points in the chromacity diagram are not fixed, and can be moved aroud to produce any color." CreationDate="2016-12-27T19:15:33.477" UserId="5724" />
  <row Id="6107" PostId="4423" Score="0" Text="@d9584 I can't speak to the hypothetical idea of a wavelength based system, and the outline I tried to answer for was why no full spectral locus display currently exists as per your first question. I also suspect that any such system would need an absolutely absurd emission both in intensity and uniform spectral output to be feasible. Given the constraints on the latter, it would need a light-year jump in technology to be even remotely feasible." CreationDate="2016-12-27T22:52:05.343" UserId="5556" />
  <row Id="6113" PostId="4407" Score="0" Text="Isn't rendering at a resolution higher than the display's output resolution, then having the resultant bitmap downsampled to the target resolution by the GPU, a very common technique for increasing overall graphics quality in games, specifically for things like anti-aliasing? I imagine it's not so odd to have a game offer significantly higher render resolutions than the display is capable of natively displaying." CreationDate="2016-12-28T14:36:54.387" UserId="5291" />
  <row Id="6114" PostId="4060" Score="0" Text="I would say there's no general rule to speak of. There are so many different color quantization algorithms developed over the years, and they all seem to have various advantages or disadvantages specifically for smooth gradient dithering, but I'm probably wrong! This is a topic that I'm very interested in. Here's a great article that compares many dithering algorithms and explains dithering in general: http://www.tannerhelland.com/4660/dithering-eleven-algorithms-source-code/" CreationDate="2016-12-28T14:45:38.603" UserId="5291" />
  <row Id="6119" PostId="4432" Score="1" Text="Wow. They probably hire toughest minds in the world to make up most distinct names for same things." CreationDate="2016-12-28T21:10:42.407" UserId="4958" />
  <row Id="6120" PostId="4432" Score="0" Text="That chart is great, thanks for having taken the time to write it down!" CreationDate="2016-12-28T21:45:46.330" UserId="110" />
  <row Id="6121" PostId="4432" Score="1" Text="&quot;*texture array - layered image*&quot; OpenGL also calls them Array Textures; the term &quot;layered image&quot; is used primarily around their attachments in FBOs. Also, you should probably mention OpenGL sampler objects and their D3D equivalent, sampler state." CreationDate="2016-12-28T23:21:42.820" UserId="2654" />
  <row Id="6122" PostId="4432" Score="0" Text="@NicolBolas Thanks. I've edited and hopefully clarified that." CreationDate="2016-12-29T04:14:50.003" UserId="48" />
  <row Id="6123" PostId="4434" Score="0" Text="Nice, this is great work!" CreationDate="2016-12-29T11:06:17.883" UserId="110" />
  <row Id="6124" PostId="4422" Score="0" Text="Comments are not for extended discussion; this conversation has been [moved to chat](http://chat.stackexchange.com/rooms/50883/discussion-on-question-by-wil-directx-openglvulkan-concepts-mapping-chart)." CreationDate="2016-12-29T11:34:17.840" UserId="231" />
  <row Id="6127" PostId="4435" Score="0" Text="This is more than thread safety. A mutex would make any global RNG thread safe but still slow and unpredictable with multiple threads. Per-pixel seed is indeed the way to go for reproducibility, although per-thread state is enough to fix the performance problem." CreationDate="2016-12-29T15:33:50.220" UserId="5717" />
  <row Id="6128" PostId="4435" Score="0" Text="With thread-safe I don't mean use of mutex etc. but just making of an RNG class that has no static members. The clib version is bad as it has a global state shared across threads. I'll fix the answer to be more clear" CreationDate="2016-12-29T15:51:48.517" UserId="1952" />
  <row Id="6129" PostId="4434" Score="0" Text="I think it would be fair to say that `VkDescriptorPool` and `ID3D12DescriptorHeap` are similar in function (in that they are how you allocate descriptors), but quite different in form, due to the differences in the overall way descriptors are handled between the APIs. Also, I imagine that the D3D12 equivalent to `VkBufferView` is typed buffers, just as for D3D11." CreationDate="2016-12-29T16:24:42.290" UserId="2654" />
  <row Id="6130" PostId="4434" Score="0" Text="You're right on the descriptor heap. Updated. Regarding the buffer views, both API have a concept of views. I couldn't tell if DX12 has broken away from the DX1 convention of typed buffers or not since I don't have much experience with DX11." CreationDate="2016-12-29T16:37:50.477" UserId="5745" />
  <row Id="6131" PostId="4435" Score="0" Text="PBRT already [has that](https://github.com/mmp/pbrt-v2/blob/master/src/core/rng.h) so the problem is likely how it is used (shared ?). I unfortunately don't know the PBRT code well enough to know what's going on." CreationDate="2016-12-29T17:04:04.390" UserId="5717" />
  <row Id="6133" PostId="4434" Score="0" Text="Could you say that D3D12 render targets are the equivalent of VkFramebuffer?" CreationDate="2016-12-30T06:03:19.123" UserId="125" />
  <row Id="6134" PostId="4434" Score="0" Text="@JorgeRodriguez - I updated response to address your question." CreationDate="2016-12-30T08:48:56.470" UserId="5745" />
  <row Id="6135" PostId="4432" Score="0" Text="Nice chart, but usually I find that `Multiple Render Targets` and `Occlusion Query` are also called the same in OpenGL, in general." CreationDate="2016-12-30T09:32:58.847" UserId="5002" />
  <row Id="6136" PostId="4437" Score="0" Text="I can't tell from the animation what the specific causes and consequences of the gimbal lock are. Could you explain more about what happens and whether it is reversible?" CreationDate="2016-12-30T12:11:07.313" UserId="231" />
  <row Id="6137" PostId="4437" Score="0" Text="It halped! Thank you for taking some time out for this." CreationDate="2016-12-30T13:06:37.233" UserId="2096" />
  <row Id="6138" PostId="4437" Score="2" Text="Oh sorry, rotation in this case is around axis (shown as circles) and by rotating on one axis, you rotate the other axis. The consequence is that with normal rotation sphere (the sphere in the animation) in animation, you don't see these axis being rotated, so if you get a gimbal lock, one axis is the same as another one, but with a normal rotation sphere it still tries to rotate as if that axis wasn't the same as another one, this means that it uses weird values to get to that rotation. I thought that the full post made that clear and that Ankit had some knowledge about it, @trichoplax" CreationDate="2016-12-30T14:58:35.853" UserId="4908" />
  <row Id="6139" PostId="4437" Score="0" Text="For me, the extra explanation in your comment makes a big difference, so it might be worth editing it into the answer for future readers (comments aren't guaranteed to stay around)." CreationDate="2016-12-30T15:16:59.590" UserId="231" />
  <row Id="6140" PostId="4437" Score="0" Text="Gimbal lock has nothing to do with matrices." CreationDate="2016-12-30T19:32:16.730" UserId="38" />
  <row Id="6141" PostId="4432" Score="0" Text="@CpCd0y Yeah, they're colloquially called that, but my intent here was to say what those things are called / how they're represented in API-ese." CreationDate="2016-12-30T22:02:14.560" UserId="48" />
  <row Id="6142" PostId="4437" Score="0" Text="The way I see it, is that knowing a little bit about how the rotation matrix is made from Euler angles can be useful to understanding Gimbal locking @joojaa  But you do have a point." CreationDate="2016-12-30T22:18:29.197" UserId="4908" />
  <row Id="6143" PostId="4437" Score="1" Text="@bram0101 sure. Or you can actually not think so abstractly and think of it as actual physical 3 axis gimbal" CreationDate="2016-12-30T22:27:50.870" UserId="38" />
  <row Id="6144" PostId="4432" Score="0" Text="@NathanReed: MRT is not used in the OpenGL specification, but &quot;occlusion query&quot; very much is. The term &quot;samples passed&quot; is merely one kind of occlusion query; there are also &quot;any samples passed&quot; and &quot;conservative any samples passed&quot;." CreationDate="2016-12-31T03:26:10.050" UserId="2654" />
  <row Id="6145" PostId="4435" Score="0" Text="A thread-safe RNG is not enough because that it only solves the race condition. But it serializes the execution and cache invalidation will still be present." CreationDate="2016-12-31T04:22:16.790" UserId="2448" />
  <row Id="6146" PostId="4435" Score="0" Text="I'm not suggesting thread-safe implentation of a RNG class but implemeting RNG class that you use thread-safely, by not sharing the state across threads." CreationDate="2016-12-31T04:48:29.783" UserId="1952" />
  <row Id="6147" PostId="4437" Score="0" Text="&quot; The xyz Euler angles can be converted to matrices so that it can be used in the rotation. That is where something called rotation order comes in &quot; cleared my confusion. Thanks." CreationDate="2016-12-31T05:42:47.707" UserId="2096" />
  <row Id="6148" PostId="4439" Score="0" Text="Have you seen that drawing works if you take out the degenerate triangles? If not, then what makes you think that degenerate triangles have *anything* to do with your problem? &quot;*The geometry itself is correctly formed because it works on Nvidia cards.*&quot; No, that's not a reasonable conclusion to come to from that evidence. &quot;Works on NVIDIA&quot; only means &quot;works on NVIDIA&quot;. There's plenty of stuff NVIDIA's GL implementations do that are outside the specification." CreationDate="2016-12-31T17:41:26.053" UserId="2654" />
  <row Id="6149" PostId="4439" Score="1" Text="&quot;*On ATI it displays the gray scissor clear box but that's it.*&quot; What &quot;gray scissor box&quot; are you referring to?" CreationDate="2016-12-31T17:42:33.573" UserId="2654" />
  <row Id="6150" PostId="4439" Score="0" Text="The only way to resolve problems like this is to boil them down to the simplest elements: draw one triangle. Get that working. Then keep adding elements until you find the one that causes things to disappear. That's your problem." CreationDate="2016-12-31T17:44:28.373" UserId="2654" />
  <row Id="6151" PostId="4439" Score="0" Text="I do not suspect that the degenerates have anything to do with the issue. I don't think I said that I did. I was simply describing the format of the geometry that is being sent that is not displaying correctly.By saying that the geometry itself is correctly formed what I meant is that the indices create triangle strips with the correct vertex indices specified in the correct order which produce the correct number of triangles with the correct alternating winding order. I was simply referring to the actual vertex data in `heightMapVertexData`. It is what i expect it to be." CreationDate="2016-12-31T17:52:30.003" UserId="5763" />
  <row Id="6152" PostId="4439" Score="0" Text="&quot;*I do not suspect that the degenerates have anything to do with the issue. I don't think I said that I did.*&quot; But that is what you ask in the title of the question: &quot;Are ATI cards lacking some feature that would allow them to draw triangle strips connected by degenerates with a single draw call using a vao?&quot; So either that's what you're asking about or the title is misleading." CreationDate="2016-12-31T17:55:49.543" UserId="2654" />
  <row Id="6153" PostId="4439" Score="0" Text="I am very familiar with debugging things from the ground up by adding one element at a time. That's how I got the system working on my Nvidia cards in the first place. That and reading the OpenGL spec.&#xA;&#xA;Unfortunately I have no access to ATI hardware where I am right now so that's not an option for debugging this issue for another few months when I can get back home where I have ATI hardware.&#xA;&#xA;I was hoping there was some simple well known reason ATI cards don't work with what I am doing that I just missed. That way I could get more eyes on my application sooner." CreationDate="2016-12-31T17:58:22.927" UserId="5763" />
  <row Id="6154" PostId="4439" Score="0" Text="I have changed the title to be less specific about the geometry being drawn." CreationDate="2016-12-31T18:01:26.533" UserId="5763" />
  <row Id="6155" PostId="4439" Score="0" Text="The gray scissor box I'm referring to is the one drawn by:" CreationDate="2016-12-31T18:03:17.857" UserId="5763" />
  <row Id="6156" PostId="4439" Score="0" Text="glEnable(GL_SCISSOR_TEST)&#xA;glEnable(GL_MULTISAMPLE)&#xA;glClearColor(background.r, background.g, background.b, background.a)&#xA;glScissor(xPosition, flippedY, width, height)&#xA;glClear(GL_COLOR_BUFFER_BIT or GL_DEPTH_BUFFER_BIT)" CreationDate="2016-12-31T18:03:40.903" UserId="5763" />
  <row Id="6158" PostId="4439" Score="0" Text="Ok. I have added a better description of the gray scissor box to the question as you recommended." CreationDate="2016-12-31T21:37:34.640" UserId="5763" />
  <row Id="6159" PostId="4439" Score="0" Text="Could AMD be optimizing out vertex attributes, throwing things off?" CreationDate="2016-12-31T22:41:01.130" UserId="2500" />
  <row Id="6160" PostId="4439" Score="0" Text="Yes the ATIs (and Nvidias) optimize out attributes if they aren't used in the body of the shader. `if (positionAttribute.location &gt;= 0) {` guards against enabling specific attributes if they are optimized out by the shader compiler. This had no impact on the output on ATI cards but it did fix an 0x501 error from the Nvidia cards when trying to enable an inactive attribute. The Nvidias still rendered fine regardless of the `0x501 GL_INVALID_VALUE` error however." CreationDate="2016-12-31T22:48:39.403" UserId="5763" />
  <row Id="6161" PostId="4439" Score="0" Text="In the case of the shaders in this question both vertex attributes shown here `positionAttribute` and `uvAttribute` are non-negative. The code is shared in other locations and the guard against the negative attributes is for those other cases. The 0x501 I mentioned was from those other cases as well." CreationDate="2016-12-31T22:54:37.843" UserId="5763" />
  <row Id="6162" PostId="4439" Score="0" Text="AMd cards are capable of drawing using this technique; there is either a driver bug or some in spec variability. Uninitialized variables in shaders were a common case for a while (NV defaulted vectors to 0001 and AMD left garbage in them) and we already discussed compiler variability in optimizing out/placing attributes. If I think of anything else..." CreationDate="2017-01-01T01:00:30.800" UserId="2500" />
  <row Id="6163" PostId="4439" Score="0" Text="In glClear, shouldn't you be using bit wise or (|) not logical &quot;or&quot; (which is like ||, right)? Doubt it's anything..." CreationDate="2017-01-01T01:43:19.143" UserId="2500" />
  <row Id="6164" PostId="4439" Score="0" Text="It's Kotlin. I didn't think the language was too important in the post in the is case since the GL calls are all the same. I'll update the question to mention that. `or` in this way is bitwise in kotlin. `||` is logical or in Kotlin." CreationDate="2017-01-01T12:35:51.620" UserId="5763" />
  <row Id="6165" PostId="4442" Score="0" Text="You mean accurate atmospheric scattering? Why not render vertices which form skydome which is centered at camera position and color it in fragment directly?" CreationDate="2017-01-01T14:32:07.170" UserId="4958" />
  <row Id="6166" PostId="4444" Score="0" Text="Great Answer but wouldn't having a hemisphere around the entire scene centered at the camera that has a sky texture (created with a sky rendering model) on the inside do the trick fine? I mean I will of course represent the sun as a disk light and apply some sort of physically based color transform on the disk light (will have to check a siggraph paper I read for the actual name). What are the advantages of using the method I described over the method you said?" CreationDate="2017-01-01T18:24:19.897" UserId="5256" />
  <row Id="6167" PostId="4444" Score="1" Text="Using the method I described, the scene will appear illuminated by the whole environment. Think of the indirect illumination and bounces between the scene geometry too. What you are describing is a model with a texture, and no emission. If you only set emission on the disk and illuminate the scene just with that, it is the same as having an area light on that solid angle of the disk, you are not emitting the color of the whole sky onto the objects and making that radiance bounce between them as indirect illumination." CreationDate="2017-01-01T18:35:22.263" UserId="5519" />
  <row Id="6168" PostId="4444" Score="1" Text="Of course, using the sun light source separated from the sphere would work if you wanted to simulate the whole atmosphere physically, with scattering. But it would be way more involved than my answer and I doubt that is what you want. Generally, we pick an image of the sky and emit that image onto the whole scene as light (the process I described)." CreationDate="2017-01-01T18:42:18.053" UserId="5519" />
  <row Id="6171" PostId="4450" Score="0" Text="Yeah CPU cores. I just edited my question. Thank you" CreationDate="2017-01-02T18:28:13.587" UserId="2096" />
  <row Id="6172" PostId="4454" Score="1" Text="Could you let us know what you mean by &quot;doesn't get correctly loaded&quot;? What specifically goes wrong that indicates this?" CreationDate="2017-01-02T23:30:19.500" UserId="231" />
  <row Id="6173" PostId="4454" Score="0" Text="Hi @trichoplax, the problem is that the array doesn't contain any data. I'm sure of this because the data I'm trying to load is used to calculate the colours of objects using tristimulus values, but the object is always black. In fact, if I tested now that if I try to pass an array of 4 values and the block in the shader contains for example a single variable of type vec4, the code above works. Thanks." CreationDate="2017-01-02T23:33:45.547" UserId="2237" />
  <row Id="6174" PostId="4456" Score="0" Text="Thank you @Nicol_Bolas. Now it's clear (and I will use a standard uniform to pass my data to the shader). I will delete my other post on stack overflow, but can you remove the down vote, please? Thank you again." CreationDate="2017-01-02T23:58:42.610" UserId="2237" />
  <row Id="6175" PostId="4456" Score="0" Text="@FabrizioDuroni a number of people have viewed this post. We don't know who the downvoter was." CreationDate="2017-01-03T00:11:04.987" UserId="231" />
  <row Id="6176" PostId="3713" Score="0" Text="Convert radians to revolutions by multiplying by 1/(2pi); have your texture represent one complete revolution of sin/cos/whatever and lookup with GL_REPEAT. Please post what kind of speedups you get - different HW has very different performance characteristics - AMD has generally supported single cycle sin/cos instructions through HW lookup tables and texture lookup tables were, at least at one time, a slowdown. Keeping the table small so it lives in cache as GroverManheim suggested, is important." CreationDate="2017-01-03T01:04:27.483" UserId="2500" />
  <row Id="6177" PostId="4455" Score="4" Text="Many raytracers implement analytical shapes. In fact we started 3d thatway and progressed to dicing of analytical shapes and only then to using polygons." CreationDate="2017-01-03T04:57:01.723" UserId="38" />
  <row Id="6178" PostId="4455" Score="1" Text="You may be interested to read about the [ellipsoid rendering of the Ecstatica series](http://gdalgorithms-list.narkive.com/qbLh34tI/ellipsoid-engine-used-in-game-ecstatica-how-its-done) back in the 1990s.&#xA;&#xA;![Ecstatica screenshot](http://vignette4.wikia.nocookie.net/ecstatica/images/f/f7/Ecs_firstencounter.jpg/revision/latest?cb=20120813160048)" CreationDate="2017-01-03T09:28:43.610" UserId="2817" />
  <row Id="6179" PostId="4459" Score="1" Text="Have you got any additional information about the overlay? Perhaps it's not plain alpha blending, but another operator. You could try some of the [Photoshop layer operator equations](http://photoblogstop.com/photoshop/photoshop-blend-modes-explained) instead." CreationDate="2017-01-03T11:26:27.667" UserId="2817" />
  <row Id="6180" PostId="3713" Score="0" Text="When you say &quot;really old iDevice&quot; how old do you mean? For example, [page 8 of the series 6](http://cdn.imgtec.com/sdk-documentation/PowerVR+Series6+Compiler.Instruction+Set+Reference.pdf) gives the &quot;fred&quot; instruction which is meant for trig range reduction (admittedly for Radians), which can then be followed by fsinc. It's been a while since I worked on these (i.e. developing the HW algorithms) but can't imagine there's a much faster way of calculating sine/cosine." CreationDate="2017-01-03T11:28:56.160" UserId="209" />
  <row Id="6181" PostId="4460" Score="1" Text="Are you talking about on a PC (where the GPU has its own memory) or a mobile platform (where the GPU accesses main memory directly)?" CreationDate="2017-01-03T11:34:07.330" UserId="2041" />
  <row Id="6182" PostId="4462" Score="0" Text="convex object of are there concave parts?" CreationDate="2017-01-03T14:16:27.747" UserId="137" />
  <row Id="6183" PostId="4462" Score="0" Text="@ratchetfreak I uploaded the pic of the objects. :) Just curious, what is your consideration by asking that question?" CreationDate="2017-01-03T14:31:54.747" UserId="2687" />
  <row Id="6184" PostId="4462" Score="1" Text="seeing if the GJK algorithm would work. It requires convex objects." CreationDate="2017-01-03T14:37:42.190" UserId="137" />
  <row Id="6185" PostId="4465" Score="0" Text="Yeah I expected such answer. Wait for GPU - I mean that draw call as any other function returns after its finishes work, in this case let's say - ordered work. Yeah there is some confusion here." CreationDate="2017-01-03T15:13:51.317" UserId="4958" />
  <row Id="6186" PostId="4465" Score="1" Text="@narthex: &quot;*returns after its finishes work*&quot; That all depends on how you define &quot;its work&quot;. The &quot;work&quot; of a draw call is to drop a token into the GPU's command stream. And sometimes not even that much. In all likelihood, by the time that function returns, [nothing has even *started* to render, let alone finished](https://www.khronos.org/opengl/wiki/Synchronization)." CreationDate="2017-01-03T15:23:51.260" UserId="2654" />
  <row Id="6187" PostId="4465" Score="0" Text="_OpenGL Rendering Commands are assumed to be asynchronous._ I feel like noob now." CreationDate="2017-01-03T15:42:09.103" UserId="4958" />
  <row Id="6188" PostId="4455" Score="1" Text="For modeling more general curved surfaces 3d splines are also commonly used, such as Bézier patches and NURBS surfaces. Most graphics applications just approximate these curves using triangles, but there are methods for displaying them that do not require such approximations." CreationDate="2017-01-03T22:06:59.027" UserId="4768" />
  <row Id="6189" PostId="3713" Score="0" Text="If sin instruction and texture lookup are expensive, you might also try approximating sin with a polynomial" CreationDate="2017-01-03T22:22:35.723" UserId="1952" />
  <row Id="6190" PostId="4466" Score="0" Text="Thanks.. Will take a look and confirm whether it satisfy my needs or not.. :)" CreationDate="2017-01-04T01:09:50.980" UserId="2687" />
  <row Id="6191" PostId="4450" Score="0" Text="Could you be a little more specific about what you have tried or found so far?" CreationDate="2017-01-04T09:31:20.760" UserId="182" />
  <row Id="6192" PostId="4469" Score="2" Text="The fuzzy look could come from the denoiser that they use. Renderman, the renderer that Pixar and probably Disney uses, has gotten a denoiser a while back. It has been used in Finding Dory, and could be used in Moana if they used Renderman to render it." CreationDate="2017-01-04T10:38:31.070" UserId="4908" />
  <row Id="6193" PostId="4469" Score="0" Text="My favorite source that describes what Dan says below is found [here](https://renderman.pixar.com/view/basic-computer-graphics-2) incidenttaly its a pixar publication (document is a bit old but is still valid for the theory). @bram0101 why wouldn't Disney use their own renderer... Disney owns Pixar remember." CreationDate="2017-01-04T10:57:04.207" UserId="38" />
  <row Id="6194" PostId="4469" Score="1" Text="Does anyone know what data compression scheme/format the cinematic releases use? Could it potentially be an artefact of skimping on the bit rate?" CreationDate="2017-01-04T11:21:04.717" UserId="209" />
  <row Id="6195" PostId="4469" Score="0" Text="Disney has also developed their own renderer, https://www.disneyanimation.com/technology/innovations/hyperion  @joojaa" CreationDate="2017-01-04T14:06:03.223" UserId="4908" />
  <row Id="6196" PostId="4470" Score="1" Text="Doesn't this belong on Physics.SE?" CreationDate="2017-01-04T18:31:26.297" UserId="5792" />
  <row Id="6197" PostId="4472" Score="3" Text="Strictly speaking, light does travel through metals but gets attenuated very rapidly, so that it doesn't penetrate more than a few microns below the surface. (Very thin layers of metal _are_ partially transparent—the gold film on spacesuit helmets, for instance.) That's what the imaginary component of the IOR measures: the rate of attenuation. And Fresnel's law applies just as much to metals as to anything else, as seen in the other answers." CreationDate="2017-01-04T22:03:19.810" UserId="48" />
  <row Id="6198" PostId="4462" Score="2" Text="I think a typical approach would be to use a BSP tree to split the object into concave parts, then use BVH sweep and prune (broad-phase) + GJK (narrow-phase) to get exact collision." CreationDate="2017-01-04T22:23:09.090" UserId="48" />
  <row Id="6199" PostId="4471" Score="0" Text="Also, they don't send primary rays out in a regular grid pattern (which is effectively what you get by increasing resolution)." CreationDate="2017-01-05T00:47:34.720" UserId="2654" />
  <row Id="6200" PostId="4471" Score="0" Text="I wonder if there are adaptive AA techniques which decide the successive sample positions based on previous samples rendered for the pixel" CreationDate="2017-01-05T01:25:33.237" UserId="1952" />
  <row Id="6201" PostId="4462" Score="0" Text="@NathanReed But the broad-phase you mention will require both objects to be collided. Am I right? I prefer to define a collision, if distance between two objects are &lt;= certain value." CreationDate="2017-01-05T01:46:48.700" UserId="2687" />
  <row Id="6202" PostId="4462" Score="2" Text="@Bla... You can inflate all the bounding boxes (or bounding spheres or whatever) by the tolerance, or build the tolerance into the broad-phase intersection tests." CreationDate="2017-01-05T01:53:55.270" UserId="48" />
  <row Id="6203" PostId="4477" Score="1" Text="I have seen a few people apply machine learning to BRDF's.  Here's one such thing: http://www.iaeng.org/IJCS/issues_v42/issue_1/IJCS_42_1_04.pdf" CreationDate="2017-01-05T05:05:25.083" UserId="56" />
  <row Id="6204" PostId="4471" Score="3" Text="@JarkkoL Yes, that's the usual way of doing it. The cost of the adaptive sampler is much less than the cost of additional primary rays, and it lets you get better results with fewer rays." CreationDate="2017-01-05T10:20:38.837" UserId="2041" />
  <row Id="6205" PostId="4471" Score="2" Text="@NicolBolas That's a good point, so I've added it (and JarkkoL's clarification) to the answer." CreationDate="2017-01-05T10:26:20.250" UserId="2041" />
  <row Id="6206" PostId="4470" Score="0" Text="Although many computer graphics questions involve physics, this is clearly a question looking for answers from computer graphics experts, and would not be a good fit on physics.SE." CreationDate="2017-01-05T10:40:23.393" UserId="231" />
  <row Id="6207" PostId="4473" Score="0" Text="Yep, thank you for adding these questions; some of them I already had them on my list. I am a bit puzzled how to use or create a proper DB for this as it seems to require a lot of accurate resources. I was hoping for an alternate/possible solution.." CreationDate="2017-01-05T11:44:40.893" UserId="5780" />
  <row Id="6208" PostId="4473" Score="0" Text="@4673_j This is why even Adobe does not implement this function. Even though their users would benefit (and they would get the DB in no time by asking users). Basically its not entirely feasible at current date." CreationDate="2017-01-05T11:47:17.193" UserId="38" />
  <row Id="6209" PostId="4473" Score="0" Text="Hm.. I might have a partial solution in my case, as there might be ~10 different monitors/screens. I'm searching now how to extract that information from the system. Thanks for your effort. I think this kind of &quot;feature/info&quot; has to be provided by the OS, not just the resolution; as the OS has direct access to all physical components." CreationDate="2017-01-05T11:55:15.507" UserId="5780" />
  <row Id="6210" PostId="4473" Score="0" Text="@4673_j Its good that this info did not exist back in 1995 or oherwise the web would still be designed like a print publication. Its a good thing that the os does not try to react to my monitors DPI or it would defeat many normal operations." CreationDate="2017-01-05T11:58:13.053" UserId="38" />
  <row Id="6211" PostId="4474" Score="0" Text="This is exactly the kind of detail I was hoping for when posting the question; thanks a lot! I've tried running the numbers again with the complex values, and I get results a lot more close to expected.&#xA;So what you're describing about electrostatic equilibrium is basically $\nabla B=0$?" CreationDate="2017-01-06T00:23:18.867" UserId="182" />
  <row Id="6212" PostId="4482" Score="1" Text="&quot;*You might want to convert your code to use that if possible*&quot; His code *is using that*. `glCreateTextures` is specific to the ARB_DSA function. He's using an odd mixture of the two, likely because he couldn't find a `glTextureImage2D` in the ARB version (since the ARB wants to discourage non-immutable textures)." CreationDate="2017-01-06T01:11:18.870" UserId="2654" />
  <row Id="6213" PostId="4482" Score="1" Text="@NicolBolas Aha, thanks, I hadn't caught that. I've updated the answer again." CreationDate="2017-01-06T03:12:37.617" UserId="48" />
  <row Id="6214" PostId="4483" Score="2" Text="Do you have a reference picture of the image you expect to get? It could be easier to first replicate an exiting image and compare your result." CreationDate="2017-01-06T05:47:28.517" UserId="182" />
  <row Id="6215" PostId="4483" Score="1" Text="@JulienGuertault thank you so much for the suggestion! That was brilliant and I wish I had thought of it earlier. That would've helped with a lot of hair pulling. See my edited post. T" CreationDate="2017-01-06T06:32:27.000" UserId="5764" />
  <row Id="6217" PostId="4484" Score="0" Text="Is your CPU code calling into GL on multiple threads? My first guess would be that there's nothing wrong with the shader and it's a cache invalidation issue caused by some non-thread-safe use of your intermediate textures." CreationDate="2017-01-06T14:54:14.840" UserId="2041" />
  <row Id="6218" PostId="4484" Score="0" Text="Thanks, and no, the CPU code is executed without multithreading. By the way: Now I tried the following: &#xA;&#xA;vec2 basePos	= vec2(mod(pos.x,256),mod(pos.y,192));&#xA;&#xA;where one instance is 256x192 and i still get this fault. It seems that there is a limitation on the number of reads you can perform on a texel?" CreationDate="2017-01-06T15:27:01.333" UserId="5399" />
  <row Id="6219" PostId="4484" Score="0" Text="Additionally i found out, that the position of the black boxes varies randomly, when using the mod(...) version" CreationDate="2017-01-06T15:34:40.530" UserId="5399" />
  <row Id="6221" PostId="4483" Score="1" Text="I've edited out the &quot;one more thing&quot; section now that you've posted it as a separate question. Notice that the separate question now appears in the &quot;Linked&quot; section on the right hand side of this page, because that question contains a link to this question." CreationDate="2017-01-07T12:20:26.467" UserId="231" />
  <row Id="6222" PostId="4411" Score="0" Text="Thank you joojaa - you seem to know what you are talking about. If I may ask, have you coded Laplacian Smoothing in C++? How can I reach out to you for technical help?" CreationDate="2017-01-07T18:02:13.237" UserId="5670" />
  <row Id="6223" PostId="4483" Score="0" Text="In case it's helpful, here's a good short read about path tracing basics with some simple c++ source code accompany it: http://blog.demofox.org/2016/09/21/path-tracing-getting-started-with-diffuse-and-emissive/" CreationDate="2017-01-07T18:30:39.693" UserId="56" />
  <row Id="6224" PostId="4491" Score="3" Text="Convert the TRS form back to a matrix and check if it's &quot;close enough&quot; to the original matrix?" CreationDate="2017-01-07T22:07:50.357" UserId="48" />
  <row Id="6225" PostId="4492" Score="0" Text="This sounds incorrect. Roughness doesn't cause diffuse, it only affects it due to geometric occlusion. A fine polished marble slate still has the same a strong diffuse, just with slightly different repartition of brightness depending on the view angle." CreationDate="2017-01-08T03:18:08.770" UserId="182" />
  <row Id="6226" PostId="4490" Score="2" Text="That's an interesting question, but a diagram would help to visualize situation." CreationDate="2017-01-08T11:05:50.293" UserId="4958" />
  <row Id="6227" PostId="4494" Score="1" Text="Thank you so, so much! Now everything makes a lot more sense to me. That paper is a wonderful resource too as my next step will be implementing glass after I have mirror done." CreationDate="2017-01-08T21:23:39.773" UserId="5764" />
  <row Id="6229" PostId="4492" Score="0" Text="Roughness might not be the right word for it, but the principle is there." CreationDate="2017-01-09T04:00:33.890" UserId="5816" />
  <row Id="6230" PostId="4496" Score="0" Text="when you use interger type with shift, same as float-type" CreationDate="2017-01-09T10:12:46.850" UserId="5823" />
  <row Id="6232" PostId="4434" Score="0" Text="Inspired by this topic, I wrote single header implementations of both Vulkan and DX12 renderers:&#xA;&#xA;renderers:https://github.com/chaoticbob/tinyrenderers" CreationDate="2017-01-09T11:23:24.617" UserId="5745" />
  <row Id="6233" PostId="4484" Score="1" Text="In your short version, the shaders seem fine. Garbage in a depth or stencil buffer can easily do this kind of thing (make sure depth testing is disabled). On the GL bug side, have you tried forcing a cpu renderer or a different vendor's GPU?" CreationDate="2017-01-09T15:56:37.817" UserId="2500" />
  <row Id="6234" PostId="4502" Score="0" Text="Thank you very much! You saved me some headache's. You are right. I completely overseen the vkUpdateDescriptorSets method. And in my setup, the pipelines won't change at all. So I can build them all upfront." CreationDate="2017-01-09T21:06:17.350" UserId="5825" />
  <row Id="6235" PostId="4503" Score="0" Text="Does it work better if the plane is facing the camera? I don't think you're giving it the correct Z. I don't know that particular unproject function but I suspect it should be Z from your depth buffer which is certainly not constant with the plane orientation shown." CreationDate="2017-01-10T01:45:11.073" UserId="5717" />
  <row Id="6236" PostId="4503" Score="0" Text="I get a similar result with the plane facing the camera.. I'm not sure how to get that z value from the depth buffer. But a value different from 1 just seem to make it worse." CreationDate="2017-01-10T05:52:19.800" UserId="3434" />
  <row Id="6237" PostId="4503" Score="0" Text="You are not passing the view matrix. In the StackOverflow link you posted, it states that you should be passing the ModelView matrix on the 'model' parameter. That makes sense, since the function cannot guess which view matrix you are using and it needs to know that to reverse all the transformations." CreationDate="2017-01-10T10:40:57.370" UserId="5519" />
  <row Id="6238" PostId="4503" Score="0" Text="Ok, yes good point. I think I assumed since the function takes in the window dimensions it had the parameters needed for the &quot;view&quot;. But this makes more sence, I didn't understand why the model matrix was a parameter. Will try this later, thanks." CreationDate="2017-01-10T10:44:55.610" UserId="3434" />
  <row Id="6239" PostId="4484" Score="0" Text="Thanks @Daniel, I disabled depth testing and nothing changed. And currently I don't have the possibility to use another GPU or do CPU rendering. I found a workaround for my problem. But now I run into another problem with two uniform variables: For the first 5 instances they are read correctly and the 6 is faulty. In the end I guess, that I made a fault in setting up the OpenGL stuff in my C++ code." CreationDate="2017-01-10T10:48:45.847" UserId="5399" />
  <row Id="6240" PostId="4507" Score="0" Text="The diagram suggests you are looking for the shortest distance between vertices of one polygon and another, which will not always be the same as the shortest distance between the polygons. For example, all of the connections in the diagram are longer than if you made them perpendicular to the edge to the right, rather than meeting a vertex on the right. Is your question specifically looking for vertex to vertex connections?" CreationDate="2017-01-10T11:52:10.643" UserId="231" />
  <row Id="6242" PostId="4510" Score="1" Text="What is it that you don't understand or want to do? This is hard to read because it's javascript without any sane abstractions... but at its core, it's only a cross product to compute the normal and addition to average the normals on shared vertices." CreationDate="2017-01-10T19:13:09.497" UserId="5717" />
  <row Id="6243" PostId="4510" Score="1" Text="Seconded. I don't see a question so have no idea what it is you are asking about." CreationDate="2017-01-10T19:32:03.820" UserId="56" />
  <row Id="6244" PostId="4510" Score="1" Text="Never mind the first line and that is Javascript.  I know what the code does. I know how to calculate normal in theory.  But in practice, in the code, I do not understand the index management mechanism. I miss the sense of what is done." CreationDate="2017-01-10T19:33:58.750" UserId="4981" />
  <row Id="6245" PostId="4503" Score="0" Text="I tried with the view matrix. See edits." CreationDate="2017-01-10T19:48:13.203" UserId="3434" />
  <row Id="6246" PostId="4507" Score="0" Text="Yes you are right, I'm searching for vertex-vertex connections." CreationDate="2017-01-10T22:04:48.187" UserId="5828" />
  <row Id="6247" PostId="4511" Score="2" Text="Using a paint program that has a &quot;nearest neighbor&quot; resize mode would give you a quick approximation, though it may not select the exact same rows/columns as your code. I think it would be pretty easy to prototype in Python using [pillow](https://python-pillow.org/) for image manipulation." CreationDate="2017-01-10T22:36:30.300" UserId="48" />
  <row Id="6248" PostId="4510" Score="2" Text="Ok, I can't write an answer anymore but as best I can tell, `vs` is a vertex array and `ns` a corresponding normal array. Both are raw float arrays so they are indexed with `i*3+0` (`i*3+x` in the code) for x component of vector number i, `i*3+1` for y, `i*3+2` for z. `ind` is an indirection array which contains vertex (and thus normal too) indices. It defines the triangle strips (the topology) within the arbitrary set of vertices. `v1` and `v2` are two edges of the triangle." CreationDate="2017-01-11T00:07:38.680" UserId="5717" />
  <row Id="6249" PostId="4511" Score="0" Text="Don't ntsc and pal have overscan areas - could you prepare your video with the decimated/filtered images in the overscan area? It would take 1 scan line of PAL to hold your entire image." CreationDate="2017-01-11T01:04:58.240" UserId="2500" />
  <row Id="6250" PostId="4510" Score="2" Text="I have tried to reword the question to make it clearer. Feel free to edit or revert changes if I changed the intended meaning." CreationDate="2017-01-11T06:51:39.927" UserId="182" />
  <row Id="6251" PostId="4511" Score="1" Text="*&quot;I cannot find any examples of how such downscaling will behave.&quot;*   Did you ever play old 3D games like Doom and see the hideous aliasing artefacts on walls/objects in the distance? That's what it will be like." CreationDate="2017-01-11T09:40:47.443" UserId="209" />
  <row Id="6252" PostId="4503" Score="0" Text="Does the plane really have a model matrix equal to identity (i.e. no scales, etc)? Also, glm::unproject returns a vector in object coordinates (model space). How are you using that returned vector when determining where it is in the world?" CreationDate="2017-01-11T12:23:32.477" UserId="5519" />
  <row Id="6254" PostId="4503" Score="0" Text="The plane is just to visualize the xy world plane (made up of lines).&#xA;I take the x and y value from the returned vector and create a translation matrix with 0.0 as z translation. No scale and rotation.&#xA;That gives me the model matrix which is sent to the shader with ViewM and ProjM." CreationDate="2017-01-11T12:27:00.147" UserId="3434" />
  <row Id="6255" PostId="4513" Score="0" Text="Are you specifically looking to perform the rendering 4 times, or just to render once and display the result in 4 different places?" CreationDate="2017-01-11T12:50:07.020" UserId="231" />
  <row Id="6256" PostId="4513" Score="0" Text="Perform the rendering 4 times. What I wrote is just a simplified example, for example I want the main camera to render a car at the full window, and another camera render the car from the side in a small square in one of the corners." CreationDate="2017-01-11T12:59:31.270" UserId="5609" />
  <row Id="6257" PostId="4513" Score="0" Text="If I understood this correctly so you need to split your window to for example 4 parts and in each part render other parts of the scene like [that](http://archive.gamedev.net/archive/reference/programming/features/3dmfc/viewerearly.jpg)?" CreationDate="2017-01-11T14:15:03.163" UserId="4958" />
  <row Id="6258" PostId="4513" Score="0" Text="Yes, but not strictly 4 parts. I want it to be flexible.&#xA;Flexible size and position of viewport rectangle." CreationDate="2017-01-11T14:38:04.867" UserId="5609" />
  <row Id="6259" PostId="4514" Score="0" Text="Thank you, but a little confused.&#xA;&quot;check with your projection matrix what values comes out at the near-plane&quot; I'm no quite sure how I would do that...&#xA;And in your first equation, isn't the first and second component of v the x and y in &quot;clip space&quot;?" CreationDate="2017-01-11T15:00:42.003" UserId="3434" />
  <row Id="6260" PostId="4514" Score="0" Text="If you transform a point $[0, 0, np]$ from camera space to NDC, what's the value you get for z-coordinate. And to your second question, no, they are in NDC. You have to multiply by w (i.e. near-plane distance in your case) to get them into clip-space" CreationDate="2017-01-11T15:14:31.830" UserId="1952" />
  <row Id="6261" PostId="4516" Score="0" Text="I made an edit. Yes voxelization can be also done as precomputation. But it seems to have nice performance also in real time as I implemented it for occlusion purporses in RSM based GI. I know that precomputation is crucial for performance but it is just a idea of thesis  - study of &quot;real timeness&quot;, I mean calculated each frame. A screen space GI sounds like oxymoron." CreationDate="2017-01-11T15:21:54.343" UserId="4958" />
  <row Id="6262" PostId="4516" Score="0" Text="Scene complexity - actually just one model probably sponza. Same complexity to get comparable results." CreationDate="2017-01-11T15:26:34.547" UserId="4958" />
  <row Id="6263" PostId="4516" Score="0" Text="Performance depends on the scene complexity you need to voxelize per frame, if you can call it real time. You could also call path tracing real time technique given simple enough scene and/or beefy enough hardware, and poor enough quality (:" CreationDate="2017-01-11T15:26:42.080" UserId="1952" />
  <row Id="6264" PostId="4515" Score="0" Text="So with glViewPort() I can define where I want to draw next, and with glBlendFunc I can define how the GPU should blend the overlapping areas (framebuffers) with each other. Am I correct?" CreationDate="2017-01-11T15:30:29.633" UserId="5609" />
  <row Id="6265" PostId="4515" Score="1" Text="Yes, you are free to parametrize that. As parameters of `viewport` are x, y of left bottom corner of chunk  and width, height of chunk. With blending you can experiment." CreationDate="2017-01-11T15:36:45.650" UserId="4958" />
  <row Id="6266" PostId="4515" Score="1" Text="why blending? how is it related to the question ?" CreationDate="2017-01-11T15:38:59.450" UserId="5364" />
  <row Id="6268" PostId="4515" Score="0" Text="@Tudvari you can do it in more modern way" CreationDate="2017-01-11T17:27:15.027" UserId="4958" />
  <row Id="6270" PostId="4515" Score="0" Text="and what is that way?" CreationDate="2017-01-11T17:42:00.107" UserId="5609" />
  <row Id="6271" PostId="4515" Score="0" Text="@Tudvari I edited my answer." CreationDate="2017-01-11T17:45:26.173" UserId="4958" />
  <row Id="6273" PostId="4508" Score="0" Text="Searching for the nearest point is an easy task. There is another problem. What if the nearest point cause an intersection with another polygon. Or what if the connection intersects the first polygon itself. Please look at the second image." CreationDate="2017-01-11T20:11:17.843" UserId="5828" />
  <row Id="6274" PostId="4519" Score="0" Text="This is interesting because TAA between frames is cheap but hard to implement.  I used temporal blending but on voxel grid to get smooth light movement which is quite easy to implement in this case." CreationDate="2017-01-11T21:17:15.123" UserId="4958" />
  <row Id="6275" PostId="4518" Score="4" Text="Hello! Can you explain (or better yet, show!) how it isn't working, and how you expect it to work?" CreationDate="2017-01-11T21:26:26.927" UserId="56" />
  <row Id="6276" PostId="4519" Score="0" Text="I know the usage of TAA on shadows in screen space or volumetric fog rendering but never thought it can be applied to GI in that way." CreationDate="2017-01-11T21:29:44.110" UserId="4958" />
  <row Id="6278" PostId="4515" Score="3" Text="You don't need blending or multiple framebuffers. Rendering will not write to any pixels outside the current glViewport rectangle, so you can just set up and draw each viewport in turn. BTW you can also use the [scissor test](https://www.khronos.org/opengl/wiki/Scissor_Test) to restrict clears to a certain rectangle, in case you want overlapping viewports." CreationDate="2017-01-12T01:02:54.347" UserId="48" />
  <row Id="6279" PostId="4519" Score="0" Text="Remember me if you publish a famous paper (;" CreationDate="2017-01-12T03:16:44.017" UserId="56" />
  <row Id="6282" PostId="4515" Score="2" Text="Blending has nothing to do with this and makes the answer more confusing. The setting up of the viewports is all that is needed" CreationDate="2017-01-12T15:34:46.420" UserId="3332" />
  <row Id="6283" PostId="4515" Score="0" Text="@NathanReed No problem feel free to edit  or add info about scissor test" CreationDate="2017-01-12T15:48:10.990" UserId="4958" />
  <row Id="6287" PostId="4411" Score="0" Text="Is there a way to contact members privately? I am trying to get some help from &quot;joojaa&quot;." CreationDate="2017-01-12T18:13:07.170" UserId="5670" />
  <row Id="6288" PostId="4527" Score="0" Text="&quot;immediately get the up, right(*), forward vectors&quot;: could you please specify which rows or columns represent the X, Y or Z axis, and why?" CreationDate="2017-01-12T18:38:44.687" UserId="4718" />
  <row Id="6289" PostId="4411" Score="0" Text="@Jay i have once done a Laplacian smoothing  algorithm but in 2D. But i dont know if im very good at helping you with your code." CreationDate="2017-01-12T19:11:45.493" UserId="38" />
  <row Id="6290" PostId="4527" Score="0" Text="let me know if that helps nbro!" CreationDate="2017-01-12T19:31:53.480" UserId="56" />
  <row Id="6291" PostId="4527" Score="0" Text="Yes, that was helpful, thanks! I've another doubt. You say &quot;you can look at it and immediately get the up, right(*), forward vectors&quot;, you're saying of the local coordinate system? According to the answer provided by @Oliver, this matrix defines a new coordinate system with respect to another, but this is often implicit. This kind of makes sense to me if I imagine at the end that this matrix represents a transformation. But what I'm confused about now is if the columns (or rows) of the transformation matrix actually define the basis of the new space... I should go back to review this." CreationDate="2017-01-12T19:47:49.870" UserId="4718" />
  <row Id="6292" PostId="4527" Score="0" Text="Yes. In my example, you can use the columns to get the basis vectors of the coordinate system it describes.  The columns are in the coordinate system of whatever the parent coordinate system is.  If there is no parent coordinate system, they are in global space.  Else, they are in the coordinate system of the parent. HTH but i understand that sometimes it takes time and multiple attempts to absorb new concepts :P" CreationDate="2017-01-12T19:54:42.953" UserId="56" />
  <row Id="6293" PostId="4527" Score="0" Text="When you say &quot;The columns are in the coordinate system of whatever the parent coordinate system is&quot; you mean &quot;the columns are in the vector space of whatever...&quot;?" CreationDate="2017-01-12T19:57:11.667" UserId="4718" />
  <row Id="6294" PostId="4529" Score="0" Text="Amazing answer, thanks a bunch (:" CreationDate="2017-01-12T20:37:46.727" UserId="56" />
  <row Id="6295" PostId="4527" Score="0" Text="You are over thinking it. Look at the identity matrix. The first column says that the x axis of the transform is 1,0,0.  If you had some rotation / scaling matrix, it may give you a different answer like 0,2,0, but that is still the x axis of the transform that the matrix represents.  Matrices can be parented off of each other, but that is probably too confusing to visualize as you are just trying to learn things.  Just imagine that if there was a matrix hierarchy, that all the matrices are multiplied together into one matrix M then you can ignore hierarchies. So, col 1 = x axis. Easy stuff (:" CreationDate="2017-01-12T21:18:29.960" UserId="56" />
  <row Id="6296" PostId="4527" Score="0" Text="I'm not really overthinking it, in my opinion. I'm just trying to put together things from linear algebra and the terminology used in computer graphics. In computer graphics, people talk about spaces and coordinate systems almost interchangeably, but that's clearly not correct from a linear algebra's point of view. The &quot;parenting&quot; thing is another thing I've never heard in linear algebra and I guess it just another vague term." CreationDate="2017-01-12T21:29:13.383" UserId="4718" />
  <row Id="6297" PostId="4528" Score="0" Text="Just thinking out loud, it may be possible to use some equations for [spherical lenses](http://www.physicsinsights.org/simple_optics_spherical_lenses-1.html) and treat it like calculating defocus blur. The caustic brightness would go like $1/r^2$ of the circle of confusion, or something like that." CreationDate="2017-01-12T22:04:24.320" UserId="48" />
  <row Id="6298" PostId="4527" Score="0" Text="Fair enough! If you want more rigorous details, I'll leave that to someone else. I'm not real clear on vector spaces, but am clear on matrices as coordinate systems. I'm more a game programmer than a mathematician. True, you can be both, but in my case, I'm stronger in one vs the other hehe." CreationDate="2017-01-12T22:39:17.257" UserId="56" />
  <row Id="6299" PostId="4528" Score="0" Text="That sounds really promising.  I like how that seems like it could be exact for spheres, whereas the answer about using surface normal and refractive index would be decent approximations for general shapes." CreationDate="2017-01-12T22:40:35.067" UserId="56" />
  <row Id="6300" PostId="4493" Score="0" Text="Thank you. This is the kind of thing I was looking for. There is an edge case I have found which is not detected - a negative scale in the original transform, but from what I've read I'm not sure its possible to detect this." CreationDate="2017-01-13T00:17:14.220" UserId="4509" />
  <row Id="6301" PostId="4493" Score="1" Text="@sebf You can check the sign of the determinant of the 3×3 submatrix. That will detect if there's a handedness flip, which is caused by a negative scale along either 1 or 3 axes. However, if there was a negative scale along 2 axes originally, that's equivalent to a 180° rotation, so it's not possible in that case to tell the difference just from the final transform matrix." CreationDate="2017-01-13T00:35:30.210" UserId="48" />
  <row Id="6303" PostId="4527" Score="0" Text="@nbro that is a good separate question. But basically it goes like this linear algebra deals with one transform from one space to another. If you have 3 spaces (for convenience because tranforms are easy, other reasoning not so easy), A, B and C. Then you would need to know which transform goes from which space to what space. Now if you know transform A-B and A-C  you also know all other combinations. So we store those transforms for later use. This is herarchially as if B and C belomg under A and indeed if you had a space W and a transform W-A then changed W-A then B and C would move with A." CreationDate="2017-01-13T06:08:37.293" UserId="38" />
  <row Id="6304" PostId="4523" Score="0" Text="It does not matter if its row oriented column oriented or arbitrary otder or inverse its still the same case. This is not because of math but what we are trying to acieve. Math does not deal with the why as much as you like to believe, math deals with properties of abstract things, the interpretation has to come from oitside of math. But its better you smartly ask the space question you originally tried ton" CreationDate="2017-01-13T07:33:01.333" UserId="38" />
  <row Id="6305" PostId="4527" Score="0" Text="@joojaa I didn't under this last part &quot;then changed W-A then B and C would move with A. &quot;" CreationDate="2017-01-13T11:40:50.577" UserId="4718" />
  <row Id="6306" PostId="4527" Score="0" Text="@nbro they are defined in relation to A therefore if that relation does not change they will transform as if a fixed part of A" CreationDate="2017-01-13T11:49:46.717" UserId="38" />
  <row Id="6307" PostId="4527" Score="0" Text="@joojaa I'm going to ask a separate question." CreationDate="2017-01-13T12:39:50.313" UserId="4718" />
  <row Id="6308" PostId="4532" Score="0" Text="&quot;*What is it meant by matrix hierarchy?*&quot; It is a [hierarchy](http://www.dictionary.com/browse/hierarchy) of matrices." CreationDate="2017-01-13T14:01:35.140" UserId="2654" />
  <row Id="6309" PostId="4532" Score="1" Text="You should read [this scene graph page](https://en.wikipedia.org/wiki/Scene_graph). The scene graph is where the parent and hierarchy concepts come from." CreationDate="2017-01-13T14:07:50.670" UserId="5717" />
  <row Id="6310" PostId="4532" Score="0" Text="@NicolBolas I know what's a hierarchy, and I know that it starts to make sense to talk about matrices hierarchies, if there are parent-child relationships between matrices." CreationDate="2017-01-13T14:09:11.563" UserId="4718" />
  <row Id="6311" PostId="4531" Score="0" Text="&quot;*This index can be used in the Vertex Shader to set the Viewport to which the scene is rendered.*&quot; No, it cannot. Well, not without the ARB_shader_viewport_layer_array extension, or the AMD equivalents. None of those are standard in GL 4.5. You are perhaps thinking of Geometry Shaders." CreationDate="2017-01-13T14:38:50.593" UserId="2654" />
  <row Id="6312" PostId="4532" Score="1" Text="BTW, if you dont get a good answer here nbro, mathematics stack exchange may be able to help more!  http://math.stackexchange.com/" CreationDate="2017-01-13T16:59:38.267" UserId="56" />
  <row Id="6313" PostId="4533" Score="1" Text="I now established that I will use stm32f767 which – not looked myself yet – has DSP commands. I might do some computations after all, clock is 216 MHz and DMA is said to be flexible. I will read ADV7280 output clocked at 54 MHz, though (interlace -&gt; progressive enabled)." CreationDate="2017-01-13T17:45:04.403" UserId="5832" />
  <row Id="6314" PostId="4535" Score="2" Text="It took me a second to understand what you meant by a rotation gizmo. I added an image of one in case it's helpful (i think you can't add images until you have more reputation sadly!).  If you have a more appropriate image in mind, feel free to let me know and i can change the image." CreationDate="2017-01-13T18:54:45.193" UserId="56" />
  <row Id="6315" PostId="4535" Score="0" Text="Thanks for adding the image Alan. You are right, I think it will make things clearer." CreationDate="2017-01-13T18:57:27.513" UserId="5841" />
  <row Id="6316" PostId="4532" Score="0" Text="@AlanWolfe Yes, thanks for trying to help!" CreationDate="2017-01-13T19:30:28.490" UserId="4718" />
  <row Id="6317" PostId="4540" Score="0" Text="UV-mapping the surface?" CreationDate="2017-01-14T02:57:15.713" UserId="48" />
  <row Id="6318" PostId="4540" Score="0" Text="Hrm yeah I guess that's true if you have   a unique range of u and v per face!" CreationDate="2017-01-14T03:13:50.313" UserId="56" />
  <row Id="6319" PostId="4536" Score="0" Text="When you say simulation, do you mean simulating the physics of things like hair or rendering things more accurately?" CreationDate="2017-01-14T05:41:19.520" UserId="5256" />
  <row Id="6320" PostId="4536" Score="0" Text="I mean things like simulating the motion of hair and water and things yeah, not the light physics." CreationDate="2017-01-14T05:43:28.783" UserId="56" />
  <row Id="6321" PostId="4541" Score="0" Text="Anything you deem necessary" CreationDate="2017-01-14T10:59:07.507" UserId="38" />
  <row Id="6322" PostId="4540" Score="0" Text="2 time 2 angle pairs? See the third rotation about the own axis is not needed to represent a arrow in any direction." CreationDate="2017-01-14T11:00:48.843" UserId="38" />
  <row Id="6323" PostId="296" Score="2" Text="&gt; &quot;If you are quite good at writing programs in e.g. Java, you won't have that much trouble learning C++&quot; — this is not true. *Everybody* will have colossal amounts of trouble learning *and using* C++. Comparing it to Java or any other safe language is completely inappropriate. For other languages you listed (Objective-C, Swift, or some other object-oriented language) that statement is mostly true, though." CreationDate="2017-01-14T13:19:32.250" UserId="5848" />
  <row Id="6324" PostId="31" Score="0" Text="keep in mind that `mozjpeg` compressor has a special trick to produce less distortion on these kinds of images. (and it's more efficient in general than usual jpeg writers too.) did you try it?" CreationDate="2017-01-14T13:43:18.547" UserId="5848" />
  <row Id="6325" PostId="4542" Score="0" Text="So, essentially, the screen is the real $2D$ screen (or image on the screen) whereas $X$ would be half of the opening width of the virtual &quot;panel&quot; or &quot;board&quot; created by the opening angle of the camera in the $3D$ world (coordinate system)? So, clearly, this virtual &quot;panel&quot; or &quot;board&quot; maybe not cover the whole screen, right?" CreationDate="2017-01-14T13:48:21.087" UserId="4718" />
  <row Id="6326" PostId="4540" Score="0" Text="That's what I was thinking too with the spherical coordinates. You don't need roll! But getting the angles takes some trig.  However, this lets you do a ray intersection against a generic object as a table lookup so maybe the trig ops aren't a bad trade off.  Nathan Reed's idea doesn't have that though which is interesting." CreationDate="2017-01-14T14:30:11.073" UserId="56" />
  <row Id="6327" PostId="4542" Score="0" Text="@nbro With the maths here, the &quot;board&quot; has to cover the whole screen, but in general, yes, they don't have to be the same. It's more usual to call this the *image plane*: the plane onto which the 3D scene is projected to create the image." CreationDate="2017-01-14T15:27:38.857" UserId="2041" />
  <row Id="6328" PostId="4542" Score="0" Text="The image plane can be thought as a plane in the $3D$ world, right?" CreationDate="2017-01-14T15:29:14.983" UserId="4718" />
  <row Id="6329" PostId="4542" Score="0" Text="@nbro Yes, that's right." CreationDate="2017-01-14T15:31:23.497" UserId="2041" />
  <row Id="6330" PostId="4535" Score="0" Text="Fwiw I think you are right. Using color based selection isn't going to work for this." CreationDate="2017-01-14T15:34:05.063" UserId="56" />
  <row Id="6331" PostId="4542" Score="0" Text="Sorry for asking too many question, but I would like to make this thing clear, and books or articles around do not make it clear, IMHO. How would the math work if for example the _image plane_ was smaller than the screen? Another question. Is the _image plane_ in the &quot;world space&quot; or in the &quot;camera space&quot; (if this question even makes sense)? I mean, the _image plane_ coordinates are usually represented using a triple of coordinates, so I suppose they are represented in the world coordinate system..." CreationDate="2017-01-14T15:34:14.480" UserId="4718" />
  <row Id="6332" PostId="4542" Score="0" Text="I've a slide where we want to find the $3D$ coordinates corresponding to the $2D$ coordinates (or two indices) of a pixel... so it seems to me that the _image plane_ can be thought really as the $3D$ counter-part of the $2D$ screen..." CreationDate="2017-01-14T15:36:47.693" UserId="4718" />
  <row Id="6333" PostId="4542" Score="0" Text="This Wiki article https://en.wikipedia.org/wiki/Image_plane seems to have helped a little bit. Apparently the _image plane_ is also called the _screen space_, so I guess that its coordinates are defined with respect to the local coordinate system of the screen space (and not with respect to the world coordinate system). I would still appreciate if you could answer my questions." CreationDate="2017-01-14T15:46:21.087" UserId="4718" />
  <row Id="6334" PostId="4544" Score="1" Text="From a [dictionary](http://www.dictionary.com/browse/monitor): monitor - _the screen component of a computer, especially a free-standing screen_. So, in English these words have the same meaning? I don't know. I my native language monitor is generally a piece of hardware and screen is a flat part of the monitor onto which viewport picture is projected." CreationDate="2017-01-14T17:40:54.250" UserId="4958" />
  <row Id="6335" PostId="4545" Score="0" Text="So, do you think that if I talk about screen (referring to the real screen of a device) and screen space (or image plane, which is the infinite abstract geometric plane that identifies) in the same sentence, there could be ambiguities. I'm basically writing something explaining that the screen space is somehow mapped to the screen, but after reading your answer, it seems a little bit ambiguous." CreationDate="2017-01-14T18:00:53.663" UserId="4718" />
  <row Id="6336" PostId="4545" Score="0" Text="By the way, just to be even sure. The screen space is a $3D$ object with respect to the camera, right? I know in computer graphics people talk about the camera space (which I know it's a synonym for eye or view space), but now it's not clear the difference between camera space and screen space. I thought that the screen space was, as you said, an abstract plane on the $3D$ world, but with respect to the camera..." CreationDate="2017-01-14T18:04:27.887" UserId="4718" />
  <row Id="6337" PostId="4536" Score="1" Text="... although light physics is sometimes a part of the more advanced simulation. For instance, in the movie interstellar, they rendered the black hole based on what we know of black hole physics and actually ended up publishing a research paper about it!" CreationDate="2017-01-14T18:22:43.947" UserId="56" />
  <row Id="6338" PostId="4546" Score="0" Text="R11F_G11F_B10F is an RGB format." CreationDate="2017-01-15T01:44:27.053" UserId="2654" />
  <row Id="6339" PostId="4546" Score="0" Text="Keep in mind that the docs don't list all possible formats. There are often vendor-specific formats, and formats that came after the docs were written." CreationDate="2017-01-15T03:04:50.213" UserId="3003" />
  <row Id="6340" PostId="4545" Score="0" Text="&quot;However, a built-in display such as the one in a laptop, tablet, or phone is never called a monitor.&quot; That is not true. The display on a laptop, tablet or phone is sometimes referred to as a monitor in casual parlance in US English, at least." CreationDate="2017-01-15T03:06:47.597" UserId="3003" />
  <row Id="6341" PostId="4511" Score="0" Text="Note that PAL and NTSC are a 4:3 aspect ratio (despite NTSC seeming like 3:2  - it has non-square pixels at 720x480). 32x16 is 2:1, so you will either be stretching the video, or pillar boxing it. In my opinion, pillar box is far better, but given the abstractness of your description, maybe you'd prefer to stretch it?" CreationDate="2017-01-15T03:34:38.620" UserId="3003" />
  <row Id="6343" PostId="4552" Score="0" Text="Offcourse it is possible to unmult this  apha if one so wishes. Im a bit unsure about whether this answers the question though, incredibly vague as it is." CreationDate="2017-01-15T07:56:33.693" UserId="38" />
  <row Id="6344" PostId="4545" Score="0" Text="@nbro screen space and camera space are usually different things." CreationDate="2017-01-15T08:03:24.053" UserId="38" />
  <row Id="6345" PostId="4552" Score="0" Text="@joojaa It isn't quite as straightforward as you have to be certain to preserve unpremult values at RGB 0.0. The only time this is required is on colour operations, otherwise associated alpha is always the correct format." CreationDate="2017-01-15T08:11:15.523" UserId="5556" />
  <row Id="6346" PostId="4553" Score="0" Text="I did check out the presentation, it said 'omission of effects results in images a little bit too dark' I'm a little curious as to what that really means." CreationDate="2017-01-15T09:17:54.053" UserId="5256" />
  <row Id="6347" PostId="4552" Score="0" Text="Sure, but alpha in images is a problematic concept in many situations anyway." CreationDate="2017-01-15T09:47:06.293" UserId="38" />
  <row Id="6348" PostId="4546" Score="0" Text="@NicolBolas Yeah because it has 32 bits overall, now I see it." CreationDate="2017-01-15T10:19:44.533" UserId="4958" />
  <row Id="6349" PostId="4553" Score="0" Text="It means several things: One is that path tracing cannot sample all possible light/scene interactions (see section 8.3 of Eric Veach's thesis). The other is that for production purposes, scenes are often rendered without caustic paths and a limited number of bounces (omitting those paths can significantly reduce noise). Terminating those paths removes light from the final result, making it darker than it would be if all light transport paths were calculated." CreationDate="2017-01-15T11:29:40.427" UserId="4546" />
  <row Id="6350" PostId="4545" Score="0" Text="@joojaa But objects in both spaces are represented with $3$ coordinates, even though in the case of screen space usually $z$ is fixed, e.g., to $1$ (or $-1$), am I right?" CreationDate="2017-01-15T11:46:37.853" UserId="4718" />
  <row Id="6352" PostId="4557" Score="3" Text="Discouraging people from experimenting in graphics and putting it on par with shipping hand crafted crypto algorithms is ridiculous." CreationDate="2017-01-15T19:34:55.247" UserId="56" />
  <row Id="6353" PostId="4557" Score="0" Text="@AlanWolfe Given that there are quite a few extremely brilliant PhD types out there that have already spent countless hours and effort solving the problems in the original question, I find your context of ridiculous ridiculous. Not to discourage one from filling their boots and attempting to reinvent the wheel." CreationDate="2017-01-15T20:51:54.340" UserId="5556" />
  <row Id="6355" PostId="4557" Score="0" Text="You should hear the simple hacks recommended by active graphics researchers. Such as &quot;dot product RGB, it really does work amazingly well&quot; from Peter Shirley." CreationDate="2017-01-15T20:56:23.097" UserId="56" />
  <row Id="6356" PostId="4557" Score="0" Text="@AlanWolfe I am sure there are some. Sadly the use of IPT and JCh for example, are almost trivial and yield remarkably good results. I edited the post and comment as I realized it read as pure snark. It wasn't intended as such. Dot product is also colour space dependent sadly, and it too fails." CreationDate="2017-01-15T20:58:05.607" UserId="5556" />
  <row Id="6357" PostId="4557" Score="0" Text="Apologies if my response was also snark. Thanks for the info Troy!" CreationDate="2017-01-15T21:02:01.033" UserId="56" />
  <row Id="6358" PostId="4557" Score="0" Text="@AlanWolfe I would be interested in the context of what you are trying to solve. There is a tremendous amount of top shelf research going on at the moment, specifically relating to game programming and motion picture work. I might be able to point to a more specific bit of code." CreationDate="2017-01-15T21:03:17.003" UserId="5556" />
  <row Id="6359" PostId="4557" Score="2" Text="I've moved on but there's application of wave function collapse for procedural image and content creation. It works in part by exact matching pixels so works best with pixel art. I was looking at seeing it be able to do softer matching for use with more realistic images, or for less strict procedural content rules. Check out this link for the basic thing: https://github.com/mxgmn/WaveFunctionCollapse" CreationDate="2017-01-15T23:04:42.363" UserId="56" />
  <row Id="6360" PostId="4557" Score="0" Text="@AlanWolfe Wow. I have already seen your work. It is pretty incredible. Would be remarkable to see you extend it." CreationDate="2017-01-15T23:25:11.960" UserId="5556" />
  <row Id="6361" PostId="4557" Score="2" Text="It's not my work but I was trying to extend it. I totally agree, it's cool stuff!  Off topic but here's my unrelated work hehe. http://blog.demofox.org/2016/02/22/gpu-texture-sampler-bezier-curve-evaluation/" CreationDate="2017-01-15T23:26:58.830" UserId="56" />
  <row Id="6362" PostId="4531" Score="0" Text="@Nicol, Thanks for this hint! I forgot to mention, that you have to include the extension. I will edit my answer." CreationDate="2017-01-16T07:26:44.073" UserId="5399" />
  <row Id="6363" PostId="4556" Score="0" Text="I'm voting to close this question as off-topic because it doesn't seem to be about computer graphics, but rather video/image/signal processing." CreationDate="2017-01-16T14:04:26.587" UserId="182" />
  <row Id="6364" PostId="4558" Score="0" Text="&quot;*Swapping buffers doesn't really copy anything these days.*&quot; I wouldn't be too sure. Lots of implementations swap via copy, especially if your application is windowed." CreationDate="2017-01-16T16:13:36.873" UserId="2654" />
  <row Id="6366" PostId="4558" Score="0" Text="@NicolBolas Well OK, if you're going into a compositor, there will probably be a copy *somewhere* along the line. I was more thinking of the case where the output is going into the display controller directly. My point is that OP was thinking that each `eglSwapBuffers` or equivalent is pausing while it copies a bunch of data, but the problem is something completely different." CreationDate="2017-01-16T16:43:50.140" UserId="2041" />
  <row Id="6367" PostId="4556" Score="2" Text="I think it's an interesting question. Essentially temporal aliasing problem which is too often ignored by CG people. There are a huge number of algorithms out there which look cool on still frames but break down when you start animating input parameters." CreationDate="2017-01-16T19:20:57.480" UserId="5717" />
  <row Id="6368" PostId="4556" Score="1" Text="I meant quantization, not aliasing, but I still think it's worth discussing why this fails to be smooth and what can be done about it besides the obvious &quot;get a toolkit with the proper floating point zoom support&quot;." CreationDate="2017-01-16T19:30:47.860" UserId="5717" />
  <row Id="6369" PostId="4558" Score="0" Text="You only explained what I already knew.&#xA;My question is, Why does my screen slow when I do not swap the buffers?" CreationDate="2017-01-16T19:48:21.557" UserId="5852" />
  <row Id="6370" PostId="4558" Score="0" Text="Lol I tried my code again and now it doesn't slow down the screen. Anyway, thanks for all." CreationDate="2017-01-16T20:16:31.603" UserId="5852" />
  <row Id="6371" PostId="4563" Score="1" Text="I have not more than 15 score points so I cannot vote up your awesome answer. Thanks!!!" CreationDate="2017-01-17T00:26:54.780" UserId="5852" />
  <row Id="6372" PostId="4563" Score="1" Text="No worries. Glad I could help." CreationDate="2017-01-17T00:27:52.163" UserId="3003" />
  <row Id="6373" PostId="4564" Score="1" Text="The MIP map level is the (base 2) logarithm of the scaling of the texture and so the top level map will also get all the &quot;negative&quot; values, i.e., when magnification of the texture is occurring, so it's not that &quot;unfair&quot; :-)&#xA;&#xA;Further, the aim of MIP mapping is primarily to eliminate aliasing though at the risk of over filtering. Choosing ceil thus aims for &quot;no aliasing&quot; but, having said that, you should be able to set a LOD bias to shift this to suit your taste." CreationDate="2017-01-17T12:00:42.780" UserId="209" />
  <row Id="6374" PostId="4321" Score="0" Text="There is a whole Python package for color science that includes some transformations: http://colour-science.org/ ." CreationDate="2017-01-17T14:53:41.950" UserId="5870" />
  <row Id="6375" PostId="4556" Score="0" Text="We have a [tag for image-processing](http://computergraphics.stackexchange.com/questions/tagged/image-processing), and there is support for the place of image processing in our scope on Meta [here](http://meta.computergraphics.stackexchange.com/questions/38/is-this-site-only-about-3d-topics) and [here](http://meta.computergraphics.stackexchange.com/questions/9/how-do-we-draw-the-line-between-questions-that-are-appropriate-for-computer-grap)." CreationDate="2017-01-17T15:06:35.160" UserId="231" />
  <row Id="6376" PostId="4565" Score="0" Text="What have you tried so far? If you describe what you already know how to do we'll have more chance of giving a relevant answer. For example, are you more stuck on how to solve this mathematically, or how to express it in JavaScript?" CreationDate="2017-01-17T16:07:17.027" UserId="231" />
  <row Id="6377" PostId="4565" Score="1" Text="Oh, thanks. I'm a competent JavaScript programmer. It's the math. I'll put what I've tried in the question so it's easier to read." CreationDate="2017-01-17T16:11:06.143" UserId="5871" />
  <row Id="6378" PostId="4565" Score="2" Text="What you're after is [vector projection](https://en.wikipedia.org/wiki/Vector_projection). Do you understand basic vector math? If not, I'm sure someone can break it down to the the basic operations." CreationDate="2017-01-17T16:23:03.913" UserId="5717" />
  <row Id="6380" PostId="4569" Score="1" Text="Just checking: do you have updated graphics drivers installed? And you're not using remote desktop or anything? Do other graphics applications work (e.g. 3D games) on the machine?" CreationDate="2017-01-17T21:47:08.967" UserId="48" />
  <row Id="6381" PostId="4565" Score="0" Text="I don't understand vector math, but it looks like the code I'm using actually works. I think the problem I'm having is elsewhere." CreationDate="2017-01-17T22:03:15.633" UserId="5871" />
  <row Id="6382" PostId="4569" Score="0" Text="I very recently updated the NVIDIA drivers (version 376.62). I'm not using a remote desktop, and it's a work machine so I don't know if other programs work. The program I'm trying to run does display properly at first, but then crashes." CreationDate="2017-01-17T22:27:38.867" UserId="5874" />
  <row Id="6383" PostId="4565" Score="0" Text="Your $y_p$ and $x_p$ look reversed from what I would expect.  Is that intentional? If not, that might be the source of whatever problems you are having." CreationDate="2017-01-17T23:51:20.963" UserId="56" />
  <row Id="6384" PostId="4565" Score="0" Text="Math.atan2 takes (y,x) as arguments: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/atan2/. Is that what you mean?" CreationDate="2017-01-18T01:37:35.317" UserId="5871" />
  <row Id="6385" PostId="4295" Score="0" Text="@MBReynolds In a mathematical sense, normals are as _vectors_ as points or directions. The problem here is that the transformations that we apply to the points of a surface to transform them do not apply to the normals." CreationDate="2017-01-18T13:05:23.133" UserId="4718" />
  <row Id="6386" PostId="4295" Score="2" Text="surface normals are bivectors, not vectors.  We can find a normal by the cross product of two vectors, the result is a bivector.  SEE Per Vogensen's: https://gist.github.com/pervognsen/c6b1d19754c2e8a38b10886b63d7bf2d" CreationDate="2017-01-18T13:55:49.310" UserId="2831" />
  <row Id="6392" PostId="4559" Score="0" Text="THis answer really needs a picture" CreationDate="2017-01-19T07:49:33.577" UserId="38" />
  <row Id="6393" PostId="4573" Score="3" Text="Yes you should use the BRDF when you intersect with something inside the  glass that isn't the glass. Are you going to use the same light direction or are you going to refract that too ? (as objects inside the glass are light by a refracted lightsource" CreationDate="2017-01-19T08:00:28.267" UserId="3073" />
  <row Id="6395" PostId="4573" Score="0" Text="I can undelete it, i skim read your question and didn't realise you already understood how to cast rays through refractive materials, so my answer is mostly redundant." CreationDate="2017-01-19T08:04:17.300" UserId="3073" />
  <row Id="6397" PostId="4572" Score="1" Text="Cook-torrance is popular these days, it takes micro geometry into account so attenuates differently to lambert." CreationDate="2017-01-19T08:48:24.647" UserId="3073" />
  <row Id="6401" PostId="4566" Score="0" Text="a quaternion is built as `quat(cos(angle/2), axis*sin(angle/2))` and you know that the dot product us equal to cos(angle) times the product of the lengths and the cross product is the axis times sin(angle) times the product of the lengths. Normalize it and you can easily get the quaternion of the same rotation but with double the angle. To get the proper quaternion you can do `slerp(unit, quat, 0.5)` which after inlining ends up as `normalize(unit+quat);`" CreationDate="2017-01-19T10:18:51.607" UserId="137" />
  <row Id="6405" PostId="4572" Score="1" Text="Disney's BRDF is pretty much the top of the line as far as diffuse BRDFs go, yes." CreationDate="2017-01-19T18:30:25.120" UserId="3470" />
  <row Id="6406" PostId="4508" Score="0" Text="Either I must hear overlooked the second image or you added after I typed my response. Either way, in 3D this is typically solved with Ray tracing and an appropriate spatial subdivision, and I would expect the same to work in 2D." CreationDate="2017-01-11T20:45:23.587" UserId="4546" />
  <row Id="6407" PostId="4576" Score="0" Text="To make this into an answer, could you explain the algorithm?" CreationDate="2017-01-19T19:00:20.697" UserId="231" />
  <row Id="6408" PostId="4181" Score="0" Text="I would have another question or doubt. You talk about a matrix $M$ that transforms an object to world space. This matrix encodes the operations I've described in my question, but in the same question I'm not talking about transforming object to world spaces or world to object spaces: I'm simply talking about transformations (encoded as matrix multiplications) applied to an object. What's confusing to me is that you're talking about these transformations from spaces to spaces, whereas in my question I'm simply applying transformations to an object..." CreationDate="2017-01-19T19:25:43.210" UserId="4718" />
  <row Id="6409" PostId="4182" Score="0" Text="Could you please point me to the right sources to read more about this parent-child relationships? I would like to understand this from the mathematical view point, if it makes sense, which I think it doesn't, because you talk about spaces in computer graphics in a different way than people in mathematics talk for example about vector spaces, that's I find this notation and terminology in computer graphics very ambiguous." CreationDate="2017-01-19T19:32:33.107" UserId="4718" />
  <row Id="6410" PostId="4182" Score="0" Text="@nbro like i said it has nothing to do with mathematics. It has to do with practical use of modeling. Its only purpose is to make all vector spaces reachable by the computert so that you the user dont have to book keep all transforms separately. Why you would want the spaces in the first place is just a question of practicality. Its a tool to make the modeling easy." CreationDate="2017-01-19T19:38:05.387" UserId="38" />
  <row Id="6411" PostId="4182" Score="0" Text="But then how can I imagine a space in computer graphics? Can I imagine it as being a coordinate system defined with respect to &quot;something&quot;?" CreationDate="2017-01-19T19:40:57.540" UserId="4718" />
  <row Id="6413" PostId="4182" Score="0" Text="@nbro its a relationship to the the previous entry in your model tree (not a binary tree just a tree like a filesystem or a xml file), where the root is the world that contains everything." CreationDate="2017-01-19T19:43:30.130" UserId="38" />
  <row Id="6414" PostId="4182" Score="0" Text="By relationship I assume you're talking about transformations (matrix multiplications in practice), since at the very end this is exactly what happens. Are you saying that a space would be a transformation with respect to the previous &quot;what&quot;? Sorry for being so pedantic, but I just would like to make this thing clear, and after having read a lot of resources, no one has been able to really make this thing clear, but maybe I just didn't read the appropriate ones." CreationDate="2017-01-19T19:47:22.153" UserId="4718" />
  <row Id="6415" PostId="4182" Score="0" Text="@nbro previous transformation in the chain of events, previous coo9rdinate system. You can not tell form mathematics what this thing is because its your job to describe this thing, mathematics does not know." CreationDate="2017-01-19T19:57:00.160" UserId="38" />
  <row Id="6416" PostId="4182" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/52088/discussion-between-joojaa-and-nbro)." CreationDate="2017-01-19T19:58:11.470" UserId="38" />
  <row Id="6419" PostId="4578" Score="0" Text="It might help to describe the reason that you need this. There are [many different centres of a triangle in 2D](https://en.wikipedia.org/wiki/Triangle_center) so I imagine there may be many approaches for a sphere surface triangle. If we know what your underlying purpose is we will have a better idea of whether &quot;average&quot; is the best approach." CreationDate="2017-01-20T01:29:34.900" UserId="231" />
  <row Id="6423" PostId="4579" Score="2" Text="@trichoplax sorry about that I edited this myself a bit to fix the mistakes/typos." CreationDate="2017-01-20T08:28:10.123" UserId="1608" />
  <row Id="6424" PostId="4532" Score="0" Text="Please check https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/" CreationDate="2017-01-20T09:30:26.850" UserId="1608" />
  <row Id="6425" PostId="4523" Score="0" Text="Please check https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry" CreationDate="2017-01-20T09:30:55.617" UserId="1608" />
  <row Id="6427" PostId="4579" Score="0" Text="Great Answer! The Diagram and the additional explanation was great, I think since you're an author of the scratchapixel tutorials you should add this so people with the same question aren't in doubt. Also, will you guys do a photon mapping lesson? That would be really awesome!" CreationDate="2017-01-20T10:32:19.057" UserId="5256" />
  <row Id="6428" PostId="4523" Score="0" Text="@user18490 Yes, in this case, this question arose after reading a lesson from scratchpixel. In my opinion it's not understandable enough the explanation there." CreationDate="2017-01-20T10:32:33.570" UserId="4718" />
  <row Id="6429" PostId="4532" Score="0" Text="@user18490 I've read that lesson about geometry. There's nothing which talks about the concept I'm describing in this question, from what I remember." CreationDate="2017-01-20T10:38:04.227" UserId="4718" />
  <row Id="6430" PostId="4466" Score="1" Text="What almost satisfy my need is this paper: Accurate and Fast Proximity Queries Between Polyhedra Using Convex Surface Decomposition.." CreationDate="2017-01-20T11:01:01.880" UserId="2687" />
  <row Id="6431" PostId="4579" Score="1" Text="@Arjan. I didn't write this lesson but I will pass on your comment. Also yes we have a plan to write a lesson on Photon Mapping but only in Volume 3, we need to finish Volume 2 this year... If you wish to support this initiative please promote it around you))" CreationDate="2017-01-20T11:16:05.167" UserId="1608" />
  <row Id="6432" PostId="4532" Score="0" Text="@nbro that's actually true, though it's pretty clear that if you ask the question you don't understand what matrices are? so maybe you should start from there? But you are right about this website, maybe you should contact them and ask them to add this to the lesson?" CreationDate="2017-01-20T11:32:19.337" UserId="1608" />
  <row Id="6434" PostId="4290" Score="1" Text="In complement to all the other answers and because other people already answered this question in length elsewhere you can check:&#xA;&#xA;https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/transforming-points-and-vectors" CreationDate="2017-01-20T11:49:26.210" UserId="1608" />
  <row Id="6435" PostId="4579" Score="0" Text="I'd like to contribute to this project in a more meaningful way, are there any ways to contact your team? I messaged the scratchapixel facebook page but I didn't get a reply." CreationDate="2017-01-20T12:02:57.173" UserId="5256" />
  <row Id="6436" PostId="4583" Score="0" Text="You probably also need to know about matrix inverses ;)" CreationDate="2017-01-20T12:08:07.667" UserId="38" />
  <row Id="6437" PostId="4532" Score="1" Text="@user18490 no nbro understands what matrices are he just didnt undersatand why you would have hierarchies. He is looking at it form very mathematical perspective. There is no such concept in nonapplied mathematics. Having chains of transforms makes only sense for modeling reasons." CreationDate="2017-01-20T12:09:59.790" UserId="38" />
  <row Id="6439" PostId="4584" Score="0" Text="Are you looking for pairs of (point, face) that are separated by less than x, or a list of all points and faces that are less than x from some point/face on the other patch?" CreationDate="2017-01-20T14:12:59.767" UserId="231" />
  <row Id="6440" PostId="4584" Score="0" Text="Should be the later." CreationDate="2017-01-20T14:17:09.473" UserId="2687" />
  <row Id="6441" PostId="4579" Score="0" Text="Really? I will let them know. I think we have a contact email at the bottom of the website's home page)" CreationDate="2017-01-20T15:48:21.557" UserId="1608" />
  <row Id="6442" PostId="4586" Score="0" Text="Thank you, I do understand the difference between the two and I understand how they both work, I was just curious as to whether such a thing was exclusive" CreationDate="2017-01-20T16:00:09.673" UserId="5887" />
  <row Id="6443" PostId="4586" Score="0" Text="So it's not))) now I hope you know." CreationDate="2017-01-20T16:00:35.163" UserId="1608" />
  <row Id="6444" PostId="4585" Score="1" Text="I think that user18490 answered the question quite well, but just in case you would like to see some actual examples, here are two tutorials that do reflection without ray casting or ray tracing. They used OpenGL for it.   https://www.youtube.com/watch?v=GADTasvDOX4     and    https://www.youtube.com/watch?v=xutvBtrG23A     basically one renders the reflection out through a camera that is positioned mirror like and the other uses cube maps for reflections. Cube maps can be rendered out for every frame to be able to get reflections of the actual dynamic scene. Hope that helped a bit!" CreationDate="2017-01-20T16:07:00.340" UserId="4908" />
  <row Id="6445" PostId="4587" Score="0" Text="This is only one technique though among others. But the question is more fondamental. The fact is you can also use rasterisation to &quot;solve&quot; for visibility between a shaded point and the rest of the scene for reflection &quot;rays&quot; if you really want to. This is highly ineffective but possible. So as the OP said in his comment &quot;they are not exclusive&quot;." CreationDate="2017-01-20T18:02:48.117" UserId="1608" />
  <row Id="6446" PostId="4589" Score="0" Text="But you miss the fondamental point to some extent. Cube maps/planar reflections are not accurate nor are most of the &quot;tricks&quot; that are used with rasterisation to mimic reflections/refractions. The point is **you can** compute true reflections/refractions using rasterisation, not fake ones. You can use a technique similar to radiosity cube maps for instance, and I believe that's what the OP needs to know." CreationDate="2017-01-20T21:48:12.960" UserId="1608" />
  <row Id="6447" PostId="4589" Score="1" Text="@user18490 You seem to be taking a very narrow interpretation of the question, focusing purely on whether perfect reflections are theoretically possible, regardless of practicality. I'm coming at it from a different angle: techniques useful in production real-time graphics." CreationDate="2017-01-20T23:22:04.947" UserId="48" />
  <row Id="6448" PostId="4591" Score="0" Text="Thank you for your reply Nicol. I`ve fixed the argument mistake and found out the root cause. I have specified 'transformation' as my uniform value and was trying to use 'transform'.... It works llike a charm now." CreationDate="2017-01-21T00:39:31.387" UserId="5890" />
  <row Id="6449" PostId="4586" Score="0" Text="If they are useful and relevant to the question, feel free to include specific links. It's only [link only answers](http://meta.stackexchange.com/a/8259/258647) that are problematic." CreationDate="2017-01-21T11:15:37.290" UserId="231" />
  <row Id="6450" PostId="4588" Score="0" Text="Can this be extended to give a list of all faces that are closer than x to the other mesh, or will it only work to give a single pair of faces?" CreationDate="2017-01-21T11:20:42.250" UserId="231" />
  <row Id="6451" PostId="4586" Score="0" Text="I think these 2 links to scratchapixel tutorials will be good on learning about the differences about Rasterization and Ray Tracing. https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-overview" CreationDate="2017-01-21T13:22:33.673" UserId="5256" />
  <row Id="6453" PostId="4593" Score="1" Text="`glGetIntegerv(GLFW_CONTEXT_VERSION_MAJOR` I'm pretty sure that doesn't work. OpenGL doesn't know what to do with `GLFW_CONTEXT_VERSION_MAJOR`." CreationDate="2017-01-21T18:43:27.343" UserId="2654" />
  <row Id="6455" PostId="4589" Score="0" Text="]-) yes because i think this is what the OP wants to know. But I think your answer is complementary so it's good. The question is whether you can computer &quot;accurate&quot; reflection/refractions with a given algorithm. I would say that if it's not possible with a given visibility algorithm, unless you fake it (aka reflection look like reflections but are not correct geometrically, physically) then I would say this algorithm can not handle things such as reflection/refractions, which is the OP's questions." CreationDate="2017-01-22T09:32:51.783" UserId="1608" />
  <row Id="6457" PostId="4597" Score="0" Text="There are plenty of articles out there on the web on initialising of lines for example http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter22.html. This was a very popular topic in the 80's so you might find good articles in books like Graphics Gem. It's more a matter of Google, searching and reading yourself. But that's a VERY well document topic." CreationDate="2017-01-22T09:36:54.640" UserId="1608" />
  <row Id="6458" PostId="4600" Score="0" Text="Could you link the other several books?" CreationDate="2017-01-22T10:22:11.803" UserId="5256" />
  <row Id="6459" PostId="4597" Score="1" Text="@user18490 its a very well documented topic, only many of those documents contain systematic flaws and assuptions. So it has a lot of documentation but lot of it is simply misleading." CreationDate="2017-01-22T13:27:15.337" UserId="38" />
  <row Id="6460" PostId="4596" Score="0" Text="[Discussion on meta about cross site duplicates and whether they are a problem](http://meta.computergraphics.stackexchange.com/questions/250/how-do-we-deal-with-duplicates-of-so-questions)" CreationDate="2017-01-22T14:26:03.410" UserId="231" />
  <row Id="6461" PostId="4599" Score="0" Text="Although recommendations are off topic on the main site (as they are on nearly every Stack Exchange site), you are welcome to discuss recommendations in [chat]." CreationDate="2017-01-22T14:36:26.430" UserId="231" />
  <row Id="6462" PostId="4596" Score="1" Text="If you have additional questions, please post them as new questions rather than editing them into an existing one. This way the answers to separate questions can be compared independently. You can still include links between the questions if they are related." CreationDate="2017-01-22T17:39:32.763" UserId="231" />
  <row Id="6463" PostId="4597" Score="0" Text="@jooja, I respect your opinion but if there is a topic on which there is a lot of  information it is on the topic of line anti-aliasing. It is one of the first topics that was ever researched in CG and you will find algorithms for doing this specially that were developed in the 70s/80s. I pointed the OP to Graphics Gems books in which he/she will find information. On such basic questions you have to encourage OPs to search on Google before asking such questions on here (and show they researched the topic before coming here). Maybe you can also write an answer)" CreationDate="2017-01-22T19:04:59.580" UserId="1608" />
  <row Id="6464" PostId="4597" Score="1" Text="@user18490 point is that lot treatises of computer graphics neglect to define scope and limits of their assumptions. So for example many coverage based calculations forget to mention 1. That you can not assume box filter being the best possible filter. 2. Algorithm is only right is you draw one line/curve but they do not stack up correctly (leading to the conflation issue many vector engines exhibit, even tough we have known of the cause since 1980's), 3. Your output  frame buffer is not linear. etc etc." CreationDate="2017-01-22T19:21:40.667" UserId="38" />
  <row Id="6466" PostId="25" Score="0" Text="I still use gDEbugger from time to time for older GL versions. CodeXL is the more modern version of it and can be found here: https://github.com/GPUOpen-Tools/CodeXL&#xA;Note that there's a comprehensive list of graphics debugging tools on the apitrace page here: https://apitrace.github.io/#links" CreationDate="2017-01-23T13:52:24.060" UserId="5908" />
  <row Id="6467" PostId="4601" Score="0" Text="This solution is kinda obvious, and far from the question asked. Goal of the question is to find a solution that operates to the *uncombined* tranform components, before creating any model matrix." CreationDate="2017-01-23T14:56:55.413" UserId="5855" />
  <row Id="6468" PostId="4601" Score="0" Text="But you can apply this solution to uncombined transform components.&#xA;You can of course simplify it, e.g. change sign in position instead of multiply it by matrix but idea is the same.&#xA;Applying it to model matrix will apply it also to mesh vertices and you don't need to change models." CreationDate="2017-01-23T15:03:59.267" UserId="3123" />
  <row Id="6469" PostId="4601" Score="1" Text="Of course if you apply this matrix to scale you don't need to worry about meshes but scale like `(1,1,-1)` isn't intuitive." CreationDate="2017-01-23T16:26:18.133" UserId="3123" />
  <row Id="6470" PostId="4569" Score="0" Text="Can anyone help me out with this?" CreationDate="2017-01-23T17:55:42.033" UserId="5874" />
  <row Id="6471" PostId="4603" Score="0" Text="`If you are truly normalizing the data, the only cause of overflow I can think of is due to rounding issues which can definitely happen.` Yeah, but  If I put value 400 to this formula (which can happen when I add a high number to the image) I get a normalized value of 555 which is not in the expected range :D There's something wrong with these extremes O.o" CreationDate="2017-01-23T18:52:40.677" UserId="5910" />
  <row Id="6473" PostId="4601" Score="0" Text="Modifying mesh vertices for some simple transformation doesn't seem a good idea, seems more like it will complicate things to a new level." CreationDate="2017-01-23T18:55:58.677" UserId="5855" />
  <row Id="6474" PostId="4603" Score="0" Text="No problem.  Let's say that after adding 400 to each channel of each pixel, the minimum value seen in any channel is 400 and the maximum is 555.  You would then run through every channel value of every pixel and apply the normalization formula.&#xA;&#xA;Output = (Input - 400) / (555-400)&#xA;&#xA;If you plug an input value of 400 into that equation you get an output of 0.  If you plug an input value of 555 into that equation you get an output value of 1.  For values between 400 and 555 you'll get values between 0 and 1.&#xA;&#xA;This result should then be multiplied by 255 to make it [0,255]." CreationDate="2017-01-23T18:56:10.043" UserId="56" />
  <row Id="6475" PostId="4603" Score="0" Text="Now you've said something complately opposite to your answer, cause you've said that I should use the old image's extremes and now, that new ones :D But according to your explanation - it works, I just tested it (swapped so the app takes extremes from the new image). Strange results, cause the image looks a little bit brighter, even if I added 1000 to it. Will test some and give some time for the question cause I really need to be sure that this is the correct way. There's no talking with my stubborn teacher, he can't even explain what he meant." CreationDate="2017-01-23T19:04:51.223" UserId="5910" />
  <row Id="6476" PostId="4603" Score="1" Text="I think the confusion is coming from the addition and the normalization being seen as a single operation.  it's really two operations.&#xA;1) Add 400 to each channel in a pixel.  You will need the pixel values to be in some storage type which can handle values outside of the 0-255 range. Floats could be a good quick and easy choice. Doubles if you need precision. Fixed point if you need speed.&#xA;2) Now do the normalization step. Find min and max values, normalize all values (all channels of all pixels) muliply by 255.&#xA;3) A final third pass does a clamp and conversion from float back to uint8." CreationDate="2017-01-23T19:13:52.507" UserId="56" />
  <row Id="6477" PostId="4603" Score="0" Text="That's what I'm doing right now. I read the first image, store it in the array A, now, I add a value 100 to each pixel, now I copy the array to array B and save array A to see results (before normalization). I find extremes on array B, normalize array B, clamp it and save it as the &quot;after&quot; image. Here's a link to all three images: https://drive.google.com/open?id=0B9DujHxzmzAySl9SU2VKNjhsNDA&#xA;&#xA;I have them in PCX format cause I chose it as a working for lessons (it's  the least complicated one :D). You'll need a photoshop or pcx viewer to view it, but you probably know that ;D" CreationDate="2017-01-23T19:29:12.553" UserId="5910" />
  <row Id="6478" PostId="4601" Score="0" Text="When I think about it now, maybe I was wrong. With translation and rotation like this meshes should look alright and no changing in culling should be needed." CreationDate="2017-01-23T21:23:09.497" UserId="3123" />
  <row Id="6482" PostId="4578" Score="0" Text="Could you detail a little more what the context is (why you are trying to do this), what your constraints are (for example, do you know what the sphere center and radius are, or do you only have the triangle?) and what formula you are using (random floating points values are a lot harder to read than equations)?" CreationDate="2017-01-25T03:42:34.323" UserId="182" />
  <row Id="6483" PostId="4606" Score="1" Text="dupe of one of the following: http://computergraphics.stackexchange.com/q/2508/137 http://computergraphics.stackexchange.com/q/259/137 http://computergraphics.stackexchange.com/q/280/137" CreationDate="2017-01-25T09:13:16.710" UserId="137" />
  <row Id="6487" PostId="4603" Score="0" Text="@Spectre It might be smarter to just display both images and screenshot the comparison. You can host a single jpg easily. But looking at your images I think your example is correct. Note that normalizing after addition undoes the addition because you subtract `fmin`." CreationDate="2017-01-26T03:05:41.927" UserId="3204" />
  <row Id="6488" PostId="4610" Score="1" Text="The effect is typically called &quot;bloom&quot;. Here's [a previous question on bloom implementation](http://computergraphics.stackexchange.com/questions/4221/bloom-in-directx), and [another related question](http://computergraphics.stackexchange.com/questions/3706/tweaking-a-glow-shader-to-make-it-look-better)." CreationDate="2017-01-27T02:26:02.647" UserId="48" />
  <row Id="6489" PostId="4610" Score="1" Text="Heh, I just noticed that second one was one you posted. :) Anyway, the key to getting large bloom radius is to do repeated steps of downsampling and blurring—kind of like making a mip chain—then summing together all the blurred textures in the final pass." CreationDate="2017-01-27T02:32:40.660" UserId="48" />
  <row Id="6490" PostId="4610" Score="0" Text="@NathanReed Yah! Still working on it. Using that technique got me something quite good looking but it can't run real-time. I have however added a button that freezes the game and then exports it with all the pretty post processing using that technique." CreationDate="2017-01-27T05:18:36.323" UserId="2308" />
  <row Id="6491" PostId="4617" Score="4" Text="_&quot;Because traditional dithering algorithms involve giving information from each pixel to its neighbours, they don't parallelise well&quot;_  That's error diffusion dithering but there is also ordered dither which can be done independently on each pixel" CreationDate="2017-01-27T17:29:40.420" UserId="209" />
  <row Id="6492" PostId="4617" Score="0" Text="oh yeah, I did think of ordered dither for the old use case of cheap alpha blending, but I don't think it's used much nowadays (for that purpose) so I decided not to include it." CreationDate="2017-01-27T17:31:46.207" UserId="2041" />
  <row Id="6493" PostId="4617" Score="0" Text="It's in some hardware eg mapping from 24bit to 16bit frame buffers." CreationDate="2017-01-27T17:36:21.817" UserId="209" />
  <row Id="6494" PostId="4612" Score="1" Text="Is there a fast way to downscale a texture? I generally create an offscreen FBO of the smaller size and then have the most generic full-screen texture renderer render onto the smaller FBO. After that I have the original texture and a FBO with the smaller texture. Is that the proper way to do it?" CreationDate="2017-01-28T04:24:46.707" UserId="2308" />
  <row Id="6495" PostId="4612" Score="0" Text="@J.Doe: That's how I do it too." CreationDate="2017-01-28T06:53:05.113" UserId="182" />
  <row Id="6496" PostId="4616" Score="2" Text="Why do you think it does not store correctly? Does your `GL_DEPTH_COMPONENT` is also 32 bit float type?" CreationDate="2017-01-28T10:28:36.200" UserId="4958" />
  <row Id="6497" PostId="4617" Score="0" Text="@Dan Good answer) but jittering is not specific to ray-tracing. It's about randomising in a way the position of samples within the pixel space and can be used with rasterisation as well." CreationDate="2017-01-28T12:28:29.450" UserId="1608" />
  <row Id="6498" PostId="4615" Score="2" Text="So you are into ray tracing and you are worried about creating an OpenGL window? :)" CreationDate="2017-01-28T12:52:30.877" UserId="4958" />
  <row Id="6499" PostId="4618" Score="0" Text="Thanks! I think I will do this with DirectX. So first I need to load the image as a texture on a quad from a specified file path (where the image is being rendered to) and then as the image is being rendered it will show up on the screen. Will I manually need to refresh the image as it is being rendered to see the progress (unload, delete, reload &amp; repeat within DirectX) or is this unnecessary? I did this in C# with a picturebox but I got an error since I cannot read and write the file simultaneously or something like that, will the same happen here?" CreationDate="2017-01-28T13:00:08.580" UserId="5256" />
  <row Id="6500" PostId="4618" Score="0" Text="I'm on Windows by the way." CreationDate="2017-01-28T13:00:30.210" UserId="5256" />
  <row Id="6501" PostId="4612" Score="1" Text="If memory usage is not an issue, you can use summed area tables to get arbitrary sized box filtering in just two lookups. I don't have the links right here, but if I'm not mistaken, people have extended that to approximate Gaussian blur with a small series of SAT lookups." CreationDate="2017-01-28T13:00:43.373" UserId="4546" />
  <row Id="6502" PostId="4618" Score="0" Text="@ArjanSingh if I understand correctly, this answer will give you somewhere to send your partial image as you create it, rather than displaying the saved file. Treat them as two separate things. Display the image pixel by pixel as you create it in memory, and save it to file at the end when it is complete." CreationDate="2017-01-28T14:00:19.057" UserId="231" />
  <row Id="6503" PostId="4615" Score="0" Text="If you really don't want to work with anything except .ppm files, you could render to an array of initially zeroed pixel values in memory, and save a series of .ppm files at regular intervals (probably more like one image per row rather than one image per pixel, otherwise you'll have millions of image files in the folder). You'll then be one step closer to the approach described in the answer." CreationDate="2017-01-28T14:06:58.837" UserId="231" />
  <row Id="6504" PostId="4615" Score="0" Text="@Arjan &amp; Trichoplax. Yes that's actually a good idea. I was thinking trying that one day but never got the time which is to write the file to disk and let some JavaScript if your browser checking every say 2s the file on disk, reload it display it in your browser)). It would work not sure about the efficiency probably poor but with canvas now it's easy (don't even need WebGL)." CreationDate="2017-01-28T14:30:44.460" UserId="1608" />
  <row Id="6505" PostId="4618" Score="0" Text="@trichoplax How can I copy the data from the image buffer to a DirectX texture? Loading, and displaying the texture isn't hard but how do I create the texture with the pixel data? Should I make this a separate question?" CreationDate="2017-01-28T14:42:34.333" UserId="5256" />
  <row Id="6506" PostId="4618" Score="1" Text="@ArjanSingh does sound like a separate question." CreationDate="2017-01-28T14:46:30.603" UserId="231" />
  <row Id="6507" PostId="4620" Score="0" Text="If I wrote my ray tracer as a .DLL and used P/Invoke in C# to use it, could I then use the image buffer to send the image buffer to the picturebox? This is quite similar to what you said which got me thinking if I could do this with C#." CreationDate="2017-01-28T14:50:53.220" UserId="5256" />
  <row Id="6508" PostId="4620" Score="0" Text="Yeah probably. I'm not into c# but Visual Studio with c# libs is pretty robust IDE too." CreationDate="2017-01-28T14:52:44.720" UserId="4958" />
  <row Id="6509" PostId="4618" Score="0" Text="@ArjanSingh Though I have to say Arjan that they are plenty of tutorials out there on how to do that ... &quot;How can I copy the data from the image buffer to a DirectX texture? ...&quot;.  I think it is important you also look for such tutorials before asking your questions here and only ask if you really can't find... but that's a very common topic." CreationDate="2017-01-28T15:00:06.260" UserId="1608" />
  <row Id="6510" PostId="4620" Score="0" Text="@narthex: it's a solution but saying that dealing with Qt is easy is not so true if you are a beginner in programming. Yes QtCreator simplified things a lot but you already introduced to the OP a lot of complex concepts (like signal/slots), etc which will take time to digest. The OP is interested in learning graphic, and so learning a graphics API is more a priority than learning Qt;-))) He can do this later ... + plus keep in mind that Qt UI are drawn in OpenGL and DX)))" CreationDate="2017-01-28T15:02:33.843" UserId="1608" />
  <row Id="6511" PostId="4620" Score="0" Text="After looking online, I'm gonna write my ray tracer as a .DLL then use the data from the image buffer use `PictureBox.SetPixel(x, y, color)` I wanted to give my Ray Tracer a UI so writing the ray tracer in .DLL was inevitable. The OpenGL/DirectX method seems quite interesting and if the SetPixel method is too slow I'll just go with that." CreationDate="2017-01-28T15:11:18.157" UserId="5256" />
  <row Id="6512" PostId="4620" Score="0" Text="@user18490 OP stated that he is looking for simple solution to view his image in UI application and this is in my opinion the simplest and involves the least lines of code instead of tedious graphical API. Also  making UI application as OP posted in image not only in Qt invevitably involves learning  such concepts as GUI thread and some kind of signals or events." CreationDate="2017-01-28T15:21:53.410" UserId="4958" />
  <row Id="6514" PostId="4621" Score="1" Text="Needs more clarification. Are `a` and `b` intended to be points defining the line? How are the barycentric coordinates defined relative to those points? Is `point` assumed to be _on_ the line, or might it be somewhere off the line? What is the intended role of `epsilon`?" CreationDate="2017-01-29T00:04:15.003" UserId="48" />
  <row Id="6515" PostId="4619" Score="2" Text="Note that the method by Jakob et al. relies on rendering of tabulated BSDF data in some specialised Fourier basis representation. For details, also refer to the corresponding [technical report](http://rgl.s3.eu-central-1.amazonaws.com/media/papers/Jakob2014Comprehensive_1.pdf).&#xA;An open-source implementation is also available in the newest, 3rd edition of [PBRT](https://github.com/mmp/pbrt-v3/). The BSDF files can be generated with [layerlab](https://github.com/wjakob/layerlab/) in Python." CreationDate="2017-01-29T00:07:57.050" UserId="1930" />
  <row Id="6516" PostId="4621" Score="0" Text="yes, a and b are intended to be points defining the line. yes, How are the barycentric coordinates defined relative to those points? point can be on or somewhere off the line. epsilon is the error range" CreationDate="2017-01-29T00:25:58.703" UserId="5943" />
  <row Id="6518" PostId="4617" Score="0" Text="@user18490 I know it's not specific to ray-tracing, but that was the most obvious example. I'd welcome an edit adding an unrelated example to bulk up that section." CreationDate="2017-01-29T09:01:41.177" UserId="2041" />
  <row Id="6519" PostId="4623" Score="0" Text="Step 1: Stop confusing the act of creating a buffer with the act of using that buffer with a VAO." CreationDate="2017-01-29T14:58:39.057" UserId="2654" />
  <row Id="6520" PostId="4625" Score="0" Text="Thank you. I made it so that the &quot;shared VBOs&quot; are only calling glBufferData once, then they are rebounded and set to a vertex attribute for each VAO. So I don't need to call glBufferData for 10 time with the same data:)" CreationDate="2017-01-29T16:06:53.177" UserId="3434" />
  <row Id="6521" PostId="4621" Score="0" Text="Would you expect the barycentric coordinates of the point to be the same if the point is projected to the line?" CreationDate="2017-01-29T17:08:25.487" UserId="2941" />
  <row Id="6522" PostId="4621" Score="0" Text="yes, barycentric coordinates of the point should be the same if the point is projected to the line" CreationDate="2017-01-29T20:12:19.987" UserId="5943" />
  <row Id="6523" PostId="4627" Score="0" Text="Very similar question: [Mirror Reflections: Ray Tracing or Rasterisation?](http://computergraphics.stackexchange.com/q/4585/48)" CreationDate="2017-01-29T20:12:52.383" UserId="48" />
  <row Id="6524" PostId="4621" Score="1" Text="Baryenctric coordinates are used for shapes likes triangles or more complex shapes. Not sure how you can define them with respect to a line? Doesn't make a lot of sense to me? It would be good if you could edit your question and explain in more details what you are looking for (what's the problem at hand)" CreationDate="2017-01-29T21:26:36.653" UserId="1608" />
  <row Id="6525" PostId="4626" Score="0" Text="Maybe best asking your question on Game Development Stack Exchange" CreationDate="2017-01-29T21:28:13.413" UserId="1608" />
  <row Id="6526" PostId="4628" Score="0" Text="I just want to make an addition to this. This article explains ray marching really well, but it uses a function to generate a height map when you give it the x and y value. You can change it so that instead of that function, it reads it from the depth buffer. http://www.iquilezles.org/www/articles/terrainmarching/terrainmarching.htm" CreationDate="2017-01-29T21:28:52.213" UserId="4908" />
  <row Id="6527" PostId="4626" Score="0" Text="Good idea. http://gamedev.stackexchange.com/questions/136525/multiple-buffers-and-calling-glbuffersubdata/136526#136526" CreationDate="2017-01-30T06:54:52.577" UserId="3434" />
  <row Id="6528" PostId="4617" Score="0" Text="Thanks for the awesome answer @DanHulme ! Any reference to nice dithering and jittering ?" CreationDate="2017-01-30T07:35:53.400" UserId="2372" />
  <row Id="6529" PostId="4617" Score="0" Text="@user18490 What would be a good example of jittering in a rasterized renderer ?" CreationDate="2017-01-30T07:38:04.350" UserId="2372" />
  <row Id="6530" PostId="4617" Score="1" Text="@MatT: any renderer would implement some sort of pixel sampling as a the most common approach to oversampling (to fight the main issue that you get with point sampling which is aliasing). The idea is to create several samples in your pixel and check whether the triangles these samples are contained within the rendered triangles. Then you accumulate their results using some sort of filtering. REYES (old), OpenGL, they all offer that feature. Check seminal paper: http://graphics.pixar.com/library/StochasticSampling/paper.pdf" CreationDate="2017-01-30T10:26:23.327" UserId="1608" />
  <row Id="6531" PostId="4620" Score="0" Text="@Narthex: very true and I don't try to argue with you or pick up a fight)). There are strong divergence of opinion on the topic. I worked for 20 years with many engineers on developing complex software and I just point out that from my experience it is better to take the time to learn yourself the concepts your describe than just using a very complex API such as Qt that hides a lot of these things for you and yet that you have to use. I strongly disagree with the idea that these concepts are hard to learn. This is my opinion I have proven it many times in my work, but won't go on a crusade." CreationDate="2017-01-30T10:30:36.010" UserId="1608" />
  <row Id="6532" PostId="4628" Score="0" Text="i will keep that in mind. Thank you for taking out some time for answering." CreationDate="2017-01-30T12:56:28.400" UserId="2096" />
  <row Id="6538" PostId="4630" Score="0" Text="What exactly are you looking for here? This looks like Dot-graph territory, not API territory. Obviously, since Dot is just a text file, you can write it with any programming language. Are you looking for a program to create such things or a programmatic way to generate such an image?" CreationDate="2017-01-30T17:33:26.417" UserId="2654" />
  <row Id="6539" PostId="4621" Score="0" Text="Barycentric coordinates apply to all SIMPLICES (plural of simplex).  A line is a 1 dimensional simplex." CreationDate="2017-01-30T19:40:55.330" UserId="56" />
  <row Id="6541" PostId="4617" Score="1" Text="@MatT: Wiki in fact has a good article -&gt; https://en.wikipedia.org/wiki/Supersampling" CreationDate="2017-01-30T21:51:13.847" UserId="1608" />
  <row Id="6543" PostId="4635" Score="0" Text="Your example is not visible. Do you work in 2D or 3D?" CreationDate="2017-01-30T21:53:01.200" UserId="1608" />
  <row Id="6545" PostId="4636" Score="0" Text="Thank you for this detailed answer. Good input for me to start planing my setup." CreationDate="2017-01-30T22:09:10.037" UserId="3434" />
  <row Id="6546" PostId="4635" Score="0" Text="Strange, I can see it. 2D. Here is another link to the example: http://bit.ly/2kIc4Wg" CreationDate="2017-01-30T22:30:51.333" UserId="5961" />
  <row Id="6547" PostId="4630" Score="0" Text="I'm voting to close this question as off-topic because it is not about computer graphics. I'm am not sure what would be a good place on SE to ask it though. Also, as Nicol points out, it need to be better formulated." CreationDate="2017-01-31T03:01:23.627" UserId="182" />
  <row Id="6549" PostId="4628" Score="0" Text="No problem. If the answer did satisfy you, don't forget to accept the answer. Otherwise, we can try to discuss further the question ;)" CreationDate="2017-01-31T07:06:31.467" UserId="5002" />
  <row Id="6550" PostId="4638" Score="0" Text="Could you please give me a snippet code example that shows how to use one VAO for multiple VBOs? All examples I find on the internet call to glVertexAttribPointer before drawing a buffer, but if you have to call that method everytime you want to draw a buffer, then VAOs doesn't make sense." CreationDate="2017-01-31T07:36:35.220" UserId="5852" />
  <row Id="6551" PostId="4640" Score="0" Text="As you imply, a plane can be represented by its normal, N, and a scalar, d. A point X is on the plane if  N.X+d = 0.  &#xA;As for inside and outside typically you define a rule, e.g., that the normal points &quot;out&quot; from the plane which just means that if N.X+d &gt; 0, then X is outside the plane, while N.X+d&lt;0, implies it's inside." CreationDate="2017-01-31T09:40:37.783" UserId="209" />
  <row Id="6554" PostId="4640" Score="0" Text="Clearly my brain is not in gear this morning. Will delete." CreationDate="2017-01-31T10:24:39.147" UserId="209" />
  <row Id="6555" PostId="4642" Score="0" Text="If you represent your plane by the Normal and a scalar you can avoid the unnecessary expensive of the vector subtract." CreationDate="2017-01-31T11:10:05.117" UserId="209" />
  <row Id="6556" PostId="4642" Score="0" Text="@SimonF. Sure ... feel free to edit my answer and add that as a quicker solution... I just made an answer)." CreationDate="2017-01-31T11:29:02.997" UserId="1608" />
  <row Id="6557" PostId="4642" Score="0" Text="Given the number of typos I'm making today ( &quot;expensive of the&quot;, good grief!), I don't think I dare.  :-)" CreationDate="2017-01-31T13:52:44.580" UserId="209" />
  <row Id="6558" PostId="4638" Score="0" Text="I added some code per OP's request. Hope it's ok. It will need to be approve by Nicol to show up" CreationDate="2017-01-31T16:46:02.490" UserId="1608" />
  <row Id="6561" PostId="4642" Score="1" Text="@SimonF - I'll do it for you..." CreationDate="2017-01-31T19:59:13.457" UserId="1608" />
  <row Id="6562" PostId="4631" Score="0" Text="yEd is also nice. altough in this case creating a dot file is the way to go." CreationDate="2017-01-31T20:59:38.480" UserId="38" />
  <row Id="6564" PostId="4640" Score="0" Text="@Mojomo. Please accept the answer if it is satisfactory otherwise people will not make the effort to keep answering questions;-)))" CreationDate="2017-02-01T07:14:48.757" UserId="1608" />
  <row Id="6565" PostId="4638" Score="1" Text="@4dr14n31t0r Th3 G4m3r Note that in the code below you will need to make the call all these functions each time in the rendering loop if you render more than 1 object. Also please accept an answer if acceptable." CreationDate="2017-02-01T07:24:08.307" UserId="1608" />
  <row Id="6566" PostId="4611" Score="1" Text="Can you not 'monte-carlo' the material layers? E.g. Weight each layer according to their reflectivity and pick one at random based on that.  Deeper layers will need some attenuation based on the sum of absorption of all layers above them." CreationDate="2017-02-01T08:59:27.680" UserId="3073" />
  <row Id="6574" PostId="4642" Score="0" Text="@user18490 yes, that answers my question. Thank you" CreationDate="2017-02-01T15:58:49.493" UserId="5943" />
  <row Id="6575" PostId="4635" Score="0" Text="You can also raytrace lines. Cast a ray perpendicular to line and test the lines in the scene. Like ray-tracing but in 2D. That would work and would give you accurate results." CreationDate="2017-02-01T20:03:15.827" UserId="1608" />
  <row Id="6576" PostId="4648" Score="0" Text="What platform? What libraries are you familiar with? Do you have any requirements or limitations in doing this? Your question as posed seems a bit too broad. Can you narrow it down a bit?" CreationDate="2017-02-02T06:20:00.547" UserId="3003" />
  <row Id="6577" PostId="4648" Score="0" Text="I just edited my question again. Thanks." CreationDate="2017-02-02T06:25:24.880" UserId="2096" />
  <row Id="6578" PostId="4648" Score="1" Text="You could try SDL as that supports software rendering / direct FB access and is in some ways simpler to use. Otherwise you could stick with openGL and use texImage2D to upload your software framebuffer to the GPU." CreationDate="2017-02-02T09:38:42.257" UserId="3073" />
  <row Id="6579" PostId="4648" Score="0" Text="Thanks for mentioning about the SDL. And yes texImage2D can be used to put image in gpu but then what to do? Is there any function in OpenGL so that i can copy the image data directly to default framebuffer?" CreationDate="2017-02-02T09:47:06.780" UserId="2096" />
  <row Id="6580" PostId="4642" Score="0" Text="One thing we should point out that is, if using floating point, it is unlikely, *in general*, that you will ever get  _(vDotPlaneNormal == 0.0)_.  You may need to add a &quot;within epsilon&quot; test" CreationDate="2017-02-02T10:04:08.660" UserId="209" />
  <row Id="6582" PostId="4648" Score="1" Text="On win32 you can get away with GDI, flushing memory to screen is relatively fast. Look for CreateDIBSection" CreationDate="2017-02-02T11:20:17.607" UserId="5364" />
  <row Id="6583" PostId="4649" Score="0" Text="guis in games usually boil down to textured squares." CreationDate="2017-02-02T14:08:38.250" UserId="137" />
  <row Id="6584" PostId="4649" Score="0" Text="But how about programs?" CreationDate="2017-02-02T14:23:33.627" UserId="5985" />
  <row Id="6585" PostId="307" Score="0" Text="On your UBO link: &quot;Lastly, they can be used to share information between different programs. So modifying a single buffer can effectively allow uniforms in multiple programs to be updated.&quot;" CreationDate="2017-02-02T18:07:02.573" UserId="5254" />
  <row Id="6587" PostId="4635" Score="0" Text="In your picture, Segment 2 does face a polygon of a different colour; Polygon D, but you say it should return false for Segment 2.  This implies you need to see if the path is occluded by polygons of the same colour, right?" CreationDate="2017-02-02T23:27:43.193" UserId="2941" />
  <row Id="6588" PostId="4654" Score="0" Text="&quot;*If I on the other hand choose OpenCL, will it be possible for other people to use my implementation without installing OpenCL on their computer?*&quot; That's just as true of OpenGL. The only difference being that GL typically comes with graphics drivers, while CL typically requires an explicit download." CreationDate="2017-02-03T01:54:14.327" UserId="2654" />
  <row Id="6589" PostId="4648" Score="0" Text="To supplement my answer: You will need to draw a full screen quad with texture mapping enabled, you can follow a simple texture mapping tutorial for this, it's fairly straight forward." CreationDate="2017-02-03T04:17:56.890" UserId="3073" />
  <row Id="6590" PostId="4635" Score="0" Text="Yes. That's another piece of it. The segment will return false if the _closest_ edge which the edge is facing is of the same type." CreationDate="2017-02-03T06:22:33.780" UserId="5961" />
  <row Id="6591" PostId="4656" Score="0" Text="what if the vertex used both matrices (blended) to calculate the normal" CreationDate="2017-02-03T08:35:46.477" UserId="137" />
  <row Id="6592" PostId="4657" Score="0" Text="This is not much different to the one above, is it? :-) The point is that you need to output really small alphas to keep as much of Bg as possible and rely on large scales of output Colours to compensate for the suppression that those alphas will cause to the accumulated colour. Making sure that the denominator is not zero as well." CreationDate="2017-02-03T09:02:00.963" UserId="270" />
  <row Id="6594" PostId="4654" Score="0" Text="Why was my question downvoted?" CreationDate="2017-02-03T18:27:44.983" UserId="5989" />
  <row Id="6595" PostId="4654" Score="0" Text="@NicolBolas That may be true. So what frameworks do media players and other video software usually use to accelerate video rendering?" CreationDate="2017-02-03T18:34:35.123" UserId="5989" />
  <row Id="6596" PostId="4657" Score="0" Text="There is a difference. Note that the alpha is multiplied by the color in the numerator and that fixes the problem" CreationDate="2017-02-03T20:03:52.660" UserId="5881" />
  <row Id="6597" PostId="4661" Score="0" Text="Thanks for taking out some time for answering the question!" CreationDate="2017-02-04T17:54:24.943" UserId="2096" />
  <row Id="6598" PostId="4662" Score="0" Text="The full project is at https://github.com/kourbou/pythree-gtk. The lecture I used was [here](https://www.youtube.com/watch?v=mpTl003EXCY)." CreationDate="2017-02-04T20:46:23.770" UserId="5998" />
  <row Id="6599" PostId="4661" Score="0" Text="@Ankit np, the rules are that you accept this as an answer if you do;)" CreationDate="2017-02-05T10:29:51.077" UserId="1608" />
  <row Id="6600" PostId="4659" Score="0" Text="I am not too sure to understand your question. SH coefficients assuming you rendered them in a pre-pass already encode visibility information (assuming you are talking about the SH coefficients computed at the vertex of your objects, not the SH coefficients of your env map for example). So are you asking how to convolve the SH coeffs from the env map with the SH coeffs from the vertices?" CreationDate="2017-02-05T10:33:48.910" UserId="1608" />
  <row Id="6601" PostId="4659" Score="0" Text="@user18490 Yeah convolve two sets but nevermind, why would I need that actually,  reconstruction and multiplication will work." CreationDate="2017-02-05T10:49:26.050" UserId="4958" />
  <row Id="6603" PostId="4662" Score="0" Text="Please refer to https://www.scratchapixel.com/lessons/3d-basic-rendering/get-started and https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix. There is ample information on the perspective projection process out there (and helas debugging people's program is not what a forum is about). Eventually that's teacher's jobs)))" CreationDate="2017-02-05T11:01:28.473" UserId="1608" />
  <row Id="6604" PostId="4670" Score="0" Text="That's one of the problems you have. If you try to debug code without knowing what the library you use do, then you will not be able to fix much. Use libraries you either understand or write your own code for this." CreationDate="2017-02-05T12:39:50.210" UserId="1608" />
  <row Id="6605" PostId="4670" Score="0" Text="@user18490 I wrote my own code... I just had to translate my code from Java into Python here and I don't use `numpy` specifically." CreationDate="2017-02-05T12:41:09.183" UserId="6003" />
  <row Id="6606" PostId="4670" Score="0" Text="Please check the reference I provided you. Do you understand the difference between column and row-major order matrices? Also how can you tell the matrix you give above is column-major?" CreationDate="2017-02-05T12:44:44.427" UserId="1608" />
  <row Id="6607" PostId="4669" Score="0" Text="I'm not familiar with Maya, so I wouldn't know what it looks like there. If it's relevant to you, I wrote the code to load Wavefront models exported from Blender; they load/render fine, so there's no issue there. I did mention that I have a working look-at matrix applied to the camera (which needs 3 parameters, not 2: `eyePosition`, `targetPosition`, and `upDirection` vectors). The transformation is actually contained in a `SceneNode` that the `Camera`, or `Dog`, is attached to, hence the question for a way to make it work regardless of which one needs to 'look' at some specified direction." CreationDate="2017-02-05T12:50:09.070" UserId="6003" />
  <row Id="6608" PostId="4670" Score="0" Text="@user18490 I'm going over the reference (it's 5am here, though), which is trying to explain how a view-matrix works, but that should be kept in the *other* question... don't mix stuff up cross-questions. Also, I understand the difference between row and column-major matrices... not sure what you're trying to get at?" CreationDate="2017-02-05T12:52:29.397" UserId="6003" />
  <row Id="6609" PostId="4669" Score="0" Text="I already replied to your question. Take the dog eye as the `eyePosition` of your `lookout`  method. If your camera is looking the other way around then apply a negative scale along the camera z-axis. There is no reason one or the other solution should not work. Again if you understand what the lookAt method works, you wouldn't ask the question;-) So do you understand what it does?" CreationDate="2017-02-05T12:55:15.577" UserId="1608" />
  <row Id="6610" PostId="4670" Score="0" Text="I don't know what you mean by mixing stuff. You have code that doesn't work. You say this matrix you provide (in form of code) is column-major. I tell you 'no' it's not. The reference I pointed you to, explains how a perspective matrix is actually built, which is what you are trying to do. The fact that it's 5am your place is not relevant to the question: tare the time to digest the theory (before posting anything else). Then it will help you fix your code." CreationDate="2017-02-05T12:59:18.693" UserId="1608" />
  <row Id="6611" PostId="4669" Score="0" Text="I *am* using the dog's  eye as `eyePosition` when trying to turn it towards the target position. I'm also using camera position as `eyePosition` when creating a separate look-at for the camera. However, the logic that allows the camera to look in the correct direction (i.e. face the target) leaves the dog facing in the opposite direction; the dog is not the camera position. The camera is looking at the dog which is trying to look at something else. If I scale the matrix, then the camera looks in the opposite direction I intend it to." CreationDate="2017-02-05T13:03:55.553" UserId="6003" />
  <row Id="6612" PostId="4670" Score="0" Text="@user18490 You're coming across as if you thought I had posted this question. The code I based this answer on actually does work." CreationDate="2017-02-05T13:46:32.437" UserId="6003" />
  <row Id="6614" PostId="4670" Score="0" Text="sorry I got you and the OP mixed up indeed. Though I see that now you have corrected the matrix to be column-major ordered;-)))" CreationDate="2017-02-05T14:00:28.367" UserId="1608" />
  <row Id="6615" PostId="4670" Score="0" Text="@user18490 Yes, it had been a translation typo. See why 6am *is* relevant? ;)" CreationDate="2017-02-05T14:01:58.877" UserId="6003" />
  <row Id="6616" PostId="4670" Score="0" Text="All understood." CreationDate="2017-02-05T14:02:31.187" UserId="1608" />
  <row Id="6617" PostId="4670" Score="0" Text="Alright thank you @ray ! It works perfectly. I have no idea why the lecture had column-major matrices. I ended up using your matrix." CreationDate="2017-02-05T16:05:30.137" UserId="5998" />
  <row Id="6618" PostId="4671" Score="0" Text="Yes, I'm sorry. I'm not actually in a Computer Graphics class, just trying to teach myself. May I ask why this matrix (from the reference) uses -f/(f-n) and -f*n/(f-n)? Do you end up with the same results with the matrix @ray gave?" CreationDate="2017-02-05T16:08:07.080" UserId="5998" />
  <row Id="6619" PostId="4665" Score="1" Text="Thank you! That makes sense. Intuitively I knew it was required but I didn't recognize that it was an optimization." CreationDate="2017-02-05T16:38:18.003" UserId="6001" />
  <row Id="6620" PostId="4671" Score="0" Text="@Kourbou: doesn't the matrix look the same to you? I you learn computer graphics it is important you learn how to read equations (as well as code)" CreationDate="2017-02-05T17:25:03.033" UserId="1608" />
  <row Id="6621" PostId="4665" Score="3" Text="Just to make sure this is explicit.. not only is cosine weighted hemisphere an optimisation because it takes fewer instructions, it's also an optimisation because it converges more quickly. It takes fewer samples to get a better result. This is a form of importance sampling." CreationDate="2017-02-05T21:06:42.837" UserId="56" />
  <row Id="6622" PostId="4665" Score="0" Text="Exactly - that was my desire as I'm trying to reduce the number of samples for indirect calculations in my lightmapper." CreationDate="2017-02-05T22:30:06.860" UserId="6001" />
  <row Id="6627" PostId="4655" Score="0" Text="Thanks. So do you mean that scatter-gather algorithms work better in OpenCL than in OpenGL?" CreationDate="2017-02-06T07:42:54.883" UserId="5989" />
  <row Id="6629" PostId="4655" Score="0" Text="Which part of the algorithm don't you think would be well suited for an OpenGL shader? The algorithm uses interpolation by combining the pixels in a small neighborhood linearly with coefficients that it gets from a lookup table. The table key is obtained by looking at the strength, angle and spread respectively of the gradients in a local neighborhood and combining the quantizations of those values. It also calculates the local binary patterns feature." CreationDate="2017-02-06T07:58:19.903" UserId="5989" />
  <row Id="6630" PostId="4670" Score="0" Text="@kourbou In mathematical texts I've come across, the &quot;standard&quot; convention has always been to use column-major orderings. What's really important (I think) is to be aware of which convention the systems you're using have adopted, be aware of the differences in expected results, and to be consistent (i.e. don't mix up row/column-major orderings)." CreationDate="2017-02-06T11:02:24.810" UserId="6003" />
  <row Id="6631" PostId="4669" Score="0" Text="I've made several updates to the question, which I think now does a better job at explaining the issue, removing ambiguity by showing the code I've been using, and showing the reference on which it was based, etc. I did go over your reference, but it explains the same concepts. I did spot what looks like a typo: it says &quot;(mind the direction of this vector: it is `To − From` not `From − To`)&quot;, but then you go on to write the line of code  `Vec3f forward = Normalize(from - to);`, which contradicts what you were trying to clarify. Let me know if you still think my question can be improved." CreationDate="2017-02-06T12:07:12.190" UserId="6003" />
  <row Id="6632" PostId="4669" Score="0" Text="I replied in the edit." CreationDate="2017-02-06T12:24:22.183" UserId="1608" />
  <row Id="6633" PostId="4669" Score="0" Text="Thanks for replying quickly. I'm wondering if my updated post clarified anything? Your edit seems to only be about what I initially thought could be was a potential typo. My look-at matrix is meant to be used with OpenGL. Does my look-at matrix look incorrect to you? From what I understand in what you insisted so much that I read (which is not the first article/reference I had consulted), it should work just fine regardless of whether the `eyePosition` is that of the `Dog` or the `Camera`, which is what I always understood to be the case, until I ran into the issue I'm trying to solve." CreationDate="2017-02-06T12:32:28.203" UserId="6003" />
  <row Id="6634" PostId="4669" Score="0" Text="Just saw your other edit; the book reference is explicitly states that it's using column-major vectors and matrices, not row-major." CreationDate="2017-02-06T12:32:52.820" UserId="6003" />
  <row Id="6635" PostId="4669" Score="0" Text="@Ray. Then if they use column-major ordering the way the matrix is written down is just plain wrong. This is not a good book. Can you point the reference please?" CreationDate="2017-02-06T12:34:01.213" UserId="1608" />
  <row Id="6636" PostId="4669" Score="0" Text="Well, I would've expected the `side` vector to be the left-most column only, not the top row (i.e. the transpose of that is what I would've expected), but I don't see the reference stating that it was transposing the pictured matrix. It could've been a typo or something in its errata." CreationDate="2017-02-06T12:37:13.863" UserId="6003" />
  <row Id="6637" PostId="4669" Score="0" Text="@Ray: why are the vector for the camera coordinate system in row form and the translation part in column form? If it's column-major matrix vectors should be written as **columns**. I gave you source code. That's how it's done in OpenGL. Many things can go wrong in your code, so follow this example and you will get it right ... code uses row-major matrices. The matrix from your ref is ALL wrong. It writes the vectors of the coordinate system in row form and the translation in column form. It's a mix of both!!! WRONG WRONG you won't go anywhere with that. You need to understand how matrices work" CreationDate="2017-02-06T12:42:01.123" UserId="1608" />
  <row Id="6638" PostId="4669" Score="0" Text="I had been sent a draft copy of the reference, so any &quot;title&quot; I provide would be meaningless, and would probably be a tangential discussion." CreationDate="2017-02-06T12:42:23.850" UserId="6003" />
  <row Id="6639" PostId="4669" Score="0" Text="Just saw your last edit. Will go over it." CreationDate="2017-02-06T12:44:25.860" UserId="6003" />
  <row Id="6640" PostId="4669" Score="0" Text="I understand how matrices work; please don't confuse a problem specific to the look-at matrix with not knowing anything about matrices. It comes across as condescending, even if that's not your intention :/" CreationDate="2017-02-06T12:46:14.383" UserId="6003" />
  <row Id="6641" PostId="4669" Score="0" Text="@Ray: you mind this useful as well: http://stackoverflow.com/questions/4124041/is-opengl-coordinate-system-left-handed-or-right-handed. I am just trying to help you. You assume you know but you don't if you can't see that the matrix you publish on this forum is not a row-major or column-major order I am sorry but I believe yes you should first really understand this and has someone who has 20 years of CGI programming behind if you don't want to learn then that's up to you, but I am just trying to mentor you here... and if I come as condescending maybe being humble is good too;-)" CreationDate="2017-02-06T13:50:45.467" UserId="1608" />
  <row Id="6642" PostId="4668" Score="0" Text="oIts the exactly same computation." CreationDate="2017-02-06T14:10:03.280" UserId="38" />
  <row Id="6643" PostId="4669" Score="0" Text="The reason the rows and colums are weirdly flipped is that they have done a 90 degree turn/flip in the matrix. While it works, it is a bit magick and should be avoided IMHO. Which is also why op is unable to do what he wants." CreationDate="2017-02-06T14:20:57.527" UserId="38" />
  <row Id="6644" PostId="4655" Score="0" Text="The part you described sounds like it involves sampling in an area around the pixel currently being operated on to calculate a value to then use in a lookup table. So you your &quot;gathering&quot; by sampling in an area around the current pixel. Then your doing a dependent texture lookup. Both of those can be expensive depending on the hardware you're running on. If the area sampled in both the input texture and the lookup table is small enough, it might not be a bottleneck, but I suspect that's where it would happen. It seems OpenCL might be a better fit for that reason (to me)." CreationDate="2017-02-06T17:18:56.600" UserId="3003" />
  <row Id="6647" PostId="4675" Score="0" Text="you probably can't unless you flatten all the data in a single batch and draw that at once which would work if they all have the same shader." CreationDate="2017-02-06T22:24:09.813" UserId="1608" />
  <row Id="6649" PostId="4676" Score="1" Text="Is VSync enabled? It could be the CPU is waiting for the next available frame. If the GPU takes say 20ms to complete its tasks, the CPU will wait for the next frame which would occur at ~33ms (16.6x2ms), meaning the GPU will idle for ~13ms." CreationDate="2017-02-07T03:06:21.333" UserId="3073" />
  <row Id="6650" PostId="4673" Score="0" Text="For sake of OpenGL 2.X, I'll probably stick with CPU skinning. Good to hear I'm not misunderstanding things." CreationDate="2017-02-07T03:24:59.937" UserId="5991" />
  <row Id="6651" PostId="4668" Score="0" Text="@joojaa &quot;olts&quot;? Not sure I understood your comment." CreationDate="2017-02-07T03:58:05.867" UserId="6003" />
  <row Id="6652" PostId="4668" Score="0" Text="Typo, it should read 'its'. The source you use uses a transpose trick just clear the matrix. Usually you shouldnt do this many steps beween calls as it confuses other programmers . Anyway the code is the same minus the extra matrix manipulation. I wont be writing code because your code does not tell me what all your conventions are." CreationDate="2017-02-07T04:49:40.500" UserId="38" />
  <row Id="6654" PostId="4668" Score="0" Text="@joojaa When you say &quot;minus the extra matrix manipulation&quot;, do you mean that the matrix from the reference above should simply be transposed so that it ends up in a column-major order or were you referring to something else? My intention is to use column-major matrices, but the one above was arranged in row-major based on how it was shown in the reference (which could've been a consistency issue fixed later in the ref) I've been going over my stuff and re-checking my understanding and other things, which is why it's taking a bit longer than usual for me to reply to comments. Thanks." CreationDate="2017-02-07T11:16:54.617" UserId="6003" />
  <row Id="6655" PostId="4669" Score="0" Text="@user18490 I understand and appreciate that you're trying to help, and I've been re-checking my stuff under the working hypothesis that I really understand absolutely nothing (even other refs), trying to discover which part(s) I misunderstood. I never intended to come across as arrogant, if that's your impression (really, I never would've posted a public question if that were the case), but it's a bit over the top to suggest that I know absolutely nothing about it, which is also too generic/vague to help me figure out which particular detail(s) I may have misunderstood. Hope that makes sense." CreationDate="2017-02-07T11:26:46.943" UserId="6003" />
  <row Id="6656" PostId="4668" Score="0" Text="@rayits From the look of it its  inverted after initial placement. See the ivnerse of a orthogonal matrix (a pure rotation) is its transpose. And ofcourse the inverse of a move is just negative move. Thats why it looks like its wrog order. Its right, just that many operations are stacked in one line of code." CreationDate="2017-02-07T13:09:50.567" UserId="38" />
  <row Id="6659" PostId="4669" Score="0" Text="@user18490 Well, I've been going over my other references (including OpenGL Super Bible 6 on matrices, model-view transform, and look-at matrix topics), but I'm still unable to figure out what part I've misunderstood to such a spectacular degree. (The code in edit-3 did not work for me.) I had noticed SB6 referring to the 'look-at' matrix as the 'view matrix' (p.77, 79) and thought that both were the same thing until I got burned (look-at and view matrices get built differently), so now I think of them as different matrices. I mention this in case it's comes across as related to my issue." CreationDate="2017-02-08T09:23:27.527" UserId="6003" />
  <row Id="6660" PostId="4669" Score="0" Text="@user18490 BTW, I'm also familiar with [this other article](http://www.3dgep.com/understanding-the-view-matrix/#The_View_Matrix) on the topic, which I had read a while back, but I'm re-reading yet again for good measure." CreationDate="2017-02-08T09:24:47.733" UserId="6003" />
  <row Id="6661" PostId="4682" Score="0" Text="which ones are missing? It looks correct at first glance." CreationDate="2017-02-08T09:34:48.043" UserId="137" />
  <row Id="6663" PostId="4682" Score="0" Text="[I put a hand-drawn circle into each pentagon (blue from first iteration and black for second iteration](https://i.stack.imgur.com/96y1X.png) I counted 25 black circles and 5 blue ones." CreationDate="2017-02-08T13:20:14.277" UserId="137" />
  <row Id="6664" PostId="4684" Score="1" Text="carefull vulkan and opengl expect different Z bound in clipspace. GLM should have a #define for that somewhere." CreationDate="2017-02-08T13:43:35.713" UserId="137" />
  <row Id="6665" PostId="4682" Score="0" Text="The ones in the middle of each large side. The tips from these pentagons would stick out of the big pentagon. If I add one more recurrence one sees the &quot;cracks&quot; opening up." CreationDate="2017-02-09T00:24:01.297" UserId="6023" />
  <row Id="6666" PostId="4683" Score="0" Text="I know it's not possible to tile the plane only with pentagons. I'm trying to use pentagons and rhombuses (for the gaps), which is possible in another configuration, but not in the above one. If you use the code I provided with n_iter=3, the figure generated will have big cracks of white space in it. I'm trying to avoid that." CreationDate="2017-02-09T00:44:03.217" UserId="6023" />
  <row Id="6667" PostId="4683" Score="0" Text="One strategy for growing the pentagon &quot;lattice&quot; above, would be to add another pentagon to each side of the small pentagons whenever possible, without overlapping one another. It would be like a crystal growth. One gets a quasicrystal like in this link: http://www.pnas.org/content/93/25/14271/F4.large.jpg, in this case one needs 3 basic shapes. I think though the recursive method won't work." CreationDate="2017-02-09T00:56:56.663" UserId="6023" />
  <row Id="6668" PostId="4683" Score="0" Text="Ah, sorry, I misunderstood what you were trying to do. I'll see if I can come up with something that addresses your actual question (no promises!)." CreationDate="2017-02-09T05:13:34.417" UserId="3003" />
  <row Id="6669" PostId="4689" Score="0" Text="which matrix lib?" CreationDate="2017-02-09T11:03:04.430" UserId="137" />
  <row Id="6670" PostId="4689" Score="0" Text="@ratchetfreak vecmath" CreationDate="2017-02-09T11:05:49.667" UserId="6029" />
  <row Id="6671" PostId="4688" Score="0" Text="I needed something similar but used a biased approach in the end: Fibonacci spiral sampling (Quasi-Monte Carlo)." CreationDate="2017-02-09T11:17:16.930" UserId="2287" />
  <row Id="6672" PostId="4685" Score="0" Text="Let me get this straight. So the 3 arguments to lookAt are eye, center, up.  Eye is the position of the camera.  I'm kind of confused on what center does.  Up is the where the top of the camera is pointing?" CreationDate="2017-02-09T13:19:08.063" UserId="6025" />
  <row Id="6673" PostId="4691" Score="1" Text="And where is the stratification? This is just uniform cosine weighted sampling?" CreationDate="2017-02-09T13:29:54.207" UserId="2287" />
  <row Id="6674" PostId="4685" Score="0" Text="`center` is the point the eye is looking at, i.e. the point that will be in the middle of the screen. `up` is the direction (not a point) the top of the camera is pointing; i.e. if you draw a line from `center` in the direction `up`, this line will be in the 12 o'clock position on the screen." CreationDate="2017-02-09T13:49:41.377" UserId="2041" />
  <row Id="6675" PostId="4692" Score="0" Text="I just looked up homography on wikipedia and it looks to me, for 2D, it's performing a mapping much like perspective texturing which means (using homogeneous coordinates) you'll need a 3x3 matrix where the bottom row isn't just 0,0,1.  You can't form that with just rotations and translations as their matrices will always have 0,0,1 in the bottom row and, hence, so will their products." CreationDate="2017-02-09T13:49:51.433" UserId="209" />
  <row Id="6676" PostId="4691" Score="1" Text="The projection preserves stratification. I will update the answer with that precision." CreationDate="2017-02-09T13:53:37.973" UserId="5717" />
  <row Id="6677" PostId="4691" Score="0" Text="How does one control the number of samples per stratum? Or do we need to?" CreationDate="2017-02-09T14:49:17.190" UserId="6001" />
  <row Id="6678" PostId="4691" Score="1" Text="@Steven I think you'll usually want one sample per stratum (eg. one sample per grid cell in 2D) to make best use of stratified sampling. Unless you have some reason to use a fixed stratification with a variable number of samples." CreationDate="2017-02-09T17:55:37.713" UserId="5717" />
  <row Id="6679" PostId="4693" Score="0" Text="Although it's the case for the obvious mapping here, not all mappings preserve stratification. The [Box-Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform) is a common one which does not." CreationDate="2017-02-09T17:58:12.093" UserId="5717" />
  <row Id="6680" PostId="4693" Score="1" Text="@Olivier Box–Muller maps e.g. disjoint boxes in the 2D input space to disjoint annular sectors of the 2D output space, so it preserves stratification in that sense at least. Do you mean that it doesn't if you consider it as a random 1D mapping?" CreationDate="2017-02-09T18:12:17.870" UserId="48" />
  <row Id="6682" PostId="4693" Score="0" Text="I ended up successfully using this technique - I simply used my cosineWeightedSample(u,v) function and stratified my u,v inputs. Thanks" CreationDate="2017-02-09T22:55:53.107" UserId="6001" />
  <row Id="6683" PostId="4693" Score="0" Text="@NathanReed yes, I believe you can put it that way. You can't generate a properly stratified 1D normal distribution with it. I suppose you could consider the 2D output stratified, if in a weird way." CreationDate="2017-02-10T00:06:17.437" UserId="5717" />
  <row Id="6684" PostId="4698" Score="1" Text="If you've found the solution, please go ahead and answer your question. :)" CreationDate="2017-02-10T10:32:10.340" UserId="182" />
  <row Id="6685" PostId="4689" Score="0" Text="Have you printed out the original matrix to ensure its constructed OK? Can you step through code to check the scalar multiplication operator is being called and verify that a temporary matrix/vector isn't being constructed? Can you post a link to this vecmath library?" CreationDate="2017-02-11T00:19:18.350" UserId="2941" />
  <row Id="6686" PostId="4503" Score="0" Text="You guys are really not making much of an effort. A bit of Googling would have pointed you to https://www.scratchapixel.com/lessons/3d-basic-rendering/computing-pixel-coordinates-of-3d-point" CreationDate="2017-02-11T15:12:09.910" UserId="1608" />
  <row Id="6687" PostId="4698" Score="0" Text="You have ample resources on the Web to learn about basic geometry and basic principle of 3D rendering such as https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/math-operations-on-points-and-vectors and https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading for an introduction to cross product and shading. It will be **very** hard for to try to get something as advanced as bump mapping working if you don't even understand what a cross product is. Learn the basic first, then progress step by step. People are not here to teach." CreationDate="2017-02-11T15:27:54.290" UserId="1608" />
  <row Id="6688" PostId="4704" Score="0" Text="if you want to model the sun as an area light then you can also make it a directional light but instead of a single angle make it centered around a cone" CreationDate="2017-02-11T16:00:31.973" UserId="137" />
  <row Id="6689" PostId="4698" Score="4" Text="@user18490 although we expect people to attempt to solve things themselves first, this question shows effort and describes the calculations and learning done so far, and I don't see a problem with it. I like your links to relevant resources but I strongly disagree with &quot;People are not here to teach&quot;. There are lots of people here who put huge amounts of their time in to writing very helpful explanations in their many answers." CreationDate="2017-02-12T01:06:30.010" UserId="231" />
  <row Id="6690" PostId="4698" Score="0" Text="@trichoplax. I understand your point but try in a constructive way to prove my point. Site like stack overflow are essentially there for people to ask questions and people to answer these questions. While you can always argue that any question is a valid question, answering things such as &quot;how do we render an image of a 3D model&quot; or in that case, &quot;how I do I do bump mapping on a sphere while I don't even know what a cross product is&quot; are just for people willing to help on these forum, questions that are unarguably too large and can't be answered unless you write a book. That's teaching." CreationDate="2017-02-12T08:24:19.817" UserId="1608" />
  <row Id="6691" PostId="4710" Score="0" Text="Dude i've read all of those. And maybe i forgot to mention but i am working with floats. What do you expect when i say everything is in range [0,1] surely it can't be 0 and 1. &#xA;&#xA;Again i think you failed to understand my questions 1) and 3). I've read books and i know Ks and Kd are blah blah blah coefficients. But what exactly are those when it comes to putting numbers? Are those the diffuse and specular color or something else? Let me update my question with books references." CreationDate="2017-02-12T09:05:45.763" UserId="6046" />
  <row Id="6692" PostId="4710" Score="0" Text="@wandering-warrior: your commend is border line polite. I am not a dude to start with. Please stay within the limits of correctness. Second I spent a share amount of time answer your question so I would appreciate some appreciation of that. If you feel I don't answer your problem, maybe you start questioning whether your questions are properly formulated." CreationDate="2017-02-12T09:10:37.233" UserId="1608" />
  <row Id="6693" PostId="4710" Score="0" Text="Well sorry i can't tell your gender sitting in front of my LED. My native language isn't english so didn't know people get pissed off on the net if called a 'dude'. Yes i feel you don't answer the question because you didn't read it carefully. I clearly wrote &quot;If Ks and Kd are the surface color or the reflectance ratio and if they are the ratio what their value should be?&quot; You failed to answer what their value should be?" CreationDate="2017-02-12T09:18:03.270" UserId="6046" />
  <row Id="6694" PostId="4710" Score="2" Text="Sorry, cant help it but I'm *considering* downvoating based on op's attitude and not on the questions merit. Dude, loose the attitude and have a little courtesy. We're all here to help.@user18490 made a more than decent effort to answer your question." CreationDate="2017-02-12T10:19:32.433" UserId="3128" />
  <row Id="6696" PostId="4687" Score="0" Text="Thank you for the guidance. FBM seems like a suitable noise function to recreate the footage." CreationDate="2017-02-12T11:55:14.707" UserId="6028" />
  <row Id="6697" PostId="4696" Score="0" Text="Thank you for the guidance. The codebase you linked and the sample you provided helped me understand and implement my own function." CreationDate="2017-02-12T11:57:50.093" UserId="6028" />
  <row Id="6698" PostId="4710" Score="0" Text="@wandering-warrior: I answered your question very precisely. But you need to get outside of the context of the code that you are using. I said that Kd and Ks are coefficients (they are not colors) and that should take values between 0 and 1 and that their sum should be ideally equal to 1. I also mentioned that you need to multiply these coefficients by the functions diffuse() and specular() which return the diffuse and specular responses of the surface. These functions are defined by shading model you use. If you are not familiar with these concepts you need to study them first." CreationDate="2017-02-12T12:08:02.057" UserId="1608" />
  <row Id="6699" PostId="4710" Score="0" Text="*ahem* Ok you answered half.  &quot;For example if i have a color rgb(66, 170, 244) which is somewhat light blue. What should the Ks and Kd be for the above color?&quot; If these aren't the color where can i get their values? Any reference?" CreationDate="2017-02-12T12:47:49.320" UserId="6046" />
  <row Id="6700" PostId="4698" Score="0" Text="@user18490 I agree we do not want questions that are too broad. It seems we disagree on whether this particular question is too broad (I see the body of the question as more specific than the broad title). However, this is a community decision, and you are part of the community, so I encourage you to suggest closing the question, and then the community will vote on whether to close. [More information on flagging to close questions here.](http://meta.stackoverflow.com/questions/334115/without-close-flag-privileges-how-do-i-bring-a-low-quality-question-to-the-comm/334118)" CreationDate="2017-02-12T13:39:22.857" UserId="231" />
  <row Id="6702" PostId="4709" Score="1" Text="CGPP is dated regarding PBR and statements like Kd+Ks&lt;=1 isn't correct, assuming they stand for diffuse &amp; specular albedo. It's rather that integral of BRDF over hemisphere &lt;= 1 for energy conservation" CreationDate="2017-02-12T14:18:49.130" UserId="1952" />
  <row Id="6703" PostId="4710" Score="1" Text="@wandering-warrior please be respectful to the people who are giving their time to help you. Whether you intended so or not, the repeated use of the phrase &quot;you failed&quot; comes across as aggressive and ungrateful. As for the confusion over the term &quot;dude&quot;, it's worth noting that it is a [**slang term with a long history**](https://en.wikipedia.org/wiki/Dude) and a wide range of meanings, from complimentary to insulting. It's safest to avoid slang terms here as the intended meaning will not always match the understood meaning." CreationDate="2017-02-12T14:48:42.383" UserId="231" />
  <row Id="6705" PostId="4709" Score="0" Text="@JarkkoL - does that mean when i normalize blinn phong, there is no need for Kd+Ks &lt;= 1?" CreationDate="2017-02-12T15:18:05.287" UserId="6046" />
  <row Id="6706" PostId="4710" Score="0" Text="@Erik Thanks for speaking out about this. Hopefully I've made it clear that disrespectful behaviour is not acceptable here. Note that you can flag any question, answer or comment for moderator attention, and we will appreciate the chance to step in and calm things down." CreationDate="2017-02-12T15:51:04.800" UserId="231" />
  <row Id="6707" PostId="4704" Score="0" Text="Thanks for the answer. What I basically try to calculate is the Sun direct irradiance at few surface points(in a room floor). Knowing the clock time and site latitude, longitude I can get sun angular position. Is it then valid to use this single Sun direction for all the sensors(assuming I go with your first answer, Sun as directional source)?" CreationDate="2017-02-12T20:01:24.350" UserId="6041" />
  <row Id="6708" PostId="4704" Score="1" Text="Ratchet, did I understand it correctly that you mean instead of sending a shadow ray to a fixed direction, sampling the direction from a solid angle around that fixed direction?" CreationDate="2017-02-12T20:08:37.037" UserId="6041" />
  <row Id="6709" PostId="4713" Score="0" Text="I was suspicious of precision problems, only I was looking at compression until I got overwhelmed. Thank you so much, this helped me fix it." CreationDate="2017-02-12T20:46:53.437" UserId="6049" />
  <row Id="6710" PostId="4698" Score="1" Text="I was attempting to derive from first principles how to bump map a ray-traced sphere, and had what I thought was a reasonable question (exactly how do you know which order u x v or v x u will get you a normal pointing away from the sphere center.) I don't consider my question a question in basic geometry..." CreationDate="2017-02-13T00:40:04.110" UserId="6033" />
  <row Id="6711" PostId="4698" Score="0" Text="Also, I wasn't able to find an answer to my question in any of the 3D graphics texts I had or on the Web. If you've got such a link, please post it." CreationDate="2017-02-13T00:47:44.100" UserId="6033" />
  <row Id="6712" PostId="4712" Score="1" Text="I actually posted a relevant answer on a more general SO question. http://stackoverflow.com/questions/12524623/what-are-the-practical-differences-when-working-with-colors-in-a-linear-vs-a-no/12894053#12894053" CreationDate="2017-02-13T09:53:15.420" UserId="2041" />
  <row Id="6715" PostId="4703" Score="0" Text="Are you looking to preserve the *ratio* between angles, to keep things in proportion, or to preserve the exact angle values? In general the angles cannot be preserved as angles around a point do not necessarily add up to 360 degrees in a non-planar surface." CreationDate="2017-02-13T16:42:38.533" UserId="231" />
  <row Id="6716" PostId="4703" Score="0" Text="The ratio. I will edit the post. Thank you for pointing this out." CreationDate="2017-02-13T17:13:07.877" UserId="6044" />
  <row Id="6717" PostId="4703" Score="0" Text="@trichoplax Angles around a point do add up to 360° as long as the surface is smooth (a small patch around the point looks like a plane). You might be thinking of the angles inside a polygon? For instance, the interior angles of a triangle don't add up to 180° on a curved surface in general." CreationDate="2017-02-13T22:20:27.567" UserId="48" />
  <row Id="6720" PostId="4703" Score="0" Text="Please see Nathan's comment which explains my misunderstanding (thanks Nathan - that is indeed what I was confusing it with)" CreationDate="2017-02-14T14:45:05.090" UserId="231" />
  <row Id="6721" PostId="2004" Score="0" Text="Have you found any demos with Maximum mipmaps? Thanks." CreationDate="2017-02-14T15:31:06.090" UserId="5756" />
  <row Id="6722" PostId="4703" Score="0" Text="@NathanReed I'm doubting myself now, but I think my error was in terminology (talking about surface instead of mesh), and that the intended point stands. Although the smooth surface that is being approximated by a triangle mesh is locally planar, the mesh itself, as a polyhedron, does not have angles around each vertex that sum to 360°. So an angle preserving projection can be made from a surface to a plane, but the angles are dependent on which surface was chosen to fit to the vertices of the mesh prior to projection (unless the original surface is known). Is this relevant here?" CreationDate="2017-02-14T16:09:47.237" UserId="231" />
  <row Id="6723" PostId="4718" Score="1" Text="`max_vertices = 93) out; //MAX 128` What does that mean? You specified a max of 93, so why does your comment say otherwise?" CreationDate="2017-02-14T16:44:53.840" UserId="2654" />
  <row Id="6724" PostId="4718" Score="3" Text="Also, it's not clear what this subdivision form is trying to do. It looks vaguely like you're linearly interpolating between the triangle positions (in clip space, no less). I'm not sure what that's supposed to accomplish or what the &quot;correct&quot; normals for that would even be." CreationDate="2017-02-14T17:02:22.967" UserId="2654" />
  <row Id="6727" PostId="4723" Score="1" Text="Not sure I'm understanding your question. Are you asking how do you sample from the mixture of $n$ lights? If you want to sample two or more lights, you would just repeat the procedure to sample a single light (with fresh random numbers), right? Or is there something else you're asking?" CreationDate="2017-02-14T23:18:15.520" UserId="48" />
  <row Id="6728" PostId="4721" Score="0" Text="Thanks for the suggestions. I've read Valve's paper on Radiosity and Normal Mapping. They describe projecting the results of the radiosity onto the three basis vectors. They don't distinguish between direct and indirect lighting. I don't think you want to consider normal mapping in the baker - my understandings is that the point of the three basis is so that the normals can access the full hemisphere of radiosity at runtime when performing bump mapping." CreationDate="2017-02-15T04:54:26.423" UserId="6001" />
  <row Id="6729" PostId="4716" Score="0" Text="Thanks. I'm struggling to follow the derivation somewhat. If I'm not wrong, every entry of the $H_1$ matrix should be multiplied by a factor of $\frac{1}{8}$." CreationDate="2017-02-15T05:08:55.283" UserId="5905" />
  <row Id="6732" PostId="4703" Score="0" Text="For meshes angles do not add up to 360°." CreationDate="2017-02-15T14:12:25.827" UserId="6044" />
  <row Id="6734" PostId="4721" Score="0" Text="Of course. The whole point of Radiosity is that it is indirect, so Normal mapping isn't really part of the total energy distribution solution. Of course, you could subdivide the surface even further to the level of individual bumps (in the normal map), but at that point you're at a subpixel level and that brings whole lot of other issues. Then again, if you had a CUDA Radiosity shader, it should still be doable." CreationDate="2017-02-15T16:05:52.540" UserId="6063" />
  <row Id="6735" PostId="4721" Score="0" Text="Wait - what? Light maps are almost always lower resolution than the diffuse and bp maps. That's why i am trying to generate directional lightmaps to encode the static lighting so i can &quot;light&quot; the normal maps at runtime." CreationDate="2017-02-15T16:22:35.593" UserId="6001" />
  <row Id="6736" PostId="4721" Score="0" Text="Yeah, well that's the decision that only you can make, as only you know how high/low frequency the diffuse maps are and what is the actual visual style you are aiming for. But with Radiosity, you really don't want to use low-resolution lightmaps. I mean, the shadow boundary that Radiosity produces is one of the most beautiful things in computer graphics. You don't want to loose that to save some memory, now do you ? Do you happen to have any screenshots you are willing to share so we know what exactly are we talking about here ?" CreationDate="2017-02-15T16:26:52.767" UserId="6063" />
  <row Id="6738" PostId="4727" Score="0" Text="Have you checked the MaxSDK ? I imagine here may be a good start &gt; http://docs.autodesk.com/3DSMAX/16/ENU/3ds-Max-SDK-Programmer-Guide/index.html?url=files/GUID-40ED5D02-BBCF-4EE3-9EE7-E59425B49CBB.htm,topicNumber=d30e2631" CreationDate="2017-02-16T08:05:48.203" UserId="3073" />
  <row Id="6739" PostId="4727" Score="3" Text="Alternatively you could use the open source AssImp library to import scenes and render them inside your own program, rather than getting your program to interoperate with Max, which I guess would involve more effort." CreationDate="2017-02-16T08:07:52.733" UserId="3073" />
  <row Id="6740" PostId="4723" Score="0" Text="Thanks Nathan, you are correct you as to do the procedure again for multiple lights. But I wonder if the above mixture density allows one to pick two or more samples from the distribution." CreationDate="2017-02-16T12:08:29.090" UserId="6041" />
  <row Id="6741" PostId="4729" Score="0" Text="Thanks, this kind of method for fractionally distributing brightness among pixels is what I was looking for. Has this technique a name? I  thought to add brightness to pixels based on the distance between their center and the point coordinates (up to a certain threshold) but this is simpler." CreationDate="2017-02-16T14:18:26.937" UserId="6072" />
  <row Id="6742" PostId="4726" Score="1" Text="I'm not sure I've understood what's going wrong, but couldn't you just repeatedly render to a texture and then just draw a single textured quad when your texture is how you like it?" CreationDate="2017-02-16T16:55:58.680" UserId="2041" />
  <row Id="6743" PostId="4726" Score="2" Text="Have you remembered to glClear() between each iteration of the loop (or ensure that you're drawing over every pixel in your &quot;function to adjust&quot;)?" CreationDate="2017-02-16T16:56:59.633" UserId="2041" />
  <row Id="6744" PostId="4313" Score="0" Text="Thank you, you explained it really well and I was able to visually understand what you just wrote. Also, sorry for replying so late. -Sami" CreationDate="2017-02-16T17:08:08.920" UserId="5513" />
  <row Id="6747" PostId="4727" Score="1" Text="What would you like to compare your scenes with? With the output of the internal 3DS Max renderer? I was told by a Corona developer that the MaxSDK was painful to work with and also not well documented (at least in previous versions), so be prepared for that ;-)" CreationDate="2017-02-16T19:15:53.963" UserId="2479" />
  <row Id="6748" PostId="4726" Score="1" Text="I have tried doing glCear(GL_BACK_LEFT) between each iteration but it is not clearing, I'm wondering if I'm missing a line of code before the glClear, or if I'm calling it right. I think the problem is that the back buffer isn't clearing" CreationDate="2017-02-16T21:09:07.387" UserId="5183" />
  <row Id="6750" PostId="4731" Score="0" Text="It doesn't show a different thumbnail for me when downloaded." CreationDate="2017-02-17T05:48:42.393" UserId="3003" />
  <row Id="6751" PostId="4731" Score="1" Text="Probably this feature got lost when imgur reapplied compression. Some formats embed a separate thumbnail, if they do then nothing stops the thumbnail from being a different picture." CreationDate="2017-02-17T06:03:53.650" UserId="38" />
  <row Id="6752" PostId="4731" Score="0" Text="also possibly its same as http://thume.ca/projects/2012/11/14/magic-png-files/" CreationDate="2017-02-17T06:10:56.710" UserId="38" />
  <row Id="6753" PostId="4730" Score="0" Text="What do you think would be faster and easier to implement? Using the MaxSDK or using Asslmp to import the scenes into my ray tracer? If I use the Asslmp method will it just import geometry will I have to define materials &amp; textures myself?" CreationDate="2017-02-17T07:38:37.913" UserId="5256" />
  <row Id="6754" PostId="4730" Score="2" Text="AssImp imports materials and uvs. IIRC AssImp uses a key/value system for materials, so it can handle every material parameter type you can imagine. For me personally, I would prefer to import via AssImp, I hear MaxSDK can be frustrating." CreationDate="2017-02-17T09:59:17.703" UserId="3073" />
  <row Id="6755" PostId="4730" Score="2" Text="Yeah, I've worked with the Max SDK before. It's badly designed and implemented, full of pitfalls and undocumented &quot;features&quot;, and a lot of the sample code doesn't work. I'd recommend avoiding it if you can. That said, I don't know how good AssImp is, and I don't know if it will evaluate materials for you or if you'll have to reimplement all the materials yourself - and of course I wouldn't expect it to work with other Max shader or light plugins." CreationDate="2017-02-17T11:05:58.453" UserId="2041" />
  <row Id="6756" PostId="4726" Score="2" Text="There's a clear mask that affects which buffers are cleared. But I'm just guessing at things here, since you haven't posted a minimal, verifiable example." CreationDate="2017-02-17T16:47:30.573" UserId="2041" />
  <row Id="6758" PostId="4733" Score="0" Text="Actually I read that it is possible with PNG files, but the thing which amused me and brought me here is that, how this trick is applied to JPEG format." CreationDate="2017-02-18T14:43:02.830" UserId="6074" />
  <row Id="6759" PostId="4737" Score="2" Text="Have you made any headway on this by yourself, or any ideas?" CreationDate="2017-02-19T18:32:26.620" UserId="2941" />
  <row Id="6760" PostId="4737" Score="3" Text="Its pretty common to feel this way in higher education, i know i did. In reality its a issue with yourself. Sure the educators aren't stellar but that is simply because your coming nearer the boundary of know how and your ability to work in several different levels of abstractions and several levels of uncertainty comes to play. (something that is very poison to certain mind frames. You'll get over this in about 5 years). The question itself is quite good. If you ever do any robotics, 3D graphics or engineering you'll be glad. It measures wether you undertood the 3 first thing you were taught." CreationDate="2017-02-19T18:56:18.617" UserId="38" />
  <row Id="6761" PostId="4736" Score="0" Text="Well, kind of, if your target is to do the half toned image. But the target is usually to do the original image and your now 2 steps away form achieving that. Its a bit like swapping a fillet stake dinners ingredients with that of a sandwich and saying its easier to do. Sure." CreationDate="2017-02-19T19:05:20.150" UserId="38" />
  <row Id="6762" PostId="4723" Score="1" Text="Are you worried about anything in particular with just repeating the process multiple times? Were you concerned about sometimes choosing the same light more than once?" CreationDate="2017-02-19T20:54:59.087" UserId="231" />
  <row Id="6763" PostId="4676" Score="0" Text="Hey PaulHK sorry my stack overflow account here is brand new, and something messed up with my registration so I cannot comment on my own post. Your idea that it is v-sync is an interesting one. *[converted from an answer - rest of content too long for a comment moved to the question - trichoplax]*" CreationDate="2017-02-07T05:13:19.720" UserId="6016" />
  <row Id="6764" PostId="4723" Score="0" Text="@trichoplax. No. I just want to understand mathematically how one choose multiple samples from a merge density." CreationDate="2017-02-20T12:53:42.807" UserId="6041" />
  <row Id="6765" PostId="4736" Score="0" Text="Vector formats also support colour gradients etc, for which palette reduction would be highly detrimental.There have been papers that describe ways of making use of this." CreationDate="2017-02-20T15:12:33.060" UserId="209" />
  <row Id="6766" PostId="4716" Score="0" Text="Thanks for the link, but in general [link-only answers are discouraged on StackExchange](http://meta.computergraphics.stackexchange.com/questions/98/how-should-we-deal-with-link-only-answers-here). I'm not suggesting you paste the entire derivation in, but perhaps you could summarize the key points (e.g. assumptions that lead to the specific coefficients) in your answer?" CreationDate="2017-02-20T16:00:30.567" UserId="48" />
  <row Id="6768" PostId="4733" Score="1" Text="@AdeshTamrakar Transparency is supported in [JPEG 2000 format](https://en.wikipedia.org/wiki/JPEG_2000)." CreationDate="2017-02-20T16:32:31.600" UserId="48" />
  <row Id="6770" PostId="4741" Score="3" Text="Is the region always convex, or might it be concave?" CreationDate="2017-02-21T00:49:07.873" UserId="48" />
  <row Id="6771" PostId="4741" Score="1" Text="It could be either - it's guaranteed to be enclosed, though." CreationDate="2017-02-21T00:50:45.137" UserId="6100" />
  <row Id="6772" PostId="4740" Score="1" Text="For generating SDFs, you can brute force it doing something like: For every empty pixel, find the nearest 'solid' pixel by searching the entire bitmap (smallest distance wins). Not a cheap method but plenty of room of optimisation later. For downscaling you should be taking averages of distance over the area you are sampling from." CreationDate="2017-02-21T04:13:14.657" UserId="3073" />
  <row Id="6773" PostId="4740" Score="0" Text="Will using ctx.drawImage would work for the downscaling?" CreationDate="2017-02-21T04:55:55.473" UserId="6099" />
  <row Id="6775" PostId="4740" Score="0" Text="Also, how do I normalize the distance representation into apha values" CreationDate="2017-02-21T07:10:51.820" UserId="6099" />
  <row Id="6776" PostId="4740" Score="0" Text="If you are encoding as 8bit alpha you would normalise with something like    Alpha[n] = distField[n] * 255 / MaxDist;    were MaxDist is the largest distance in your field, this requires you to pass MaxDist as a uniform to your shader so it can unpack your alpha buffer properly. You could maybe get away with not not needing MaxDist if you have fixed texture dimensions. Ideally you should be using a floating point format though." CreationDate="2017-02-21T07:16:01.577" UserId="3073" />
  <row Id="6777" PostId="4742" Score="0" Text="One method to make this work faster is to perform a prepass on your array of vertices, duplicating or merging very close vertices and removing vertices that are in a straight line between two neighbouring vertices. They won't contribute to the resulting mesh in a meaningful way so they don't need to be considered. The amount of error you want to consider valid is up to you. This is a good approach when dealing with user input which is inherintly 'dirty'" CreationDate="2017-02-21T16:41:57.947" UserId="6001" />
  <row Id="6778" PostId="4744" Score="0" Text="Better question would be, should you have scene graph in rendering engine at all ;)" CreationDate="2017-02-22T04:04:18.427" UserId="1952" />
  <row Id="6779" PostId="4753" Score="0" Text="It should be $L = \frac{d\phi}{d\omega dAcos(\theta)}$" CreationDate="2017-02-22T13:54:47.150" UserId="1952" />
  <row Id="6780" PostId="4753" Score="0" Text="Yeah, sorry, fixed that now." CreationDate="2017-02-22T14:14:19.107" UserId="1831" />
  <row Id="6781" PostId="4747" Score="0" Text="Interesting, thanks for that. I'm using rust and GLium, which I don't think has bindings to this but I could look into contributing a patch." CreationDate="2017-02-22T17:38:33.153" UserId="6100" />
  <row Id="6782" PostId="4743" Score="1" Text="This is a really neat technique thanks for that. I think I managed to find an off-the-shelf tesselator I can use, but I might have to try this out just because it's a nifty idea." CreationDate="2017-02-22T17:40:27.177" UserId="6100" />
  <row Id="6783" PostId="4756" Score="0" Text="I know calculating irradiance, for example, cancels out the term, but this doesn't answer my question regarding radiance itself.  More precisely, how can it approach infinity when theta is approaching 90." CreationDate="2017-02-22T17:53:08.380" UserId="1831" />
  <row Id="6784" PostId="4740" Score="0" Text="Hi Paul, thank you for your answer. Can you elaborate on the downscaling?" CreationDate="2017-02-22T18:03:20.280" UserId="6099" />
  <row Id="6785" PostId="4756" Score="1" Text="Because if the flux stays constant, you need infinite radiance that there's any flux through a surface that's perpendicular to the light source" CreationDate="2017-02-22T18:05:59.653" UserId="1952" />
  <row Id="6788" PostId="4753" Score="1" Text="I've posted an answer to this, but have been thinking about it and think I can see what you might be getting at. If there was some differential flux (even a teeny weeny bit) at a grazing angle and $\theta$ was **very close** to $90°$, then the radiance would be very large because of the very small denominator. Is this what you're getting at? If so, I'm not sure what the explanation would be and wonder &quot;is there any surfaces that would  have a significant $d\phi$ at a grazing angles?&quot;" CreationDate="2017-02-22T20:11:00.083" UserId="2941" />
  <row Id="6791" PostId="4756" Score="0" Text="It makes sense that the light &quot;has&quot; to be more intense in this context, indeed." CreationDate="2017-02-22T21:25:36.877" UserId="1831" />
  <row Id="6794" PostId="4760" Score="0" Text="billboards, flat squares rotated to always face the camera" CreationDate="2017-02-23T08:13:40.710" UserId="137" />
  <row Id="6795" PostId="4761" Score="1" Text="The divide by 2 is because you're taking the neighbouring vertices height, which happen to be 2 units apart (see the diagram in your second link which lables in/out and left/right neighbour vertices). It's to normalise height (Y) with X/Z." CreationDate="2017-02-23T09:11:48.560" UserId="3073" />
  <row Id="6797" PostId="4744" Score="0" Text="@JarkkoL why wouldn't you? how could you otherwise impose relative movements in the scripting of a game engine for instance?" CreationDate="2017-02-23T13:53:58.917" UserId="2287" />
  <row Id="6798" PostId="4352" Score="0" Text="In fact you should consider the complete EM spectrum. Furthermore, the question does not specify whether a human eye must perceive the resulting images." CreationDate="2017-02-23T13:57:14.153" UserId="2287" />
  <row Id="6799" PostId="4744" Score="0" Text="@Matthias just explicitly update transformation of such objects. relative movement is extremely rare in practice and should be dealt as an exception, not as part of common engine data structure." CreationDate="2017-02-23T14:03:19.187" UserId="1952" />
  <row Id="6801" PostId="4767" Score="0" Text="Where are you getting this infinity from? Is this a formula you're using, or something you've read, or something about a particular shader?" CreationDate="2017-02-24T12:58:23.320" UserId="2041" />
  <row Id="6802" PostId="4767" Score="0" Text="@DanHulme Question updated." CreationDate="2017-02-24T13:39:03.750" UserId="5944" />
  <row Id="6803" PostId="4767" Score="0" Text="What's your maths background? Are you familiar with the Dirac delta distribution?" CreationDate="2017-02-24T13:40:04.643" UserId="2041" />
  <row Id="6804" PostId="4767" Score="0" Text="@DanHulme Haven't heard of that... Learning CG always makes me worry about my math." CreationDate="2017-02-24T13:44:02.973" UserId="5944" />
  <row Id="6806" PostId="4769" Score="0" Text="Thanks for your time and patience! I've understand what you mean. However, I doubt that the total area under the BRDF curve(you mean the surface in the hemisphere coordinate system?) is the albedo. In my another question [Energy conservation of BRDF](http://computergraphics.stackexchange.com/questions/4768/energy-conservation-of-brdf), the last equation $\forall\Phi:\int_{\Omega_x}f_r(x,\Phi\to\Theta)cos(N_x,\Theta)d\omega_\Theta\le1$ is the necessary and sufficient condition for energy conservation.(To be continued)" CreationDate="2017-02-24T16:37:34.243" UserId="5944" />
  <row Id="6807" PostId="4769" Score="0" Text="So I think albedo maybe equal $\int_{\Omega_x}f_r(x,\Phi\to\Theta)cos(N_x,\Theta)d\omega_\Theta$ (I'm not sure since I haven't found any relative document for the time). An additional factor $cos(N_x,\Theta)$ is in the formula. From this formula, if we consider the situation that it equals 1 and the BRDF is constant, we can get the BRDF equals $\frac{1}{\pi}$ but not $\frac{1}{2\pi}$ since $\int_{\Omega_x}cos(N_x,\Theta)d\omega_\Theta=\pi$." CreationDate="2017-02-24T16:37:41.743" UserId="5944" />
  <row Id="6808" PostId="4394" Score="0" Text="Not sure if you're still visiting this site, but I've got a question about your Fresnel equation and have posted it [here](http://computergraphics.stackexchange.com/questions/4771/fresnel-and-specular-colour)" CreationDate="2017-02-24T21:27:40.853" UserId="2941" />
  <row Id="6809" PostId="4768" Score="0" Text="I'm working through the same book and coincidentally went through that chapter (again) in the last few days. Are you aware of the [UC Davis lectures on YouTube](https://www.youtube.com/playlist?list=PLslgisHe5tBPckSYyKoU3jEA4bqiFmNBJ) that use this book? Unfortunately the lecturer doesn't address your questions specifically in the [BRDF lecture](https://www.youtube.com/watch?v=ytRrjf9OPHg). I was formulating an answer for you but I'm not **fully** understanding it either, so I'll not reply as I don't want to misinform. I will try to write my thoughts about it when I have time though..." CreationDate="2017-02-24T23:12:27.960" UserId="2941" />
  <row Id="6811" PostId="4772" Score="0" Text="Thanks. So to come up with the specular colour for gold, I'd need to lookup the wavelengths for red, green, and blue components. With those wavelengths I lookup the corresponding refractive indices for gold. Then plug them in the formula for $R_0$ and those 3 values are the components of the specular colour?" CreationDate="2017-02-25T00:56:53.017" UserId="2941" />
  <row Id="6812" PostId="4772" Score="2" Text="@PeteUK Yep, as an easy way, you can use representative wavelengths for R, G, B like that. Probably a more accurate way is to integrate $R_0(\lambda)$, multiplied by one of the [CIE color matching functions](https://en.wikipedia.org/wiki/CIE_1931_color_space#CIE_RGB_color_space), over wavelength." CreationDate="2017-02-25T00:59:43.433" UserId="48" />
  <row Id="6819" PostId="4779" Score="0" Text="Thanks! So your saying I need to create a Win32 Console Application -&gt; Import all Files and I should be good to go?" CreationDate="2017-02-26T12:12:36.847" UserId="5256" />
  <row Id="6820" PostId="4779" Score="0" Text="Try this first: https://msdn.microsoft.com/en-us/library/jj620919.aspx Then drag and drop your files and your main function instead. (And remember what I said about Precompiled Headers)" CreationDate="2017-02-26T12:43:27.577" UserId="2287" />
  <row Id="6821" PostId="4777" Score="0" Text="Do you mean subsurface scattering?" CreationDate="2017-02-26T13:07:04.467" UserId="2287" />
  <row Id="6823" PostId="4737" Score="0" Text="Hello PeteUK. Yes, I have edited the question and added my progress so far." CreationDate="2017-02-26T22:37:10.360" UserId="5513" />
  <row Id="6824" PostId="4772" Score="1" Text="@PeteUK In complement to the answer posted by -Nathan, I would cite this article (https://seblagarde.wordpress.com/2011/08/17/feeding-a-physical-based-lighting-mode), by Sebastién Lagarde, where he briefly discusses the convertion between IOR and RGB values to be used in the Fresnel equation (Schlick's approximation). The ZIP file containing the source code of the converter is disguised as a PDF and the link can be found in the section &quot;Specular color&quot;." CreationDate="2017-02-27T04:32:50.693" UserId="5681" />
  <row Id="6825" PostId="4772" Score="0" Text="Taking a closer look at the conversion program (IOR-&gt;RGB) I mentioned above, I see that it actually converts IOR values into sRGB (gamma compressed) values. Wouldn't it be the case to convert just to RGB, since the final rendered (path traced) image will be gamma compressed?" CreationDate="2017-02-27T04:46:58.820" UserId="5681" />
  <row Id="6826" PostId="4772" Score="0" Text="@ChristianPagot Perhaps it's for storing in a sRGB specular color texture. The GPU would then decode to linear RGB when it's sampled." CreationDate="2017-02-27T04:53:13.213" UserId="48" />
  <row Id="6828" PostId="4781" Score="1" Text="As I understand it, this is the math used for the &quot;french curve form&quot; https://en.wikipedia.org/wiki/Euler_spiral&#xA;I guess you would start by understanding that, an apply curve-fitting to that type of functions.&#xA;How easy it is to learn curve-fitting depends on your math background. But the literature should be easy to find." CreationDate="2017-02-28T07:25:35.960" UserId="3434" />
  <row Id="6834" PostId="4764" Score="0" Text="To be honest, I didn't read the code. Can you first clarify your question?" CreationDate="2017-02-28T11:49:07.603" UserId="6125" />
  <row Id="6835" PostId="4781" Score="0" Text="@remi000 So the method you are recommending would be similar to regression/statistical curve fitting (maybe with a more appropriate distance/error function than those typically used in statistics), and simply using Euler spiral/French curve functions as the function being fit?" CreationDate="2017-02-28T16:44:58.193" UserId="6136" />
  <row Id="6836" PostId="4764" Score="0" Text="Sorry, things got muddled while writing. How do I find the position of all joints for an inverse kinematic chain that is longer than 2 segments?" CreationDate="2017-02-28T18:14:40.730" UserId="2076" />
  <row Id="6837" PostId="4764" Score="0" Text="I think you can perhaps find explicit formulas here http://courses.csail.mit.edu/6.141/spring2011/pub/lectures/Lec14-Manipulation-II.pdf starting on slide 32. Tell me if it's not what you're looking for." CreationDate="2017-02-28T19:41:17.920" UserId="6125" />
  <row Id="6838" PostId="4786" Score="0" Text="what I have a problem with is the &quot;stride&quot; value in glVertexAttribPointer. Why would you not tightly pack an array?" CreationDate="2017-02-28T19:58:18.517" UserId="5476" />
  <row Id="6839" PostId="4746" Score="0" Text="Thanks! For now, I have the scene graph on the main thread, but it would be nice to have it in a web worker then. I would also like to do things like `node.rotation.x = 30` from the main thread, and have that update the scene graph node in the worker (effectively the main thread API is just an interface that updates the actual node in the worker). Does that seem like a good approach? I'm going to try it here: https://github.com/trusktr/infamous" CreationDate="2017-02-28T20:37:15.440" UserId="4991" />
  <row Id="6840" PostId="4786" Score="1" Text="when interleaving data by using a array of structs then the stride is the sizeof the struct." CreationDate="2017-02-28T20:46:17.737" UserId="137" />
  <row Id="6841" PostId="4764" Score="0" Text="It's close, but not exactly it. These formulas illustrate a typical 3 joint IK solve (e.g. arm, leg) with a single bend. I'm looking for the formula that factors in additional joints and bends in the IK solve (e.g. 4 joints with 2 bends)." CreationDate="2017-03-01T02:18:30.270" UserId="2076" />
  <row Id="6842" PostId="4781" Score="0" Text="Yes something like that. I remember there is some very general method for curve fitting. And you can use this method on any kind of curve after defining the curve-parameters of which can vary (http://web.iitd.ac.in/~pmvs/courses/mel705/curvefitting.pdf)&#xA;I've never used one of these French or Hip curves, but I guess you need to move it around to draw the exact curve you want, so this would be multiple segments like you said.. This might make it more complex.&#xA;Anyway, this sounds like an interesting problem!" CreationDate="2017-03-01T07:33:29.097" UserId="3434" />
  <row Id="6844" PostId="4787" Score="0" Text="Do you mean underconstrained?" CreationDate="2017-03-01T12:25:08.923" UserId="6125" />
  <row Id="6845" PostId="4787" Score="0" Text="@StinkySkunk oh yes" CreationDate="2017-03-01T12:46:01.710" UserId="38" />
  <row Id="6846" PostId="4788" Score="0" Text="at some point you have no choice but to call glDraw* multiple times with different uniforms." CreationDate="2017-03-01T15:07:14.863" UserId="137" />
  <row Id="6847" PostId="4788" Score="0" Text="I don't think that you can treat these 3 objects as one because they belong to different `vaos`. You can't bind more than one `vao` at the time." CreationDate="2017-03-01T16:29:05.997" UserId="4958" />
  <row Id="6848" PostId="4787" Score="0" Text="To be honest, I'm not sure which option I'm searching for. What I do know is that I'm after a planar solution that moves like this: &#xA;&#xA;https://youtu.be/3sqd_CZNq7s?t=6s&#xA;&#xA;It's short, but I'm looking at the part where the color chain is red-orange-yellow-blue." CreationDate="2017-03-01T18:12:40.767" UserId="2076" />
  <row Id="6849" PostId="4787" Score="0" Text="That does not seem very useful @GregGunn It does not even seem to solve any ik. Atleast we do not know if it is because we cant see the target.  It looks more like a system that moves the links based on proxility which is more like hbrid fk/ik. the only reason we even suspect its ik is because the title says so... Titles lie." CreationDate="2017-03-01T19:00:52.033" UserId="38" />
  <row Id="6850" PostId="4787" Score="0" Text="@joojaa If that reference is my goal—regardless of it's IK or not—how would you suggest I build something that moves like that?" CreationDate="2017-03-01T19:30:16.597" UserId="2076" />
  <row Id="6851" PostId="4737" Score="0" Text="I'm doubting that the third column should be all zeros. It's late here and will try to look at it more tomorrow, but think that column should be the basis vector for your $N$." CreationDate="2017-03-02T00:05:51.620" UserId="2941" />
  <row Id="6852" PostId="4737" Score="0" Text="I'm also doubting that $1$ in row 2, column 4 of your rotation matrix. That's the translation part of the matrix." CreationDate="2017-03-02T00:09:34.523" UserId="2941" />
  <row Id="6853" PostId="4789" Score="0" Text="You also need to invert the matrix in the end" CreationDate="2017-03-02T02:59:47.487" UserId="1952" />
  <row Id="6854" PostId="4789" Score="0" Text="@JarkkoL, I don't know why it needs to invert, can you explain?" CreationDate="2017-03-02T04:06:18.087" UserId="6140" />
  <row Id="6855" PostId="4789" Score="0" Text="OP is asking for view (world-&gt;camera) matrix, while you are building object-&gt;world matrix" CreationDate="2017-03-02T04:13:51.003" UserId="1952" />
  <row Id="6856" PostId="4781" Score="1" Text="Just so you know, the Bezier curve was created so that designers didn't have to deal with French curves anymore since they are imprecise and the resulting curves are hard to communicate between people. Also, if you are looking to fit data points with a polynomial, you should check out least squares fitting, which is an O(1) operation - no looping or gradient descent type stuff required. One other thing, bezier curves can be approximated with line art, like with strings. Check out figure 2 here: https://plus.maths.org/content/bridges-string-art-and-bezier-curves" CreationDate="2017-03-02T06:06:51.307" UserId="56" />
  <row Id="6857" PostId="4789" Score="0" Text="@JarkkoL Thank you very much, I misunderstood the transformation. already change the answer." CreationDate="2017-03-02T06:08:12.630" UserId="6140" />
  <row Id="6858" PostId="4787" Score="0" Text="@GregGunn There is a inifinite mumber of solutions that does something. _You_ need to decide what is useful, i cant help you if there is no aim other than a abstract &quot;something&quot;. You need to have a clear vision of what problem your ik is trying to solve. Onnce you can describe what the vide does with its 4th link and why thats useful then i can help. The trick to underconstrained problems is that you need to do the reasoning because the computer or anybody else who does not k.ow the purpose and specification can not do it for you." CreationDate="2017-03-02T06:43:18.940" UserId="38" />
  <row Id="6859" PostId="4789" Score="0" Text="This is great. I will check with my lecturer and once he approves, I'll accept the answer." CreationDate="2017-03-02T10:33:50.193" UserId="5513" />
  <row Id="6860" PostId="4797" Score="0" Text="You are talking about GL_LINEAR_MIPMAP_LINEAR filtering method, but if you use GL_NEAREST_MIPMAP_LINEAR instead then you first do a nearest interpolation of the higher-res and lower-res textures, and then mix the resulting colors giving more preference to the nearest mipmap and less preference to the furthest. Am I correct?" CreationDate="2017-03-03T00:30:08.370" UserId="5852" />
  <row Id="6861" PostId="4797" Score="1" Text="Actually, for nearest, you would just pick either the high res or low res texture (which ever is nearest), and then within that texture do a normal 2D nearest-neighbor look-up." CreationDate="2017-03-03T00:51:02.213" UserId="3003" />
  <row Id="6862" PostId="4789" Score="0" Text="@S.A Thank you, your question is very interesting." CreationDate="2017-03-03T09:34:23.553" UserId="6140" />
  <row Id="6863" PostId="4798" Score="1" Text="Without looking at your code I'd guess the error is perspective correction, similar to [this question](http://computergraphics.stackexchange.com/questions/4079/perspective-correct-texture-mapping?rq=1)" CreationDate="2017-03-03T10:22:11.410" UserId="2041" />
  <row Id="6864" PostId="4793" Score="0" Text="Unrelated, but your `imageArray2` loop would be faster as `for (int i = 0; i &lt; 100 * 100 * 3; i += 3) { imageArray2[i] = 0; imageArray2[i + 1] = 0; imageArray[i + 2] = 255; }` because it avoids a division inside the loop. I think it's clearer this way too." CreationDate="2017-03-03T10:35:24.940" UserId="2041" />
  <row Id="6865" PostId="4793" Score="2" Text="Obviously mipmaps are turned off in the program as-is: the thing you're debugging is why you get a black screen when you uncomment the line that turns them on. First rule of GL debugging is to use [`glGetError`](https://www.khronos.org/registry/OpenGL-Refpages/gl4/html/glGetError.xhtml) everywhere and see if any of the gl* function calls are failing." CreationDate="2017-03-03T10:37:35.947" UserId="2041" />
  <row Id="6866" PostId="4798" Score="0" Text="you are interpolating in screenspace, you should be interpolating in clipspace." CreationDate="2017-03-03T11:42:49.877" UserId="137" />
  <row Id="6867" PostId="4796" Score="1" Text="I think you can do this vith a L-system. Like the penrose tiling exaple on [this page](http://www.kevs3d.co.uk/dev/lsystems/), see examples on the right. (a L-system is pretty easy to program, if you dont know how its one of those cassic CS things). An early alpha version of a L-System generator for illustrator can be found [here](https://bitbucket.org/joojaa/jooillustratorscripts/src/99aaf05584e32a9c694bbdb0857169201ff9af26/jooLSystem.jsx?at=Lsys&amp;fileviewer=file-view-default)" CreationDate="2017-03-03T12:10:00.967" UserId="38" />
  <row Id="6868" PostId="4795" Score="0" Text="Thank you, this is very helpful and gives me a lot of jumping-off points. I considered splines, but they aren't typically used in this drafting context AFAIK, so I don't think they would be ideal for at least some of the applications." CreationDate="2017-03-03T16:13:43.990" UserId="6136" />
  <row Id="6869" PostId="4800" Score="1" Text="Fantastic reference. Always fascinating to read original papers which introduced new technologies that we all but take for granted these days." CreationDate="2017-03-03T22:09:16.563" UserId="6145" />
  <row Id="6870" PostId="2286" Score="1" Text="The [Disney BRDF paper](https://disney-animation.s3.amazonaws.com/library/s2012_pbs_disney_brdf_notes_v2.pdf) section 5.3 refers to the equation in your answer and then goes on to specify a different model. I don't claim to understand any of it as I've just started my glossy BRDF implementation!" CreationDate="2017-03-03T23:02:17.397" UserId="2941" />
  <row Id="6871" PostId="4793" Score="0" Text="@Dan Hulme How can I turn on mipmaps? The tutorial I'm following doesn't make mention of how to turn on/off mipmaps." CreationDate="2017-03-03T23:36:23.977" UserId="5852" />
  <row Id="6872" PostId="4793" Score="1" Text="Setting the `GL_TEXTURE_MIN_FILTER` to `GL_NEAREST` means the mipmaps won't be used, which is why you only see level 0 when you do this." CreationDate="2017-03-04T09:47:34.247" UserId="2041" />
  <row Id="6873" PostId="4803" Score="0" Text="If it were me, I'd avoid OpenGL and use Metal, DirectX 12, or Vulkan, as OpenGL is now a legacy library that probably won't be updated in the future." CreationDate="2017-03-04T20:58:08.330" UserId="3003" />
  <row Id="6874" PostId="4803" Score="0" Text="@user1118321 I plan to move to Vulkan after the project, OpenGL is just more covered and accessible right now. Also, my professor doesn't have experience in Vulkan sadly." CreationDate="2017-03-05T01:35:50.443" UserId="6167" />
  <row Id="6875" PostId="4806" Score="0" Text="Thank you! This helps a lot to get to a good starting point. I really like the idea of Fast Ray-Liquid intersection testing, but might be out of scope. I'm going to look into it." CreationDate="2017-03-05T03:16:59.317" UserId="6167" />
  <row Id="6876" PostId="4657" Score="7" Text="In order to help people assess whether to use this approach, could you add an explanation of why this solves the problem with the approach in the question?" CreationDate="2017-03-05T13:38:49.680" UserId="231" />
  <row Id="6877" PostId="4812" Score="0" Text="But the scale factor is 3 dimensional and the tex coordinate is 2 dimensional." CreationDate="2017-03-06T07:55:50.450" UserId="5609" />
  <row Id="6878" PostId="4812" Score="0" Text="For example if the scaling factor is 1, 10, 1, then the texture coordinates of a face will be scaled based on which face we render currently. Like if we render the top quad it will be scaled x = 1 and z = 1. If we look at a side quad it will scaled by x = 1 and y = 10 or z = 1 and y = 10. How can I check which face are working on?&#xA;Or do I overcomplicate it?" CreationDate="2017-03-06T08:03:08.383" UserId="5609" />
  <row Id="6881" PostId="4801" Score="0" Text="I deleted my answer as I think that a blending solution won't fix this problem.  I think you probably need to utilize the stencil buffer somehow to avoid writing the darkness to the same area twice." CreationDate="2017-03-06T15:26:17.937" UserId="3332" />
  <row Id="6882" PostId="4801" Score="0" Text="Yes, I have thought the same. I will pursue that idea for the moment. Thanks for trying." CreationDate="2017-03-06T15:28:36.117" UserId="6164" />
  <row Id="6883" PostId="4812" Score="0" Text="Without knowing more about your architecture and seeing more of your code, I don't have any way to answer that question. I would assume that you know which faces you're drawing when. You do need to know which face you're drawing and what the scale factors are in order to do this correctly." CreationDate="2017-03-06T17:17:20.910" UserId="3003" />
  <row Id="6884" PostId="4814" Score="0" Text="Very nice @StinkyShunk ! I came up to this already too but this explains it well tp.  I've planned to answer the question myself but now I think It'll accept your answer and post mine according 3D as additional information too." CreationDate="2017-03-06T18:33:40.403" UserId="6165" />
  <row Id="6886" PostId="4814" Score="0" Text="After all, thank you very much @StinkySkunk. I really hope this helps other people trying to understand it out there too." CreationDate="2017-03-06T19:14:43.197" UserId="6165" />
  <row Id="6887" PostId="4814" Score="0" Text=":) I am happy I could confirm your thinking. Please do post your more thorough answer as well for the benefit of others!" CreationDate="2017-03-07T00:07:21.773" UserId="6125" />
  <row Id="6888" PostId="4796" Score="0" Text="Based on the answer by @Mikhail V and by looking at the fractal structure given in my code with number of iterations up to 5, I wonder if one could make a &quot;fractal complement&quot; (or dual) to fill in the gaps/cracks, since they are also self-similar. Then one could patch both fractals together in the end." CreationDate="2017-03-07T01:46:23.227" UserId="6023" />
  <row Id="6889" PostId="4813" Score="0" Text="Usually the printer will tell you the minimum DPI required. I believe that most printers use 300 DPI for printing photos. They may let you get away with less (hence the bad and average categories), but their hardware prints at 300 dpi so your image should have enough pixels for that." CreationDate="2017-03-07T05:39:43.930" UserId="3003" />
  <row Id="6891" PostId="4818" Score="0" Text="Do I really need 3 correspondences, or is 2 enough?    &#xA;&#xA;Additionally, how can I get the angle of rotation from this?" CreationDate="2017-03-07T11:30:49.107" UserId="6175" />
  <row Id="6892" PostId="4818" Score="1" Text="2 is enough but the third lets you verify that it's indeed a rotation. As for angle you can project the point and it's rotated point onto the axis get the direction to the points and dot product." CreationDate="2017-03-07T11:35:55.860" UserId="137" />
  <row Id="6893" PostId="4818" Score="0" Text="I guess two can have degenerate cases if they're coplanar with the axis of rotation, so three can help, but three can also be bad if they're still coplanar with the axis.  Like for a triangle on a door that rotates open, all the &quot;midpoint-planes&quot; are the same." CreationDate="2017-03-07T11:41:46.897" UserId="6175" />
  <row Id="6894" PostId="4818" Score="1" Text="@ginsunuva there is a solution for that, see edit. The only remaining issue is a degenerate triangle which is parallel with the rotation axis but that has multiple solutions." CreationDate="2017-03-07T11:43:54.037" UserId="137" />
  <row Id="6895" PostId="4818" Score="0" Text="Great, also how do I actually do the project/find angle thing? How can I get two points to be in this new 2d coordinate system defined by the orthogonal plane to the rot-axis, so I can do a dot product?" CreationDate="2017-03-07T12:42:01.103" UserId="6175" />
  <row Id="6896" PostId="3675" Score="0" Text="Great answer! I completely agree with shadows and ambient occlusion being the main factors contributing to the &quot;3D-ness&quot; of the other apps. Fog is less important for small, enclosed spaces but is required for performance in large, open spaces and is rather trivial to implement." CreationDate="2017-03-07T13:51:14.027" UserId="6145" />
  <row Id="6898" PostId="4812" Score="0" Text="@user1118321 Could you expand on the part about passing a scale matrix into the fragment shader? How do you use that to adjust U,V coords for non-uniform scaling? Is it possible?" CreationDate="2017-03-07T13:59:25.173" UserId="6145" />
  <row Id="6899" PostId="4816" Score="0" Text="Marvelous. Thanks!" CreationDate="2017-03-07T15:38:39.830" UserId="6179" />
  <row Id="6900" PostId="4820" Score="2" Text="Welcome to the CGSE! To get relevant help, you will have to add more context to your question. What tool or language are you using? Is &quot;Processing&quot; some setting of that tool?" CreationDate="2017-03-07T15:56:34.323" UserId="2479" />
  <row Id="6901" PostId="4820" Score="1" Text="Thanks for your welcome. Processing is a tool for developing basic computer graphics programs. I edited my post." CreationDate="2017-03-07T16:00:56.363" UserId="6183" />
  <row Id="6902" PostId="4812" Score="0" Text="OK, I've added an example above." CreationDate="2017-03-07T17:20:05.297" UserId="3003" />
  <row Id="6903" PostId="4816" Score="0" Text="One more question - how did you count aspect ratio of these images?" CreationDate="2017-03-07T22:27:35.540" UserId="6179" />
  <row Id="6904" PostId="4801" Score="1" Text="You could try using Max blending : http://stackoverflow.com/questions/2143690/is-it-possible-to-achieve-maxas-ad-opengl-blending" CreationDate="2017-03-08T06:41:27.103" UserId="3073" />
  <row Id="6905" PostId="4812" Score="0" Text="I couldn't figure out how could I know which face I am drawing. The drawing part of my architecture is simple: Load the vertices, normals, indices, uv-s from an .obj file, then just push it to the shader. The vertex shader will multiple it with MVP, then the fragment shader samples from teh texture based on the UV coord. Somehow I have to get that which face looks towards which axis before the MVP multiple. (because for example a &quot;rotate around Y axis 90 degree&quot; would make this very hard, if not impossible)&#xA;Maybe I could something with the indices?" CreationDate="2017-03-08T09:38:08.610" UserId="5609" />
  <row Id="6906" PostId="4812" Score="0" Text="Can you modify the .obj file? If so, you can group sets of polygons in meaningful ways using [the `g` command](https://en.wikipedia.org/wiki/Wavefront_.obj_file#Referencing_materials). This will allow you to make 2 triangles into a single cube face, for example. Then, when reading it in, you can know which face is which." CreationDate="2017-03-08T16:52:12.200" UserId="3003" />
  <row Id="6907" PostId="4813" Score="0" Text="Keep in mind that those suggestions are mostly useless. You won't see anywhere near 300 DPI from common commercial printers on typical photo paper. It's closer to 100-150 DPI in my experience. And you won't be able to see even that much on larger prints from a reasonable viewing distance so you can get away with less." CreationDate="2017-03-08T20:53:03.950" UserId="5717" />
  <row Id="6908" PostId="4827" Score="1" Text="There looks to be a number of rather odd things in your answer, but _&quot;Jpg uses fast fourier transform&quot;_ is certainly incorrect.   JPEG uses Discrete Cosine Transforms (DCTs). There are several reasons why this is a better choice than FFT but that's a little difficult to explain with limited time and the restrictions of S.O. comments :-)." CreationDate="2017-03-09T09:57:40.637" UserId="209" />
  <row Id="6909" PostId="4827" Score="0" Text="@SimonF Thanks for your comment, I believe DCT is a special form of FFT since DCT only uses cosine waves but FFT uses both sine and cosine waves, you may set the sine expansion =0  and solve the equation using the constraint to have a DCT out of A FFT, but YES YOU ARE RIGHT the exact name of algorithm is DCT for jpeg lossy data. Thanks again" CreationDate="2017-03-09T12:59:10.150" UserId="537" />
  <row Id="6911" PostId="4830" Score="1" Text="Maybe because the implementation modifies in place running LtR? That would mean the pixel above got changed and one got inserted to the left but not to the right." CreationDate="2017-03-09T13:23:47.710" UserId="137" />
  <row Id="6917" PostId="4830" Score="0" Text="Ok, but modifying in place would mean that, looking at the 2nd picture, the downmost circled blank pixel turns into a red one. Then, the green seed right next to it should take that information and turn into a red one too, but it remains green..." CreationDate="2017-03-09T15:06:11.080" UserId="5406" />
  <row Id="6918" PostId="4832" Score="1" Text="Please don't ask the same question on multiple sites. It means that any answers get spread across different places, making that information harder to find in future." CreationDate="2017-03-09T17:02:26.930" UserId="2041" />
  <row Id="6919" PostId="4828" Score="1" Text="There's not a lot of information to go on here. What OS are you on? What are you trying to take a screenshot of? What is it going to be output on? What makes you think that it's even possible to take a screenshot at a higher resolution than the screen? Also, you mention rendering in your last sentence. What are you rendering? And how are you rendering it?" CreationDate="2017-03-10T03:45:10.670" UserId="3003" />
  <row Id="6920" PostId="4828" Score="0" Text="I guess you could divide your view fustrum into a grid and render it to several separate render targets. Your post processing shaders need to be able to deal with sub sections (say a full screen vignette would have UV coordinate inputs, you would be passing in sub-rectangles of UVs to render sections of it), you should be able to restitch the output textures together without artefacts." CreationDate="2017-03-10T08:34:43.047" UserId="3073" />
  <row Id="6921" PostId="4828" Score="0" Text="@user1118321 I edit post with more details.&#xA;@PaulHK In your solution screenspace effects like `SSR` will be broken due to lack information." CreationDate="2017-03-10T11:49:16.753" UserId="3123" />
  <row Id="6922" PostId="4830" Score="0" Text="[Somewhat related](http://computergraphics.stackexchange.com/questions/2102/is-jump-flood-algorithm-separable) but definitely not a duplicate (disclaimer: I wrote an answer to it)" CreationDate="2017-03-10T21:00:23.380" UserId="231" />
  <row Id="6924" PostId="4828" Score="0" Text="What information do your post processing effects require? Do they just need the immediately neighbouring pixels or more distant data?" CreationDate="2017-03-10T21:13:36.843" UserId="231" />
  <row Id="6925" PostId="4835" Score="0" Text="Ahh, that's it! So when a pixel finds a colored neighbor it computes the distance between that pixel and the initial seed (whose position it knows from the neighbor). This wasn't clear to me that there is a difference between a colored pixel and a seed. Thank you, you helped me a lot with that!&#xA;&#xA;But one question remains: what about the diagonal pixels in the 4x4 example? They have the same distance to both initial seeds..." CreationDate="2017-03-10T21:23:25.777" UserId="5406" />
  <row Id="6926" PostId="4835" Score="4" Text="Glad it makes sense now! The diagonals are ambiguous in the definition of the voronoi diagram: the paper mentions (on the first page) that for equidistant seeds, they choose the seeds arbitrarily. Maybe it will look nicer if you consistently choose one color over the other, I have no idea!" CreationDate="2017-03-10T21:36:46.803" UserId="6125" />
  <row Id="6927" PostId="4835" Score="0" Text="Alright, just overlooked it. Thanks again!" CreationDate="2017-03-10T21:44:01.503" UserId="5406" />
  <row Id="6930" PostId="4837" Score="1" Text="Could you post some code and a screen shot of what it looks like, and perhaps an example of what you want it to look like?" CreationDate="2017-03-11T06:48:55.653" UserId="3003" />
  <row Id="6931" PostId="4836" Score="1" Text="Do you already have code to do this? If so, have you profiled it to see what its performance is like and where any potential slow-downs might be? As it's asked right now, there are a lot of possible answers, and it's a quite broad question." CreationDate="2017-03-11T06:51:44.690" UserId="3003" />
  <row Id="6932" PostId="4836" Score="0" Text="No because the results to the experiment will greatly affect how I design my engine. I am completely redesigning my graphics engine and I am trying to have it with my new method in mind." CreationDate="2017-03-11T07:01:14.080" UserId="2308" />
  <row Id="6933" PostId="4837" Score="0" Text="@user1118321 Good idea. Added!" CreationDate="2017-03-11T07:01:51.457" UserId="2308" />
  <row Id="6934" PostId="4828" Score="0" Text="@trichoplax Post processes need information from whole screen (some needs few previous frames but in this case it can be ignored)" CreationDate="2017-03-11T10:30:08.897" UserId="3123" />
  <row Id="6935" PostId="3784" Score="0" Text="I like your explanation very much. I get the arguments that there must be the inverse solid angle factor in BRDF, but what about the cosine factor? If we could drop the cosine term from BRDF, then we could drop if from the integral in the rendering equation, couldn't we? The only reason I can see is in the correct/current formulation the denominator can be seen as irradiance..." CreationDate="2017-03-11T11:50:05.027" UserId="5249" />
  <row Id="6936" PostId="4811" Score="0" Text="So for designing a PDF for picking a light, is there a specific method that's proven to be effective? Or does everyone have their own combination of techniques such as the output power, orientation, etc as you mentioned? Also, will it make biased if I do that? Or does dividing by the PDF keep it unbiased?" CreationDate="2017-03-11T16:17:44.600" UserId="5764" />
  <row Id="6937" PostId="4816" Score="0" Text="For real, @Nicol Bolas, how did you determine that 1000x1333 image has aspect ratio of 3:4? Even this site https://www.ninjaunits.com/calculators/aspect-ratio/ does not determine this." CreationDate="2017-03-11T17:59:30.180" UserId="6179" />
  <row Id="6938" PostId="4816" Score="1" Text="@funguy: The aspect ratio of an image is the ratio of the image's width to its height. You don't need a website to divide 1000 by 1333; you only need a calculator. Note that this ratio is just the ratio of the image itself; some image formats have metadata that specify what aspect ratio the image should be *stretched* to in order to correctly reproduce what it stores. But absent such metadata, the assumption is that the ratio of the pixels is how the image should be displayed." CreationDate="2017-03-11T18:04:59.343" UserId="2654" />
  <row Id="6940" PostId="4816" Score="0" Text="Yes, @Nicol Bolas, but this is what I want to understand, how to find out aspect ratio with a calculator. If you divide 1000/1333 you will get something like 0.75018... but still, how to add up this final result - 3x4?" CreationDate="2017-03-11T19:26:40.317" UserId="6179" />
  <row Id="6941" PostId="4811" Score="0" Text="Unfortunately, I don't know any production-proven approaches in detail and even PBRT proposes only very simple methods. I extended the answer with what I know. And, just to be clear, for picking a light source you need to use probability, not PDF (probability density function)." CreationDate="2017-03-11T21:52:16.563" UserId="2479" />
  <row Id="6942" PostId="4836" Score="1" Text="Are you only drawing one quad (with the texture tiled)? I think a diagram or what you want to achieve as the end result would help." CreationDate="2017-03-12T04:13:53.533" UserId="2707" />
  <row Id="6943" PostId="4840" Score="0" Text="So cos(theta) would equal to 0.8. Thanks!" CreationDate="2017-03-12T09:28:04.413" UserId="5513" />
  <row Id="6945" PostId="4838" Score="1" Text="Just to be clear, you want the integer ratio and not the actual value? For example, the aspect ratio is just `width` / `height`, so for 1000x1200, it's 1000/1200 = 0.8333... But if you want to print it out as a ratio with a format like 16:9, then it's slightly more complicated (though not much). But it's not clear from your question which one you're asking about." CreationDate="2017-03-12T19:03:31.093" UserId="3003" />
  <row Id="6946" PostId="4838" Score="0" Text="Yes, it is 1000/1200 = 0.8333, but how you actually get 5:6 by knowing just  this - 1000/1200 = 0.8333 ?" CreationDate="2017-03-12T21:06:23.100" UserId="6179" />
  <row Id="6948" PostId="4845" Score="0" Text="Can you give an example with numbers? Would appreciate." CreationDate="2017-03-12T23:12:01.090" UserId="6179" />
  <row Id="6949" PostId="4837" Score="1" Text="Could you expand the explanation of what you intend? Do you want the objects to be opaque in all cases *except* when they are behind a trail? Does this apply only to the trails of other objects, or also to an object's own trails?" CreationDate="2017-03-13T01:00:19.190" UserId="231" />
  <row Id="6950" PostId="4845" Score="0" Text="Plain awesomeness. Thanks a lot!" CreationDate="2017-03-13T08:00:48.703" UserId="6179" />
  <row Id="6951" PostId="4844" Score="0" Text="I wonder what the $b$ without $2$ is then. I've seen it used somewhere, but cannot find it now." CreationDate="2017-03-13T16:56:19.970" UserId="6029" />
  <row Id="6952" PostId="4557" Score="0" Text="@AlanWolfe The research you linked is incredible! Thanks for posting." CreationDate="2017-03-13T21:33:16.357" UserId="6145" />
  <row Id="6954" PostId="4852" Score="0" Text="So the divide by w brings the 4d coordinates to 3d and then the 3d points are orthographically projected to 2d? So a perspective projection really also encompasses a final orthographic projection?" CreationDate="2017-03-14T10:08:22.523" UserId="6230" />
  <row Id="6955" PostId="4845" Score="0" Text="What about other value 1000x1333 it does not add up. It seems like a few people are saying that aspect ratio of this is 3:4, but if I try to count it is always 1000x1333. How they get 3:4 for this?" CreationDate="2017-03-14T11:31:56.570" UserId="6179" />
  <row Id="6956" PostId="4842" Score="0" Text="Do you have to use vertex normals as opposed to face normals? This makes the problem considerably more difficult. That said, the center of the triangle is always going to be the average of the values at the 3 vertices of the triangle, regardless of the size or direction of the triangle. If you know how to calculate the values at each vertex, you can simply interpolate t=0.5 between all three values." CreationDate="2017-03-14T13:52:29.050" UserId="6145" />
  <row Id="6957" PostId="4842" Score="0" Text="@Dan No, it's flat shading, so it should be by face normal." CreationDate="2017-03-14T15:25:54.490" UserId="2707" />
  <row Id="6958" PostId="4845" Score="0" Text="That's a case of integer rounding. Technically, 3:4 would be a ratio of 0.75, whereas 1000:1333 is 0.750187546886722... So it's very close to 3:4, but not exact. So another way you could approach this is to have a table of common aspect ratios (like 1:1, 2:1, 3:4, 4:5, 16:9, etc.) and then find the one that's closest. Or you could round the result of the width / height to some decimal amount and then compare." CreationDate="2017-03-14T16:37:34.137" UserId="3003" />
  <row Id="6959" PostId="4840" Score="1" Text="For flat shading you'd want to use the triangle's true, geometric normal vector, right? Not the average of the vertex normals or something." CreationDate="2017-03-14T16:58:44.670" UserId="48" />
  <row Id="6960" PostId="4855" Score="0" Text="Thanks for your answer. My aim is to calculate daylighting from the visible sky, seen through the glazing portal,on few measurement points. The transmitted ray will hit a different patch of sky with a different distribution than the initial shadow ray. And as you said this will give wrong results." CreationDate="2017-03-14T17:13:53.010" UserId="6041" />
  <row Id="6961" PostId="4849" Score="0" Text="Thanks for the answer but I have to admit that I don't understand everything. Could you put some concrete example or code so that I can't see more clearly please ?" CreationDate="2017-03-14T17:30:32.940" UserId="5770" />
  <row Id="6962" PostId="4850" Score="0" Text="What do you mean by &quot;the same side&quot;? I know the general idea of &quot;how to do it&quot; but I don't know how it happens in practice. Povray gives such an accurate and pretty result, I don't understand how it can possibly do it." CreationDate="2017-03-14T17:36:20.940" UserId="5770" />
  <row Id="6963" PostId="4840" Score="0" Text="Nathan, so cos(theta) is equal to 0.8 still, right?" CreationDate="2017-03-14T17:40:56.410" UserId="5513" />
  <row Id="6964" PostId="4842" Score="0" Text="@aces So you're agreeing with me? Lol. Your comment contradicts itself.The OP specified 3 normals, one for each vertex, in his question." CreationDate="2017-03-14T17:44:33.870" UserId="6145" />
  <row Id="6965" PostId="4855" Score="0" Text="I am not sure any light tracing methods would help either as sampling the infinite area light source(sky) for an indoor scene is not efficient, and considering the window portal as the light source with the background sky distribution wouldn't help either as you sample the window area with a sample direction towards the room but still needs to trace one transmitted ray back to the sky to account for the glazing. Is this correct?" CreationDate="2017-03-14T18:01:18.603" UserId="6041" />
  <row Id="6967" PostId="4840" Score="0" Text="@NathanReed We don't have the winding direction, so we could potentially compare the average with both flat geometric normal vectors and pick one. What if the triangle has some sort of weighted normal distribution and the vectors are given to compensate for this? This might be too general of a case I'm thinking of though..." CreationDate="2017-03-14T19:28:58.263" UserId="2707" />
  <row Id="6968" PostId="4842" Score="0" Text="@Dan Sorry, I interpreted your comment incorrectly." CreationDate="2017-03-14T19:31:07.473" UserId="2707" />
  <row Id="6969" PostId="4840" Score="0" Text="I suspect you're overthinking it, since this sounds like a homework problem. :) But it's true the winding direction needs to be considered, or we need to check light vs normal and do double-sided lighting." CreationDate="2017-03-14T19:32:16.863" UserId="48" />
  <row Id="6970" PostId="4840" Score="0" Text="I do like your suggestion though. :) I'll incorporate it into the answer." CreationDate="2017-03-14T19:33:06.447" UserId="2707" />
  <row Id="6971" PostId="4840" Score="0" Text="Actually, I decided to compare it against the average normal vector. The reasoning behind this is that the supplied vertex normals can help us determine the winding order if the triangle is single-sided." CreationDate="2017-03-14T20:31:46.313" UserId="2707" />
  <row Id="6972" PostId="4849" Score="0" Text="Ok. I haven't got time today, but will try to add some more info soon(ish)." CreationDate="2017-03-15T12:10:07.650" UserId="209" />
  <row Id="6973" PostId="4842" Score="0" Text="@aces No big deal. Hopefully your answer helps the OP!" CreationDate="2017-03-15T16:16:15.100" UserId="6145" />
  <row Id="6975" PostId="4863" Score="0" Text="But it's required only if one maps a texture to the object?" CreationDate="2017-03-16T06:40:49.887" UserId="6029" />
  <row Id="6976" PostId="4842" Score="0" Text="My lecturer told me this was wrong. The question asks to use flat shading, which is a different equation." CreationDate="2017-03-16T09:33:21.493" UserId="5513" />
  <row Id="6977" PostId="4856" Score="0" Text="My lecturer told me this method is not answering the question, because the question asks for flat shading." CreationDate="2017-03-16T09:33:51.503" UserId="5513" />
  <row Id="6978" PostId="4866" Score="0" Text="To my understanding, this is how to calculate the RGB intensity values using flat shading. I would really appreciate seeing what others have to say about this." CreationDate="2017-03-16T09:45:52.650" UserId="5513" />
  <row Id="6980" PostId="4849" Score="0" Text="What you do suggest is that I keep using my method of constructing triangles but that I should use a spatial subdivision algorithm to &quot;fasten&quot; the calculation?" CreationDate="2017-03-16T17:00:50.857" UserId="5770" />
  <row Id="6982" PostId="4869" Score="0" Text="The (surface normal x Light intensity) / diffuse part, shouldn't that be N.L * lightColour * diffuseColour ?" CreationDate="2017-03-17T07:56:16.103" UserId="3073" />
  <row Id="6983" PostId="4869" Score="0" Text="I didn't add a divide anywhere in the equation. It is lightIntensity*(diffusive reflection coefficient * (N.L))" CreationDate="2017-03-17T08:33:59.497" UserId="5513" />
  <row Id="6984" PostId="4869" Score="0" Text="@PaulHK Is it possible you could give a reference to where you found the equation for flat shading?" CreationDate="2017-03-17T08:56:44.863" UserId="5513" />
  <row Id="6985" PostId="4869" Score="0" Text="OpenGL's fixed function pipeline, I found a decent reference here &gt; http://www.cs.cmu.edu/afs/cs/academic/class/15462-s09/www/lec/02/lec02b.pdf" CreationDate="2017-03-17T09:33:54.467" UserId="3073" />
  <row Id="6986" PostId="4869" Score="0" Text="This equation doesn't consider flat or smooth shading, that is down to you to use either a fixed normal per triangle or interpolated vertex normals." CreationDate="2017-03-17T09:34:51.223" UserId="3073" />
  <row Id="6987" PostId="4849" Score="0" Text="It seems to me you have two basic options though, in some senses, they can both converge to an 'equivalent' result: (a) Interval arithmetic, to weed out regions followed by Newton-Rhapson to solve for the ray intersection or (b) dice (i.e. tessellate)  your object into &quot;sufficiently small&quot; triangles that you don't see discontinuity artefacts (as in your first image). In a sense these are &quot;similar&quot; since in (a) each Newton-R iteration approximates the surface locally as a plane (like a small triangle). I suspect b will be much easier to implement but tricky to estimate ideal subdivision level." CreationDate="2017-03-17T10:19:44.837" UserId="209" />
  <row Id="6989" PostId="4873" Score="0" Text="Thanks for your answer. You mentioned that brdf samples can be shared between direct and indirect. Could you explain this a bit more please." CreationDate="2017-03-18T10:00:31.967" UserId="6041" />
  <row Id="6990" PostId="4873" Score="0" Text="You're welcome, @ali. I extended the answer. If not clear enough, please, let me know." CreationDate="2017-03-18T14:44:41.553" UserId="2479" />
  <row Id="6991" PostId="4868" Score="0" Text="Are you trying to implement a general-use robust path tracer or is it intended to be used in a special (and fairly difficult) scenarios like the one you showed in your picture?" CreationDate="2017-03-18T14:48:00.813" UserId="2479" />
  <row Id="6992" PostId="4873" Score="0" Text="Well, a bit struggling to get your idea, but I guess that's what I used to do in this situation: on doing the direct lighting, as the solid angle for visible part of the sky and the external obstruction are same, I could use one pdf to compute the contribution of the both: either from the sky(if no hit) or from the external obstruction. The problem arises on computing the radiance of the external hit as it needs recursive calls(a separate complete path) to get the correct estimate. This works though makes it quite slow and a complex code; it is also advised not to do indirect in direct part." CreationDate="2017-03-18T16:17:19.310" UserId="6041" />
  <row Id="6993" PostId="4868" Score="0" Text="No no aim for implementing a general-use path tracer. Yet this is sort of a typical scenario in doing daylight calculation and I try to improve the efficiency of path tracer here. I came across an exercise in pbrt book which suggests to flag, and then sampling certain objects which are important source of indirect, along with brdf sampling(I have updated the question with this exercise, please take a look); and thought this could be a solution to my case." CreationDate="2017-03-18T16:29:40.347" UserId="6041" />
  <row Id="6994" PostId="4874" Score="2" Text="You seem to keep using the words without any idea of what they mean. While &quot;building&quot; and &quot;compiling&quot; are often used synonymously, I have never seen &quot;rendering&quot; being used as a synonym for them. It's overall unclear what you're really asking about. Perhaps you should get some more programming experience that could better inform you as to what all of these terms mean." CreationDate="2017-03-18T19:03:15.077" UserId="2654" />
  <row Id="6995" PostId="4868" Score="0" Text="I've edited for appearance only - I didn't want to change the equations without checking with you first. I'm guessing that in both the description and the pseudocode it should say $W2 = p_2 / (p_2 + p_3)$ rather than $W2 = p_2 / p_2 + p_3$ (since that would evaluate the division before the addition)." CreationDate="2017-03-19T00:44:43.883" UserId="231" />
  <row Id="6996" PostId="4848" Score="0" Text="For the specific problem outlined in red in the first image, it looks like you might need to check which side of the surface you are approaching before deciding which way the normal should point, otherwise some regions will be shaded incorrectly. This shows up in particular for a Möbius strip, since there must be a point at which the normals switch direction due to the twist. There must be some adjacent triangles that have almost opposite normal directions (like the light and dark adjacent triangles in the image)." CreationDate="2017-03-19T01:14:16.007" UserId="231" />
  <row Id="6997" PostId="4848" Score="0" Text="Yes, I thought that as well but I am setting a `isInside` flag in the triangle intersection method. If it is true I then negate the normal vector. The flag is set if `det &lt; 0` using an intersection method I don't recall the name for now" CreationDate="2017-03-19T02:33:45.410" UserId="5770" />
  <row Id="6998" PostId="4848" Score="0" Text="I am using the [Möller–Trumbore intersection algorithm](https://www.wikiwand.com/en/M%C3%B6ller%E2%80%93Trumbore_intersection_algorithm)" CreationDate="2017-03-19T03:04:25.550" UserId="5770" />
  <row Id="6999" PostId="4868" Score="0" Text="Many thanks @trichoplax. It is correct the way you have written it. My apology as I am new to this group and couldn't find correct fonts/style for maths." CreationDate="2017-03-19T06:45:59.497" UserId="6041" />
  <row Id="7000" PostId="4868" Score="0" Text="There is a guide to using MathJax for mathematical formatting [over on Worldbuilding Stack Exchange](https://worldbuilding.meta.stackexchange.com/questions/607/how-do-i-add-mathematical-notation-using-latex-mathjax), but there is no obligation to use MathJax - I just edited it in to make it more readable." CreationDate="2017-03-19T13:46:48.897" UserId="231" />
  <row Id="7001" PostId="4867" Score="1" Text="It would be great to have a summary of what made the difference and how it helped." CreationDate="2017-03-19T15:13:27.723" UserId="231" />
  <row Id="7003" PostId="4863" Score="1" Text="@mavavilj yes texture coordinates are not relevant if you are not using textures." CreationDate="2017-03-19T15:20:16.160" UserId="231" />
  <row Id="7005" PostId="4857" Score="1" Text="Are you asking which properties should be treated differently at different distances?" CreationDate="2017-03-19T16:05:11.967" UserId="231" />
  <row Id="7006" PostId="4857" Score="0" Text="Exactly, that's what I meant :)" CreationDate="2017-03-19T16:09:44.123" UserId="4646" />
  <row Id="7007" PostId="4857" Score="1" Text="It might help to edit to explain this - it took me a while to guess what you meant." CreationDate="2017-03-19T16:12:25.123" UserId="231" />
  <row Id="7008" PostId="4876" Score="1" Text="You are awesome! you always answer all my questions! Thanks a lot! PS: You have to make an opengl tutorial." CreationDate="2017-03-19T20:42:19.047" UserId="5852" />
  <row Id="7009" PostId="4771" Score="1" Text="Inspired by your question, I've posted a [related one](http://computergraphics.stackexchange.com/q/4877/5681) regarding the use of more accurate Fresnel approximations in a RGB-based path tracer.  I am just commenting it here in the case you are interested." CreationDate="2017-03-19T21:06:30.083" UserId="5681" />
  <row Id="7010" PostId="4771" Score="0" Text="Brilliant. I have tried to implement gold appearance but don't think it looks right and have been reading more about Fresnel, extinction coefficients, etc. I'm not on this full time so it's going slow. Thanks." CreationDate="2017-03-19T22:32:53.300" UserId="2941" />
  <row Id="7011" PostId="4868" Score="0" Text="ali, just to be sure, do I get it right that P2 (the black part of the image) is the geometry which blocks light from the white background visible on your image? And, what do you mean &quot;P2 is same PDF but to account for external surfaces contributions&quot;? I have problems understanding this sentence. Is it the same as P1, or is it somehow changed P1?" CreationDate="2017-03-19T23:22:52.830" UserId="2479" />
  <row Id="7013" PostId="4866" Score="0" Text="There are multiple problems with the normal. The first is that it needs to be normalized. The second issue is that it points in the opposite direction as the vertex normals. Maybe that's somehow correct, but since you weren't given a winding order, I would pick the direction that's similar to the direction of the vertex normals:&#xA;http://computergraphics.stackexchange.com/questions/4839/how-do-you-calculate-costheta-for-diffuse-in-flat-shading" CreationDate="2017-03-20T01:50:33.427" UserId="2707" />
  <row Id="7014" PostId="4842" Score="0" Text="It isn't a &quot;different equation.&quot; The only difference is how you treat the normals (and possibly the interpolation of vertex information such as color, but that's not necessary in this question since the vertices don't have individual colors)." CreationDate="2017-03-20T01:53:16.443" UserId="2707" />
  <row Id="7015" PostId="4856" Score="0" Text="This does answer the question. The only difference between flat shading and smooth shading in this question is the normal you use in the lighting calculation. That's why I edited your earlier question." CreationDate="2017-03-20T01:59:01.477" UserId="2707" />
  <row Id="7016" PostId="4856" Score="0" Text="Also, I treat the diffuse reflectance coefficients as the diffuse color (maybe there's some miscommunication here?) because it doesn't make sense for them to be three-component vector of arbitrary values. It's used in energy conservation. The same thing is applicable to the diffuse light attenuation you put in you question; it should only be a single value (although the calculation is the same for this question)." CreationDate="2017-03-20T02:03:36.663" UserId="2707" />
  <row Id="7017" PostId="4880" Score="0" Text="They look fine to me, but be aware of row/column order of your C++ side matrices. Although as you say, your symptoms are only slightly wrong so it's probably some other issue." CreationDate="2017-03-20T10:08:24.443" UserId="3073" />
  <row Id="7018" PostId="4868" Score="0" Text="First thanks for your attention :). The black part of the image, marked as P2, is external buildings that blocks light from the sky for indoor surfaces. Yes P2 is same pdf as P1; P2 is 1/(window area) to sample points on the window to estimate the contribution of external buildings. P1 is same pdf, 1/(window area), again to sample points on window opening; but this time they are shadow rays to get an estimate from the visible sky(direct light);" CreationDate="2017-03-20T11:13:10.697" UserId="6041" />
  <row Id="7019" PostId="4868" Score="0" Text="Its essentially one estimator in both cases with same pdf, one to get the direct light, the other the indirect from the external buildings. The only different is the Visibility test in the two estimators. In P1 any shadow rays that hits the external objects are dismissed whereas in the case of P2 any ray that returns no intersect is ignored(as the domain is on external buildings). I hope this explains better." CreationDate="2017-03-20T11:13:13.713" UserId="6041" />
  <row Id="7020" PostId="4880" Score="0" Text="I cant get it to handle the lightingdirection correctly" CreationDate="2017-03-20T11:14:44.540" UserId="6255" />
  <row Id="7021" PostId="4795" Score="2" Text="@WillGoldie POmax has added a section on his fabulous bezier curve page that actually adresses how to simplify beziers as arcs which might be of use for you: https://pomax.github.io/bezierinfo/#arcapproximation" CreationDate="2017-03-20T11:44:32.707" UserId="38" />
  <row Id="7022" PostId="4857" Score="0" Text="Thanks, you are totally right! I tried to rephrase the question, please do tell if it's still hard to understand." CreationDate="2017-03-20T12:00:58.493" UserId="4646" />
  <row Id="7023" PostId="4881" Score="0" Text="I will keep that in mind but I was talking about generalised methods" CreationDate="2017-03-20T12:36:39.603" UserId="5506" />
  <row Id="7024" PostId="4880" Score="0" Text="You could fix your normals to point towards the camera (e.g. 0,0,-1) then try a forward light dir of 0,0,1 without using the normal matrix to at least eliminate that as a problem." CreationDate="2017-03-20T13:05:11.300" UserId="3073" />
  <row Id="7025" PostId="4880" Score="0" Text="To add : Doing the above is to produce predictable results without using a normal matrix, you can then re-introduce the normal matrix to this scheme as a method for testing the normal matrix in your shader" CreationDate="2017-03-20T13:36:19.780" UserId="3073" />
  <row Id="7026" PostId="4881" Score="0" Text="@newin When you say generalised, do you mean non-flat surfaces, or multiple flat surfaces? Do you require distortion of the reflections? Multiple reflections of mirrors in other mirrors? It would help to edit the question to specify what you do need and what you don't need, then answers can take that into account and focus on just what is necessary." CreationDate="2017-03-20T20:49:48.910" UserId="231" />
  <row Id="7028" PostId="4881" Score="1" Text="@trichoplax I edited my question with much more precisions I hope that's enough." CreationDate="2017-03-20T22:13:49.327" UserId="5506" />
  <row Id="7030" PostId="4885" Score="0" Text="Yes, you are right. When I multiply position with model only, and not view, I get the right angle. Thank you very much. (I think I understand too)" CreationDate="2017-03-21T08:35:29.057" UserId="6255" />
  <row Id="7031" PostId="4880" Score="0" Text="I was sending the wrong matrix in" CreationDate="2017-03-21T08:46:31.727" UserId="6255" />
  <row Id="7032" PostId="4880" Score="1" Text="@Charlie If you've now worked out what is wrong, could you please write it up as an answer? It will be helpful to any future visitors who have a similar problem." CreationDate="2017-03-21T11:19:14.023" UserId="2041" />
  <row Id="7033" PostId="4880" Score="0" Text="@DanHulme I modified the values in the original, but ill post a clarification too" CreationDate="2017-03-21T12:08:46.737" UserId="6255" />
  <row Id="7034" PostId="4880" Score="1" Text="Don't edit the original to include the answer. It makes it harder for other visitors to see what the original problem was. They made it easy to answer your own question to avoid any confusion over what is the question and what is the answer." CreationDate="2017-03-21T12:43:16.230" UserId="2041" />
  <row Id="7035" PostId="4883" Score="1" Text="With you can also insert debug messages into the opengl stream with push/pop debug group to get some context. The opengl objects created can also be named with glObjectLabel." CreationDate="2017-03-21T13:07:06.040" UserId="137" />
  <row Id="7038" PostId="4890" Score="3" Text="The simplest option is to get higher resolution of your shadow map in areas you care about." CreationDate="2017-03-22T09:04:51.003" UserId="137" />
  <row Id="7039" PostId="4890" Score="3" Text="Percentage Closer Filtering" CreationDate="2017-03-22T12:58:04.123" UserId="3470" />
  <row Id="7042" PostId="4895" Score="0" Text="maybe it's the scale, when the height you are timing is 1000 m then falling is going to seem slow." CreationDate="2017-03-23T14:17:37.010" UserId="137" />
  <row Id="7043" PostId="4895" Score="0" Text="I didn't think about that.  I have an 800x800 window and I have the viewport mapped at 1 meter per pixel.  I just googled and it takes around 19 seconds to fall from 800 meters." CreationDate="2017-03-23T14:30:18.810" UserId="6025" />
  <row Id="7044" PostId="4895" Score="1" Text="You should print out all the values to debug this code. The first bug is that `last` is only initialized once: it's the time of the first frame, not the time of the previous frame." CreationDate="2017-03-23T17:01:39.963" UserId="2041" />
  <row Id="7045" PostId="4895" Score="1" Text="actually, it's initialized every frame.  I did debug." CreationDate="2017-03-23T18:36:25.493" UserId="6025" />
  <row Id="7046" PostId="4902" Score="2" Text="You will see circles, due to diffraction" CreationDate="2017-03-24T01:46:08.027" UserId="6290" />
  <row Id="7047" PostId="4890" Score="0" Text="PCF isn't really that good. In fact, PCF is the naive brute force solution for shadow map filtering. It definitely does not deserve to be called &quot;the go to&quot; solution. Even the simplest exponential shadow maps can be linearly filtered without destroying performance." CreationDate="2017-03-24T01:47:10.407" UserId="6285" />
  <row Id="7048" PostId="4890" Score="0" Text="A realistic solution for a game engine is something more like using cascade shadow map partitioning in conjunction with a linearly filterable depth representation that you can pre-filter to the desired degree of smoothness." CreationDate="2017-03-24T01:51:54.443" UserId="6285" />
  <row Id="7051" PostId="4905" Score="0" Text="Yeah, I've read that article. But does this phenomenon occur if we consider idealized film? Where there is no shadowing by the pixel sensor edges and no attenuation due to nonzero color filers thickness. I'm asking about phenomenon caused just be geometry." CreationDate="2017-03-24T20:08:14.883" UserId="5249" />
  <row Id="7052" PostId="4899" Score="1" Text="The problem is that there are hundreds of OpenGL calls in my codebase. Is there a list of OpenGL calls which have as side effect the allocation/deallocation of memory?" CreationDate="2017-03-24T20:18:33.017" UserId="5225" />
  <row Id="7053" PostId="4899" Score="1" Text="Regarding my prior comment, I'm assuming `glBindBuffer` is the key function call to look out for?" CreationDate="2017-03-24T20:23:58.273" UserId="5225" />
  <row Id="7054" PostId="4902" Score="0" Text="What if we assume infinitesimal pinhole and classic optics (no diffraction)." CreationDate="2017-03-24T20:41:42.987" UserId="5249" />
  <row Id="7055" PostId="4906" Score="1" Text="Could you add some clarification? Does &quot;looks like it's spinning on its y axis&quot; mean that it spins only during the process of rotating to face the camera, and is stationary thereafter? If the objects already end up facing the camera, could you explain how they currently move and what is unsatisfactory about it?" CreationDate="2017-03-25T13:00:24.043" UserId="231" />
  <row Id="7056" PostId="4906" Score="1" Text="It would also help to see the code if you are able to post it." CreationDate="2017-03-25T13:08:23.527" UserId="231" />
  <row Id="7057" PostId="4899" Score="1" Text="It depends on which flavor of OpenGL your program uses, I would suggest to monitor `glGenBuffers` / `glDeleteBuffers`, (see the OpenGL wiki's [Buffer Object page](https://www.khronos.org/opengl/wiki/Buffer_Object) ). Also `glCreateProgram` /  `glDeleteProgram`, `glGenSamplers` / `glDeleteSamplers`, etc" CreationDate="2017-03-25T13:26:07.680" UserId="110" />
  <row Id="7058" PostId="4899" Score="0" Text="In my code, there are 20 calls to `glGenBuffers`, and only one call to `glDeleteBuffers`. Does that automatically identify a problem, in that these calls should be 1 for 1 with each other?" CreationDate="2017-03-25T13:28:34.017" UserId="5225" />
  <row Id="7059" PostId="4899" Score="0" Text="No, it is not necessary a problem, you have to check the handles of the objects generated versus deleted, and see if they match." CreationDate="2017-03-25T13:32:52.043" UserId="110" />
  <row Id="7060" PostId="4899" Score="1" Text="A good-practice for multi-platform graphics application (rendering engines, etc) is to wrap the graphics API specific functions under a generic layer (for example, your program has a structure/class `MyAppBuffer` from which are derived `MyAppOpenGLBuffer`, `MyAppDirectXBuffer`, etc. It has the benefit of having all the graphics API functions in a single place, making it easier to monitor. You could try to refactor your code in a similar way." CreationDate="2017-03-25T13:36:35.343" UserId="110" />
  <row Id="7062" PostId="4907" Score="1" Text="Could you elaborate on the problems? Also just to be sure, this doesn't have anything directly to do with OpenGL right?" CreationDate="2017-03-25T16:13:41.017" UserId="4776" />
  <row Id="7063" PostId="4907" Score="0" Text="I am facing problem in implementing object picking using obb algorithm.I am not able to understand what does t1 and t2 means and how the algorithm is working. And for opengl i am using glut library if you explain the algorithm that would be enough." CreationDate="2017-03-25T16:32:12.700" UserId="6320" />
  <row Id="7064" PostId="4815" Score="1" Text="The densest known packing of the regular pentagon is [slightly different](http://blogs.ams.org/visualinsight/2014/12/01/packing-regular-pentagons/)." CreationDate="2017-03-25T22:55:03.243" UserId="106" />
  <row Id="7065" PostId="4908" Score="0" Text="thanks for this i just have couple of questions....first what are the equations of this plane and for 3d i should have a pair of z planes right?" CreationDate="2017-03-26T13:48:13.903" UserId="6320" />
  <row Id="7073" PostId="4905" Score="1" Text="Yes, that's the &quot;natural vignetting&quot; on the wiki page" CreationDate="2017-03-27T03:26:20.610" UserId="1952" />
  <row Id="7074" PostId="4907" Score="0" Text="First of go with GLFW library rather than going with freeglut And check out this tutorial... http://antongerdelan.net/opengl/raycasting.html" CreationDate="2017-03-27T05:17:16.937" UserId="2096" />
  <row Id="7075" PostId="4914" Score="5" Text="That is an anisotropic specular reflection (dominant in 1 direction), an effect usually from brushed metals." CreationDate="2017-03-27T05:37:09.193" UserId="3073" />
  <row Id="7076" PostId="4914" Score="2" Text="check here &gt; http://sungsoo.github.io/papers/brdf-mdf.pdf" CreationDate="2017-03-27T06:34:22.613" UserId="3073" />
  <row Id="7078" PostId="4912" Score="1" Text="Could you add a link to the paper? That way it will be easier for someone to help you." CreationDate="2017-03-27T08:54:25.253" UserId="2041" />
  <row Id="7079" PostId="4907" Score="1" Text="@dkoder Can you edit the question to summarize what you understand already and where specifically you're having trouble? Currently your question is too unclear to really answer. :(" CreationDate="2017-03-27T17:02:34.117" UserId="48" />
  <row Id="7080" PostId="4908" Score="0" Text="Yes for 3D you'd just have to add z as well, the concept stays the same. I can write you a short snippet of the algorithm. Do you have a preferred language? C, Python or?" CreationDate="2017-03-28T00:33:58.307" UserId="4776" />
  <row Id="7081" PostId="4916" Score="0" Text="First of all, have you tries other computers and verified that it isn't your GPU that's the problem? Because I'm facing a similar problem, because my GPU is starting to get a few years behind it. Second of all. Any additional context would greatly help. Did you write the application? Reaching any limits? What does the shader look like? Checked for OpenGL errors?" CreationDate="2017-03-28T00:36:55.363" UserId="4776" />
  <row Id="7082" PostId="4916" Score="0" Text="I'd look at your post processing first, the black artefact at the start looks like it is being blurred each frame by something. Maybe you are rendering something into the wrong buffer somehow ?" CreationDate="2017-03-28T02:36:40.477" UserId="3073" />
  <row Id="7083" PostId="4916" Score="0" Text="FWIW, I have seen issues like this when NaNs enter the pipeline, either by being passed from the CPU or being calculated on the GPU. I can't say whether that's your issue or not, but it did end up looking similar to this." CreationDate="2017-03-28T04:36:25.420" UserId="3003" />
  <row Id="7084" PostId="4920" Score="0" Text="Please dont ask for oppinions. They dont give out clear answers. Yes, it works." CreationDate="2017-03-28T05:05:54.023" UserId="38" />
  <row Id="7085" PostId="4919" Score="4" Text="seems to me that this kind of review is a full half of a PhD thesis" CreationDate="2017-03-28T05:08:32.367" UserId="38" />
  <row Id="7086" PostId="4918" Score="1" Text="Why scan it? I mean most likely there is a model of the bottle somewhere (ask manufacturer) or then modeling it from scratch which usually is not a big deal. Also scanning is more work than taking a hundred pictures." CreationDate="2017-03-28T05:13:20.123" UserId="38" />
  <row Id="7087" PostId="4916" Score="0" Text="@Vallentin I have lots of shaders and framebuffers... that's why i didn't provide any code and in this code there are not many hints of anything.. . There was this issue when my renderer was forward, then I rewrited it to deferred, added some color attachments ping pong (to apply post effects, fog, light) and it worsen the condidtion. There are some gl errors (1280) when adding color attachments to framebuffers which I don't understand because they work at the end. Yeah I didn't check it on other computer. My GPU is quite new (gtx970)" CreationDate="2017-03-28T08:28:39.633" UserId="4958" />
  <row Id="7088" PostId="4916" Score="0" Text="@PaulHK I'll check postprocess and btw white flashes in first gif are from eye adaptation effect. There is pass to calculate average luminance (downsample to 1x1) and these error values seem to confuse this." CreationDate="2017-03-28T08:38:33.037" UserId="4958" />
  <row Id="7089" PostId="4916" Score="0" Text="This is just a renderer done in free time to test some advanced graphic techniques, nothing pro." CreationDate="2017-03-28T08:45:18.113" UserId="4958" />
  <row Id="7090" PostId="4916" Score="0" Text="@PaulHK Yes increasing blur sigma is also increasing the size of squares, so artifacts get blurred." CreationDate="2017-03-28T08:49:14.683" UserId="4958" />
  <row Id="7091" PostId="4918" Score="1" Text="I don't think a rendering approach will work well for you in this case, as a lot of the bottles on your belt will be crushed, which is quite hard to simulate." CreationDate="2017-03-28T08:57:17.907" UserId="2041" />
  <row Id="7092" PostId="4915" Score="0" Text="In deferred way you first loop through your models and render albedo, normals and positions to framebuffer color attachments and then apply all your lights in single draw call irrelevantly to number of models." CreationDate="2017-03-28T11:03:20.850" UserId="4958" />
  <row Id="7093" PostId="4789" Score="0" Text="Hi, I just made a new question: http://computergraphics.stackexchange.com/questions/4917/final-transformation-matrix-to-transform-world-coordinate-into-vrc The problem I am having with your final transformation matrix is that it is not following the steps my lecture slides are telling me. My lecture slides tell me to multiply the rotation matrix with the translation matrix to find the final transformation matrix." CreationDate="2017-03-28T11:18:59.830" UserId="5513" />
  <row Id="7094" PostId="4918" Score="0" Text="@DanHulme I dont know we did it for testing a reverse vending machine for testing and it seemed to work quite well. Altough it is true that real thing has different levels of data. But it was quite much easier to have digital reolicates of 2000 bottles than actually go and buy them. as quite many of the models were available from the vendor with some emailing and asking nicely." CreationDate="2017-03-28T14:17:17.487" UserId="38" />
  <row Id="7095" PostId="4918" Score="0" Text="@joojaa Yeah, in general it can be a really effective technique, but if it's for post-consumption plastic bottles you can't expect them to be the same shape as new bottles" CreationDate="2017-03-28T14:24:42.787" UserId="2041" />
  <row Id="7096" PostId="4918" Score="0" Text="@DanHulme Depends on where you are, in Finland where nearly all bottles and cans are recycled they will in fact all be 90% intact by the time they return as manhandling them means you will not get the deposit back. Glass bottles will be reused as bottles." CreationDate="2017-03-28T14:27:44.393" UserId="38" />
  <row Id="7098" PostId="4920" Score="0" Text="@joojaa Some answers can be based on widely-held community opinions called &quot;conventions&quot;, which I may not know of, which is why I'm asking. After thinking about it, it doesn't seem like I want *every* shader instance to run a tween that I may only want for a handful of vertices." CreationDate="2017-03-28T17:55:35.973" UserId="4991" />
  <row Id="7099" PostId="4920" Score="0" Text="Im fine with the other parts except is it a good idea. Since good is not well defined. This is not a convention thing as its merely a optimization problem, neither is clearly better in a context free situation. But we can not optimize in all possible contexts." CreationDate="2017-03-28T18:33:00.740" UserId="38" />
  <row Id="7100" PostId="4922" Score="0" Text="Could you explain what you do know about Gouraud shading so we can see what level of explanation you require?" CreationDate="2017-03-28T20:50:01.207" UserId="231" />
  <row Id="7101" PostId="4922" Score="0" Text="Aaand yet another homework..." CreationDate="2017-03-28T21:21:01.400" UserId="2479" />
  <row Id="7102" PostId="4916" Score="0" Text="Do you have a method to visualise all your buffers individually ?" CreationDate="2017-03-29T07:18:17.637" UserId="3073" />
  <row Id="7105" PostId="4877" Score="0" Text="I'm looking forward to going through this when time permits. Your results look good compared to reference. I'd be interested to see the images from your renderer with Schlick's. In my renderer, I'm trying to use image based lighting. My gold (with Schlick's) doesn't look great, but I'm not sure if that's because of the BRDF, or the lighting. Think I might move to area lights (is that what you use?), and work in a more controlled environment until I've got things right." CreationDate="2017-03-29T08:40:42.400" UserId="2941" />
  <row Id="7106" PostId="4922" Score="0" Text="@trichoplax I edited the question. I have a basic understanding of Gouraud shading. I think the intensity values change as you go through the polygon. I don't really know how to answer the question or where to start though as the symbols used in videos and research I have done do not match the information given in the question." CreationDate="2017-03-29T09:03:32.827" UserId="5513" />
  <row Id="7107" PostId="4926" Score="0" Text="Great, so we both agree we have to multiply the rotation and translation matrices. I'm going to have to check with my lecturer though which order to do it. He also multiplies the last column of the final transformation matrix (M) by -1 (except for the 1 in M44). Thanks anyways." CreationDate="2017-03-29T10:09:35.977" UserId="5513" />
  <row Id="7108" PostId="4926" Score="0" Text="Can you share the result after checking with your lecturer. Thanks." CreationDate="2017-03-29T10:12:55.893" UserId="6140" />
  <row Id="7109" PostId="4922" Score="1" Text="As an aside, you also need to say whether the colour interpolation is linear in screen space (as would have been the case when the technique was first used) or linear in world (i.e. perspective correct in screen space) as would be the case with modern rendering hardware. (Not entirely sure, but Dreamcast might have been one of the first to do the latter)" CreationDate="2017-03-29T11:05:59.930" UserId="209" />
  <row Id="7110" PostId="4926" Score="0" Text="Yeah, once I get it sure." CreationDate="2017-03-29T12:03:53.283" UserId="5513" />
  <row Id="7111" PostId="4922" Score="0" Text="@Simon F not really sure. This is a past exam question. I literally copied and pasted all the information from that question. This was all the information given." CreationDate="2017-03-29T12:05:38.497" UserId="5513" />
  <row Id="7112" PostId="4922" Score="0" Text="In that case, assuming the light intensity is &quot;1.0&quot;, then given constant, diffuse coefficients and a constant direction *to* the light, normalise your light direction, take the dot product with each of your (unit) normals, clamp each to be positive if necessary, average those scalar vals (because you're at the centre of the triangle) add on the ambient, and you can then multiply this by the diffuse surface colour." CreationDate="2017-03-29T13:46:57.390" UserId="209" />
  <row Id="7116" PostId="4928" Score="0" Text="Yes, I have added a new computeshader.hlsl" CreationDate="2017-03-29T17:07:26.910" UserId="6349" />
  <row Id="7117" PostId="4885" Score="0" Text="It is worth mentioning that placing lightposition into viewspace was an even better solution ;)" CreationDate="2017-03-29T20:33:44.697" UserId="6255" />
  <row Id="7122" PostId="4884" Score="0" Text="If you have found a solution, please post it as an answer rather than editing it into the question. This allows voting on answers. [Self answering is actively encouraged.](http://computergraphics.stackexchange.com/help/self-answer)" CreationDate="2017-03-30T07:01:12.357" UserId="231" />
  <row Id="7124" PostId="4908" Score="0" Text="Sorry for the late reply...python code would be nice" CreationDate="2017-03-30T16:21:13.487" UserId="6320" />
  <row Id="7128" PostId="4937" Score="0" Text="There are several blogs talking about derivatives and it would be cool to have some feedback from people who implemented them in their production engine and why they choose that method." CreationDate="2017-03-31T08:23:22.367" UserId="2372" />
  <row Id="7129" PostId="4937" Score="2" Text="One of the big factors of why things don't immediately get adopted is inertia and that the existing solution being good enough." CreationDate="2017-03-31T08:56:47.627" UserId="137" />
  <row Id="7133" PostId="4918" Score="0" Text="@joojaa Asking the manufacturer is actually the best idea. You're probably right, i was under the impression that i would need at least a couple of thousand images of each bottle. Can you tell me more about the work you did with the reverse vending machine? How did you guys go about generating the data?" CreationDate="2017-03-31T14:28:52.960" UserId="6340" />
  <row Id="7135" PostId="4941" Score="1" Text="Have you tried https://groups.google.com/forum/#!forum/skia-discuss? You're unlikely to get help on CGSE as the question isn't about directly computer graphics, but about using a specific API and file I/O. See http://computergraphics.stackexchange.com/help/on-topic" CreationDate="2017-04-01T08:17:51.377" UserId="2941" />
  <row Id="7139" PostId="4877" Score="0" Text="@PeteUK Yes, I am using only one area light source (a sphere actually). I did not include rendered images with Schlick's so as not to clutter the question. However, I can send you equivalent images rendered with Schlick's Fresnel formulation along with the precise description of the scene so that you can reproduce on your renderer and compare. My email is available on my Github page (see my profile)." CreationDate="2017-04-01T14:42:49.873" UserId="5681" />
  <row Id="7140" PostId="4877" Score="0" Text="Very kind, email on the way..." CreationDate="2017-04-01T16:24:44.620" UserId="2941" />
  <row Id="7141" PostId="4942" Score="0" Text="How are you calculating the contrast ratio after you perform the edge detection? What results are you getting? Also did you leave out half a paragraph right below the images? There's a sentence fragment at the end." CreationDate="2017-04-02T02:20:31.480" UserId="3003" />
  <row Id="7142" PostId="4942" Score="0" Text="@user1118321 I moved it to the bulleted list at the end, nice catch." CreationDate="2017-04-02T03:00:12.897" UserId="6373" />
  <row Id="7143" PostId="4949" Score="1" Text="Thank you for telling me what the name of the operation is; I found a plugin for paint.net called &quot;Modify Channels&quot;, and it serves my purpose perfectly ^^ (; As for the flipping thing, I suppose that would have worked for the indexed image; however, it would not work for anything like swapping just two channels on a sprite sheet, for one example. I probably could've used a more imaginative image than that indexed one for the purpose of this question, but being honest I was frustrated + thus in a rush when I initially posted this question. Though that 'rush' ended up taking quite a while &gt;.&gt; xD" CreationDate="2017-04-02T05:41:07.743" UserId="6376" />
  <row Id="7144" PostId="4942" Score="1" Text="How does your luminosity conversion work? It seems the contrast labelling does not match the actual luminosity output you have made (the red F00 is much darker than the grey 777, even though the label indicates that the F00 should have a higher contrast ratio).." CreationDate="2017-04-03T10:43:54.860" UserId="3073" />
  <row Id="7145" PostId="4942" Score="0" Text="@PaulHK I bet you're onto it. It's the browser's default `luminanceToAlpha`, which is probably fouling up somewhere along the line." CreationDate="2017-04-03T12:37:47.213" UserId="6373" />
  <row Id="7146" PostId="4950" Score="0" Text="Depending on the field of work, a graphics programmer may have zero knowledge of GPU architecture, or may have very detailed knowledge, or anywhere in between. The term &quot;computer graphics programmer&quot; is too broad to be able to give a meaningful answer to this question. You may be able to guide your research by finding examples of the type of thing you want to be able to work on, which will help narrow down which knowledge areas are required/beneficial." CreationDate="2017-04-03T12:49:11.373" UserId="231" />
  <row Id="7147" PostId="4950" Score="0" Text="@trichoplax just updated the description. Hopefully it's a little less broad." CreationDate="2017-04-03T14:25:01.213" UserId="6384" />
  <row Id="7148" PostId="4950" Score="0" Text="Maybe that people run systems that are diverse and a few years old. Relying on the newest prototypes might cause more harm than good, as they have yet to become standard" CreationDate="2017-04-03T15:33:24.143" UserId="6255" />
  <row Id="7149" PostId="4942" Score="1" Text="Indeed, it's typical for luminance to give green the most weight and blue the least. For example, [BT.709 luma](https://en.wikipedia.org/wiki/Rec._709#Luma_coefficients)." CreationDate="2017-04-03T15:38:00.563" UserId="5717" />
  <row Id="7151" PostId="4954" Score="0" Text="Why not density level set on the voxel structure?" CreationDate="2017-04-04T14:30:25.980" UserId="38" />
  <row Id="7152" PostId="4950" Score="1" Text="You might enjoy learning about PowerVR-style tile-based renderers.  The usual books (H&amp;P, etc) don't cover that.  As far as writing OpenGL applications, there is often a matching between vendor-specific extensions and vendor-specific hardware.  Take a look at mobile GPU vendor extensions, programmable blending is the first that comes to mind." CreationDate="2017-04-04T17:47:09.437" UserDisplayName="user3412" />
  <row Id="7153" PostId="1755" Score="0" Text="Plenty of L-System examples (including trees other plants) are available in [VLab](http://algorithmicbotany.org/virtual_laboratory/) (for mac) and [L-Studio](http://algorithmicbotany.org/virtual_laboratory/) (for PC). You can reproduce the figure by downloading their software and executing its L grammar (which is partially available in the book)." CreationDate="2017-02-03T07:36:32.633" UserId="5881" />
  <row Id="7154" PostId="4952" Score="0" Text="I've guessed that the capitalised &quot;PI&quot; was referring to the constant $\pi$ as I couldn't think of an acronym that would fit. If this is not your intention please edit to correct." CreationDate="2017-04-04T21:00:56.893" UserId="231" />
  <row Id="7155" PostId="4932" Score="0" Text="I suppose I'd like to animate any numbers of particles to randomly chosen end points. It could be 100000 particles (where a particle could possibly have multiple vertices). It's a personal project for fun, so no time limit. I don't need the data on the CPU side, but I just need to know when the tween is complete on the CPU side." CreationDate="2017-04-04T23:14:38.323" UserId="4991" />
  <row Id="7156" PostId="4920" Score="0" Text="@joojaa Suppose we're animating 100000 vertices to randomly-chosen end points." CreationDate="2017-04-04T23:15:37.567" UserId="4991" />
  <row Id="7157" PostId="4958" Score="3" Text="The effect will also be more noticeable at the terminator as it would add some variation to an otherwise perfectly smooth curve." CreationDate="2017-04-05T05:38:49.563" UserId="3073" />
  <row Id="7158" PostId="4922" Score="0" Text="I would have thought you would take the average of the 3 vertex normals to get the triangle centre normal. Then normalise your light direction (1,2,2). So (in psuedo/glsl) saturate(dot( (Na+Nb+Nc)/3, normalize(L) ) ) * diffuseColor;  ?" CreationDate="2017-04-05T08:27:22.913" UserId="3073" />
  <row Id="7159" PostId="3800" Score="0" Text="wasn't the original `El` name derived from &quot;emissive light&quot; ? which was not a convolved term, but an additive term in front of the integral. Irradiance should be named `Ir` no ?" CreationDate="2017-04-05T09:14:00.997" UserId="1614" />
  <row Id="7160" PostId="3800" Score="0" Text="@v.oddou E is the usual variable for irradiance that I've seen used in texts and papers. Emissive light is usually denoted $L_e$ (L is radiance)." CreationDate="2017-04-05T13:37:02.617" UserId="48" />
  <row Id="7161" PostId="4961" Score="0" Text="hey that's very interesting. that's probably way easier since our engine is more based on d3d ways. I'll look into that tomorrow" CreationDate="2017-04-05T13:58:50.483" UserId="1614" />
  <row Id="7162" PostId="4957" Score="0" Text="Why the vertical and not the horizontal should be used? The 50px and the 30px is horizontal measurement in the frame..&#xA;And the sizes are what I got by the algorithm, maybe the it not 100% accure" CreationDate="2017-04-05T14:02:35.427" UserId="3305" />
  <row Id="7163" PostId="4958" Score="2" Text="Even if it is a shadowside slope of a mountain gets less light rhan one without. Even self shadowing by some method might be noticeable." CreationDate="2017-04-05T14:18:44.130" UserId="38" />
  <row Id="7164" PostId="4953" Score="0" Text="Hey, I mentioned it in the EDIT that always returning 90 degrees was the mistake I did. But even after that, I got the results as shown in the image. Most of the cylinders went to their correct position but some have deviated a little for some reason. Could be an implementation error again. Thanks for your answer :)" CreationDate="2017-04-06T08:39:04.137" UserId="5121" />
  <row Id="7165" PostId="4965" Score="1" Text="Do you have any recommendation for a library that is simple (can be implemented using only c++ code, no additional software and/or designers), fast, c++ oriented (opposed to written for c, but usable) and lightweight (without non gui modules) ?" CreationDate="2017-04-06T10:40:08.937" UserId="5215" />
  <row Id="7166" PostId="4965" Score="1" Text="Myself I got used to working with QtDesigner IDE + Qt libs, where I design gui using built in designer and subclass widget to get gl context and then use glew to get gl extensions. It is only c++. There are more compact c++ libs for only gui making, like wxwidgets, maybe you should check that out, I don't have much experience with them though." CreationDate="2017-04-06T13:20:22.720" UserId="4958" />
  <row Id="7170" PostId="4967" Score="0" Text="Some of your pros seem to be about not using precomputed tangent space (i.e. deriving the tangent space from UV derivatives per-pixel), which AFAIK is a separate design choice, independent from the choice of derivative maps vs normal maps." CreationDate="2017-04-06T21:32:47.497" UserId="48" />
  <row Id="7171" PostId="4970" Score="2" Text="Are you familiar with differentials in calculus, like $dy/dx$ and such? Are you familiar with the concept of solid angle?" CreationDate="2017-04-06T23:33:58.497" UserId="48" />
  <row Id="7172" PostId="4970" Score="0" Text="@NathanReed yes I am familiar with dy/dx but I am not familiar with concept of solid angle." CreationDate="2017-04-07T00:31:14.203" UserId="6282" />
  <row Id="7173" PostId="4970" Score="0" Text="@NathanReed ok soI did more research and I think I have better understanding of solid angle. Solid angle ranges from 0~4 pi. But what does differential of solid angle of the normal vector at &quot;i&quot; mean?" CreationDate="2017-04-07T05:58:46.217" UserId="6282" />
  <row Id="7174" PostId="4970" Score="0" Text="It looks like an integral that's been quantified." CreationDate="2017-04-07T08:40:24.780" UserId="137" />
  <row Id="7175" PostId="4968" Score="0" Text="For some reason glClientWaitSync always fails for me, if timeout is non-0. Even if it's 1, then there's console warning:&#xA;WebGL: INVALID_OPERATION: clientWaitSync: timeout &gt; MAX_CLIENT_WAIT_TIMEOUT_WEBGL" CreationDate="2017-04-07T11:59:24.640" UserId="6404" />
  <row Id="7176" PostId="4967" Score="0" Text="Thanks for the comment @NathanReed Are you talking about the fact of calculating tangents and binormals ? Could you tell me more about precomputed tangent space ?" CreationDate="2017-04-07T15:12:59.450" UserId="2372" />
  <row Id="7177" PostId="4971" Score="0" Text="Also, metalness is often a binary value, and assuming that you will not use roughness gradients (so as to see the impact of reduced precision), you could store metalness and roughness in a single 8 bit channel (1 bit for metalness and 7 for roughness)." CreationDate="2017-04-07T15:33:34.287" UserId="270" />
  <row Id="7178" PostId="4972" Score="1" Text="Could you be a little more specific as to what you don't really understand? Is it how the waves are generated and the parameters? I'm sure many people here could help you." CreationDate="2017-04-07T19:12:53.157" UserId="5256" />
  <row Id="7179" PostId="4968" Score="0" Text="Did you verify that you're using WebGL 2.0?" CreationDate="2017-04-07T21:54:48.370" UserId="4776" />
  <row Id="7180" PostId="4968" Score="0" Text="Of course, context is WebGL2RenderingContext, and I use other WebGL2.0-only features." CreationDate="2017-04-08T00:10:38.807" UserId="6404" />
  <row Id="7181" PostId="4968" Score="0" Text="Check whether `sync` is `0` or `null`. Also [check whether your browser supports it](https://developer.mozilla.org/en-US/docs/Web/API/WebGL2RenderingContext/fenceSync#Browser_compatibility)." CreationDate="2017-04-08T00:13:30.870" UserId="4776" />
  <row Id="7182" PostId="4972" Score="0" Text="@ArjanSingh Yes! I updated the question." CreationDate="2017-04-08T02:49:27.947" UserId="6407" />
  <row Id="7183" PostId="4808" Score="3" Text="Specifically what kind of constraints do you want to include?" CreationDate="2017-04-08T05:29:49.060" UserId="48" />
  <row Id="7184" PostId="4968" Score="0" Text="`sync` works fine, I can get it's status with `getSyncParameter`. It is only the `clientWaitSync` that has issues. Chrome 57 in macOS 10.12.4 on MacBook mid-2012." CreationDate="2017-04-08T10:25:07.883" UserId="6404" />
  <row Id="7185" PostId="4968" Score="0" Text="Try and check `gl.MAX_CLIENT_WAIT_TIMEOUT_WEBGL` maybe `gl.clientWaitSync()` just isn't allowed in Chrome on MacOS." CreationDate="2017-04-08T16:46:35.120" UserId="4776" />
  <row Id="7187" PostId="4978" Score="0" Text="Since the objective of the chapter is to implement Lambert could you suggest an alternative implementation? Where does the grey color from the sphere come from, is it set anywhere?" CreationDate="2017-04-10T10:54:42.507" UserId="5256" />
  <row Id="7189" PostId="1793" Score="1" Text="this is a bit late, but [here](https://i.stack.imgur.com/nKqSd.jpg) you can see a STATIC mesh with 24.000.000 Vertices in Blender.&#xA;And i can rotate it SMOOTH with 40 FPS. I think it is just amazing what modern graphic cards can do." CreationDate="2017-04-09T15:42:59.287" UserId="6420" />
  <row Id="7191" PostId="4983" Score="0" Text="What is the error you get? Do you have the debug layer turned on?" CreationDate="2017-04-10T22:00:24.873" UserId="48" />
  <row Id="7192" PostId="4983" Score="0" Text="Yes, I have debug layer enabled.&#xA;when this is called :&#xA;&#xA;ThrowIfFailed(D3DX12SerializeVersionedRootSignature(&amp;computeRootSignatureDesc_madhu, featureData_madhu.HighestVersion, &amp;signature1, &amp;error1));&#xA;&#xA;an exception occures, stepping through the code I see that the handle returned into the signature is invalid" CreationDate="2017-04-10T22:38:55.377" UserId="6349" />
  <row Id="7193" PostId="4983" Score="0" Text="Ok. So is there any output from the debug layer about the error? If you're using Visual Studio, it would show up in the Output pane. Alternatively, is there any error message generated in the `error1` blob?" CreationDate="2017-04-10T22:44:28.947" UserId="48" />
  <row Id="7194" PostId="4983" Score="0" Text="Not sure how to check the error from &quot;error1&quot; blob.&#xA;But the return value is E_INVALIDARG- one or more arguments are invalid." CreationDate="2017-04-10T22:50:55.130" UserId="6349" />
  <row Id="7195" PostId="4983" Score="0" Text="You can print the error blob to the output pane like this: `if (error1) { OutputDebugStringA((char *)error1-&gt;GetBufferPointer());	}`" CreationDate="2017-04-10T23:04:43.280" UserId="48" />
  <row Id="7196" PostId="4983" Score="0" Text="Shader register range of type UAV (root parameter [1], visibility ALL, descriptor table slot [0]) overlaps with another shader register range (root parameter[0], visibility ALL, descriptor table slot [0]).&#xA;&#xA;&#xA;&#xA;This is the error that is printed. &#xA;Is there something that I could do to manage this?" CreationDate="2017-04-10T23:18:34.163" UserId="6349" />
  <row Id="7197" PostId="4985" Score="0" Text="That makes sense, though I doubt I'm running out of VRAM honestly because last time I checked these textures are only ~40MB each or so and a 960M has 4GB so there should plenty. Also yeah that whole prioritize uninitialized and first in/first out system of managing the tiles should always make sure it updates the oldest freed tile. I'll try that call though to see if it works." CreationDate="2017-04-11T00:34:59.973" UserId="4837" />
  <row Id="7198" PostId="4985" Score="0" Text="Well I implemented the `glInvalidateTexSubImage` thing when the tiles are freed (assuming I did it right, it calls it on each mipmap level for the layer in question) and it doesn't seem to change anything. I'm going to keep it because it probably will be helping the GPU in some way but I guess sending something to NVIDIA might be the best option now (I'll likely try to make a minimal example of it in a small program later when I have time). For now I'm also going to try initializing the entire texture upfront to see if that helps." CreationDate="2017-04-11T17:58:25.537" UserId="4837" />
  <row Id="7199" PostId="4984" Score="0" Text="Yes this solved the problem. Thank you" CreationDate="2017-04-11T21:12:41.733" UserId="6349" />
  <row Id="7200" PostId="4989" Score="0" Text="Probably you have problems in D3dcompilefromfile. I would suggest you to print the return of GetAssetFullPath and verify if It has the expected file location." CreationDate="2017-04-11T21:32:50.467" UserId="303" />
  <row Id="7201" PostId="4989" Score="0" Text="More info:&#xA;when i printed the debug error it says the following:&#xA;D3D12GetDebugInterface: This method requires the D3D12 SDK Layers for Windows 10, but they are not present on the system.&#xA;&#xA;would this mean something?" CreationDate="2017-04-11T21:42:27.780" UserId="6349" />
  <row Id="7202" PostId="4989" Score="0" Text="You dont have directx sdk properly installed or dont have It linked correctly on the binaries of your program. You should check the linker setup in your compilation routine." CreationDate="2017-04-11T22:13:52.373" UserId="303" />
  <row Id="7203" PostId="4989" Score="0" Text="But then the NbodyGravity sample runs perfectly well. If the SDk was not properly configured, how is that possible?" CreationDate="2017-04-11T22:23:05.020" UserId="6349" />
  <row Id="7204" PostId="4989" Score="0" Text="If the sample is running, then you should check if your program is linked with the same libraries as the sample." CreationDate="2017-04-11T22:26:05.793" UserId="303" />
  <row Id="7205" PostId="4989" Score="0" Text="Yes, all i am doing is adding code to the same sample. i am not deleting or changing any path as such" CreationDate="2017-04-11T22:56:03.413" UserId="6349" />
  <row Id="7206" PostId="4991" Score="0" Text="If the second error message is accurate, you'd need to download the mentioned SDK layers and see if there is still a problem. If the second error message is not accurate then we'll need to know that before investigating." CreationDate="2017-04-12T07:05:10.973" UserId="231" />
  <row Id="7207" PostId="4989" Score="2" Text="Next time you want to add more information to your question, you should use the &quot;edit&quot; link instead of posting a whole new question." CreationDate="2017-04-12T08:55:37.637" UserId="2041" />
  <row Id="7208" PostId="4982" Score="0" Text="Could it be some kind of false sharing: the memory you're overwriting with the texture upload shares cache lines with parts of the atlas that the GPU is reading at the same time? What size &amp; shape is the region of the atlas you're overwriting?" CreationDate="2017-04-12T09:04:46.847" UserId="2041" />
  <row Id="7210" PostId="4982" Score="0" Text="@DanHulme Since its a 2D Array Texture I just load in each mipmap level of a layer I want to overwrite. I have 50 or so layers of the 512 layer texture in use by the GPU and the layers which cause the odd performance issues when loading are right next to them since while finding new layers it wraps around from the end after walking through all the uninitalized layers back to the oldest &quot;free&quot; page in a list, which happens to be right next to that active range of texture layers. I thought it had something to do with that but the fact that it only happens the first time made me think otherwise." CreationDate="2017-04-12T16:26:47.467" UserId="4837" />
  <row Id="7211" PostId="4982" Score="0" Text="Also I'm going to attempt to make a minimal example of this problem once I have some time to as I am making a lot of assumptions most likely about what is going on, not to mention all of the other things going on in this project which could be affecting this texture loading (there's a lot of places where I use `glClientWaitSync` and whatnot for synchronization but I doubt those would cause issues with something like this)." CreationDate="2017-04-12T16:34:47.830" UserId="4837" />
  <row Id="7212" PostId="4987" Score="0" Text="The orientation trick from your first paragraph works perfectly as long as the object is closed and all faces have coherent orientation. It is my favorite solution." CreationDate="2017-04-12T17:32:23.597" UserId="5717" />
  <row Id="7213" PostId="4995" Score="3" Text="Why not just take the curves that Inkscape outputs and convert them to poly lines? It's fairly simple to convert a Bezier curve to a poly line." CreationDate="2017-04-13T05:34:17.240" UserId="3003" />
  <row Id="7214" PostId="4990" Score="0" Text="It looks like your perspective projection calculation is wrong. To compute a 2d screen coordinate from a 3d vector I would use something roughly like   Y' = Y / (Z + nearZ);  rather than Y' = 400 - Z * cos(rayAngle);" CreationDate="2017-04-13T06:51:54.463" UserId="3073" />
  <row Id="7217" PostId="4995" Score="0" Text="Why specifically a polyline? It's a much worse approximation to a curve than a Bezier spline. Regardless, I don't think you'll find a tool that will give you a &quot;clean&quot; polyline output without needing any manual tweaking." CreationDate="2017-04-13T07:16:57.893" UserId="2041" />
  <row Id="7218" PostId="4994" Score="0" Text="Nice answer. The OP also asked about Cook-Torrance importance sampling which this answer doesn't touch upon." CreationDate="2017-04-13T08:22:53.040" UserId="2941" />
  <row Id="7219" PostId="4994" Score="5" Text="I updated the answer to add a section about Cook-Torrance" CreationDate="2017-04-13T14:25:05.987" UserId="310" />
  <row Id="7220" PostId="4991" Score="0" Text="I think you can download SDK layer in &quot;Settings -&gt; Apps and Features -&gt; Manage optional features&quot;." CreationDate="2017-04-13T15:37:00.133" UserId="67" />
  <row Id="7221" PostId="4985" Score="0" Text="You could also try to use PBOs to map memory client side from the driver. Then you'd fill up the PBOs, unmap the memory, and queue up a `glCompressedTexSubImage2D` to let the driver do it's own upload." CreationDate="2017-04-14T06:24:35.600" UserId="197" />
  <row Id="7222" PostId="4985" Score="0" Text="@Mokosha Yeah I am probably going to do that for my next project, I was just not aware that was a thing at the time of making this and due to time constraints I can't exactly implement it now." CreationDate="2017-04-14T06:36:44.987" UserId="4837" />
  <row Id="7224" PostId="4995" Score="0" Text="Section 4 of  the paper mentions that polylines can clearly define the contour segments. Those segments are used to compute the local Shape Context descriptors." CreationDate="2017-04-14T14:00:03.810" UserId="6435" />
  <row Id="7225" PostId="4995" Score="0" Text="I couldn't link the paper in my question because I am new. Here it is:&#xA;&#xA;https://www.cg.tuwien.ac.at/research/publications/2014/Guerrero-2014-TPS/" CreationDate="2017-04-14T14:00:29.533" UserId="6435" />
  <row Id="7226" PostId="4998" Score="0" Text="I understand that I need a matrix (9 numbers) for each color. So I will need a matrix for red, a matrix for green, and a matrix for blue. However you are telling me these 9 numbers has to be y0,y−11,y01,y11,y−22,y−12,y02,y12,y22 but I don't understand how to calculate these matrix values. I don't understand what you mean by&quot; nx3 matrix b with the color at that point?&quot; What is &quot;n&quot; here? Do you mean just RGB values?" CreationDate="2017-04-15T02:26:50.430" UserId="6282" />
  <row Id="7227" PostId="4999" Score="1" Text="Can you post the resulting images? Is it a significant difference, or is it possible that the different ways of calculating the values lead to slight differences in rounding here and there and lead to images that are very similar, but not identical?" CreationDate="2017-04-15T03:16:21.960" UserId="3003" />
  <row Id="7228" PostId="4998" Score="0" Text="You don't need to do linear regression. Since SH form an orthonormal basis, each SH's coefficient will be equal to the inner product of the cubemap with the SH: the spherical integral of (cubemap * SH). Or in other words, sum (pixel color * SH evaluated at that pixel's direction vector * solid angle of the pixel) over all pixels in the cubemap." CreationDate="2017-04-15T05:45:21.223" UserId="48" />
  <row Id="7229" PostId="4999" Score="0" Text="@user1118321 I've updated the question with many more details, please give a look at it" CreationDate="2017-04-15T11:05:04.900" UserId="6454" />
  <row Id="7230" PostId="5002" Score="0" Text="Can you give an example of what you think isn't legible in regular 3D text rendering? There are certainly other techniques, but it's not clear what you're asking about." CreationDate="2017-04-15T15:29:24.370" UserId="3003" />
  <row Id="7231" PostId="5002" Score="0" Text="For example, the program [Virtual Desktop](https://www.oculus.com/experiences/rift/911715622255585/) shows all of your screens in VR space using the HTC Vive, Oculus Rift, etc. The text is barely legible in these environments (i.e., not good enough to use VR as a replacement for physical monitors)." CreationDate="2017-04-15T16:17:24.090" UserId="5225" />
  <row Id="7232" PostId="5002" Score="0" Text="So you mean the 2D text rendered in a 3D space? The video on the page you linked to didn't strike me as looking terrible. When they were browsing the web, it was quite legible. (Of course I'm looking at it on my laptop, not on Vive, so maybe it's worse in real life?)" CreationDate="2017-04-15T16:22:11.580" UserId="3003" />
  <row Id="7233" PostId="5002" Score="0" Text="Yes -- much worse in real life. You can read text for a few minutes if you squint your eyes, but it's not good enough to fully replace a PC screen by any stretch." CreationDate="2017-04-15T18:24:55.240" UserId="5225" />
  <row Id="7235" PostId="4995" Score="0" Text="@user1118321 No it's not? Can you elaborate on how to do that? How do you even parse the svg path attribute?" CreationDate="2017-04-17T05:50:49.660" UserId="6435" />
  <row Id="7236" PostId="5002" Score="0" Text="Super sampling could help. Applies to everything, not just text." CreationDate="2017-04-17T06:01:46.727" UserId="67" />
  <row Id="7237" PostId="4995" Score="1" Text="I've added an answer explaining how it would work." CreationDate="2017-04-17T06:13:26.903" UserId="3003" />
  <row Id="7238" PostId="5005" Score="0" Text="Thank you! Could you also tell me if this is possible to do in MATLAB? If so, how?" CreationDate="2017-04-17T06:46:11.477" UserId="6435" />
  <row Id="7240" PostId="5005" Score="0" Text="I haven't used MATLAB in like 20 years, so I'm not really the one to ask. However, you can probably ask MATLAB questions on [Stack Overflow](http://stackoverflow.com/)." CreationDate="2017-04-17T16:11:46.790" UserId="3003" />
  <row Id="7241" PostId="5005" Score="0" Text="Ok. What platform is the SVG DOM interface implemented on? I can't decide what to use for this project.&#xA;&#xA;Also I don't think what you're suggesting will work for the kind of path I have. I have many curves and control points in one single d attribute." CreationDate="2017-04-17T17:11:15.853" UserId="6435" />
  <row Id="7242" PostId="5005" Score="0" Text="Sorry, I mean many curves and move to commands in a single d atrribute" CreationDate="2017-04-17T17:19:07.407" UserId="6435" />
  <row Id="7244" PostId="5001" Score="0" Text="Hi. Thank you for detailed instruction. It helps a lot to understand the paper. However I am wondering how you arrived to &quot;cosine lobe in SH&quot; are these numbers ratio given to you in the paper? If so I am not being able to find that section." CreationDate="2017-04-17T21:37:52.423" UserId="6282" />
  <row Id="7245" PostId="4998" Score="0" Text="What do you mean by 9x3 matrix x of the unknown coefficients(9 for each color)? So there are three rows? And each row is all the same number that represents either color red or green or blue? Is my job then just multiplying matrix A to mat x to get matrix of 3x3 b?" CreationDate="2017-04-17T22:10:28.580" UserId="6282" />
  <row Id="7246" PostId="5001" Score="0" Text="@BlueBug In diffuse lighting lambertian model ray is reflected at many angles rather than as in specular, in one direction. So, in order to approximate this and evaluate irradiance at given point the convolution on cosine lobe is performed - this means in case of SH - division by, for each of SH order 1, 2, 3 - [pi, (2/3)*pi, pi/4]. I hope it helps, check also [this](https://seblagarde.wordpress.com/2012/01/08/pi-or-not-to-pi-in-game-lighting-equation/) or [this](https://en.wikipedia.org/wiki/Lambert%27s_cosine_law)" CreationDate="2017-04-17T22:49:06.833" UserId="4958" />
  <row Id="7247" PostId="5001" Score="0" Text="I meant not divison but multiplication, sorry." CreationDate="2017-04-18T00:06:15.890" UserId="4958" />
  <row Id="7249" PostId="5001" Score="0" Text="After reading your answer I edited my question to ask you with some description. I don't understand why you don't do the green part of the equation. Are you already doing it but then I am not being able to see what you are doing? Or is it that there are more than one way to make SH map?" CreationDate="2017-04-18T00:59:47.847" UserId="6282" />
  <row Id="7250" PostId="5005" Score="0" Text="It should work just fine for many curves and move to commands. Each bezier can be handled as above to turn it into poly lines. they will connect with the next one in the sequence. As for which platform to use, that's not something I can answer. I've done SVG parsing using a standard XML parser in Objective-C (Apple's NSXMLParser), but it could be done in any language for which you have an XML parsing library. Good luck!" CreationDate="2017-04-18T02:02:34.350" UserId="3003" />
  <row Id="7251" PostId="5005" Score="0" Text="I see. I am just going to try to get contour segments from a raster image and not bother with this. This seems way too complicated" CreationDate="2017-04-18T03:28:55.323" UserId="6435" />
  <row Id="7252" PostId="5006" Score="1" Text="try some values, the coefficient that is near 1 when the point inside is close to a vertex belongs to that vertex." CreationDate="2017-04-18T08:04:32.533" UserId="137" />
  <row Id="7253" PostId="5006" Score="1" Text="@ratchetfreak I did that, but I'm also curious how to mathematically find the respective coefficient." CreationDate="2017-04-18T09:01:48.193" UserId="2687" />
  <row Id="7255" PostId="5001" Score="1" Text="Instead of weighting each cube map sample equally, it would be better to weight by the solid angle of each pixel. Otherwise, you'll over-weight the corners and under-weight the face centers." CreationDate="2017-04-18T20:04:41.240" UserId="48" />
  <row Id="7257" PostId="5003" Score="1" Text="I wouldn't worry so much about generating images directly. Rather, write an application that does what you need in whatever language feels most appropriate, then capture the rendered output to an image (e.g. similar to what happens when you click &quot;Print&quot; while viewing a web page). As far as what library you can or should use, I have no idea (nor is this the appropriate place to ask). The latter problem seems rather special purpose and I suspect you'll be mostly starting from scratch." CreationDate="2017-04-19T12:48:16.093" UserId="6145" />
  <row Id="7264" PostId="4994" Score="0" Text="For example GGX, to sample spherical coordinates angle cos(θ) we use the importance sampled formula to calculate the angle and use that in GGX as usual right? Or does the formula replace GGX entirely?" CreationDate="2017-04-19T15:22:30.663" UserId="5256" />
  <row Id="7265" PostId="5005" Score="0" Text="@MrWarlock616 its far  less complicated than implementing your own tracer. is just text processing. but then you most likely will not need to implement this as such applications allready exist, like say inkscape." CreationDate="2017-04-19T21:10:26.157" UserId="38" />
  <row Id="7266" PostId="4994" Score="2" Text="I added a section to help answer your questions. But, in short, your first method is correct. You use the sampling formula to generate a direction, then you use that new direction in the normal GGX formula *and* to get the pdf for the Monte Carlo formula." CreationDate="2017-04-19T21:26:38.637" UserId="310" />
  <row Id="7267" PostId="5009" Score="0" Text="Awesome, I'm looking for this kind of explanation. BTW, there is small mistake, the $V_1$ should be $V_0$, $V_2$ should be $V_1$ and $V_3$ should the vector pointing to $T$. Hopefully you have the time to fix that error. Thanks once again, hopefully many people benefit from this." CreationDate="2017-04-20T05:04:12.603" UserId="2687" />
  <row Id="7268" PostId="5009" Score="1" Text="Argh, @Bla... yes i will redraw the pictures. Anyway i am feeling that the variables in the code could have more understandable names. $V_0$ - $V_2$ offcourse being reasonable names but would be much better to name them with same nomenclature as your points." CreationDate="2017-04-20T05:34:08.433" UserId="38" />
  <row Id="7269" PostId="5009" Score="0" Text="I just changed the variable names, hopefully now it becomes clearer." CreationDate="2017-04-20T06:32:07.133" UserId="2687" />
  <row Id="7270" PostId="5014" Score="0" Text="You might already know this, but caustics can be rendered with regular old path tracing as well. It just takes a lot of samples to look decent." CreationDate="2017-04-20T06:41:32.280" UserId="56" />
  <row Id="7271" PostId="5014" Score="0" Text="What are the benefits of using photon mapping as opposed to regular path tracing? Is it just render times or will I be getting a significant difference in terms of how good it looks?" CreationDate="2017-04-20T07:45:54.590" UserId="5256" />
  <row Id="7272" PostId="5001" Score="0" Text="@NathanReed Where? During accumulation to the buffer?" CreationDate="2017-04-20T08:58:27.287" UserId="4958" />
  <row Id="7273" PostId="5001" Score="0" Text="Right, when computing the SH coefficients from the cube map." CreationDate="2017-04-20T13:21:01.773" UserId="48" />
  <row Id="7274" PostId="5012" Score="0" Text="I think you can also use [ID3D12Resource::ReadFromSubresource](https://msdn.microsoft.com/en-us/library/windows/desktop/dn914415(v=vs.85).aspx)&#xA; it is a little simpler." CreationDate="2017-04-20T13:57:11.980" UserId="3123" />
  <row Id="7275" PostId="5001" Score="0" Text="@NathanReed Myself I am using pretty uniform poisson sampling to avoid biasing samples." CreationDate="2017-04-20T14:17:59.650" UserId="4958" />
  <row Id="7276" PostId="5016" Score="1" Text="What happens when you run it? Can you provide a screenshot as a visual reference?" CreationDate="2017-04-20T14:24:06.153" UserId="6145" />
  <row Id="7277" PostId="5016" Score="0" Text="@Dan I tried with the directional and positional lights turned off and only with the spotlight turned on and the cube appears to be all black, so a scene without lights, so I assumed that the spotlight doesn't work" CreationDate="2017-04-20T14:56:01.290" UserId="6401" />
  <row Id="7278" PostId="4484" Score="1" Text="Just one question your gradient texture is rectangular texture . Just wanted to know in your application code you have not accidently enabled texture filtering modes .&#xA;Only min filter GL_TEXTURE_MIN_FILTER should be set to GL_NEAREST or GL_LINEAR . Also wrapping modes for each coordinate must be either GL_CLAMP_TO_EDGE or GL_CLAMP_TO_BORDER.&#xA;Also see mipmap is not turned on .&#xA;One more thing in your shader make sure texelFetch pos is not normalised." CreationDate="2017-04-19T19:19:22.147" UserId="6475" />
  <row Id="7279" PostId="5012" Score="0" Text="But I am trying to read out of a compute buffer. I was under the impression that ReadFromSubresource is used primarily on textures and layouts." CreationDate="2017-04-20T16:37:28.903" UserId="6349" />
  <row Id="7280" PostId="1506" Score="0" Text="Great answer; thank you. I had never come across the distinction between bi- and covectors, but it clarifies things for me. Bivectors obviously have a direct geometric algebra representation; do you know if covectors also do?" CreationDate="2017-04-20T16:43:23.850" UserId="6478" />
  <row Id="7281" PostId="5016" Score="2" Text="Unfortunately I don't have the time to debug your shader for you, but I would start by removing things from the shader code incrementally until it works and backtrack from there to find the problem. Bugs that cause nothing to render at all are the worst ones to find!" CreationDate="2017-04-20T17:10:14.440" UserId="6145" />
  <row Id="7282" PostId="5005" Score="0" Text="@joojaa It's not that simple, really, think about it. I basically have to implement the SVG DOM in Matlab." CreationDate="2017-04-20T17:23:56.820" UserId="6435" />
  <row Id="7283" PostId="5005" Score="0" Text="@MrWarlock616 you dont need a DOM  (or a SAX)  to do this. It makes it is slightly more fragile without one but not really a big deal. In any case matlab has a XML DOM implementation (called **xmlread**) so your complaint is totally irrelevant. See [docs](https://se.mathworks.com/help/matlab/ref/xmlread.html)" CreationDate="2017-04-20T17:36:01.670" UserId="38" />
  <row Id="7284" PostId="5016" Score="0" Text="I've tried to remove the other two lights but I can't find the bug!" CreationDate="2017-04-20T17:55:31.783" UserId="6401" />
  <row Id="7285" PostId="5005" Score="0" Text="@joojaa I've seen the docs. I could retrieve the d attribute using xmlread and all that but I don't have the time to implement a parser for the d attribute. I just care about the point coordinates since I'm trying to replicate the results of a paper." CreationDate="2017-04-20T17:57:15.193" UserId="6435" />
  <row Id="7286" PostId="1506" Score="0" Text="@PaulDuBois I think covectors are what geometric algebra terms a dual vector. In 3D GA, the dual of a vector is a bivector; it doesn't make the distinction in the algebra. But they still transform differently: one should use the outermorphism for bivectors, and the dual transform for dual vectors. (And again, in 3D I think those come out the same, up to a scale factor.)" CreationDate="2017-04-20T20:46:23.197" UserId="48" />
  <row Id="7287" PostId="5016" Score="0" Text="Keep removing features (e.g. texturing), one at a time. Maybe try a very simple shader that just makes everything white, then start adding features back once you have something that renders." CreationDate="2017-04-20T21:04:53.473" UserId="6145" />
  <row Id="7288" PostId="5012" Score="0" Text="I never used this with `UAV` but maybe it will work, just set `DstRowPitch` and `DstDepthPitch` to buffer size (in bytes) and you will see if its work. I don't see why it shouldn't work (but maybe it don't)" CreationDate="2017-04-20T21:18:10.633" UserId="3123" />
  <row Id="7289" PostId="5016" Score="0" Text="Yeah I did it, maybe I just need a good reference in order to understand better the use of a spotlight, it's not really clear in the online material that I've found." CreationDate="2017-04-20T21:40:42.403" UserId="6401" />
  <row Id="7290" PostId="5012" Score="0" Text="I created the Input buffer with the following properties:&#xA;&#xA;ThrowIfFailed(m_device-&gt;CreateCommittedResource( &amp;defaultHeapProperties, D3D12_HEAP_FLAG_NONE, &amp;bufferDesc,&#xA;D3D12_RESOURCE_STATE_COPY_DEST, nullptr,&#xA;IID_PPV_ARGS(&amp;m_InputBuffer1[0])));&#xA;&#xA;and later on when I try to read it back as follows, the compiler throws an exception!&#xA;&#xA;UINT8* pDataBegin;&#xA;m_InputBuffer1[0]-&gt;ReadFromSubresource(reinterpret_cast&lt;void**&gt;(&amp;pDataBegin),5,5,0,0);&#xA;printf(&quot;The value pointed to is %d&quot;, *pDataBegin);&#xA;&#xA;any idea why this is happening ?" CreationDate="2017-04-21T00:43:15.963" UserId="6349" />
  <row Id="7292" PostId="5017" Score="1" Text="One approach can be to render the back faces of a mesh to a depth buffer to compute an 'optical depth' which won't work so well for highly concave meshes but looks decent enough. http://http.developer.nvidia.com/GPUGems/gpugems_ch16.html" CreationDate="2017-04-21T05:54:21.453" UserId="3073" />
  <row Id="7293" PostId="5012" Score="0" Text="`ID3D12Resource::ReadFromSubresource` doesn't allocate memory for you and it get pointer not pointer to pointer, so you need to allocate memory and cast `pDataBegin` to `void*` not `void**`. Remember to free allocated memory" CreationDate="2017-04-21T09:07:35.853" UserId="3123" />
  <row Id="7294" PostId="5012" Score="0" Text="Also i'm not sure about `D3D12_RESOURCE_STATE_COPY_DEST` but i don't see whole code so can't tell if its should be this resource state." CreationDate="2017-04-21T09:18:06.320" UserId="3123" />
  <row Id="7295" PostId="5018" Score="0" Text="It looks like you're stretching the texture rather than tiling. Tiling wouldn't deform the rounded corners, but it would give you multiple copies of the rounded corners, and you wouldn't be able to fix it the way you propose." CreationDate="2017-04-21T15:32:46.923" UserId="2041" />
  <row Id="7296" PostId="5018" Score="0" Text="@DanHulme Yeah my bad, I misspoke. Instead of increasing the tiling, I am lowering it, that's why I am having the stretch effect. But this is the expected effect behaviour. I just want to avoid the deformation caused by the stretching." CreationDate="2017-04-21T15:38:52.257" UserId="2372" />
  <row Id="7297" PostId="5018" Score="0" Text="I edited my question. Sorry about that." CreationDate="2017-04-21T15:40:55.090" UserId="2372" />
  <row Id="7298" PostId="5018" Score="1" Text="I realise your example textures are quite simple, but if you ever have to do something more complex and still want to avoid the &quot;distortion&quot;, [Seam Carving might be of interest:](http://www.win.tue.nl/~wstahw/edu/2IV05/seamcarving.pdf) When I saw it presented it almost seemed like magic." CreationDate="2017-04-21T17:01:04.310" UserId="209" />
  <row Id="7299" PostId="5019" Score="3" Text="Another good one is [Pre-Integrated Skin Shading](http://advances.realtimerendering.com/s2011/Penner%20-%20Pre-Integrated%20Skin%20Rendering%20(Siggraph%202011%20Advances%20in%20Real-Time%20Rendering%20Course).pptx) from SIGGRAPH a few years ago." CreationDate="2017-04-21T20:27:42.410" UserId="48" />
  <row Id="7302" PostId="5021" Score="0" Text="Could you expand on the requirements? When you say &quot;texture&quot;, does this require variation in light emission across the surface, or can it just be a uniformly glowing cube? Does it need to affect nearby objects or just to appear emissive alone? Writing down exactly what is required may also help you come up with ideas." CreationDate="2017-04-23T16:37:28.247" UserId="231" />
  <row Id="7303" PostId="5021" Score="0" Text="@trichoplax this is the assignment: &quot;combine the lighting with a texture (256x256) that models the emission of light by the object. Generate this texture randomly with values between 0 and 128 for each component. The amount of light emitted by the object must be added to the computed lighting&quot;" CreationDate="2017-04-23T16:58:37.423" UserId="6401" />
  <row Id="7304" PostId="5021" Score="0" Text="@trichoplax By the way, I think I have to apply a kind of &quot;glowing&quot; texture that emits light, so it should affect nearby objects" CreationDate="2017-04-23T18:54:41.777" UserId="6401" />
  <row Id="7305" PostId="5021" Score="1" Text="If it's an assignment, I'd recommend double checking exactly what is required with your teacher/tutor/lecturer so you don't end up giving yourself a bigger task than is required." CreationDate="2017-04-23T19:40:57.910" UserId="231" />
  <row Id="7306" PostId="5021" Score="0" Text="hm normally you'd do multiple passes, somehting like this: http://io7m.com/documents/glow-maps/s3.xhtml idk if you can render to a frame buffer object with webgl. I guess the question is: does webgl support frame buffer objects?" CreationDate="2017-04-24T00:14:10.500" UserId="6255" />
  <row Id="7307" PostId="5018" Score="2" Text="User interfaces like Android use something call 'nine patches' which breaks the texture into a 3x3 grid with different scaling rules such that the corners are preserved and only the interiors are scaled in 1 or both axis. Further reading here: http://en.miui.com/thread-26099-1-1.html" CreationDate="2017-04-24T05:24:39.583" UserId="3073" />
  <row Id="7308" PostId="5015" Score="0" Text="Note that you need the second edition of Physically Based Rendering: From Theory to Implementation, since the third does not contain or handle pure (like Jensen defined it) Photon Mapping." CreationDate="2017-04-24T06:26:09.390" UserId="2287" />
  <row Id="7309" PostId="5014" Score="0" Text="@ArjanSingh There is a little more to it. Using an infinite number of rays, both path tracing (unbiased) and photon mapping (consistent) will result in the same fully converged image. Photon mapping, however, is biased and will not result in the converged image on average for a finite (practical) number of rays (like path tracing) and can contain additional artefacts due to its density estimation techniques (besides the typical Monte Carlo noise)." CreationDate="2017-04-24T06:28:31.697" UserId="2287" />
  <row Id="7310" PostId="5016" Score="0" Text="UPDATE: if i delete the &quot;if(lEffect&gt;lCutOff)&quot;, the spotlight works as a positional light so I assume that with the &quot;if&quot; construct the code runs always the &quot;else&quot; part and it doesn't compute the part within the &quot;if&quot;. By the way, I can't still find what's wrong with the code :_(" CreationDate="2017-04-24T11:19:49.717" UserId="6401" />
  <row Id="7311" PostId="5021" Score="0" Text="yeah, I checked what I have to do and I have to apply a glow map through textures, but I can't find a good reference that explains the procedure :(" CreationDate="2017-04-24T11:22:14.663" UserId="6401" />
  <row Id="7312" PostId="5023" Score="0" Text="Do you mean that you have a set of 3D control points, C0, C1...Cn, and, I assume, some rotation matrix, P, such that, (P.Ci) [x] is strictly increasing, as are (P.Ci) [y] and (P.Ci) [z] ?" CreationDate="2017-04-24T11:56:14.690" UserId="209" />
  <row Id="7313" PostId="5023" Score="0" Text="No, i mean in 3D a point is defined by (x, y, z) instead of (x, y)." CreationDate="2017-04-24T12:04:53.353" UserId="2687" />
  <row Id="7314" PostId="5012" Score="0" Text="But wouldn't it be easier if i use the Map and Unmap pair?&#xA;&#xA;std::vector&lt;UINT8&gt; pDataBegin;&#xA;ThrowIfFailed(m_InputBuffer1[0]-&gt;Map(0, NULL, reinterpret_cast&lt;void**&gt;(&amp;pDataBegin)));&#xA;&#xA;but then again even this throws an error :(" CreationDate="2017-04-24T18:36:13.967" UserId="6349" />
  <row Id="7315" PostId="5012" Score="0" Text="What error do you get? Did you try `ReadFromSubresource`? Did you do steps pointed [on Microsoft page](https://msdn.microsoft.com/en-us/library/windows/desktop/dn899202(v=vs.85).aspx)?" CreationDate="2017-04-24T22:43:10.197" UserId="3123" />
  <row Id="7316" PostId="5012" Score="0" Text="I was not able to declare my heap as type D3D12_HEAP_TYPE_READBACK. Because the second I do that, I am not able to call CreateCommitResource and even create the buffer.&#xA;&#xA;If i keep the heap type DEFAULT and call Map, there is nothing returned into *pDataBegin" CreationDate="2017-04-24T23:06:21.883" UserId="6349" />
  <row Id="7317" PostId="5023" Score="0" Text="Your question is ambiguous as you don't specify monotonic in relation to what in 3D?" CreationDate="2017-04-25T13:10:58.360" UserId="1952" />
  <row Id="7318" PostId="5023" Score="0" Text="@JarkkoL You need to check the link that I provided." CreationDate="2017-04-25T15:27:48.523" UserId="2687" />
  <row Id="7319" PostId="5023" Score="0" Text="It doesn't disambiguate how you want the function to be monotonic in 3D. If you want the function to be monotonic along y-axis like in the 2D case, it's trivial extension, but if you want it to be monotonic along each axis, it's different. So which is it? Please clarify your question." CreationDate="2017-04-25T15:58:37.033" UserId="1952" />
  <row Id="7320" PostId="5023" Score="0" Text="@JarkkoL Oh, I see.. How can you tell that the function in the site is monotonic along y-axis only? I think the important step is &quot;Compute the slopes of the secant lines between successive points:&quot;, but the site only provide the 2D case. I think if I can extend that to 3D than the rest are the same. Am I right?" CreationDate="2017-04-26T01:42:41.317" UserId="2687" />
  <row Id="7321" PostId="5022" Score="2" Text="Can you post any more examples or links to similar stuff? It's not clear to me what the histogram on the right is showing, so it's difficult to say much about it." CreationDate="2017-04-26T02:42:13.947" UserId="3003" />
  <row Id="7324" PostId="5022" Score="0" Text="From further investigation I believe it's a scatterplot of gradient magnitude and intensity of the 3d texture. (The tooth) Im still having a little trouble visualizing it though." CreationDate="2017-04-27T03:40:36.103" UserId="113" />
  <row Id="7325" PostId="5028" Score="2" Text="Can we see some code for this? It sounds like you function continues recursing when it meets the RR condition so it's probably a coding problem rather than a problem with the algorithm. As a side note, it is possible implement  path tracing without needing recursion, GPU implementations rely on this." CreationDate="2017-04-27T04:24:29.463" UserId="3073" />
  <row Id="7326" PostId="5028" Score="1" Text="Have you stepped through it in a debugger to see whether the pseudo-random number is being generated the way you think? And that everything else you're doing is happening the way you think?" CreationDate="2017-04-27T04:47:58.113" UserId="3003" />
  <row Id="7329" PostId="5028" Score="1" Text="&quot;Unclear wht you're asking&quot; isn't very specific, but I'm closing this question because it can't be answered without the code that produces the error. If you [edit] your question and make it answerable, it can be reopened." CreationDate="2017-04-27T08:38:31.060" UserId="2041" />
  <row Id="7330" PostId="5028" Score="0" Text="Sorry! I've edited my question." CreationDate="2017-04-27T14:58:56.470" UserId="6515" />
  <row Id="7331" PostId="5027" Score="0" Text="I'm hardly an expert, but when graphics articles start talking about light reflections based on dielectrics and metals, they are often talking about [physically based rendering.](https://learnopengl.com/#!PBR/Theory)  Hope that helps." CreationDate="2017-04-27T17:55:32.460" UserId="6521" />
  <row Id="7332" PostId="5030" Score="0" Text="Sorry. I was experimenting with my code and forgot to edit the values. Then I copied and pasted and here is... I edited the question so now it has the correct values in the last set." CreationDate="2017-04-27T22:28:26.643" UserId="5852" />
  <row Id="7333" PostId="5030" Score="0" Text="Okay—the result is the same, just scaled differently." CreationDate="2017-04-27T23:15:30.077" UserId="506" />
  <row Id="7336" PostId="2366" Score="1" Text="Please don't post screenshots of code. It's not searchable and it prevents other people pasting your code into their own editor to test it." CreationDate="2017-04-28T08:16:17.977" UserId="2041" />
  <row Id="7337" PostId="5028" Score="0" Text="why did you comment out the depth test? it's what should prevent the stack overflow" CreationDate="2017-04-28T09:47:11.397" UserId="137" />
  <row Id="7338" PostId="5030" Score="0" Text="But all the points inside the square results in values out of the range [ -1 ,1 ]. For example  (0.1, -0.5, 0.1) -&gt; (1, -5)" CreationDate="2017-04-28T14:23:40.237" UserId="5852" />
  <row Id="7339" PostId="5035" Score="0" Text="My understanding is &quot;haze&quot; refers more to scattering due to water droplets or other small particles in the air, not the blue (Rayleigh scattering) effect from the air itself." CreationDate="2017-04-28T16:34:02.537" UserId="48" />
  <row Id="7340" PostId="5030" Score="2" Text="@4dr14n31t0rTh3G4m3r The vertex shader only runs per vertex, not for every point in the square. So the four vertices are mapped to (±2, ±2) and then the GPU draws a polygon between those vertices, which does cross the viewport. The points interior to the polygon do not go through the `v.y = v.y/v.z` transform, so the result is not `(-inf,-2] U [2,inf)` after all." CreationDate="2017-04-28T16:44:32.887" UserId="48" />
  <row Id="7341" PostId="5035" Score="1" Text="Meteorologically speaking, *haze* is specifically water, not other particles, so yes, there are multiple causes leading to the real-world effect. That's part of why it looks different in different situations, and why you hear so many words in connection with it." CreationDate="2017-04-28T16:46:50.333" UserId="2041" />
  <row Id="7342" PostId="5030" Score="0" Text="The problem was that I believed the vertex shader runs for every point in the square. Thanks a lot @Nathan and please post your comment as answer. I would like to accept it because you explained how vertex shaders run. However, now that I understand why does it happen, how could I fix this issue?" CreationDate="2017-04-28T16:49:48.593" UserId="5852" />
  <row Id="7343" PostId="5028" Score="2" Text="@ratchetfreak It is, because Russian Roulette algorithm should stop the resursion randomly (to keep the estimator unbiased). As far as I know, I should be like that." CreationDate="2017-04-28T17:12:47.760" UserId="6515" />
  <row Id="7344" PostId="5031" Score="0" Text="Thanks!  Wouldn't have guessed first quote referred to split-sum..." CreationDate="2017-04-28T21:53:20.780" UserDisplayName="user3412" />
  <row Id="7345" PostId="5025" Score="0" Text="Thank you for you recommendation @Derag. However, the values in the readback buffer are incorrect. &#xA;Can this be because of the readback buffer description, that the numbers are not being interpreted correctly?" CreationDate="2017-04-28T23:53:15.670" UserId="6349" />
  <row Id="7347" PostId="5025" Score="0" Text="What values do you get? If you get only zeros you probably have some bug in code. If buffer you pass to `Map` have different type than `UAV` buffer that you read values can vary.&#xA;And I don't think that `D3D12_RESOURCE_DESC` should be problem here as long as `D3D12_RESOURCE_DESC::Width` is set to right size and `D3D12_RESOURCE_DESC::Format` is set to `DXGI_FORMAT_UNKNOWN`" CreationDate="2017-04-29T11:07:40.650" UserId="3123" />
  <row Id="7350" PostId="5025" Score="0" Text="The definition of the readback buffer is the same as what you suggested, the buffer which the shader actually calls (or in the example above &quot;bufferB) has the following properties:&#xA;D3D12_HEAP_PROPERTIES defaultHeapProperties = { D3D12_HEAP_TYPE_DEFAULT,D3D12_CPU_PAGE_PROPERTY_UNKNOWN,D3D12_MEMORY_POOL_UNKNOWN,0,0 };&#xA;ThrowIfFailed(m_device-&gt;CreateCommittedResource(&amp;defaultHeapProperties, D3D12_HEAP_FLAG_NONE, &amp;buffersDesc1, D3D12_RESOURCE_STATE_GENERIC_READ, nullptr, IID_PPV_ARGS(&amp;m_computeBuffer)));" CreationDate="2017-05-01T16:50:18.383" UserId="6349" />
  <row Id="7351" PostId="5039" Score="0" Text="Have you tried stepping through in a debugger to see just where and how the values &quot;go crazy&quot;? If it happens only when cos &gt; 0.9999, that suggests a possible numerical precision problem. (Also, it sounds like you have two questions here—is your main question &quot;how to sample a half-vector distribution&quot; or &quot;why does this function go crazy&quot;? We can answer the former, but I don't know if it will help debug the problem you're having; for the latter, the relevant source code, example results, etc would be useful.)" CreationDate="2017-05-01T17:52:48.027" UserId="48" />
  <row Id="7352" PostId="5039" Score="0" Text="I worked on this over the last 2 days, I will update my question with the results.  I have ruled out that the cosine angle is the issue.  It more looks like an issue where the spherical coordinates are rotating about the wrong axis.  I graphed the results of my half-vector samples and I will upload a link to the image viewing their distribution." CreationDate="2017-05-01T17:57:07.563" UserId="6534" />
  <row Id="7353" PostId="5039" Score="0" Text="Also, thanks for the pointer - I updated the title accordingly." CreationDate="2017-05-01T18:19:17.813" UserId="6534" />
  <row Id="7354" PostId="5045" Score="0" Text="Thank you for that.  I put together a live HTML/WebGL test that isolates only this problem.  I am essentially using a variation of the equation you posted (but following the PBRT code to be anisotropic instead of iso).  Something strange is happening as I move off of normal incidence to this function.  My half-vectors seem to be &quot;leaning&quot;.  Not sure how else to describe it.  I posted the HTML proof that has all the math/shader code inline and commented for readability." CreationDate="2017-05-01T20:27:37.517" UserId="6534" />
  <row Id="7355" PostId="5045" Score="0" Text="Followed your advice and went back to the basics.  Implemented the cook torrence equation, verified basic inputs look right, and determined that sampling the distribution works fine, the problem must be elsewhere, outside of these basics.  Thanks for taking the time to read my question." CreationDate="2017-05-01T21:21:02.723" UserId="6534" />
  <row Id="7356" PostId="5045" Score="0" Text="@Steve your WebGL example looks smooth here. Ugly yellow in the lower left corner, more grey in the upper right. Did you change it to the simpler distribution or is it still supposed to be the broken code? I'm using chromium on linux btw. Or is the yellow &quot;blob&quot; supposed to be centered? Either way,`wo *= -wo;` looks suspicious." CreationDate="2017-05-01T22:48:16.543" UserId="5717" />
  <row Id="7357" PostId="5045" Score="0" Text="I'm actually debugging that issue right now.  I called it &quot;solved&quot; because i made an error.  I just updated to &quot;correct&quot; code and my distribution problem is back again.  If you refresh the webpage you'll see that my &quot;crossed quadrants&quot; are back" CreationDate="2017-05-01T22:50:40.923" UserId="6534" />
  <row Id="7358" PostId="5045" Score="0" Text="Currently, I am visualizing the half vectors as they come back raw from the distribution." CreationDate="2017-05-01T22:51:11.040" UserId="6534" />
  <row Id="7359" PostId="5045" Score="0" Text="@Steve Ok, I see the mess now. Will try to analyze later." CreationDate="2017-05-01T22:51:57.387" UserId="5717" />
  <row Id="7360" PostId="4340" Score="0" Text="I'm voting to close this question as off-topic because it is an end-user question about using graphics software, not about graphics algorithms or research." CreationDate="2017-05-02T08:31:31.287" UserId="2041" />
  <row Id="7361" PostId="5025" Score="0" Text="What size of buffers did you set? And what types of data? (int, float...)" CreationDate="2017-05-02T09:20:15.083" UserId="3123" />
  <row Id="7362" PostId="5010" Score="1" Text="Can you not subdivide your world into a 3d grid and if any grid has &gt;= 1 point inside it, it can be made into a voxel?" CreationDate="2017-05-02T09:40:08.457" UserId="3073" />
  <row Id="7363" PostId="5049" Score="1" Text="IIRC if you integrate the PDF over the entire domain the result must be 1." CreationDate="2017-05-02T14:55:36.033" UserId="137" />
  <row Id="7365" PostId="5059" Score="1" Text="Is your question about older hardware or recent hardware? The title says &quot;older&quot; but your actual question text seems like your asking more about more recent models. I suspect the answer will be different between the 2." CreationDate="2017-05-04T06:07:00.787" UserId="3003" />
  <row Id="7367" PostId="5059" Score="0" Text="I believe that the some of the required features (including compute! and coarse occlusion queries) and some of the required limits may be out of reach." CreationDate="2017-05-04T10:37:50.000" UserId="137" />
  <row Id="7369" PostId="5059" Score="0" Text="@user1118321: I mostly mean the most recent GPUs that don't support Vulkan. By &quot;older&quot; I just meant &quot;old enough to not support it&quot;." CreationDate="2017-05-04T12:06:45.967" UserId="6567" />
  <row Id="7370" PostId="5062" Score="1" Text="Pardon me for asking, but that looks very much like a patent diagram in which case, doesn't it explain how it's done?" CreationDate="2017-05-04T12:22:43.627" UserId="209" />
  <row Id="7371" PostId="5062" Score="0" Text="@SimonF you are correct! Unfortunately, it does not go into detail as to how it should be achieved." CreationDate="2017-05-04T12:26:03.837" UserId="6572" />
  <row Id="7372" PostId="5062" Score="2" Text="Hmm. Given a patent is supposed to be implementable by an &quot;average skilled&quot; person in the field, that doesn't sound promising.   Having said that, it looks as though the right edge of each piece is part of the convex hull. There are documented algorithms for finding that, if that's of use to you." CreationDate="2017-05-04T13:51:27.840" UserId="209" />
  <row Id="7373" PostId="5059" Score="1" Text="@Dolda2000: I don't think the question is well-founded. You list three pieces of hardware, but you assume that they don't have Vulkan implementations for non-&quot;business reasons&quot;. Do you have a basis for such an assumption?" CreationDate="2017-05-04T14:45:22.047" UserId="2654" />
  <row Id="7374" PostId="5059" Score="0" Text="@NicolBolas: Sorry, it seems I made myself misunderstood. I really just intended to speak of whatever most recent hardware that technically cannot support Vulkan. Those three specific architectures were just my best guess on what those might be, but I'm entirely open to having guessed wrong." CreationDate="2017-05-04T17:37:15.060" UserId="6567" />
  <row Id="7375" PostId="5059" Score="1" Text="@Dolda2000: But you already know why GL 3.x/D3D10-class hardware can't support Vulkan: because they don't have very obvious hardware features that Vulkan requires. So the only hardware you seem to be talking about is GL 4.x/D3D11-class hardware that doesn't have Vulkan implementations." CreationDate="2017-05-04T18:08:02.143" UserId="2654" />
  <row Id="7376" PostId="5028" Score="0" Text="I've added the pathtracing tag again, and I've left the raytracing tag too as they both seem relevant." CreationDate="2017-05-04T19:43:24.273" UserId="231" />
  <row Id="7377" PostId="5056" Score="0" Text="Nice detective work. Have you fed this back to the author?" CreationDate="2017-05-04T19:55:23.870" UserId="231" />
  <row Id="7378" PostId="5060" Score="0" Text="Great solution, which works well. Here's the (exaggerated) result of placing an additional virtual source behind the mirror: https://www.youtube.com/watch?v=A6fppB5jeAY" CreationDate="2017-05-04T20:01:09.090" UserId="6562" />
  <row Id="7379" PostId="5059" Score="0" Text="@NicolBolas What are these &quot;very obvious hardware features&quot;? I think this is an excellent question. Also, I'm not sure I follow that GL 3.x can't support Vulkan since there exist GPUs that support Vulkan and can't support GL 3.2 devices because of lacking geometry shaders." CreationDate="2017-05-05T01:52:15.437" UserId="2707" />
  <row Id="7380" PostId="5059" Score="0" Text="@aces: The &quot;very obvious features&quot; in question would be compute shaders and image load/store. Obvious due to the fact that it's right there in the Vulkan specification that these are not optional. The only GPUs that can provide these yet aren't GL 3.x capable would be *mobile* GPUs. But they don't support OpenGL of any version; they support OpenGL ***ES*** versions." CreationDate="2017-05-05T02:10:45.760" UserId="2654" />
  <row Id="7381" PostId="5059" Score="2" Text="@NicolBolas You should write this as an answer :), as well as explain what hardware changes are needed to support these." CreationDate="2017-05-05T02:31:46.450" UserId="2707" />
  <row Id="7382" PostId="5063" Score="3" Text="Yes the 0 is for GL_TEXTURE0. Don't forget to bind the texture object to texture unit 0 also." CreationDate="2017-05-05T07:07:10.187" UserId="3073" />
  <row Id="7383" PostId="5050" Score="0" Text="I wasn't think in terms of area, so thanks a lot. I still have an unresolved paradox in my poor brain. Imagine a unit hemisphere and a hemisphere of radius 2, centred at the same point. The first has surface area $2\pi$ and the second $8\pi$. The probability of picking a particular direction over these hemispheres seems the same, but it's going to be $\frac{1}{2\pi}$ and $\frac{1}{8\pi}$ respectively. I can't get my head around why it's different mathematically but _seemingly_ the same theoretically. Put another way, why does the unit hemisphere get precedence over ones of other radii?" CreationDate="2017-05-05T10:06:20.270" UserId="2941" />
  <row Id="7384" PostId="5064" Score="0" Text="Have you tried saturating the dot products before raising it to some power ? They should not go &lt; 0" CreationDate="2017-05-05T10:17:14.290" UserId="3073" />
  <row Id="7385" PostId="5064" Score="0" Text="@PaulHK Just tried it and it didn't work. But there is the possibility the specular values are less than 0 when raising it to a power." CreationDate="2017-05-05T10:26:43.137" UserId="113" />
  <row Id="7386" PostId="5059" Score="2" Text="It also raises the question in what way eg. Tesla GPUs don't support compute shaders. Since they support CUDA, why can they not support Vulkan compute shaders? Ie., what actual hardware feature do they lack that prevents them from running compute shaders." CreationDate="2017-05-05T11:57:47.993" UserId="6567" />
  <row Id="7387" PostId="5063" Score="0" Text="This is all well-covered in the [first tutorial on texturing on that very site](https://learnopengl.com/#!Getting-started/Textures)." CreationDate="2017-05-05T15:11:09.260" UserId="2654" />
  <row Id="7388" PostId="5056" Score="0" Text="I'm actually the one who screwed up the code when I translated it to webgl - so yeah :-) I definitely let myself know. What I think I found most difficult about this problem is the lack of good information or graphics that actually visualize what the algorithms produce in graphics." CreationDate="2017-05-05T17:04:50.597" UserId="6534" />
  <row Id="7390" PostId="5043" Score="0" Text="I am wondering where it would be better to put the code for Russian Roulette: in traceRay or in the indirectLighting function?" CreationDate="2017-05-05T18:02:42.320" UserId="6515" />
  <row Id="7392" PostId="5043" Score="0" Text="That sounds like a potential new question..." CreationDate="2017-05-05T19:33:58.363" UserId="231" />
  <row Id="7393" PostId="5024" Score="0" Text="Could you provide a screenshot of what it looks like?" CreationDate="2017-05-05T20:14:08.903" UserId="1981" />
  <row Id="7396" PostId="5065" Score="0" Text="@NicolBolas https://open.gl/textures states that samplers in the fragment shader are bound to texture units so wouldn't it make sense to use GL_TEXTURE0 and so on to make the code more readable" CreationDate="2017-05-06T16:33:54.397" UserId="6546" />
  <row Id="7399" PostId="5065" Score="0" Text="@NicolBolas Does my answer look correct now?" CreationDate="2017-05-06T17:34:26.883" UserId="6546" />
  <row Id="7403" PostId="5067" Score="0" Text="Also, why are multiplying the light position by the w component of the position?" CreationDate="2017-05-08T04:08:37.453" UserId="2707" />
  <row Id="7405" PostId="5067" Score="0" Text="I was about to comment the same thing. Division by W is typical for things that have had a perspective matrix performed on them. It should not be needed for the world positions of lights." CreationDate="2017-05-08T06:23:44.060" UserId="3073" />
  <row Id="7406" PostId="5067" Score="0" Text="wLiPos is a vec4, and I have to avoid dividing by zero, so I cross multiplied in the formula." CreationDate="2017-05-08T06:45:19.170" UserId="5609" />
  <row Id="7407" PostId="5067" Score="0" Text="Now I multiple by the transpose of the inverse of the modelMatrix, but nothing changed.&#xA;wNormal = (vec4(vtxNorm, 0) * transpose(inverse(ModelMatrix))).xyz;" CreationDate="2017-05-08T06:46:51.617" UserId="5609" />
  <row Id="7408" PostId="5050" Score="0" Text="I think Olivier's answer explains that well. Steradians and the unit sphere go together in the same way that radians and the unit circle go together. If you change units so that your sphere has a different radius, that radius becomes an extra factor in the integration. &quot;Choosing a direction&quot; and &quot;choosing a point on the surface of a unit sphere&quot; can be treated as the same thing." CreationDate="2017-05-08T08:56:28.300" UserId="2041" />
  <row Id="7409" PostId="5067" Score="0" Text="You don't need the W component for world transformations. You can truncate the vec4*mat4 result to a vec3." CreationDate="2017-05-08T09:25:20.640" UserId="3073" />
  <row Id="7412" PostId="5067" Score="0" Text="Oh yes, that's a typo. wLightPos is not in world, it's a vec4, so it can be in the infinity for example." CreationDate="2017-05-08T10:03:56.980" UserId="5609" />
  <row Id="7413" PostId="5067" Score="0" Text="Or you mean that wPos? But if yes, then the w component is always 1, so that multiplication shouldn't change anything." CreationDate="2017-05-08T10:06:37.417" UserId="5609" />
  <row Id="7415" PostId="5067" Score="0" Text="but if I just write eye - pos and LightPos - pos, everything stays the same." CreationDate="2017-05-08T10:34:28.327" UserId="5609" />
  <row Id="7416" PostId="5042" Score="0" Text="Thank you for explaining this and sorry for the delayed answer, but I had to find the time to read this very carefully. I feel like the only paragraph I do not completely understand is the one where you talk about fragment shaders and derivatives. What exactly is derived in screen-space X and Y? Is it the texture coordinate value?" CreationDate="2017-05-08T13:15:54.893" UserId="6535" />
  <row Id="7417" PostId="5042" Score="0" Text="@NicolaMasotti: [&quot;Derivative&quot; is a calculus term.](https://en.wikipedia.org/wiki/Derivative) In this case, it is the rate of change of the texture coordinate, across the surface of the triangle, in screen-space X or Y. If you don't know calculus, then I'm not going to be able to explain it to you in a single post." CreationDate="2017-05-08T15:52:40.840" UserId="2654" />
  <row Id="7421" PostId="5042" Score="0" Text="Fortunately I know what a derivative is. Is there any place I can look at for the exact math?" CreationDate="2017-05-08T17:24:17.720" UserId="6535" />
  <row Id="7422" PostId="5042" Score="0" Text="Also, I would propose a couple of changes to your answer, i.e. &quot;_it depends on how the polygon is mapped to the texture_&quot; instead of &quot;_it depends on how the texture is mapped to the polygon_&quot;. Also, when you say: &quot;_using coverage to scale colors that are on the border_&quot; do you actually mean &quot;_weight colors that are on the border_&quot;?" CreationDate="2017-05-08T17:31:46.863" UserId="6535" />
  <row Id="7425" PostId="5078" Score="0" Text="If you don't get an answer here, you can also try on the [Photography Stack Exchange](https://photo.stackexchange.com/)." CreationDate="2017-05-09T05:14:32.823" UserId="3003" />
  <row Id="7426" PostId="5067" Score="0" Text="How does it look if you remove the distance attenuation?  (*le[i] and / pow(length(wLight[i]), 2)  )" CreationDate="2017-05-09T06:12:25.200" UserId="3073" />
  <row Id="7427" PostId="5064" Score="0" Text="How about diffVal ? They also need saturating as they are used in your reflection calculation. I say this because it looks like your specular reflections are at N.L &lt; 0" CreationDate="2017-05-09T06:25:20.790" UserId="3073" />
  <row Id="7428" PostId="5021" Score="0" Text="It is important to note that glow is certainly not light emission! You should probably edit your question" CreationDate="2017-05-09T08:43:52.710" UserId="38" />
  <row Id="7429" PostId="5067" Score="0" Text="Didn't fix it. https://i.gyazo.com/814598d0d3afea8d9e83b6c4abb84854.png" CreationDate="2017-05-09T09:21:26.297" UserId="5609" />
  <row Id="7430" PostId="5067" Score="0" Text="If I comment out the kd part in the color: https://i.gyazo.com/0006479371f58a9931250b2d03dcd244.png&#xA;So the specular part causes it. With only ka and kd:&#xA;https://i.gyazo.com/e9630d77a2dc443aa17412a2d2cc486a.png" CreationDate="2017-05-09T09:28:14.450" UserId="5609" />
  <row Id="7431" PostId="5067" Score="0" Text="It's something about the 'cosd'" CreationDate="2017-05-09T09:37:48.657" UserId="5609" />
  <row Id="7432" PostId="5064" Score="0" Text="Have you tried regenerating your vertex normals to be sure they're accurate?" CreationDate="2017-05-09T13:43:17.977" UserId="6596" />
  <row Id="7433" PostId="5064" Score="0" Text="It was the diff value. Needed to be saturated before calculating the reflectance vector." CreationDate="2017-05-09T13:57:31.543" UserId="113" />
  <row Id="7434" PostId="5022" Score="1" Text="Is this paper publicly available? In that case it would be useful if you could provide us with a link to it." CreationDate="2017-05-09T14:10:30.250" UserId="110" />
  <row Id="7435" PostId="5085" Score="0" Text="I think it is possible but solution depends on definition of `light intensity`. So I would suggest to specify what do you mean by that" CreationDate="2017-05-09T14:59:46.400" UserId="3123" />
  <row Id="7437" PostId="5085" Score="0" Text="Are you sure? According to wiki luminous intensity is based on the luminosity function, `a standardized model of the sensitivity of the human eye`" CreationDate="2017-05-09T15:20:59.090" UserId="3123" />
  <row Id="7438" PostId="5085" Score="0" Text="Yeah.. you're right again" CreationDate="2017-05-09T15:21:19.617" UserId="6603" />
  <row Id="7441" PostId="5024" Score="0" Text="@flawr  The included image, the purple should take up the entire plot window" CreationDate="2017-05-09T18:53:17.833" UserId="6501" />
  <row Id="7442" PostId="5024" Score="0" Text="I cannot reproduce your problem, when I try it the whole axis are filled. You can of course cut the axis using `xlim([a,b])` and `ylim([c,d])` or `axis([a,b,c,d])`." CreationDate="2017-05-09T19:09:43.983" UserId="1981" />
  <row Id="7443" PostId="5086" Score="0" Text="Could you add a bit more about the wider context? Are the offset triangles to remain attached as in the original mesh? Does &quot;culled&quot; mean the original triangle is discarded, or just that the offsetting is abandoned, leaving the triangle at its original size?" CreationDate="2017-05-09T22:18:30.857" UserId="231" />
  <row Id="7445" PostId="5086" Score="1" Text="So... how does this work with meshes? Because in a mesh, a vertex has more than 2 neighbors. Or is this just for individual triangles?" CreationDate="2017-05-09T22:32:41.903" UserId="2654" />
  <row Id="7446" PostId="5086" Score="0" Text="My implementation is such that all triangles are laid out in a continuous buffer:&#xA;[P1.x, P1.y, P1.z, P2.x, P2.y, P2.z ... Pn.x, Pn.y, Pn.z] with neighboring points also laid out explicitly at attributes. This way, each vertex of each face can be calculated and manipulated without affecting neighboring faces. Nicol Bolas, yes, dealing with each triangle separately." CreationDate="2017-05-09T23:02:06.493" UserId="6596" />
  <row Id="7447" PostId="4807" Score="0" Text="Not quite clear on what you're asking here. You can use a canvas as a texture for a WebGL context in another canvas. Typically, this &quot;texture&quot;-canvas would be off-screen. There's no need to worry about aspect. Best if the dimensions of the texture are powers of 2. &quot;256x256&quot;, &quot;128x64&quot;, and &quot;1024x4096&quot; are all fine. Affect your aspect ratio using mapping coordinates and geometry." CreationDate="2017-05-09T23:17:08.767" UserId="6596" />
  <row Id="7448" PostId="5085" Score="3" Text="I suppose you could multiply the R,G,B by the inverse wavelength (frequency) of said light to approximate its power, although this assumed R,G,B are 3 discrete peaks at 3 different wavelengths when in reality the power is spread across some overlapping range." CreationDate="2017-05-10T02:12:55.230" UserId="3073" />
  <row Id="7449" PostId="5087" Score="1" Text="First thing would be to check your texture coordinates are valid. This you can do by outputting the tex.xy to gl_FragColor,you should see a red/green gradient across your triangle. Once you confirm that is working you should also make sure your texture does not contain all 0 in its alpha as you're multiplying the A channel in your final colour, you can test this by not multiplying the alpha channel from the texture." CreationDate="2017-05-10T03:50:32.750" UserId="3073" />
  <row Id="7450" PostId="5087" Score="1" Text="Also texture coordinates are usually normalised (0...1). Your 'tex' array has values higher than 1 ?" CreationDate="2017-05-10T03:53:27.993" UserId="3073" />
  <row Id="7451" PostId="5087" Score="0" Text="Hi @PaulHK, thanks for responding. I'll try that experiment. I set the texture coordinates so that the texture repeats (I'm not sure what this is called but I know this is how you use a texture to tile an object.)" CreationDate="2017-05-10T03:55:12.423" UserId="5521" />
  <row Id="7452" PostId="5087" Score="0" Text="I thought it may have been that, but I didn't see anything like glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT);   I think the default texture behaviour is clamping." CreationDate="2017-05-10T04:17:24.263" UserId="3073" />
  <row Id="7453" PostId="5087" Score="0" Text="I added the function call to set the GL_TEXTURE_WRAP=GL_REPEAT but I still see black. I also set the texture coordinates to 0, 0.5, and 1 (from 0,2, and 4) but this did not work either. Also, WebGL inspector is not working (won't turn red on a page) butt I think I've seen before that my texture loads but for some reason is not sampled." CreationDate="2017-05-10T05:12:40.250" UserId="5521" />
  <row Id="7454" PostId="5088" Score="2" Text="Usually the normal in a BRDF calculation is the normal of the surface at the intersection point. the 'if (n.dot(r.d) &lt; 0) n = n*-1.0;' bit looks strange, what is that for ?" CreationDate="2017-05-10T09:05:34.723" UserId="3073" />
  <row Id="7455" PostId="5088" Score="0" Text="Thanks for the input, and that part was given to me by my professor, but as far as I know that ensures that the normal points out from the surface, or is properly oriented." CreationDate="2017-05-10T09:30:07.807" UserId="6606" />
  <row Id="7457" PostId="5092" Score="0" Text="Not sure if I'm reading this right ? Are you proposing I switch from image-based texturing to polygonal mesh of the sphere and use perspective texturing rather ? The computational complexity (and related visual artifacts alone ) disqualify this approach altogether. Right now, until planet is close, I can stay around 60 fps. No chance with perspective texturing. That link looks like it allows to move the point of distortion up and down, so not sure how it can be used to remap the sphere points into the texture. Doesn't look like it." CreationDate="2017-05-10T14:15:25.677" UserId="6063" />
  <row Id="7458" PostId="5092" Score="0" Text="No I mean changing the source image to minimize coordinate processing" CreationDate="2017-05-10T14:18:17.023" UserId="137" />
  <row Id="7459" PostId="5092" Score="0" Text="But that would only help with the distortion around poles (which I'm not showing,  deliberately). The 3-D perspective (most visible on left/right edge)  is a real-time effect - it's essentially some nonlinear function (most probably it has a kind-of quadratic falloff, I'm guessing). Essentially, for each 2D point (x,y) on a known scanline y, we want to know the 3D point (x,y,z) - which I can easily linearly remap to the texture coordinates" CreationDate="2017-05-10T14:38:35.893" UserId="6063" />
  <row Id="7460" PostId="5090" Score="1" Text="What kind of CPU/system can hold useful 2D textures but not a short lookup table for an `asin()` approximation? A dozen entries + linear interpolation should be good enough for your application." CreationDate="2017-05-10T20:10:03.743" UserId="5717" />
  <row Id="7461" PostId="5090" Score="1" Text="It's not about lack of main RAM but about slowing down the RISC chip which gets effectively locked till a read from RAM is finished. Transferring texels is about the extent of RAM access that is doable in 30+ fps.  The local cache is only 4 KB for both code and data. Problem is, it's 32-bit internally for read/write which either wastes a lot of precious cache or performance when you unpack single bytes from 32 bits. You mention dozen values for asin. Can you elaborate? 90 degrees/12 =~7 degree step, if I get you right." CreationDate="2017-05-10T20:48:52.767" UserId="6063" />
  <row Id="7462" PostId="5090" Score="2" Text="Yes, that's what I meant, with 12 being a rough guess. You would still have some distortion compared to the real `asin()` but probably not that much. Can your CPU do multiplications in reasonable time? If so, a cubic spline would be an even better approximation (more quality for the same table size). I think this can be done with integer math but float would be easier, if you have it." CreationDate="2017-05-10T21:08:05.090" UserId="5717" />
  <row Id="7463" PostId="5090" Score="1" Text="Conversion from float to int is usually last step, when I confirm in excel that the numbers work- but is necessary as floats are not supported and I try to avoid fixed point as much as possible. The multiplication takes just 3 cycles like add or sub or bitshift. Cubic spline is a great idea. I completely forgot about those! So far I managed to avoid tables, but if there's no other way I could sacrifice , say , 100 bytes for 25 values of asin , at 4 degree resolution. Or more- it's easy to tweak table size when hunting for acceptable level of distortion." CreationDate="2017-05-10T21:49:14.470" UserId="6063" />
  <row Id="7465" PostId="5088" Score="0" Text="how about 1/n.dot(i) in your eval() function  -- if n.dot(i) is in the 0..1 range your output is going to be 1 .. infinity" CreationDate="2017-05-11T02:43:50.617" UserId="3073" />
  <row Id="7468" PostId="5088" Score="0" Text="hmm I tried removing the dot product and just sending back the ks color and nothing changed. Also I've updated my code a bit, I had a mistake in the first brdf function calls in which I was sending the r.d vector instead of -r.d (or o) vector. This has given me a new stranger error now though where the specular sphere has a black ring now." CreationDate="2017-05-11T03:30:55.847" UserId="6606" />
  <row Id="7469" PostId="2185" Score="0" Text="If the normals are varying between vertices, why should the tesselation be so important?" CreationDate="2017-05-11T04:36:00.023" UserId="6596" />
  <row Id="7470" PostId="1888" Score="0" Text="You could also specify a normal for each vertex as an attribute, not a uniform, and render the mesh all in one call." CreationDate="2017-05-11T04:49:18.350" UserId="6596" />
  <row Id="7471" PostId="2185" Score="0" Text="@Jackalope Tessellating more finely means the normals will be evaluated correctly at each tessellated vertex, and interpolated over a shorter distance between vertices, so the error due to using linear interpolation will be reduced." CreationDate="2017-05-11T04:53:13.783" UserId="48" />
  <row Id="7472" PostId="5095" Score="0" Text="Nice demo! That is _exactly_ what I'm doing right now !  I noticed you kept the radius small to keep the framerate high ;) Yes, I'm very often experimenting with how far I can push the division vs interpolation, often times I end up adjusting the dataset so I can replace division with bitshift. Since we can mirror the right half of planet, we need to cover just 90 degrees with the LUT. But, is the AngleStep per on-screen pixel, constant for those 90 degrees?So, if for current camera,equator takes up 180 pixels, e.g. half takes 90 pixels,then each pixel would correspond to 90/90 =1 degree step?" CreationDate="2017-05-11T14:10:12.003" UserId="6063" />
  <row Id="7473" PostId="5095" Score="0" Text="I think I just realized what I need to do: Create a sphere in 3D, place camera right in front of it, transform each point on equator, and note the X coordinate. That will give me the exact distribution that I need for the LUT creation. I'm not sure why I thought initially I could get away without doing that - I was hoping there would be some simple math trick,as it's a sphere. Actually, I can do it in excel, as I have a tab that I used for testing the integer version of the 3D transformations..." CreationDate="2017-05-11T14:13:44.383" UserId="6063" />
  <row Id="7474" PostId="5095" Score="1" Text="I noticed you mentioned 4KB code/data cache &amp; memory latency issues in your target platform in comments. This is very cache friendly way of rasterizing the sphere so small LUT should remain in the cache of that size, and each entry in the LUT could be stored in 1 or 2 bytes. You also don't need 1 degree precision so the LUT can be smaller (linearly interpolate between the LUT entries), so the entire LUT could fit in just ~100 bytes or less. You can calculate the LUT easily with acos(), which maps horizontal position to the surface of the cylinder for the &quot;rounded&quot; texture mapping effect." CreationDate="2017-05-11T14:37:16.127" UserId="1952" />
  <row Id="7481" PostId="5095" Score="0" Text="Thanks, I'm still trying to wrap my head around the arccos, though. I get that its numerical distribution of values is nonlinear, and that's exactly what we need to cheat. I guess, when I implement it,it'll be obvious,but to get the index to the table, I can only remap it [from the horizontal position on the screen] linearly (e.g. a point that is horizontally in the middle between the left edge and circle center would have value of 0.5, arccos of which is 60 degrees, and that corresponds to texel at position of 0.66 x TextureSegmentWidth).It may not matter,as the end result is nonlinear,anyway" CreationDate="2017-05-11T17:30:43.553" UserId="6063" />
  <row Id="7482" PostId="5086" Score="0" Text="trichoplax - &quot;Culled&quot; means thrown out, not rendered, as in a back-facing, single-sided primitive." CreationDate="2017-05-11T17:38:10.983" UserId="6596" />
  <row Id="7483" PostId="5095" Score="1" Text="You map the x-coordinate linearly to the table and fetch the u-coordinate you use to fetch from the texture. This is essentially:&#xA;for(unsigned i=0; i&lt;lut_suze; ++i) lut[i]=acos(1.0f-2.0f*float(i)/lut_size) / pi * 0.5f * tex_width;" CreationDate="2017-05-11T17:40:07.540" UserId="1952" />
  <row Id="7484" PostId="5086" Score="0" Text="Both of you seem to suggest that the GPU sees faces as &quot;tethered&quot; to other faces. As far as I know, this is not the case. The GPU sees faces as separate entities. Faces only appear to be connected because congruent vertices from neighboring faces typically have identical attributes. One common exception is face normals in a flat-shaded mesh." CreationDate="2017-05-11T17:42:38.940" UserId="6596" />
  <row Id="7485" PostId="5088" Score="0" Text="I'm not sure if this gives a clue, but the reflective sphere also seems to be lacking any shadow on the plane beneath it." CreationDate="2017-05-11T21:25:38.607" UserId="231" />
  <row Id="7486" PostId="5095" Score="0" Text="Thanks a lot, examining the data closer reveals your equation provides data for whole width of the circle, not just for diameter, correct ? I watched your video again, but I find it hard to see any quantization artifacts there. Did you use full floats or fixed point ? I find it hard to believe that texturing would be integer-only!" CreationDate="2017-05-11T21:32:38.363" UserId="6063" />
  <row Id="7488" PostId="5088" Score="0" Text="From the shadowed wall area's point of view, it will see the glowing ringed ball so it acts as a light source instead of an occluder. Energy is also not being conserved due to this problem so the scene ends up with too much light." CreationDate="2017-05-12T02:28:51.547" UserId="3073" />
  <row Id="7489" PostId="5088" Score="0" Text="One thing I would do is make a debug mode in your path tracer. In this mode you stop recursing at the first intersection and instead output the outgoing sampling direction as a colour (colour = rayDir*0.5+0.5). From there you can visualise if the out bound ray direction is being computed properly, the walls can act as a reference." CreationDate="2017-05-12T02:34:52.140" UserId="3073" />
  <row Id="7490" PostId="5088" Score="1" Text="I was finally able to solve this problem, I had a problem with the &quot;rad = rad + (receivedRadiance(y, depth + 1, true) * (PI)).mult(b) * (1 / p);&quot; statement. I changed it to this : &quot;rad = rad + receivedRadiance(y, depth + 1, flag).mult(brdf.eval(n, o, i))*(n.dot(i)/(pdf*p));&quot; I found this solution by just looking up similar projects and formulas online. In the diffuse BRDF I added &quot;pdf = i.dot(n) * (1 / PI)&quot; to account for this change, and cancel out the dot product. edit: oh I also needed my normal to be checked again o, not r.d." CreationDate="2017-05-12T06:29:34.743" UserId="6606" />
  <row Id="7491" PostId="5099" Score="0" Text="Is image 2 somehow broken? Or are you in fact trying to figure out disparity to then operate on the second image somehow. Like making colors uniform or something." CreationDate="2017-05-12T14:59:03.447" UserId="38" />
  <row Id="7492" PostId="5099" Score="0" Text="@joojaa, no, sorry for not being clearer. Image 2 is a perfect image/photo. What the aim of the algorithm is, is to take blocks of a totally different image (Image 1) - say for example comic book scans, and use that to re-draw image 2, purely for the artistic effect." CreationDate="2017-05-12T15:14:12.713" UserId="6618" />
  <row Id="7493" PostId="5099" Score="0" Text="Added example images to clarify." CreationDate="2017-05-12T15:16:36.470" UserId="6618" />
  <row Id="7494" PostId="5099" Score="0" Text="Not sure how much help this would be, but what you are after is, in effect, a version of [Vector Quantisation](https://en.wikipedia.org/wiki/Vector_quantization), i.e. VQ, except that the representative vectors aren't derived/synthesised from the source image but supplied separately. You thus don't have to do the training process." CreationDate="2017-05-12T16:17:16.493" UserId="209" />
  <row Id="7495" PostId="5099" Score="2" Text="...Or you could just grab Mosaic program (maybe from http://www.brighthub.com/multimedia/photography/articles/34691.aspx ). The only tedious part would be manufacturing a set of images to corresponding to your blocks." CreationDate="2017-05-12T16:21:05.967" UserId="209" />
  <row Id="7496" PostId="5086" Score="1" Text="@Jackalope: &quot;*Both of you seem to suggest that the GPU sees faces as &quot;tethered&quot; to other faces.*&quot; That's because, generally speaking, this is true. Most meshes don't merely have neighboring triangles use &quot;identical attributes&quot;; they reuse *the same vertices*. This could be through triangle lists that use the same index multiple times, or through triangle strips, or whatever. But generally speaking, meshes reuse neighboring vertices. Your meshes do not, but your specific case doesn't change the general case. That's why I asked for clarification." CreationDate="2017-05-12T16:44:45.493" UserId="2654" />
  <row Id="7497" PostId="5086" Score="0" Text="Thanks for that, Nicol Bolas. I was unclear about that. I'd thought that vertex data was usually expanded to explicit values before going to the GPU, ala STL-style rather than OBJ-style notation." CreationDate="2017-05-12T17:14:54.217" UserId="6596" />
  <row Id="7498" PostId="5095" Score="0" Text="Yes, it generates the LUT for full width of a scanline. Can't remember the details, but I'm pretty sure the LUT stored just integer pixel coordinates and I just point sampled it. You could do better with linear interpolation and fixed point." CreationDate="2017-05-12T19:23:22.237" UserId="1952" />
  <row Id="7499" PostId="5102" Score="0" Text="OpenGL ES is for [embedded platforms](https://www.khronos.org/opengles/) like cell phones. macOS definitely doesn't support it, and I'm pretty sure neither Linux nor Windows do either. They all support straight OpenGL, though." CreationDate="2017-05-13T03:34:23.830" UserId="3003" />
  <row Id="7500" PostId="5100" Score="1" Text="Nice work figuring this out! Really like the mathematical description at the beginning." CreationDate="2017-05-13T03:36:59.053" UserId="3003" />
  <row Id="7501" PostId="5102" Score="0" Text="Do you mean, there is no backdoor to display OpenGL ES content in a native app?" CreationDate="2017-05-13T18:12:50.370" UserId="6623" />
  <row Id="7502" PostId="5102" Score="0" Text="No there's no such backdoor. OpenGL and OpenGL ES are fairly similar, and portions of your code may work on both, but usually there are some parts that will not. For example on macOS you have a `CGLContextObject` that represents the context, whereas on iOS you use an [`EAGLContext`](https://developer.apple.com/library/content/documentation/3DDrawing/Conceptual/OpenGLES_ProgrammingGuide/WorkingwithOpenGLESContexts/WorkingwithOpenGLESContexts.html)." CreationDate="2017-05-13T20:44:07.067" UserId="3003" />
  <row Id="7503" PostId="5104" Score="0" Text="Are you looking for a memory-efficient way of producing this exact effect (particles moving into their required positions), or are you looking for similar effects that don't look quite the same but are easier/more memory-efficient to implement? For example. fading in the final text whilst simultaneously fading out animated white noise would have a similar feel, but wouldn't really look like moving particles, so we need to know precisely what you require in order to be able to answer." CreationDate="2017-05-14T15:24:37.013" UserId="231" />
  <row Id="7504" PostId="5104" Score="0" Text="The random dots dissolving into text doesn't look to me like the dots move at all. It looks like random noise which is then blended with the letters using the &quot;noise dissolve&quot; blend mode or transition." CreationDate="2017-05-14T15:45:34.570" UserId="3003" />
  <row Id="7505" PostId="5104" Score="0" Text="@trichoplax Same effect with &quot;moving&quot; binary pixels" CreationDate="2017-05-14T15:53:46.297" UserId="6628" />
  <row Id="7506" PostId="5104" Score="0" Text="@user118321 I think I understand what you mean: Wouldn't that require non-random noise blend masks (to blend _all_ pixels within a few transition frames)? That would require full frame animated masks (and the respective memory)." CreationDate="2017-05-14T15:53:51.970" UserId="6628" />
  <row Id="7507" PostId="5104" Score="0" Text="If you require individual pixels to be visibly following paths, would you be interested in solutions that save memory by using a limited number of distinct path patterns, so that some pixels are following the same path as each other, just offset by the distance between their final destinations?" CreationDate="2017-05-14T16:42:19.243" UserId="231" />
  <row Id="7508" PostId="5104" Score="0" Text="Might work if the pixels come from the same letters, but if e.g. all set top-left pixels move in synch I suspect it will be noticable. Another question is whether it would work with proportional fonts." CreationDate="2017-05-14T18:17:49.537" UserId="6628" />
  <row Id="7509" PostId="5106" Score="0" Text="Do you mean any integer or any unsigned integer?" CreationDate="2017-05-14T20:20:52.873" UserId="6546" />
  <row Id="7510" PostId="5106" Score="2" Text="@Archmede The type of names is unsigned integer, so that's what I mean." CreationDate="2017-05-14T20:32:22.047" UserId="48" />
  <row Id="7511" PostId="5085" Score="0" Text="0..255, or 0..1 equals Black to White, but 128 (or 0.5) is NOT the actual middle of value between 0 and 1 when it comes to display output. The middle is somewhere around 158, iirc." CreationDate="2017-05-15T11:45:56.110" UserId="6633" />
  <row Id="7512" PostId="4990" Score="0" Text="He needs to correct Z with cos, otherwise the world goes to the fish. That's a great pun. :) I'm talking about fisheye. He's spawning rays from a point and corrects them to a plane." CreationDate="2017-05-15T12:39:02.703" UserId="6633" />
  <row Id="7513" PostId="4990" Score="0" Text="OP, i'm not sure you provide enough meaninful information. My suggestion is to modify your renderer to output the scanlines as seen from the top and posting it. My guess is that your distances are calculated wrongly, but a) i dont do java and b) your approach seems overly complicated to me and thus i'm of limited use only." CreationDate="2017-05-15T12:44:45.880" UserId="6633" />
  <row Id="7515" PostId="5104" Score="1" Text="@trichoplax If I understand correctly, that sounds good indeed - but it'll take some time for me to try. Thanks for now, I'll report back!" CreationDate="2017-05-15T19:01:37.580" UserId="6628" />
  <row Id="7517" PostId="5104" Score="0" Text="@trichoplax Thanks, I'd like to get back to you once more questions come up." CreationDate="2017-05-15T19:12:50.243" UserId="6628" />
  <row Id="7519" PostId="5112" Score="0" Text="My first attempt to [run this on shadertoy.com](https://www.shadertoy.com/view/lslBRH) is missing a few pieces. If you know the site, could you please have a look?" CreationDate="2017-05-16T06:47:01.240" UserId="6628" />
  <row Id="7520" PostId="5010" Score="0" Text="You could use a fullscreen triangle and raymarch it. There's lots of tutorials about that. And iirc it's the preferred method." CreationDate="2017-05-16T07:06:15.170" UserId="6633" />
  <row Id="7522" PostId="5116" Score="0" Text="One key point of the near and far planes is that they set the how the depth changes throughout the region between them." CreationDate="2017-05-16T15:08:56.053" UserId="137" />
  <row Id="7523" PostId="5112" Score="0" Text="Yes, this wasn't intended to be a complete program. I was just trying to explain my method. In looking at it again, I think I'll need to revise it. I don't have time right now, but I'll try to get an example up &amp; running soon." CreationDate="2017-05-16T19:23:17.657" UserId="6596" />
  <row Id="7524" PostId="5112" Score="0" Text="Yes, of course. I'm just not able to fill in the gaps on my own without spending too much time with WebGL/Shadertoy, but I thought I'd give it a bit of a try. However, while I'm curious to see how this works and to get a glimpse at the result, it's not ultimately the approach I am looking for, since it requires complex graphics hardware and is way beyond a &quot;simple&quot; algorithm." CreationDate="2017-05-16T19:42:33.397" UserId="6628" />
  <row Id="7529" PostId="5112" Score="0" Text="Actually, almost everybody has the hardware needed to view WebGL.&#xA;http://caniuse.com/#feat=webgl" CreationDate="2017-05-17T03:50:35.177" UserId="6596" />
  <row Id="7531" PostId="5119" Score="0" Text="I want to understand the purpose of the depth buffer. I've only attempted a wireframe renderer, so I didn't consider depth when plotting the 3D points to a 2D screen (I could plot the points and lines in any order and it would look right).&#xA;&#xA;Is the purpose of the depth buffer to maintain a record of the Z distance of each point once it's been projected to 2D so that solid polygons may be drawn in the correct order? Is this why I didn't require a near/far plane in my simple wireframe visualizer?" CreationDate="2017-05-17T18:01:32.443" UserId="6646" />
  <row Id="7532" PostId="5123" Score="0" Text="Oh! yes. It worked. Thanks a lot. :D" CreationDate="2017-05-17T18:01:59.393" UserId="2096" />
  <row Id="7533" PostId="5119" Score="2" Text="@VilhelmGray Right, the depth buffer records the depth of the nearest surface at each pixel, so that when rasterizing a triangle, you can tell if it should be occluded by the previously rendered pixels or not. But if you don't care about depth sorting (because you're drawing wireframe, you've pre-sorted your triangles already, or some other reason) then there's no need for a depth buffer." CreationDate="2017-05-17T18:31:31.577" UserId="48" />
  <row Id="7535" PostId="5120" Score="0" Text="More about the context: I need to use the billboards in **points** mode. So a billboard is attached in screen space from a single vertex position." CreationDate="2017-05-17T22:21:50.433" UserDisplayName="user4613" />
  <row Id="7536" PostId="5122" Score="2" Text="In 2015 Pixar gave a great talk at SIGGRAPH about [Multi-Threading for Visual Effects](http://s2015.siggraph.org/attendees/courses/sessions/multi-threading-visual-effects.html). I'm not sure if the video is available, but if you can find it, it's worth watching. They break down their tests to do multi-threading several different ways: Per frame, per tile, per effect pass, etc. Really interesting and useful info!" CreationDate="2017-05-17T22:24:59.630" UserId="3003" />
  <row Id="7538" PostId="5124" Score="0" Text="If this site gets [stack snippets](https://computergraphics.meta.stackexchange.com/questions/26/do-we-want-stack-snippets) at some point, I'll replace the jsfiddles with stack snippets so it can all be viewed from the answer post." CreationDate="2017-05-18T00:30:02.877" UserId="231" />
  <row Id="7539" PostId="5124" Score="1" Text="Thanks for this nice demo of your initial idea. I'll post a capture of the animation tomorrow for you to integrate into your post. I think this might be called a (pseudo) random-random walk? This might be extended with more directionality at the cost of additional state memory for each pixel (3 bits). But the effect is close enough to the original's presumed particle effect. I'll try to find out if there are any RNGs/LSFR whose state can be set without iteration (for the reverse operation) and then implement this on a microcontroller. Thanks again." CreationDate="2017-05-18T05:36:22.980" UserId="6628" />
  <row Id="7540" PostId="5125" Score="0" Text="Is [wikipedia](https://en.wikipedia.org/wiki/Torus) not enough?" CreationDate="2017-05-18T15:37:40.340" UserId="1981" />
  <row Id="7542" PostId="5099" Score="1" Text="For the case X=1 you might want to take a look here: https://codegolf.stackexchange.com/questions/33172/american-gothic-in-the-palette-of-mona-lisa-rearrange-the-pixels/33206#33206&#xA;&#xA;One solution you could think of is reducing each X by X square to one colour (e.g. by averaging) and then using the algorithm presented in the linked challenge." CreationDate="2017-05-18T22:09:42.617" UserId="1981" />
  <row Id="7543" PostId="5124" Score="1" Text="I have to admit, this is the better answer. +1" CreationDate="2017-05-18T22:34:47.567" UserId="6596" />
  <row Id="7544" PostId="422" Score="1" Text="Note that (0.3, 0.59, 0.11) are the [luma coefficients](https://en.wikipedia.org/wiki/Luma_(video)) for Rec. 601 (aka Standard Def. NTSC), whereas Rec. 709 (aka High Def. ATSC) would be (0.2126, 0.7152, 0.0722). And sRGB images (which is likely what you're dealing with) are closer to Rec. 709 than Rec. 601." CreationDate="2017-05-19T02:45:55.393" UserId="3003" />
  <row Id="7545" PostId="4611" Score="0" Text="PaulHK That's exactly what I am doing in my path tracer, russian roulette for each interface between layers, so, no branching at all. Unfortunatelly, my implementation is not finished yet, so I do not have information regarding the actual performance. I've based my implementation on the paper &quot;Arbitrarily Layered Micro-Facet Surfaces&quot; by Andrea Weidlich and Alexander Wilkie, which seems to be more limited than the framework of Wenzel Jakob (pointed out in the answer by Stefan), but which is capable of generating quite good results and is much simpler to implement." CreationDate="2017-05-19T04:31:42.510" UserId="5681" />
  <row Id="7546" PostId="297" Score="0" Text="The path tracing example at the bottom is simulating total internal reflection also which adds a lot of complexity to the shading. Thin areas can appear both 'thicker' and more attenuated like the rims of the ears for example." CreationDate="2017-05-19T07:38:49.653" UserId="3073" />
  <row Id="7547" PostId="5125" Score="0" Text="Not really, because it describe everything in matter of 3+ dimensions, whereas I would like properties in the 2 dimension space. For example, I would like a formulae to calculate the distance minimal distance between two points.  &#xA;I found a way to do it but I'd like more mathematical insights, because I think it would be better and faster." CreationDate="2017-05-19T08:28:19.957" UserId="6655" />
  <row Id="7548" PostId="5125" Score="0" Text="You just see the embeddings of a torroidal space in 3d, but the space itself is 2d. *In the field of topology, a torus is any topological space that is topologically equivalent to a torus.* So a rectangle with opposing sides &quot;glued&quot; together is a toroidal space." CreationDate="2017-05-19T08:42:09.070" UserId="1981" />
  <row Id="7549" PostId="57" Score="0" Text="I do not quite see the difficulty: Couldn't you just start with any coarse initial mesh, and refine each triangle where e.g. the product of $curvature\times area$ (or e.g. the sum of the absolute angles to the neighbour triangles, or any other heuristic) exceeds some constant? In bthe basic adaptive FEM you basically try to estimate the error on each element (i.e. triangle) and refine it if the error is above some threshhold." CreationDate="2017-05-19T09:14:24.353" UserId="1981" />
  <row Id="7550" PostId="5129" Score="1" Text="It's unclear what you mean by &quot;shaderpass&quot; here. Are you rendering quads to the texture? You say that each pass should render to different locations; how are those locations determined? Is your shader trying to read from the same texture it's writing to? Please be more explicit (or post some code) as to exactly what you're trying to do." CreationDate="2017-05-19T15:53:22.607" UserId="2654" />
  <row Id="7551" PostId="5129" Score="0" Text="Actually my post is super explicit and to the point. I'm not sure why you come up with so many things not related to the question. It tells you exactly what i do and need to know. You confuse me." CreationDate="2017-05-19T17:02:49.483" UserId="6633" />
  <row Id="7552" PostId="5130" Score="0" Text="after rethinking... i'm not sure i follow. How would i blend one texture with itself? Read-modify-write in the shader isn't an option for me, it would kill performance. Remember, there is only one output texture.. and a dozen shaderpasses." CreationDate="2017-05-19T17:05:18.600" UserId="6633" />
  <row Id="7553" PostId="5129" Score="1" Text="Well, the term &quot;shaderpass&quot; is not part of OpenGL or GLSL lexicon. I've heard it used in many different contexts for radically different things. It's been used to mean &quot;multipass rendering over the same mesh&quot;, &quot;full-screen quads over a scene&quot;, and other things. There is no one definition of &quot;shaderpass&quot;, so I'm trying to ask what *you* mean by this term. As well as the other things. And FYI, whether you're reading from the texture you're rendering to is *not* unrelated to the question." CreationDate="2017-05-19T17:05:35.920" UserId="2654" />
  <row Id="7554" PostId="5024" Score="0" Text="Cutting the axis works for cutting the axis, then the resulting image takes up a quarter of the new window.   If I enter graphics_toolkit(&quot;gnuplot&quot;) I get the entire axes to be filled with the purple squares like I want.  However, if I use the line( ) function to plot a line overtop I get the following error:     lot &quot;-&quot; binary format='%float64' record=2 using ($1):($2) axes x1y1 title &quot;&quot; with lines linestyle 10 ;&#xA;           ^&#xA;           line 0: invalid command" CreationDate="2017-05-19T17:22:25.030" UserId="6501" />
  <row Id="7555" PostId="5024" Score="0" Text="If I don't use gnuplot, and instead use OpenGL (which is the default), I get the same quarter plot image of purple but plotting overtop works." CreationDate="2017-05-19T17:25:15.020" UserId="6501" />
  <row Id="7556" PostId="5024" Score="0" Text="@flawr  Please see the edited post.  Hopefully it helps and thank you very much for helping me out." CreationDate="2017-05-19T17:48:07.310" UserId="6501" />
  <row Id="7557" PostId="5129" Score="0" Text="I didn't mention that i'm reading from the output texture, therefore I'm not doing it. If I did that, I'd mention it. :/ well, a shader pass is when you have a shader doing things. The question implied that there are multiple passes Everything else is just &quot;flavour&quot; and changes nothing about it being multiple shader passes. Your attempt at accuracy is kind of appreciated, but not leading anywhere." CreationDate="2017-05-19T17:48:41.070" UserId="6633" />
  <row Id="7558" PostId="5130" Score="0" Text="@z0rberg's: &quot;*How would i blend one texture with itself?*&quot; You don't blend *textures* at all. Blending happens between the current render target and the outputs of the fragment shader." CreationDate="2017-05-19T17:52:55.907" UserId="2654" />
  <row Id="7559" PostId="5130" Score="0" Text="I must have struck a nerve somewhere in your ego. You're not helpful and actually annoying. :) sure, you wanna be super accurate, but you don't see how that only serves yourself and no one else. Can you stop being annoying? Cheers! :) ... or wait. You can have the last word, as that will make you feel better... and i will simply ignore you. :)" CreationDate="2017-05-19T18:18:41.733" UserId="6633" />
  <row Id="7560" PostId="5130" Score="0" Text="@z0rberg's: I did answer your question. How do you blend a texture with itself? You don't; you blend *the output of your rendering process* with the texture. You render with blending. You render again with blending. Both are rendered to the same image, with the latter blended on top of the former. If you find it &quot;annoying&quot; to use the right words to describe things, I really can't help you. But that's how you do what ratchet freak was talking about." CreationDate="2017-05-19T19:01:15.010" UserId="2654" />
  <row Id="7561" PostId="5129" Score="1" Text="&quot;*Everything else is just &quot;flavour&quot; and changes nothing about it being multiple shader passes*&quot; No, they're not &quot;flavour&quot;. For example, if your multiple &quot;shader passes&quot; are actually rendering *different geometry*, then I would suggest you construct the geometry for the separate passes to simply not overlap. But if they're rendering the *same* geometry, then you'd need to do something else. And that &quot;something else&quot; would be based on how each pass knows what it should and shouldn't render to. This is not sophistry; these details directly impact what my answer would be." CreationDate="2017-05-19T19:03:33.073" UserId="2654" />
  <row Id="7562" PostId="5120" Score="0" Text="Screenshots of the problem would have made this much easier to answer by more people FYI!" CreationDate="2017-05-19T21:15:12.377" UserId="56" />
  <row Id="7563" PostId="5127" Score="0" Text="You're right, I found some good stuff with &quot;toroidal space&quot; after all. And thanks for the algorithm. I think I have an optimization I will test and confirm here. I'll accept your answer after that." CreationDate="2017-05-19T21:48:02.437" UserId="6655" />
  <row Id="7564" PostId="5130" Score="2" Text="@z0rberg's Welcome to Computer Graphics Stack Exchange! Please understand that people are here to help you, and are not asking questions to annoy you. Narrowing down what you are aiming to do is part of helping you. All members of this community are expected to [Be Nice](https://computergraphics.stackexchange.com/help/be-nice). Please bear this in mind when responding to others." CreationDate="2017-05-20T00:02:37.117" UserId="231" />
  <row Id="7565" PostId="5131" Score="0" Text="Do you mean you want the object to move directly towards or away from the eye?" CreationDate="2017-05-20T00:31:11.103" UserId="231" />
  <row Id="7566" PostId="5131" Score="0" Text="Yeah along the direction of the eye" CreationDate="2017-05-20T10:06:27.693" UserId="6401" />
  <row Id="7567" PostId="5108" Score="0" Text="Do you know anything more about the structure? E.g. are there just a handful of planes, or are there many planes but just a few points per plane? Are the planes equidistant? Are the points within a plane closer to eachother than to the points of the other planes?" CreationDate="2017-05-20T13:03:04.303" UserId="1981" />
  <row Id="7568" PostId="5108" Score="0" Text="Also do you have an upper bound of the number of planes?" CreationDate="2017-05-20T14:53:16.570" UserId="1981" />
  <row Id="7569" PostId="5132" Score="0" Text="It means their values depend on two directions (usually incoming and outgoing light). With a direction being a direction in 3-space, of course, not something like &quot;up-down&quot;." CreationDate="2017-05-21T16:37:01.117" UserId="6" />
  <row Id="7570" PostId="5133" Score="1" Text="So the &quot;two directions&quot; are simply the incidence ray and the reflection/transmission ray? :)" CreationDate="2017-05-21T19:08:44.187" UserId="2736" />
  <row Id="7571" PostId="5129" Score="2" Text="@z0rberg's Please reconsider your attitude toward people you are hoping to get free help from. And please do add more details—including screenshots and relevant rendering code—to your question. We can't read your mind and it is far from clear what exactly the problem you're trying to solve is." CreationDate="2017-05-22T03:33:21.743" UserId="48" />
  <row Id="7572" PostId="5134" Score="0" Text="Some things to consider, while you wait for the answer. How would you get a unsorted dataset in order if not by sorting? So nearly every algorithm stars by organising data to suit their need. Where would you store that data? Suddenly you have tables and sorting on everything you do." CreationDate="2017-05-22T05:37:29.730" UserId="38" />
  <row Id="7574" PostId="5137" Score="0" Text="The Lumen application posted above suggests it can emulate &gt;60hz frequencies by using either horizontal clock or pixel clock simulation." CreationDate="2017-05-22T06:53:11.987" UserId="3073" />
  <row Id="7575" PostId="5137" Score="0" Text="@PaulHK yes but then you need to average frames. Most systems have vertical sync  on by default. I mean i can sort of do this on my monitor since i have sync turned off but you couldn't depend on a web application to do this since it wouldnt work for 99% of users, and you have no control over the setting." CreationDate="2017-05-22T06:59:04.463" UserId="38" />
  <row Id="7576" PostId="5137" Score="0" Text="Could you not simulate the time difference from the top-left corner ? This is (roughly) how video game emulators handle raster effects for instance. For example, the middle scanline of the frame would be roughly +8ms from the top of the frame, assuming a 16.6ms frame." CreationDate="2017-05-22T07:03:15.623" UserId="3073" />
  <row Id="7577" PostId="5137" Score="0" Text="@PaulHK Not on the platform you use in a browser, the hardware does multi buffering and you can not escape the sync even if you wanted to, as far as i understand." CreationDate="2017-05-22T07:04:29.473" UserId="38" />
  <row Id="7578" PostId="5135" Score="0" Text="Oow, thanks! That's a really nice explanation! :)" CreationDate="2017-05-22T07:36:30.463" UserId="2736" />
  <row Id="7584" PostId="5124" Score="0" Text="@Jackalope thank you! My approach is less sophisticated and I can't imagine why the memory would need to be so restricted, but assuming that it is, this was the simplest I could think of." CreationDate="2017-05-22T12:11:42.673" UserId="231" />
  <row Id="7585" PostId="5137" Score="0" Text="I guess the core of my question is what sort of calculations are being performed in order to emulate a video oscillation of &gt;60hz? Since both Lumen and my browser are running on the same display with the same refresh rate, albeit one is a native app which might mean that it is doing things that are limited in the browser..." CreationDate="2017-05-22T17:57:59.377" UserId="6674" />
  <row Id="7586" PostId="5137" Score="0" Text="@Corey well the local program has more access to the system. In any case your question does not really specify that your asking about teh emulation. Your just asking why you have problems below 60Hz. But the real answer is to actually calculate the color as it would be. But its just horribly complicated to get it right. multi sample or integrate the result of your data end run it in color correct environment that should do the trick. Only a browser can not know the colorspace." CreationDate="2017-05-22T18:01:37.387" UserId="38" />
  <row Id="7589" PostId="5140" Score="3" Text="As long as all duplicates of a vertex have the same normal, you should get smooth shading. Check that (i) the duplicated normals are indeed identical, and (b) the shader is actually doing Gouraud shading and not, say, flat shading (incorrect setting of [`glShadeModel`](https://www.khronos.org/registry/OpenGL-Refpages/gl2.1/xhtml/glShadeModel.xml) or [interpolation qualifier](https://www.khronos.org/opengl/wiki/Type_Qualifier_(GLSL)#Interpolation_qualifiers))." CreationDate="2017-05-23T02:06:24.997" UserId="106" />
  <row Id="7590" PostId="5137" Score="0" Text="The Lumen application is showing results for one frame rather than integrating all possible phases, so we should expect some kind of rolling effect for each frame. Check the red gifs in the link under the 'Frequency ranges' heading." CreationDate="2017-05-23T04:05:25.160" UserId="3073" />
  <row Id="7593" PostId="5140" Score="0" Text="If you just want to know whether this is possible, this seems ready to answer, and @Rahul's comment contains a good start on such an answer. If you want to know why your specific code isn't working, we'd need to see it in order to investigate it." CreationDate="2017-05-23T10:17:36.590" UserId="231" />
  <row Id="7594" PostId="5140" Score="0" Text="@Rahul Thank you for your hints. I check the normals and the glShadeModel and both are set correctly. I also wrote the model into an Wavefront obj file and this also results in strange lightning. Here is an image of the model http://imgur.com/a/JjLxp." CreationDate="2017-05-23T12:11:48.953" UserId="6682" />
  <row Id="7595" PostId="5144" Score="1" Text="Can you please add a link to the paper or book you're referring to, and a relevant excerpt from the code you're talking about?" CreationDate="2017-05-23T14:35:16.307" UserId="48" />
  <row Id="7596" PostId="5144" Score="0" Text="http://www.dgp.toronto.edu/people/stam/reality/Research/pdf/ns.pdf this is it I thought the links are banned or something" CreationDate="2017-05-23T14:46:40.623" UserId="4978" />
  <row Id="7601" PostId="4900" Score="0" Text="A surface of revolution of a line segment would be a disk, cylinder, or cone, no? I can't see how you would get a hyperboloid; it should be a surface of revolution of a hyperbola. I'm guessing the hyperbola is fit to pass through the two points, or something like that." CreationDate="2017-05-23T15:13:40.263" UserId="48" />
  <row Id="7602" PostId="5140" Score="0" Text="Oh..could it be that you are  seeing  &quot;Mach band-ish&quot; related&quot; artefacts that appear when you get a discontinuity in the 1st derivative of the shading? Basically, the human visual system amplifies these discontinuities (probably to save our ancestors being eaten by lions)." CreationDate="2017-05-23T16:12:31.970" UserId="209" />
  <row Id="7603" PostId="5146" Score="0" Text="Make sure you got clamp (vs wrap) texture address mode enabled when fetching from shadow map." CreationDate="2017-05-23T16:59:55.583" UserId="1952" />
  <row Id="7604" PostId="4900" Score="2" Text="@NathanReed It's not intuitive at all but it is possible. Look at the diagrams on the right side of the [wikipedia entry](https://en.wikipedia.org/wiki/Hyperboloid#Parametric_representations)." CreationDate="2017-05-23T22:31:48.730" UserId="5717" />
  <row Id="7605" PostId="5140" Score="2" Text="Going by your image, it looks like you have Gouraud shading working perfectly well. The problem is a combination of skinny triangles and poorly estimated normals, making it *look* like the shading is not smooth when really it is just varying rapidly over a narrow region." CreationDate="2017-05-24T05:06:42.087" UserId="106" />
  <row Id="7606" PostId="5147" Score="0" Text="What about if b is not 1 and 2? Another thing, where is the Neuman boundaries? Those are slip or no-slip? Is it a solid boundaries? If i  use the PCG, will something change in the boundaries? Sorry for all of these stupid questions but I want to understand. Thanks cheers!" CreationDate="2017-05-24T11:42:32.357" UserId="4978" />
  <row Id="7607" PostId="5147" Score="0" Text="@AnasAlaa if you have a new question you can post it as a question rather than a comment." CreationDate="2017-05-24T20:14:15.433" UserId="231" />
  <row Id="7608" PostId="5147" Score="0" Text="I am sorry but it's really the same question but in another form. And it's a complete of his answer too." CreationDate="2017-05-24T22:05:10.613" UserId="4978" />
  <row Id="7609" PostId="5150" Score="0" Text="Thanks! That first part was the important information for me. I missed this information in the Tutorials. Since I use textures, I only use color to &quot;tint&quot; Quads (Sprites), if you know what I mean, or to alter transparency via the alpha channel. So using a uniform was a good hunch. I am still stuck on matrices and how to do them properly on the GPU, but I am getting there." CreationDate="2017-05-24T22:08:58.997" UserId="6679" />
  <row Id="7610" PostId="5147" Score="0" Text="There's a little mistake in my previous answer, and I've editted the answer.  About Neuman boundaries, this demo code didn't implement it." CreationDate="2017-05-25T01:49:01.350" UserId="6691" />
  <row Id="7614" PostId="5153" Score="0" Text="Thank you for the clarifying answer! I understand that if we were to use a path tracer without explicit light sampling, we would never hit a point light source. So, we can basically add its contribution. On the other hand, if we sample an area light source, we have to make sure we should not hit it again with the indirect lighting in order to avoid double dip" CreationDate="2017-05-25T15:37:02.323" UserId="6698" />
  <row Id="7615" PostId="5153" Score="0" Text="Exactly! Is there any part that you need clarification on? Or there isn't enough detail?" CreationDate="2017-05-25T15:39:24.237" UserId="310" />
  <row Id="7616" PostId="5153" Score="0" Text="Also, is multiple importance sampling used only for direct lighting calculation? Maybe I missed but I didn't see another example of it. If I shoot just one ray per bounce in my path tracer, it seems that I cannot do it for the indirect lighting calculation." CreationDate="2017-05-25T15:43:01.690" UserId="6698" />
  <row Id="7617" PostId="5153" Score="2" Text="Multiple Importance Sampling can be applied anywhere you use importance sampling. The power of multiple importance sampling is that we can combine the benefits of multiple sampling techniques. For example, in some cases, light importance sampling will be better than BSDF sampling. In other cases, vice versa. MIS will combine the best of both worlds. However, if BSDF sampling will be better 100% of the time, there is no reason to add the complexity of MIS. I added some sections to the answer to expand upon this point" CreationDate="2017-05-25T18:53:35.473" UserId="310" />
  <row Id="7619" PostId="5153" Score="1" Text="It seems we separated incoming radiance sources into two parts as direct and indirect. We sample lights explicitly for the direct part and while sampling this part, it is reasonable to importance sample the lights as well as BSDFs. For the indirect part, however, we have no idea about which direction may potentially give us higher radiance values since it is the problem itself that we want to solve. However, we can say which direction can contribute more according to the cosine term and BSDF. This is what I understand. Correct me if I'm wrong and thank you for your awesome answer." CreationDate="2017-05-25T19:50:22.677" UserId="6698" />
  <row Id="7620" PostId="5153" Score="0" Text="That's correct! Very good summary" CreationDate="2017-05-25T19:52:58.853" UserId="310" />
  <row Id="7621" PostId="5147" Score="0" Text="So, if I just want to solve the pressure poisson equations with the PCG method, can I just use these boundaries in the code?" CreationDate="2017-05-25T23:07:02.193" UserId="4978" />
  <row Id="7622" PostId="5147" Score="0" Text="Yes, I think so." CreationDate="2017-05-26T01:50:29.490" UserId="6691" />
  <row Id="7623" PostId="5147" Score="0" Text="I think the answer is not correct. The two options are either homogenous Neumann boundary conditions (if the value is copied) or homogenous dirichlet boundary condition as negating the value represents a zero value in between the interface. Furthermore I doubt that you can use this function for a cg solver. To which variables do you want apply it? For a cg solver I would recommend to check Robert bridsons book fluid simulation for computer graphics, although he uses a slightly modified (superior) discretization (staggered grid)." CreationDate="2017-05-26T19:18:48.873" UserId="2695" />
  <row Id="7624" PostId="5147" Score="0" Text="I don't understand how the boundaries will be dirchlet and neuman at the same time. Bridson book is good but he has no code to implement what he said about boundaries and PCG... etc" CreationDate="2017-05-27T03:05:56.473" UserId="4978" />
  <row Id="7625" PostId="5147" Score="0" Text="Boundary conditions are either dirichlet or Neumann, I don't know from which part of the code you see this. Neumann  conditions are realized by modifying the matrix. You might want to check a finite difference book for details. This function from stam emulates a modified matrix for a gauss seidel iteration.  I fear you have to do the transition from text / equations to code. You can't just copy and paste everything from different sources..." CreationDate="2017-05-27T13:15:16.483" UserId="2695" />
  <row Id="7626" PostId="5156" Score="0" Text="Can it be assumed that the orientation of all instances of a character are identical? That is, that there is zero rotation?" CreationDate="2017-05-27T13:32:50.193" UserId="231" />
  <row Id="7627" PostId="5156" Score="0" Text="Are you able to share the raw data for people to experiment?" CreationDate="2017-05-27T13:33:37.863" UserId="231" />
  <row Id="7628" PostId="5156" Score="1" Text="@trichoplax There is a very slight consistent rotation clockwise as a scanning artifact. I've accounted for it when segmenting. There is no variation of rotation of individual characters.&#xA;&#xA;[Initial image](https://www.dropbox.com/s/01ygrkzlhylu612/ACPU-128.jpg?dl=0) [Charset encoding](http://en.wikipedia.org/wiki/GOST_10859) [Segmented](https://www.dropbox.com/s/doc2bzh155mey26/tiles.zip?dl=0)" CreationDate="2017-05-27T16:44:44.147" UserId="6705" />
  <row Id="7630" PostId="5153" Score="0" Text="Hi Richie, I enjoyed reading your answer here. While ago I posted a question on MIS but didn't get answer. I wonder if I could have your thoughts as well, thanks. https://computergraphics.stackexchange.com/questions/4868/efficiently-sampling-specific-surfaces-using-mis-in-path-tracing" CreationDate="2017-05-27T23:24:40.693" UserId="6041" />
  <row Id="7631" PostId="5147" Score="0" Text="I understand you, in the code, the boundaries are dirichlet or neuman in the code. But I doubt if I can just use the same boundaries for cg solver you can see the full code here" CreationDate="2017-05-27T23:55:11.180" UserId="4978" />
  <row Id="7632" PostId="5147" Score="0" Text="https://www.autodeskresearch.com/sites/default/files/TheArtOfFluidAnimationCode.zip&#xA;&#xA;&#xA;Here is the code and you just use the set_bnd for div in the project function to whom I want just to replace the gauss seidel with PCG." CreationDate="2017-05-28T02:42:20.940" UserId="4978" />
  <row Id="7634" PostId="4789" Score="0" Text="Why do you use the vector A(1, 1, 1) instead of the VUP vector(0,0,1) to work out the first row of the rotation matrix?" CreationDate="2017-05-27T18:09:33.243" UserId="6716" />
  <row Id="7635" PostId="4789" Score="0" Text="isnt transformation matrix M = R.T , why are you saying to transform it again. I think the question just asked for the transformation matrix" CreationDate="2017-05-27T19:51:36.523" UserId="6718" />
  <row Id="7636" PostId="4789" Score="0" Text="@link in object(camera) coordinate space, the z-axis from the eye to a point where the eye looks at. yes, it's point A, `VRP - A` equals to the coordinate of z-axis of object space in world space." CreationDate="2017-05-28T08:07:57.280" UserId="6140" />
  <row Id="7637" PostId="4789" Score="0" Text="@fahadchowdhury The question starter wants a matrix which can convert world coordinate to object coordinate, this matrix consist of two matrices, one is rotation matrix, another is translation matrix, only if the camera is looking at positive z-axis, the rotation matrix would be an identity matrix." CreationDate="2017-05-28T08:22:11.757" UserId="6140" />
  <row Id="7640" PostId="4342" Score="0" Text="How do I calculate the light direction for use in a Ray Tracer to first calculate the Direct Lighting though? Do I even need to do that? I am little confused here." CreationDate="2017-05-28T12:53:09.150" UserId="5256" />
  <row Id="7641" PostId="5167" Score="0" Text="Will try this method! Although, would you have any knowledge of how I can do it without approximating it with point lights?" CreationDate="2017-05-28T14:10:01.077" UserId="5256" />
  <row Id="7642" PostId="4342" Score="0" Text="You don't need to do that, no.  You just shoot random rays in the hemisphere, and some of them will hit the light, some of them won't.  There are techniques to shoot rays at the lights more directly while still accounting for the probability of you having chosen them randomly (to make the image look the same), but it's not required, no." CreationDate="2017-05-28T14:20:07.700" UserId="56" />
  <row Id="7643" PostId="4342" Score="0" Text="I'm still really confused (I'm sorry), but wouldn't this only work for Indirect lighting and not Direct Lighting? Could you add some code to your answer to illustrate this clearer? I looked at the source for path tracer in the blog post on GitHub but I couldn't understand what was going on." CreationDate="2017-05-28T14:31:47.553" UserId="5256" />
  <row Id="7644" PostId="4342" Score="0" Text="Also, does this mean I can remove the direct lighting raytracer component from my Renderer entirely since the path tracer can handle this?" CreationDate="2017-05-28T14:35:09.933" UserId="5256" />
  <row Id="7645" PostId="4342" Score="0" Text="The light leaving a surface in a specific direction is made up of two components: One is the $L_e$ term which is the emissive light, or how much the object glows.  That is the direct lighting. The other component is the rest of the integration, which is how much light is reflected from other lighting sources (direct and indirect lighting sources), this is indirect lighting.  Those two components are added together as light leaving the surface.  Path tracing as I described finds them both because they are treated the same (they are summed and treated as &quot;lighting&quot; not direct or indirect)." CreationDate="2017-05-28T14:38:01.137" UserId="56" />
  <row Id="7646" PostId="4342" Score="0" Text="Yes, you can remove the direct lighting code from your path tracer if you want to.  You will still get accurate results.  However, as I explained from the first couple sentences of the answer, it will take longer to make a good looking image if you do so.  It takes more complex math to have &quot;direct lighting&quot; in your path tracer, and have the results come out correctly, but it does speed things up to have that. You get better results in fewer samples.  But yeah, &quot;direct lighting&quot; code is totally optional, and path tracing works fine without it." CreationDate="2017-05-28T14:40:26.307" UserId="56" />
  <row Id="7647" PostId="5167" Score="0" Text="Unfortunately, I do not know any method for arbitrary surfaces which works with simple raytracing because it's hard to tell if an point can see the plane or not, but depending on your case you might want to try a Monte Carlo method such as Distribution Ray Tracing." CreationDate="2017-05-28T17:00:09.253" UserId="6682" />
  <row Id="7648" PostId="5168" Score="2" Text="CG has already passed the uncanny valley, as can be shown by the rising ubiquity of completely digital doubles in Hollywood blockbusters of recent years. I would imagine giving critique of a particular artwork is somewhat out of the scope of this website, but for me it's mostly the lack of detail on the clothes and the skin shader. The hair is also a bit off but hard to say exactly how." CreationDate="2017-05-28T18:10:52.757" UserId="457" />
  <row Id="7649" PostId="5168" Score="3" Text="Sometimes people geet hooked on knowing that it is cg. They then say they can spot CG. However when you then give them a mixture of images which are real and which are CG they suddenly can not spot the difference." CreationDate="2017-05-28T20:53:00.597" UserId="38" />
  <row Id="7650" PostId="5168" Score="0" Text="I would say for things to improve, the tie and shirt don't quite look right but it would take a lot of observation and the intent to find rendering issues to observe those things. In terms of the interaction of light, it has been nailed but what will need to happen next (for CG in general) will be an accurate simulation of smoke and other fluids to perfect CGI in most scenarios. This also brings up the question for me of where rendering goes next? Other than speeding up renders what can be done to improve? Is there a dead end soon approaching?" CreationDate="2017-05-28T21:03:42.723" UserId="5256" />
  <row Id="7651" PostId="5140" Score="0" Text="Does your algorithm produce a stream of triangles that you need to process immediately, or do you have the full model at hand before sending it to the visualization software? In the second case, could some processing allow you to create an indexed model (with averaged normals on each vertex)?" CreationDate="2017-05-29T09:07:09.723" UserId="6725" />
  <row Id="7653" PostId="5168" Score="0" Text="See also [this very similar question](https://computergraphics.stackexchange.com/questions/2556/what-physical-properties-are-lacking-to-keep-this-3d-scene-from-looking-like-a). I'm not closing this as a duplicate because I'm treating it as a review of the image for realism, rather than a general question about the state of the art. I've edited the title to reflect this." CreationDate="2017-05-29T12:28:13.727" UserId="231" />
  <row Id="7654" PostId="5168" Score="0" Text="@trichoplax much appreciated" CreationDate="2017-05-29T13:44:22.840" UserId="6722" />
  <row Id="7655" PostId="5140" Score="0" Text="The problem here is, that is a custom made visualisation software, which gets an input stream in a custom format from a server software, which could work on multiple computing nodes (made for high performance computing). This format doesn't support indices and the model have to be outputed as a triangle stream. But for processing it's possible to use indices, with the restriction that the whole mesh may be distributed across the server nodes." CreationDate="2017-05-29T13:49:40.380" UserId="6682" />
  <row Id="7656" PostId="5167" Score="0" Text="Maybe you could also try to sample some randomly distributed points on the plane of the area light and then check it against your surface point." CreationDate="2017-05-29T13:52:32.210" UserId="6682" />
  <row Id="7658" PostId="4536" Score="0" Text="So in terms of innovation (excluding render times and performance) there isn't much left to do for offline rendering for light transport, does this mean offline rendering has hit a dead end? Would you happen to know of any of the challenges people who work on path tracers have? Light physics is what really interests me and the fact that the focus is moving towards animation really sucks :(..." CreationDate="2017-05-29T17:06:34.440" UserId="5256" />
  <row Id="7660" PostId="5144" Score="0" Text="https://www.autodeskresearch.com/sites/default/files/TheArtOfFluidAnimationCode.zip Here is the code and you just use the set_bnd for div in the project function to whom I want just to replace the gauss seidel with PCG." CreationDate="2017-05-27T23:57:41.090" UserId="4978" />
  <row Id="7662" PostId="5156" Score="0" Text="Interesting indeed. just an idea- since you make own program, how is about setting different threshold by averaging, depending on which pixel row it is - say the top pixel row contributes pixels which are &gt;95% black, second slightly more, say, all pixels which are &gt;94% black ... and so on." CreationDate="2017-05-29T21:58:10.173" UserId="6160" />
  <row Id="7663" PostId="5156" Score="0" Text="@Mikhail That's a good idea, but it definitely requires tuning." CreationDate="2017-05-30T03:02:40.507" UserId="6705" />
  <row Id="7664" PostId="5168" Score="0" Text="The dead eyes, for me." CreationDate="2017-05-30T05:01:19.737" UserId="6679" />
  <row Id="7666" PostId="5168" Score="0" Text="Looks to clean. Doesn't suffer the artefacts which many cameras/lenses create. E.g. grain. Lighting also looks too perfectly distributed." CreationDate="2017-05-30T11:38:27.237" UserId="3331" />
  <row Id="7667" PostId="4536" Score="1" Text="Well actually I think there is more to figure out about basic lighting.  Give this a read (Disney's PBR paper from 2012) to see some open problems with basic things: https://disney-animation.s3.amazonaws.com/library/s2012_pbs_disney_brdf_notes_v2.pdf" CreationDate="2017-05-30T13:36:12.027" UserId="56" />
  <row Id="7668" PostId="5147" Score="0" Text="Where are you? I need help" CreationDate="2017-05-30T17:47:36.870" UserId="4978" />
  <row Id="7669" PostId="4536" Score="0" Text="The book &quot;physically based rendering from theory to practice&quot; also describes a lot of open problems, like better ways to describe a color spectrum or a brdf." CreationDate="2017-05-30T18:17:09.297" UserId="56" />
  <row Id="7670" PostId="4536" Score="0" Text="That actually is true, a lot of the exercises in that book point out many unsolved challenges. While they may not be the low hanging fruit I was expecting it does show a lot of the problems left with path tracers which go very deep. Is there a book like PBRT for simulations? Simulations are cool but for a novice, it's nearly impossible to learn at the moment due to the lack of resources that go step-by-step." CreationDate="2017-05-30T19:14:37.050" UserId="5256" />
  <row Id="7671" PostId="4536" Score="0" Text="I think it depends a lot on the type of simulation you want to make, but I'm not sure of any intro resources to eg fluid or smoke sim. I'd like to know of some!" CreationDate="2017-05-30T19:16:57.733" UserId="56" />
  <row Id="7672" PostId="5181" Score="0" Text="Yes there is a good tutorial called [12 steps to CFD](http://lorenabarba.com/blog/cfd-python-12-steps-to-navier-stokes/)." CreationDate="2017-05-30T19:56:35.823" UserId="38" />
  <row Id="7674" PostId="5181" Score="0" Text="This seems like a really broad question. (And I'm not sure if recommendations are allowed on this stack exchange. They're usually frowned upon.) Can you narrow down what you're having trouble understanding with fluid sims?" CreationDate="2017-05-31T05:22:55.350" UserId="3003" />
  <row Id="7675" PostId="5181" Score="0" Text="See Bridson and Müller-Fischer's notes from their SIGGRAPH 2007 course, *[Fluid Simulation for Computer Animation](http://www.cs.ubc.ca/~rbridson/fluidsimulation/)*. You should also consider picking up [Bridson's book](https://www.crcpress.com/Fluid-Simulation-for-Computer-Graphics-Second-Edition/Bridson/9781482232837) which has more detail." CreationDate="2017-05-31T08:38:52.177" UserId="106" />
  <row Id="7677" PostId="5156" Score="0" Text="Maybe it is worth posting an image with _one_ letter (before/after filtering) in native resolution. Then probably you get more chances for real proposals. Higher resolution is better. In general removing smear can become a complex task, if you want a pure programmatical way. And I suppose it would need anyway row-by-row filtering, and to filter all chars it will need vertical aligning the glyphs or finding boundaries for each glyph." CreationDate="2017-05-31T21:14:39.480" UserId="6160" />
  <row Id="7678" PostId="5156" Score="0" Text="So if your examples is the highest possible resoultion, then I'd say it is too small for post-processing.  And if something, you can always retouche the results with hand. Retouching the whole set can take a pair of evenings, but developing fully automatical software can take weeks." CreationDate="2017-05-31T21:30:00.330" UserId="6160" />
  <row Id="7679" PostId="5156" Score="0" Text="@MikhailV Retouching is always an option, if everything else fails. Now I'm doing simple averaging after estimating the offset in whole pixels. I was hoping for some kind of a smarter algorithm for offset estimation and/or averaging." CreationDate="2017-05-31T21:38:51.010" UserId="6705" />
  <row Id="7682" PostId="5184" Score="3" Text="That's kind of a big question to ask. If you're hoping for a big block of code you can just paste in, you're going to be disappointed. Perhaps you could narrow the question down to a particular part you're having trouble with. Would a clearer description of the maths help? Would a pseudocode descrption of the main formulae help?" CreationDate="2017-06-01T09:11:23.853" UserId="2041" />
  <row Id="7683" PostId="5184" Score="1" Text="Yeah, you're right I should be doing the coding myself. A clearer explanation of the mathematics of what does what (also the symbols which confuse the hell out of me) will help and pseudo code will work just fine. I'm updating the question to be more specific." CreationDate="2017-06-01T12:31:39.520" UserId="5256" />
  <row Id="7684" PostId="5192" Score="0" Text="Isn't a second texture a more dense sampling with additive blending of samples?" CreationDate="2017-06-01T15:38:16.353" UserId="4958" />
  <row Id="7685" PostId="5189" Score="0" Text="I have red that part of the article, but I fail to see the connection between that equation and the one I showed above. Perhaps my math skills arn't good enough to work that out." CreationDate="2017-06-01T15:44:30.417" UserId="2736" />
  <row Id="7686" PostId="5184" Score="0" Text="Is there a beginner friendly book that covers this though? I find this extremely tough to grasp and I honestly need a complete explanation of everything." CreationDate="2017-06-01T17:03:01.447" UserId="5256" />
  <row Id="7687" PostId="5192" Score="0" Text="No, but there is a similar way to generate blue noise textures to what you describe.  Basically you place a point and then low pass filter (blur), then put a point in the lowest valued pixel and blur again. Rinse and repeat. That's how I've heard it described but I think there must be more to it, to keep the points sharp where you placed them." CreationDate="2017-06-01T17:16:23.317" UserId="56" />
  <row Id="7688" PostId="5193" Score="2" Text="Another possible reason for the confusion is that Ambient Occlusion is most of the times rendered out into a separate pass and added in later. Making it seem like it could be a post processing effect." CreationDate="2017-06-01T18:41:24.950" UserId="4908" />
  <row Id="7689" PostId="5184" Score="1" Text="There's an index of the symbols on page 2 of the paper. I'm afraid there's no simple way to understand that method. IIRC, there are slower and more accurate methods which are simpler to understand because they actually trace multiple bounces instead of simulating them with a complex formula. You could perhaps start there." CreationDate="2017-06-01T19:29:26.297" UserId="5717" />
  <row Id="7690" PostId="5184" Score="0" Text="If there are other simpler and accurate techniques I would definitely want to start there :D! Could you link the resources here? Also, does Path Tracing already render subsurface scattering?" CreationDate="2017-06-01T19:31:49.147" UserId="5256" />
  <row Id="7691" PostId="5193" Score="0" Text="@bram0101 does that apply to Ambient Occlusion that is not using the Screen Space Ambient Occlusion approximation?" CreationDate="2017-06-01T19:48:27.437" UserId="231" />
  <row Id="7692" PostId="5186" Score="2" Text="&quot;It seems to just select the longer pair of lines over the shorter pair.&quot; What? All the red lines in the figure are the same length." CreationDate="2017-06-01T21:51:14.903" UserId="106" />
  <row Id="7693" PostId="5192" Score="2" Text="The &quot;blue noise texture&quot; is from [this page](http://momentsingraphics.de/?p=127), which also explains the relationship between blue noise sampling and the texture." CreationDate="2017-06-01T21:53:55.977" UserId="106" />
  <row Id="7694" PostId="5193" Score="2" Text="@trichoplax Yes, for example with Voxel-Cone Traced AO. In that method, the AO is added as a separate pass" CreationDate="2017-06-01T22:10:59.830" UserId="310" />
  <row Id="7695" PostId="5193" Score="1" Text="@trichoplax with offline render engines it is done a lot, especially with VFX or general 3d animation. There could be a few reasons for rendering it out separately. 1. Most shaders and lights do not have a button to turn on ao meaning you have to create a render pass for it. 2. Passes can be faster to render or easier to set up. 3. In VFX or some animation productions, they render out in a bunch of different passes to then add them together in post for more control.  I am talking about ao from offline render engines so it is ray traced ao." CreationDate="2017-06-02T05:46:31.273" UserId="4908" />
  <row Id="7696" PostId="5193" Score="0" Text="These comments suggest it is not as simple as I described, and there is a much better answer waiting to be posted..." CreationDate="2017-06-02T09:35:01.070" UserId="231" />
  <row Id="7699" PostId="5197" Score="0" Text="If you already have the ray-casting algorithm, why not cast a ray from a vertex along its edge and find whether the first intersection is before the distance to the opposite vertex? Then you can find the intersections on each edge." CreationDate="2017-06-02T14:34:25.393" UserId="2041" />
  <row Id="7700" PostId="5197" Score="0" Text="That doesn't help you where a finger of the mesh pokes through the polygon, though. Perhaps you're better off starting by classifying the mesh's vertices by which side of the plane of the polygon they're on." CreationDate="2017-06-02T14:47:06.050" UserId="2041" />
  <row Id="7701" PostId="5199" Score="0" Text="I'm afraid I don't what any of what you just said means. I have almost no background in computer graphics. I'm learning this as I go over the last year or so. I'm writing this code from scratch in C++ for a computational physics application. For reasons I won't go into, this has to run in an environment with almost nothing available. Imagine that this has to be done on a non-network-connected machine in 1997. So straight up from scratch with no special libraries." CreationDate="2017-06-02T15:06:52.907" UserId="6762" />
  <row Id="7702" PostId="5197" Score="0" Text="@DanHulme Once I've determined which mesh vertices are on which side of the plane, what would I do with that information?" CreationDate="2017-06-02T15:11:36.973" UserId="6762" />
  <row Id="7703" PostId="5197" Score="0" Text="Use it to find mesh edges that cross the plane and intersect *them* with the plane to find all the crossing points. You can use the crossing points to make a new polygon which is the section of the plane and the polyhedron. (You need to use the topology of the polyhedron to know what order to connect the edges together, because this new polygon is non-convex.) Now just intersect the two polygons in the usual way. I'm making this up as I go along but it sounds plausible." CreationDate="2017-06-02T17:06:40.873" UserId="2041" />
  <row Id="7704" PostId="5199" Score="0" Text="It comes down to the idea you suggested about projecting the mesh into the plane of the polygon. The near clipping plane part cuts off the parts of the mesh &quot;in front of&quot; the plane. Only drawing the inside faces means you'll only see the inside of the mesh where this cut has made a hole, giving you an image of the inside of the polyhedron. Finally, the stencil buffer intersects that image with your polygon. If you had a 3D library available it would be easy to write (because you don't need to do all the maths yourself) and fast (run on the GPU)." CreationDate="2017-06-02T17:10:44.303" UserId="2041" />
  <row Id="7705" PostId="5199" Score="0" Text="What is &quot;the near clipping plane part&quot;? What do you mean when you say &quot;inside faces&quot;? Does that mean triangles which partially overlap the polygon in projection? What is the &quot;stencil buffer&quot;? As to using a 3D library... no can do. Some networks are VERY selective about what code can and can't be stored on it. I don't mind doing the math of implementing an algorithm. I've had a lot of practice at that. I just need to understand the geometry of the algorithm." CreationDate="2017-06-02T17:27:06.017" UserId="6762" />
  <row Id="7706" PostId="5197" Score="0" Text="@DanHulme So it seems to me that there are two kinds of crossing points: Where an edge of the polygon crosses a triangle, and where an edge of a triangle crosses a polygon. Both kinds can be found by some kind of exhaustive search. I guess the next step would be to throw away vertices of the original polygon that were known to be outside the polyhedron. How do I use the topology of the polyhedron to order the resulting bunch of points?" CreationDate="2017-06-02T17:31:45.100" UserId="6762" />
  <row Id="7707" PostId="5185" Score="1" Text="Something that might help you out is that &quot;specular&quot; means &quot;mirror like&quot;.  So, when talking about specular and diffuse reflections, specular means mirror like reflections, what people normally call reflection.  Diffuse is just &quot;things lighting up&quot;.  They are reflecting light too, but they are reflecting from many different directions, so the result isn't anything that you can recognize an image in.  There is a scale between specular and diffuse reflection in real objects, and this is part of what the &quot;roughness&quot; parameter of PBR controls." CreationDate="2017-06-02T22:06:48.067" UserId="56" />
  <row Id="7708" PostId="5197" Score="0" Text="If I knew that off the top of my head I'd have posted an answer instead of drip-feeding suggestions one at a time!" CreationDate="2017-06-03T10:31:53.727" UserId="2041" />
  <row Id="7709" PostId="5200" Score="3" Text="What output are you looking for? You say you're going *from* some proportions of subtractive primaries, but what colour space are you trying to convert it *to*? And if the proportions have to add to 100%, how do you express the lightness of the colour?" CreationDate="2017-06-03T10:44:03.387" UserId="2041" />
  <row Id="7710" PostId="5198" Score="0" Text="My issue was that my normals were in world space. I thought otherwise, but realized when you posted this. Also, You don't necessarily need 2.0 - 1.0 in the normal retrieval section if your normals are signed." CreationDate="2017-06-03T23:15:01.397" UserId="5026" />
  <row Id="7711" PostId="5202" Score="0" Text="Really thanks for this detailed answer. Can you read Bridson paper who has a different way of boundaries from Stam's one and I can't relate them. Another thing, can we just evaluate the values of the velocity and pressure at the boundaries then put the values directly in the matrix?" CreationDate="2017-06-04T21:12:34.727" UserId="4978" />
  <row Id="7712" PostId="5188" Score="1" Text="It should be noted that this is true for all versions of GLSL, all the way back to 1.10 and even the ES versions." CreationDate="2017-06-04T23:24:27.413" UserId="2654" />
  <row Id="7713" PostId="5202" Score="0" Text="Why can't I use the same technique in the PCG Method? I mean the set_bnd" CreationDate="2017-06-05T01:30:43.107" UserId="4978" />
  <row Id="7714" PostId="5202" Score="0" Text="I edited my answer w.r.t. the equivalence between stam and bridson" CreationDate="2017-06-05T01:48:09.417" UserId="2695" />
  <row Id="7715" PostId="5202" Score="0" Text="... If you have a more detailed question, please add page numbers / equation numbers, so I can find the right spots." CreationDate="2017-06-05T01:49:28.723" UserId="2695" />
  <row Id="7716" PostId="5202" Score="0" Text="Did you try the set bnd method for pcg? I think it is unclear to which variables it needs to be applied to. You can give it a try and report about your results." CreationDate="2017-06-05T01:52:59.430" UserId="2695" />
  <row Id="7717" PostId="5202" Score="0" Text="Firstly, thank you for your detailed answer. Stam applies a no slip boundary condition for the velocity and Bridson applies slip boundary condition. I don't have a problem with the velocity right now. I am sorry I didn't put the code because i thought it's in the paper. The problem is at the pressure boundary condition which is indeed a neuman (the normal derivative of p is just zero) and it's applied in the projection step but stam applies it in a different way from Bridson. If you see the code of stam (I will modify the question to add it),  TBC in the next comment" CreationDate="2017-06-05T01:58:31.000" UserId="4978" />
  <row Id="7718" PostId="5202" Score="0" Text="If you see Stam code, you will see that he applied the boundaries as mentioned here in GPU GEMS Equation 17 and 18.    http://developer.download.nvidia.com/books/HTML/gpugems/gpugems_ch38.html.    But Bridson derives a formula to calculate the pressure of the boundary cells dependant on the velocity which is in the SIGGRAPH paper i mentioned above in chapter 4 equation number 4.10" CreationDate="2017-06-05T01:59:46.137" UserId="4978" />
  <row Id="7719" PostId="5202" Score="0" Text="Actually stam doesn't imply the boundaries in the matrix of the pcg like the math. He just finds the values of the pressure and the velocity that applies the boundaries and put them in the matrix. Actually I see not the code of the no slip of Stam which I will try to understand (the set_bnd and how it works according to the paper of cpu gems) but I didn't find a way for the free slip or no stick condition in which the fluid moves freely in the tangential direction. Thanks for your answers again and again. I would like to chat with you at any time." CreationDate="2017-06-05T02:05:46.487" UserId="4978" />
  <row Id="7720" PostId="5202" Score="0" Text="Sorry, Bridson changes the equations so that they fit the boundary conditions for the pressure and the velocity. Now i understand this. But how can we implement the free slip boundaries?" CreationDate="2017-06-05T02:19:41.760" UserId="4978" />
  <row Id="7721" PostId="5202" Score="0" Text="The gpu gems code is implementing the boundary conditions the same way as stam but again &quot;only&quot; for Jacobi iterations,  not for a cg solver." CreationDate="2017-06-05T10:25:14.187" UserId="2695" />
  <row Id="7722" PostId="5202" Score="0" Text="I edited my answer" CreationDate="2017-06-05T10:37:03.210" UserId="2695" />
  <row Id="7723" PostId="5108" Score="0" Text="If the points are noisy and some are not in a plane, could you clarify how much deviation from a plane due to noise is permitted, before a point is no longer regarded as being in that plane? For an approximate solution you would need to explain what your priorities are." CreationDate="2017-06-05T12:08:48.423" UserId="231" />
  <row Id="7724" PostId="5202" Score="0" Text="Now I understand how stam realizes the velocity boundary condition on an allocation grid. Can we use his implementation in a staggered grid? I still don't understand the relation between forcing the normal derivative to be zero in the way stam is doing and the equation that Bridson derives of the pressure at boundaries. Bridson says that when we subtract the pressure gradient with the velocity of the slip boundary conditions it will just enforce the pressure boundary but he didn't explain why. One time I asked someone and he told me that it's prescribed in the momentum and I didn't understand" CreationDate="2017-06-05T14:00:47.980" UserId="4978" />
  <row Id="7725" PostId="5202" Score="0" Text="Also there is no code implementing the slip boundaries of velocity like the code of stam is implementing the no slip. I added this in an answer by mistake lol" CreationDate="2017-06-05T14:01:50.823" UserId="4978" />
  <row Id="7727" PostId="5202" Score="0" Text="Another thing please, Stam says that the normal derivative of pressure should be zero in stick condition but Bridson says a difference thing in equ in the secondary edition book you told me about. Is the normal derivative of pressure different in the two cases? I would like to add you on Facebook or something" CreationDate="2017-06-05T15:05:28.133" UserId="4978" />
  <row Id="7728" PostId="5200" Score="0" Text="It can not work. Because that would mean tye primaries are not pure. Although mathematically you could have some fudge factor but no ink can actually work like this." CreationDate="2017-06-05T15:14:11.893" UserId="38" />
  <row Id="7729" PostId="5200" Score="0" Text="What do you mean it can't work? If you take magenta paint, cyan paint and yellow paint, you can mix the colour wheel. This is exactly what I'm talking about, you take some proportion of primary colours and mix them together.&#xA;&#xA;one part yellow to 1 part magenta and you get red and so on" CreationDate="2017-06-05T15:40:37.397" UserId="6770" />
  <row Id="7730" PostId="5207" Score="0" Text="I have edited my question, thanks for your input." CreationDate="2017-06-05T17:03:40.647" UserId="6770" />
  <row Id="7731" PostId="5200" Score="0" Text="It wont work because to make red you need 100% cyan + 100% yellow. Otherwise you get a red otherwise your formula no longer adds up on any other concentrations. As that would mean you would need a darker mixture for red than from black. So best not think percentages but parts of color mixture." CreationDate="2017-06-05T17:18:09.107" UserId="38" />
  <row Id="7732" PostId="5200" Score="0" Text="That would be true if it was represented in cmyk, but I'm not in the colour model cmyk, I need to take some ratio of primary paints and return some rgb colour code. &#xA;&#xA;The cmyk colour model as an intermediary seems unwise given that it is 0-100% for each component c m y and k and goes from white to pure , and i have no white.&#xA;&#xA;What I want is a colour wheel that covers hues and chroma. If you want red you mix an equal proportion of magenta and yellow to get secondary colour red. if you mix blue and red you're mixing all 3 cmy in some proportion, that's the definition of a primary." CreationDate="2017-06-05T17:22:46.870" UserId="6770" />
  <row Id="7733" PostId="5195" Score="0" Text="okay, that's a bit more clear but it still doesn't explain what do I do with my &quot;vec3 reflectionColour&quot; that I get from re-launching a ray in raytracing/marching or using SSR/cubemap. It's like the equations only taking into account a pointlight/spotlight but no idea for area lights or reflections. It also seems that the &quot;specular&quot; is only related to the &quot;white dot&quot; instead of reflection because of that which doesn't make sense =/" CreationDate="2017-06-05T23:57:17.883" UserId="5506" />
  <row Id="7734" PostId="5195" Score="0" Text="@newin In a PBR model, specular and reflection are different names for the same thing. It's a specular highlight when the ray hits a light, a reflection when it hits another object (or an environment). As for your reflectionColour, it depends how the ray is generated. Are you sampling direct lighting? Area lights? Integrating a BRDF against and environment map? Try reading [Progressive Path Tracing with Explicit Light Sampling](https://computergraphics.stackexchange.com/questions/5152), there's a long answer which might clear a few things up about the process." CreationDate="2017-06-06T01:04:41.147" UserId="5717" />
  <row Id="7735" PostId="5202" Score="0" Text="I still don't understand everything. Why can't I use set_bnd in staggered grids? What's the tangential component and how can I code some function like set_bnd to enforce the slip boundary? (yes or no) Does the velocity boundary condition whether it's slip or no slip affects the normal derivative of pressure? I mean that if slip, the normal derivative will be like Bridson stated and in no slip, it will be just zero?" CreationDate="2017-06-06T02:48:18.047" UserId="4978" />
  <row Id="7736" PostId="5202" Score="0" Text="Where can I read more about this? I don't find any paper say something clear about what is a boundary condition!!!" CreationDate="2017-06-06T02:51:40.587" UserId="4978" />
  <row Id="7737" PostId="5202" Score="0" Text="Why stam uses the normal derivative equals zero and Bridson uses the normal derivative of pressure to be different in page 83 in his book" CreationDate="2017-06-06T02:54:52.727" UserId="4978" />
  <row Id="7738" PostId="5202" Score="0" Text="I really want to contact you in private even Skype chat or anything you want please. I want to hit a wall with my confused head." CreationDate="2017-06-06T02:57:49.383" UserId="4978" />
  <row Id="7739" PostId="5185" Score="0" Text="You can try to read the original Cook-Torrance paper: http://inst.cs.berkeley.edu/~cs294-13/fa09/lectures/cookpaper.pdf - be sure to check the references listed in the end too, when possible (most are available online). It will take some time, but it is a sure way to understand &quot;in depth&quot; what is happening." CreationDate="2017-06-06T06:00:40.910" UserId="110" />
  <row Id="7741" PostId="5195" Score="0" Text="So if I understand correctly that &quot;vec3 reflectionColour&quot; should be treated as a light and if I want to make mesh emitting light I should just put a value &gt; 1 in the albedo and that's it ?&#xA;(I'm trying to integrate PBR in to different scenario the first is a raymarcher where I want meshes emitting light, the other is a game engine that use SSR and probably will use cubemaps)" CreationDate="2017-06-06T12:16:28.103" UserId="5506" />
  <row Id="7742" PostId="5210" Score="1" Text="Since this is a VBA-specific question, and there aren't a lot of VBA-using graphics researchers, I doubt you'll get good answers here; you might do better on [so]." CreationDate="2017-06-06T13:47:03.337" UserId="2041" />
  <row Id="7743" PostId="5210" Score="0" Text="Thanks! I was able to find what I needed with some digging anyway, but I'll be sure to post all future followup questions there." CreationDate="2017-06-06T13:51:20.090" UserId="6785" />
  <row Id="7744" PostId="5195" Score="0" Text="@newin emitted light is just added on top of everything else. It's unrelated to BRDFs. If your mesh does nothing but emit light, you don't even need to trace any further rays from it. A large albedo will amplify light (physically wrong), not create it." CreationDate="2017-06-06T14:57:19.783" UserId="5717" />
  <row Id="7745" PostId="5212" Score="0" Text="You can slerp a quaternion by repeatedly nlerping to 0.5 and binary searching. So if you can expand out the math of conversion to quaternion, the nlerp and the conversion back, then it should be possible" CreationDate="2017-06-07T13:41:56.787" UserId="137" />
  <row Id="7746" PostId="5210" Score="0" Text="If you feel the answer you discovered for yourself might be relevant to others working with computer graphics in future, you could add a self answer to this question." CreationDate="2017-06-07T14:52:21.797" UserId="231" />
  <row Id="7747" PostId="5211" Score="1" Text="If your Constructive Solid Geometry method allows intersection as well as union, then you can use this to find the internal area, and subtract that from the total to give the external area. Would that make a useful answer?" CreationDate="2017-06-07T15:01:15.320" UserId="231" />
  <row Id="7748" PostId="5211" Score="1" Text="This would depend on all the initial meshes having no internal triangles. If you can guarantee this then the method will work. For example, a hollow ball constructed from two concentric spheres would not give correct results when intersected with another sphere. However, if the construction is using only unions, then intersections can be used to subtract the area of the internal triangles." CreationDate="2017-06-07T15:08:42.777" UserId="231" />
  <row Id="7749" PostId="5212" Score="0" Text="More generally, if you can lerp from the identity to any rotation $r$, i.e. find the rotation $r^t$ with the same axis and $t$ times the angle, then you can lerp from $a$ to $b$ via $(ba^{-1})^ta$." CreationDate="2017-06-07T15:31:39.147" UserId="106" />
  <row Id="7750" PostId="5210" Score="0" Text="Will do. Once I get off this train! :-)" CreationDate="2017-06-07T17:17:12.593" UserId="6785" />
  <row Id="7751" PostId="5211" Score="0" Text="I think that this could be the answer, but for example, if one of the meshes is inside the other the one that's inside should be completely ignored. Would that be the case? I tried to use intersection as stated and found the same results." CreationDate="2017-06-07T19:50:23.823" UserId="6788" />
  <row Id="7752" PostId="5213" Score="4" Text="With or without the cooperation of the app getting injected into? :) If you have control of both apps, you could use shared memory to efficiently communicate mesh data between the processes. Otherwise, you're looking at something like shimming the graphics API." CreationDate="2017-06-07T23:01:54.360" UserId="48" />
  <row Id="7753" PostId="5213" Score="0" Text="Perhaps an ironic question as, no doubt, some effort has to be put into the GPU drivers to prevent parallel applications interfering on the shared resource." CreationDate="2017-06-08T08:24:30.840" UserId="209" />
  <row Id="7754" PostId="5211" Score="1" Text="If your intersection method returns the surface that encloses the volume enclosed by both the first mesh and the second mesh, then the intersection of the two meshes you describe will simply be the entire inner mesh, which is precisely the triangles that you wish to exclude. So it sounds like either your intersection method works differently, or there is a separate problem causing overcounting. Could you include your code so we can analyse it?" CreationDate="2017-06-08T09:15:06.203" UserId="231" />
  <row Id="7755" PostId="5213" Score="1" Text="Yes its called inter process communication. Not in any way specific to graphics programming. Do you mean directly on the GPU?" CreationDate="2017-06-08T11:07:00.347" UserId="38" />
  <row Id="7756" PostId="5211" Score="0" Text="Sure! Currently I'm using [Net3dBool](https://github.com/Arakis/Net3dBool) to perform the CSG operations. The code that performs the intersection is on the file [BooleanModeller.cs](https://github.com/Arakis/Net3dBool/blob/master/src/Net3dBool/BooleanModeller.cs), on the **getIntersection** method, wich calls the **composeSolid** method. Thanks!" CreationDate="2017-06-08T12:13:17.157" UserId="6788" />
  <row Id="7757" PostId="1993" Score="0" Text="clinfo did not work, but sudo clinfo did for me." CreationDate="2017-06-08T14:20:30.920" UserId="6798" />
  <row Id="7758" PostId="5217" Score="1" Text="I think &quot;window of 20x20 pixels&quot; applies to &quot;energy&quot; rather than to &quot;image Laplacian&quot;, i.e. you compute the Laplacian as usual, the compute its total energy (i.e. sum of squared values) on a window of 20x20 pixels. But you should add a link or reference to the research paper to be sure." CreationDate="2017-06-08T15:36:52.227" UserId="106" />
  <row Id="7759" PostId="5213" Score="1" Text="If you know the graphics API the other application is using, you can use the concept of dll hooking to inject code. For example, you could hook Present() and use the context to draw your triangles before calling the real Present(). For example: https://www.microsoft.com/en-us/research/project/detours/ or http://easyhook.github.io/ Hooking is how applications like OBS/Fraps do frame overlays and captures." CreationDate="2017-06-08T17:13:23.453" UserId="310" />
  <row Id="7760" PostId="5214" Score="1" Text="Are you looking to estimate subsurface scattering in your path tracer, or evaluate it exactly using full-volumetric scattering?" CreationDate="2017-06-08T17:17:30.390" UserId="310" />
  <row Id="7761" PostId="5215" Score="1" Text="I thought [you said](https://stackoverflow.com/q/44431751/734069), &quot;I will avoid asking more than one question once time later.&quot; That rule doesn't only apply to SO; it's all of our sites. Also, all of the other questions asked about that question still apply." CreationDate="2017-06-08T17:26:16.717" UserId="2654" />
  <row Id="7762" PostId="5214" Score="0" Text="I read that simulating full-volumetric scattering is really slow. If it is not that slow I would like to implement it. Although diffusion approximations work faster, they assume lots of things or they need ugly preprocessing steps. Have you done it before? Any suggestion would be great!" CreationDate="2017-06-08T19:22:55.860" UserId="6698" />
  <row Id="7763" PostId="5214" Score="2" Text="It can be slow, depending on the implementation. I have implemented brute-force, isotropic scattering media in my hobby path tracer. Example image: http://imgur.com/a/oy9F8. (The FPS / total frame times are in the upper left corner). Using a backscattering phase function like Henyey-Greenstein might make it converge faster for more opaque materials, but I'm not sure since I haven't tried it. If nothing else, implementing full volumetric scattering is a great way to learn what the approximations are trying to approximate." CreationDate="2017-06-08T19:57:59.250" UserId="310" />
  <row Id="7764" PostId="5214" Score="0" Text="I'll try to do a write up later tonight when I get home. In the mean time, here's the link to my path tracer. https://github.com/RichieSams/lantern It's not production ready at all. Rather, it's just for helping me learn the math behind everything. That said, it's open source under Apache 2.0, so feel free to copy and learn from it yourself." CreationDate="2017-06-08T19:59:38.570" UserId="310" />
  <row Id="7765" PostId="5214" Score="1" Text="Lastly, here are two questions that I asked here about this topic: https://computergraphics.stackexchange.com/questions/2482/choosing-reflection-or-refraction-in-path-tracing https://computergraphics.stackexchange.com/questions/2563/full-monte-carlo-volumetric-scattering" CreationDate="2017-06-08T20:01:22.773" UserId="310" />
  <row Id="7766" PostId="5215" Score="0" Text="Actually, this posting is before that comment I replied.@NicolBolas" CreationDate="2017-06-09T02:56:03.560" UserId="6660" />
  <row Id="7767" PostId="5214" Score="0" Text="Thank you! I implemented perfect refractive and reflective surfaces already. By the way, the answers to both questions are enlightening for me but I would prefer to see something you wrote up." CreationDate="2017-06-09T08:08:28.930" UserId="6698" />
  <row Id="7768" PostId="5212" Score="0" Text="Thank you for your valuable comments, Ratchet Freak and Rahul! I will take your advice and answer this question." CreationDate="2017-06-09T09:16:09.300" UserId="6791" />
  <row Id="7770" PostId="5217" Score="0" Text="That would make sense, maybe this is it. I can't tell. But that's a good idea and I will try to implement it to see.&#xA;&#xA;You're right for the link, I will add it in the post." CreationDate="2017-06-09T09:53:16.497" UserId="6662" />
  <row Id="7771" PostId="5217" Score="0" Text="+1 now that there is a link to the article." CreationDate="2017-06-09T16:55:34.060" UserId="231" />
  <row Id="7772" PostId="5192" Score="0" Text="Yeah that's where I got the image.  It doesn't give the information I'm looking for.  For instance, if you DFT the first data set, the frequency spectrum should look much like the DFT of the second, but how would you even DFT the first one?  How are these two things &quot;duals&quot; of each other in frequency space?  And can you take concepts from each and apply them to the other?" CreationDate="2017-06-09T20:34:20.627" UserId="56" />
  <row Id="7773" PostId="5222" Score="0" Text="What if there's rotation around a different axis than the normal?" CreationDate="2017-06-10T04:54:02.550" UserId="3003" />
  <row Id="7774" PostId="5222" Score="0" Text="The normal changes then." CreationDate="2017-06-10T16:11:26.143" UserId="4678" />
  <row Id="7775" PostId="5224" Score="0" Text="Very interesting. Now one question that I have is how does the transformation matrices look like (both for translation and rotation)? A matrix that if applied to all the origin points will give me the new object." CreationDate="2017-06-10T16:18:04.190" UserId="4678" />
  <row Id="7776" PostId="5224" Score="1" Text="That could be the beginning of a new question... We also have many questions tagged [transformations](https://computergraphics.stackexchange.com/questions/tagged/transformations)" CreationDate="2017-06-11T00:17:59.123" UserId="231" />
  <row Id="7777" PostId="5220" Score="0" Text="Subsurface scattering won't necessarily require ray marching if it is in a homogeneous medium. Are you specifically looking to model inhomogeneous media (fire and smoke) rather than homogeneous media (milk or glass)?" CreationDate="2017-06-11T00:25:44.353" UserId="231" />
  <row Id="7778" PostId="5226" Score="0" Text="You go buy a colorimeter from the store (or lend one... from the library), make profile and do a profile to profile conversion. Note this is NOT a linear transform. Also your srgb mode needs to be calibrated since color is only accurate if you have calibrated the monitor in place taking into account surrounding color conditions so factory cslibration is allways off." CreationDate="2017-06-11T06:49:50.697" UserId="38" />
  <row Id="7779" PostId="5226" Score="0" Text="@joojaa That's not needed for what the question asks, which is merely converting values from one color space to another using a formula. Whether the color appears correctly to the human eye on a particular monitor is not the question." CreationDate="2017-06-11T08:15:51.373" UserId="6814" />
  <row Id="7780" PostId="5226" Score="0" Text="No, it wont work." CreationDate="2017-06-11T09:01:11.767" UserId="38" />
  <row Id="7781" PostId="5226" Score="0" Text="Why not? It may not be a linear transform, but whatever the formula, we should be able to apply it to an input value to get an output." CreationDate="2017-06-11T09:14:48.137" UserId="6814" />
  <row Id="7782" PostId="5226" Score="0" Text="Because the profile is the formula and only way to obtain one is to measure that individual panel! But its not a big deal colorimeters meantbfor profile buidlding are cheap." CreationDate="2017-06-11T09:53:10.753" UserId="38" />
  <row Id="7783" PostId="5214" Score="0" Text="@RichieSams I read physically based rendering and got the intuition. There is only one part that confuses me. We cannot sample the light sources directly due to refraction at the boundaries, right? If so, do we have to wait until the ray we are tracing exit the object because otherwise there would be lots of black pixels on the image?" CreationDate="2017-06-11T13:40:01.567" UserId="6698" />
  <row Id="7784" PostId="5214" Score="0" Text="Sorry, I got busy this week. I'm writing up the answer now. But yes, you are correct. Light sampling is almost impossible from within media with traditional path tracing. Photon mapping works quite well in this area, though. Vertex Connection and Merging combines the best of both worlds: http://www.smallvcm.com/" CreationDate="2017-06-11T15:24:57.320" UserId="310" />
  <row Id="7786" PostId="5226" Score="0" Text="Again, I didn't ask for anything related to an individual panel, but about the color space itself in an ideal sense. I'm not interested in buying hardware. Please answer the question originally asked." CreationDate="2017-06-12T02:54:06.823" UserId="6814" />
  <row Id="7788" PostId="5229" Score="0" Text="Thank you for this amazing answer! The problem is that no matter what I try for the coefficients, my outputs look like a glass. Does this mean I am doing scattering part wrong? Here is the sequence of outputs: http://imgur.com/a/gg2u1&#xA;I only changed the scattering coefficient while keeping absorption the same" CreationDate="2017-06-12T07:04:07.920" UserId="6698" />
  <row Id="7790" PostId="5229" Score="1" Text="Are you sure that you're not accidentally setting the coefficient equal to the scattering distance? It should be scatteringCoefficient = (1 / scatteringDistance). Also absorptionCoefficient = -log(absorptionColor) / absorptionAtDistance. In the end, it doesn't really matter how you come up with the coefficient. That said, a lower coefficient will mean less absorption / scattering. A higher coefficient will mean less. For artists, it's a bit easier to think of it in terms of distances, so we back-calculate the coefficients from distances." CreationDate="2017-06-12T13:25:28.770" UserId="310" />
  <row Id="7791" PostId="5223" Score="0" Text="If I remember correctly, MLT usually starts with a frame from general path tracing (one way, or bi-directional). They use this image to find places with high luminance, and explore more in that area." CreationDate="2017-06-12T13:32:29.433" UserId="310" />
  <row Id="7792" PostId="5224" Score="1" Text="@trichoplax ok. My new question is posted here: https://computergraphics.stackexchange.com/questions/5230" CreationDate="2017-06-12T19:00:53.457" UserId="4678" />
  <row Id="7793" PostId="5223" Score="0" Text="Oh? I thought it starts randomly, but the probability of moving to a new location favors locations that are brighter. So I do multiple rounds from multiple initial locations to avoid the sampling getting stuck in bright areas.&#xA;&#xA;Also, I'm a student in this so I don't know a lot, but I just learned while researching about this that path tracing is not the same as ray tracing. Does that have anything to do with my problem above? Do you know if I can even do MLT using ray tracing?" CreationDate="2017-06-12T20:18:17.980" UserId="6811" />
  <row Id="7796" PostId="4994" Score="0" Text="For GGX how would I calculate/sample `wi`? I understand how to sample the spherical coordinates angle θ but for the actual direction vector how is that done?" CreationDate="2017-06-13T14:59:53.260" UserId="5256" />
  <row Id="7798" PostId="5237" Score="1" Text="the [video](https://www.youtube.com/watch?v=jdMSEJwbCCY) linked to the wiki page looks really interesting" CreationDate="2017-06-13T18:35:14.433" UserId="6829" />
  <row Id="7799" PostId="5234" Score="0" Text="Thank you so much for sharing your knowledge. Please read my clarification edit. Thanks again." CreationDate="2017-06-13T19:43:24.940" UserId="6824" />
  <row Id="7800" PostId="5232" Score="0" Text="You can actually try it and see if it results in a expected result or not. Note that matrix multiplication is only valid for the column vector comig after the matrix and row coming before due to the way matrix multiplication is defined. But you can allways transpose the vector resulting in the same thing (if the matrix chain is computed first)" CreationDate="2017-06-13T19:47:52.683" UserId="38" />
  <row Id="7801" PostId="5240" Score="0" Text="is it possible to generate a height map using 3d software such as maya or blender rather than trying to create one manually using python? and does the generated map store the 3d coordinates in each channel like you have described?" CreationDate="2017-06-14T01:59:17.187" UserId="6834" />
  <row Id="7802" PostId="5240" Score="1" Text="I haven't done it that way before, but I believe you can. (I'm usually generating them in code.) There is a [Blender Stack Exchange](https://blender.stackexchange.com) where you could ask about that. The [Video Production Stack Exchange](https://video.stackexchange.com) may have info on Maya or other apps. In my experience height maps are usually just grayscale images where brightness represents height rather than putting all 3 coordinates into the r, g, and b channels, so I'd expect most apps to output that way." CreationDate="2017-06-14T02:33:49.793" UserId="3003" />
  <row Id="7803" PostId="5240" Score="0" Text="@MarkMarkrowaveCharlton Yes all you have to do is choose a format that allows writing the zbuffer and check ot under camera settings. If really need this in a rgb image you can just write the distance to the pixel value using a node (sampler info in maya)" CreationDate="2017-06-14T05:36:24.007" UserId="38" />
  <row Id="7804" PostId="5237" Score="2" Text="Yes lenses that do this are called pericentric." CreationDate="2017-06-14T05:42:01.220" UserId="38" />
  <row Id="7805" PostId="5232" Score="0" Text="Also note that for a pure rotation matrix the inverse is a transpose." CreationDate="2017-06-14T05:56:31.980" UserId="38" />
  <row Id="7806" PostId="5232" Score="0" Text="Rather than the 3x3 matrix, I think it'd be more instructive to use a 4x4 so as to include a translation part. That way you should get a clearer understanding of ordering etc." CreationDate="2017-06-14T10:56:24.973" UserId="209" />
  <row Id="7807" PostId="5238" Score="0" Text="PBRT implementation seems like photon beam diffusion. Are photon beam diffusion and the what Solid Angle does the same? I ask because I thought PBD also take point samples on the geometry as a preprocessing step but Solid Angle slides don't do it." CreationDate="2017-06-14T12:17:21.757" UserId="6698" />
  <row Id="7809" PostId="4994" Score="0" Text="GGX is assumed to be isotropic so $\phi = \xi_{2}$. With $\phi$ and $\theta$, you can transform to cartesian coordinates to get $w_i$ http://tutorial.math.lamar.edu/Classes/CalcIII/SphericalCoords.aspx" CreationDate="2017-06-14T15:05:38.347" UserId="310" />
  <row Id="7810" PostId="5243" Score="0" Text="Any reason you're limiting yourself to a single texture? I've seen it done with multiple textures in shipping apps. (Not to say it can't be done with a single texture, but my guess is that it's easier with more than 1 texture.)" CreationDate="2017-06-15T01:44:10.347" UserId="3003" />
  <row Id="7811" PostId="5243" Score="0" Text="It's easier to get a single texture. And, I'm curious :)" CreationDate="2017-06-15T02:34:37.913" UserId="6838" />
  <row Id="7812" PostId="5243" Score="0" Text="@DanielKareh: &quot;*But how does one go about taking a single texture and getting a decent normal map out of it?*&quot; What makes you think that there's a way to convert an image into a normal map? What is the image of, exactly?" CreationDate="2017-06-15T02:42:07.420" UserId="2654" />
  <row Id="7813" PostId="5243" Score="0" Text="try reading this: https://fenix.tecnico.ulisboa.pt/downloadFile/845043405449073/Tangent%20Space%20Calculation.pdf" CreationDate="2017-06-15T05:47:50.190" UserId="6255" />
  <row Id="7814" PostId="5229" Score="0" Text="No, parameter setting part is the same. I couldn't solve the problem but if I solve it, I will comment here." CreationDate="2017-06-15T07:19:48.460" UserId="6698" />
  <row Id="7818" PostId="5243" Score="0" Text="@NicolBolas: Well, software like CrazyBump seems capable of generating normal maps of many, many surfaces from a single texture." CreationDate="2017-06-15T13:05:47.993" UserId="6838" />
  <row Id="7819" PostId="5243" Score="0" Text="@trichoplax: Sorry if my question was confusing. I'm looking for a way to generate a decent normal map from a single texture, if that's possible." CreationDate="2017-06-15T13:06:35.593" UserId="6838" />
  <row Id="7820" PostId="5243" Score="0" Text="@Charlie: I already know how to use a normal map and apply it to a surface. I'm trying to learn how to get the normal map in the first place." CreationDate="2017-06-15T13:08:59.300" UserId="6838" />
  <row Id="7822" PostId="5243" Score="0" Text="There are several  different ways to do this, easiest being converting a bump map into a normal map." CreationDate="2017-06-15T13:46:57.660" UserId="38" />
  <row Id="7823" PostId="5243" Score="0" Text="@joojaa: Alright. Any ideas on how to get a bump map from an image?" CreationDate="2017-06-15T14:16:10.877" UserId="6838" />
  <row Id="7824" PostId="5243" Score="0" Text="If the image is a bump map then the normal map is just the derivate of the bump." CreationDate="2017-06-15T14:39:06.967" UserId="38" />
  <row Id="7825" PostId="5243" Score="0" Text="@DanielKareh aha, I just use mudbox myself. You generate a normal map from a bump map." CreationDate="2017-06-15T14:43:17.323" UserId="6255" />
  <row Id="7826" PostId="5243" Score="0" Text="But, how do I get the bump map from the image. Like, if I have an image texture of say, a rock wall, how can I get a decent bump map from it, which I can then convert into a normal map." CreationDate="2017-06-15T15:00:28.350" UserId="6838" />
  <row Id="7828" PostId="5243" Score="0" Text="@trichoplax: Yes, that is correct. I'll edit the question." CreationDate="2017-06-15T17:29:21.393" UserId="6838" />
  <row Id="7829" PostId="5243" Score="0" Text="Hopefully someone can give an overview explanation as an answer here, but as a starting point there is a free open source tool called [AwesomeBump](https://github.com/kmkolasinski/AwesomeBump) if you (or anyone looking to write an answer) wants to see example code." CreationDate="2017-06-15T17:29:58.677" UserId="231" />
  <row Id="7830" PostId="5243" Score="0" Text="Thank you! Do you happen to know which file the generator is in, or will I have to look around?" CreationDate="2017-06-15T18:00:20.633" UserId="6838" />
  <row Id="7831" PostId="5230" Score="1" Text="For near minimal you store a vector for the translation and a quaternion for the rotation, so 7 elems to store.  So the matrix is simply the quaternion to rotation matrix + translation." CreationDate="2017-06-15T20:05:24.093" UserId="2831" />
  <row Id="7832" PostId="5230" Score="0" Text="Yes, that I know. But can you post how that rotation matrix really looks like? The rows and columns." CreationDate="2017-06-15T21:25:25.903" UserId="4678" />
  <row Id="7833" PostId="5246" Score="3" Text="In general it's about 10 times harder to get your code correct on the GPU than on the CPU, so regardless of where it's faster, I'd recommend you start by getting everything working on the CPU." CreationDate="2017-06-16T09:07:05.487" UserId="2041" />
  <row Id="7834" PostId="5247" Score="0" Text="Thanks. What was `Q*`?" CreationDate="2017-06-16T14:34:55.197" UserId="4678" />
  <row Id="7835" PostId="5247" Score="1" Text="It's the conjugate which is the same as inverse when unit magnitude." CreationDate="2017-06-16T16:12:08.617" UserId="2831" />
  <row Id="7836" PostId="398" Score="0" Text="&quot;and anyway there's unusually high-quality pseudocode at that link&quot;, I disagree. That pseudo code is likely not a proper implementation of Wu's algorithm even though it seems to be what was used in countless places around the web. Wu's original algorithm drew from both ends inward towards the center and was actually *faster* than Bresenham's because it performs about half as many operations even though it writes to more pixels. I am talking about Wu's actual algorithm *not* the one posted in the linked wikipedia article." CreationDate="2017-06-16T16:37:17.097" UserId="20" />
  <row Id="7837" PostId="5246" Score="0" Text="Thanks, Dan, you are quite right, I do not have enough experience writing for GPU, I just want to understand general performance ideas." CreationDate="2017-06-16T16:37:37.597" UserId="6841" />
  <row Id="7838" PostId="5246" Score="0" Text="It really depends on what your goals are.  Is this something you want to do in real time? During runtime of your game? During &quot;real time&quot; editing in the editor?  How complex are the meshes you intend on splitting?" CreationDate="2017-06-16T21:57:33.577" UserId="56" />
  <row Id="7841" PostId="5220" Score="0" Text="Are you trying to do real time or offline rendering? SSS is a surface based algorithm that uses what's called a BSSRDF. Usually the implementation only accepts the data of a few layers past the surface. Usually just a few extra texture maps. Volume rendering can do the job of SSS but it models the entire inside of a volume. Often it does this with a 3d texture (uniform grid). But sometimes it's done with other datasets like tetrahedral grids. TLDR: Volume rendering isn't used for SSS although the concepts of emission, absorption, in/out scattering are the same." CreationDate="2017-06-16T23:47:03.353" UserId="113" />
  <row Id="7842" PostId="5243" Score="0" Text="I'm afraid I'm not familiar with the software, just stumbled upon it while searching." CreationDate="2017-06-17T09:06:02.857" UserId="231" />
  <row Id="7845" PostId="5249" Score="0" Text="I'll make sure to check out the mentioned neural network approach. I guess I'll have to go back to messing around with custom image filters, considering you mentioned that those applications use artistic approaches rather than super accurate models of light transfer." CreationDate="2017-06-17T16:09:59.873" UserId="6838" />
  <row Id="7846" PostId="5243" Score="0" Text="That's okay. I'll make sure to do some searching through the files." CreationDate="2017-06-17T16:11:04.613" UserId="6838" />
  <row Id="7847" PostId="3947" Score="1" Text="Cannot imagine a normal with negative Z in screen (view) space. It would require the culling turned off which itself is a bad idea from the performance perspective." CreationDate="2017-06-17T16:57:08.867" UserId="6846" />
  <row Id="7848" PostId="5247" Score="1" Text="There was a seemly deleted comment along the lines of &quot;not useful to have the matrix&quot;.  This is true, you work with the rotation translation pair..and when transforming points you don't actually create a matrix (don't need to at least).  The matrix is to show the sub-expressions that need to be computed.  And this is assuming that the average number of points is (somewhere) greater than one.  Otherwise you're better off performing the rotation directly from the quaternion expression." CreationDate="2017-06-17T18:15:48.150" UserId="2831" />
  <row Id="7849" PostId="5247" Score="1" Text="It's more efficient to convert to a matrix instead of using quaternion directly if you transform more than one vector with the same quaternion. Also for more compact representation you don't need to store w since you are dealing with unit quaternions: w=sqrt(1-x^2-y^2-z^2)." CreationDate="2017-06-18T04:25:12.007" UserId="1952" />
  <row Id="7850" PostId="5247" Score="1" Text="I was attempting to say the matrix doesn't need to be stored..it's on the stack.  Dropping the scalar doesn't work very well: http://marc-b-reynolds.github.io/quaternions/2017/05/02/QuatQuantPart1.html" CreationDate="2017-06-18T07:13:17.563" UserId="2831" />
  <row Id="7851" PostId="5253" Score="1" Text="1. try update your system. and run `ldconfig`&#xA;2. how did you compile your code? what arguments are you using?" CreationDate="2017-06-18T17:28:50.323" UserId="2448" />
  <row Id="7852" PostId="398" Score="0" Text="@Octopus [Expresses vague skepticism, especially on the faster bit, but lacks context to refute or confirm—if this is so, sources, corrections, and edits are of course welcome.]" CreationDate="2017-06-18T23:31:25.497" UserId="523" />
  <row Id="7855" PostId="5256" Score="0" Text="I would think part of it has to do with the layout of 2d elements on the screen.  If you allow the user to freely resize the screen then the elements could overlap." CreationDate="2017-06-19T13:24:42.943" UserId="3332" />
  <row Id="7856" PostId="5256" Score="0" Text="Someone who can't build a GUI that's scaling resistant shouldn't be allowed to build GUIs at all. Also, scaling is possible, but only in steps. So a little window where things might overlap is still possible, so why not sizes in between?" CreationDate="2017-06-19T13:28:07.610" UserId="6092" />
  <row Id="7857" PostId="5256" Score="1" Text="Also, the 2d elements are often textures that are designed to be the right size for a certain screen resolution.  If the user arbitrarily blows up the screen size, those textures will stretch and look bad.  Instead, there are different textures for each supported resolution." CreationDate="2017-06-19T13:30:45.260" UserId="3332" />
  <row Id="7858" PostId="5256" Score="2" Text="This question is probably more appropriate for gamedev.stackexchange.com.  See https://gamedev.stackexchange.com/a/125316/26734" CreationDate="2017-06-19T13:34:15.813" UserId="3332" />
  <row Id="7859" PostId="5256" Score="0" Text="I posted it here because I think this is mostly a graphical issue that just happens to appear mostly in games, but you might be right. Also, the answer on that question on gamdev isn't really satisfying." CreationDate="2017-06-19T13:56:13.407" UserId="6092" />
  <row Id="7860" PostId="5256" Score="0" Text="See also: https://gamedev.stackexchange.com/a/56952" CreationDate="2017-06-19T13:58:53.293" UserId="6092" />
  <row Id="7861" PostId="5256" Score="2" Text="Not quite enough info for an answer, but we query the driver to ask for what resolutions will work in full screen mode.  The specific list of resolutions you give in dark souls 2 will vary from machine to machine. There are hardware and software (driver) limitations to what is available." CreationDate="2017-06-19T14:14:28.783" UserId="56" />
  <row Id="7862" PostId="5256" Score="0" Text="But why would they matter in windowed mode? And if the game takes all the resolution the system throws at them, does it mean the game in fact *is* resolution independent and the restriction is purely artificial?" CreationDate="2017-06-19T14:29:40.100" UserId="6092" />
  <row Id="7863" PostId="5253" Score="0" Text="I used following commands: &quot;g++ pgm1.cpp -lglut&quot;" CreationDate="2017-06-19T14:52:37.567" UserId="5099" />
  <row Id="7864" PostId="5255" Score="0" Text="I did try adding -lgl to the compilation command on seeing this suggestion on one of the internet sites. But it does not help" CreationDate="2017-06-19T14:53:41.643" UserId="5099" />
  <row Id="7865" PostId="5253" Score="0" Text="I tried running ldconfig command. I am getting a message &quot;Cant create temporary cache file/etc/ld.so.cache~: Permission denied&quot;" CreationDate="2017-06-19T15:13:12.047" UserId="5099" />
  <row Id="7867" PostId="5253" Score="0" Text="try `sudo ldconfig`. When you see Permission denied. adding sudo to the command will fix that 99% of the time(not the best habit though)" CreationDate="2017-06-19T18:23:33.920" UserId="2448" />
  <row Id="7868" PostId="5256" Score="2" Text="If you find the linked answer unsatisfying, you might be more likely to get the answer you seek if you edit the question to explain why you are not satisfied with the reasons given. That way you can avoid getting the same answer here." CreationDate="2017-06-19T18:25:28.247" UserId="231" />
  <row Id="7869" PostId="5252" Score="0" Text="Great answer, and thanks for a thorough explanation. I believe this is exactly what I'm looking for, though I'll need to reread your answer several more times to get it through in my head haha. But please fix your original M matrix to include z. Thank you again, good sir!" CreationDate="2017-06-19T18:28:43.440" UserId="6824" />
  <row Id="7870" PostId="5258" Score="0" Text="One way to generate a triangulated sphere is to use an isosphere: https://schneide.wordpress.com/2016/07/15/generating-an-icosphere-in-c/&#xA;&#xA;What do you mean by pre-defined adjacency? What's the purpose?" CreationDate="2017-06-19T19:39:27.857" UserId="310" />
  <row Id="7872" PostId="5258" Score="0" Text="I need a 3D sphere consistent with a given 3D mesh in terms of the number of vertices and face-connectivity. The given mesh is genus 1, so I guess that is possible. I have looked into icosahedron division method but the issue is face-connectivity." CreationDate="2017-06-20T04:47:45.107" UserId="6854" />
  <row Id="7873" PostId="5261" Score="0" Text="Interesting point. Does this mean Dark Souls will run distorted on screens with different aspect ratios (e. g. an old 800x600)? I think the fact that they all have the same ratio might be due to what @Alan Wolfe said in the comments, that these are only those supported by my hardware on full screen mode." CreationDate="2017-06-20T07:57:02.083" UserId="6092" />
  <row Id="7874" PostId="5263" Score="2" Text="I think you need to provide some more background to your question." CreationDate="2017-06-20T09:20:53.817" UserId="209" />
  <row Id="7875" PostId="5263" Score="0" Text="Please narrow down the question with more detail on what you are looking to achieve. It might also help to describe what information you already have about the mesh, perhaps with an example." CreationDate="2017-06-20T11:42:03.770" UserId="231" />
  <row Id="7876" PostId="5261" Score="2" Text="As a specific example, in the 2d game [slither.io](http://slither.io/) there is a max distance you can see. In portrait, you can see this distance vertically, with your horizontal view restricted. In landscape, you can see this distance horizontally, with your vertical view restricted. If you resize the window to be perfectly square, then you can see the max distance both vertically and horizontally, giving a significant advantage over other players. In addition to the time and money required to adjust to different aspect ratios, it can also become a potentially undesirable meta game." CreationDate="2017-06-20T11:50:27.963" UserId="231" />
  <row Id="7877" PostId="5211" Score="0" Text="@trichoplax I think that you're right, and the library that I'm using is computing something wrong on the CSG operations. Just for testing I tried the method powered by Carve used in Blender and it gave the results that I expected, so I'll try to use another library (maybe Carve itself) to do the operations and will answer the question acordingly if it works. Thanks!" CreationDate="2017-06-20T13:03:33.967" UserId="6788" />
  <row Id="7878" PostId="5251" Score="0" Text="Can you give a chapter or page number where $W(x,\theta) $ is described in PBR?" CreationDate="2017-06-20T13:34:06.133" UserId="6608" />
  <row Id="7879" PostId="5265" Score="0" Text="So, reading this it seems to  me you do not know how the standard 3D camera projections work. Simply you transform vectors by the transformation matrix of the projection." CreationDate="2017-06-20T14:40:53.557" UserId="38" />
  <row Id="7880" PostId="5265" Score="0" Text="Thanks for the answer! But wouldn't this give me a view-dependent projection? I'm interested in knowing exactly how to define a plane on which the mesh should be projected." CreationDate="2017-06-20T14:52:38.187" UserId="6788" />
  <row Id="7881" PostId="5266" Score="0" Text="One can backace cull the data and only consider the edges that adjoin a backface culled member." CreationDate="2017-06-20T15:11:31.440" UserId="38" />
  <row Id="7882" PostId="5266" Score="0" Text="@joojaa that's an equivalent way of generating perimeter candidates." CreationDate="2017-06-20T15:13:27.907" UserId="137" />
  <row Id="7883" PostId="5265" Score="0" Text="A view is a transform just like anything else. Nothing states that it has to be the same view as your viewport. So i take it you do not know how the camera works." CreationDate="2017-06-20T15:14:21.520" UserId="38" />
  <row Id="7884" PostId="5266" Score="0" Text="Thanks! I'll try your suggestions out and accept the answer if that is the case. One question though: which method should I use to find the normal of this arbitrary plane? For example, if I have a wall, I need to use its larger face as the arbitrary plane, so how should I compute it's normal?" CreationDate="2017-06-20T15:14:21.873" UserId="6788" />
  <row Id="7885" PostId="5266" Score="1" Text="@LiordinoNeto cross product" CreationDate="2017-06-20T15:15:28.177" UserId="137" />
  <row Id="7887" PostId="5266" Score="0" Text="@ratchetfreak this? &#xA;`meshNormal = (0,0,0)&#xA;foreach vertexNormal in meshNormals&#xA;    meshNormal = meshNormal x vertexNormal`" CreationDate="2017-06-20T15:20:35.003" UserId="6788" />
  <row Id="7888" PostId="5266" Score="0" Text="Yes but you get the area with those edges too." CreationDate="2017-06-20T15:28:22.260" UserId="38" />
  <row Id="7889" PostId="5266" Score="0" Text="@joojaa sorry, but I think I didn't understand. Are you talking about the arbitrary plane's normal calculation with the cross product? What edges exactly?" CreationDate="2017-06-20T15:32:00.527" UserId="6788" />
  <row Id="7890" PostId="5252" Score="0" Text="Thanks @MatthewL for noticing that error. Just fixed it now!" CreationDate="2017-06-20T15:54:23.963" UserId="250" />
  <row Id="7891" PostId="5266" Score="0" Text="@LiordinoNeto I wasnt talking to you no handle means i am talking to ratchett. Anyway face normal is different from vertex normals. Face normal is formed out of 2 triangle edges. Note this is why i asked about you knowing how the 3D view rendering works since now we are in fact telling you how the viewport rendering works in a roudabout way as the operations needed are exactly the same." CreationDate="2017-06-20T16:01:37.917" UserId="38" />
  <row Id="7892" PostId="5266" Score="0" Text="@joojaa no problem, thanks for the explanation!" CreationDate="2017-06-20T16:34:53.243" UserId="6788" />
  <row Id="7893" PostId="5211" Score="0" Text="Thanks for the update, and a self answer if it works would be great!" CreationDate="2017-06-20T18:38:29.417" UserId="231" />
  <row Id="7894" PostId="5265" Score="1" Text="For the area, why don't you just add up areas of the triangles in the original 3D mesh? What you need the projection for? The way you describe it sounds like an overcomplicated solution to a fairly straightforward problem. Or is it because you don't know how to calculate the area of a triangle in 3D?" CreationDate="2017-06-21T02:25:00.510" UserId="1952" />
  <row Id="7896" PostId="5251" Score="0" Text="I'am using a book 'Advanced Global Illumination 2ed' (page 89 chapter 4.2)  and referring to &quot;dspace5.zcu.cz/bitstream/11025/15417/1/Willems_94.pdf&quot; (page 4, 5 the potential equation)" CreationDate="2017-06-21T07:38:58.917" UserId="6806" />
  <row Id="7897" PostId="5238" Score="1" Text="There are two things to distinguish here: The function to integrate (BSSRDF) and the method of integration. The first one are functions like the dipole, multipole, photon beam diffusion, etc. The second are methods like Jensen's hierarchical octree or Solid Angle's importance sampled ray tracing." CreationDate="2017-06-21T08:36:52.163" UserId="4546" />
  <row Id="7898" PostId="5263" Score="1" Text="Sorry to be a pain, but what defines &quot;good topology&quot; vs &quot;bad topology&quot;? Are you after the effectiveness of each polygon, or how regular the mesh is?" CreationDate="2017-06-21T09:09:53.740" UserId="209" />
  <row Id="7899" PostId="5265" Score="0" Text="@JarkkoL thanks for the suggestion. Summing up the areas of all the triangles of the original 3D mesh surface gives me the **surface area**, which I'm already calculating correctly. I don't know exactly how this is called, but what I need is a way to compute the planar area filled by this same mesh to apply in an architectural context." CreationDate="2017-06-21T12:12:47.627" UserId="6788" />
  <row Id="7900" PostId="5265" Score="1" Text="Ok, so your question really is &quot;how do you calculate the projected area of a 3D object on an arbitrary 2D plane&quot;? If the 3D object is convex, then it's quite simple (ignore backfacing triangles and project triangles using dot-product), but it's much more difficult for concave objects because you have to account for (partial) occlusion to avoid &quot;double counting&quot; areas." CreationDate="2017-06-21T12:59:22.987" UserId="1952" />
  <row Id="7901" PostId="5265" Score="0" Text="@JarkkoL yes, you got it right. I'm working now on the backface culling with dot-product. Thanks!" CreationDate="2017-06-21T14:07:24.833" UserId="6788" />
  <row Id="7902" PostId="5265" Score="0" Text="Just did the backface culling and now it's giving me results near what I expect. I suspect that the difference may be because I'm not accounting yet to partial occlusion. Will check on it to see what can be done. Thanks for the insights, everyone!" CreationDate="2017-06-21T17:50:19.067" UserId="6788" />
  <row Id="7904" PostId="5266" Score="0" Text="I didn't implemented this yet, but indeed it seems like the most straightforward approach to be taken and would give the expected results, so I'm accepting your answer." CreationDate="2017-06-21T17:51:54.900" UserId="6788" />
  <row Id="7905" PostId="5263" Score="0" Text="good topology is a low polygon mesh with faces in the right location such that when animated the mesh deforms properly. i want to generate a low poly mesh FROM the high poly mesh with good topology" CreationDate="2017-06-21T20:12:51.150" UserId="6834" />
  <row Id="7906" PostId="5263" Score="0" Text="&quot;Suitable for animation&quot; is much more meaningful than &quot;good&quot;, and different things can be good in different contexts. If you edit the question to describe more about the task you are facing, including that info from your comment, it will be much easier to know what type of answers you are seeking." CreationDate="2017-06-21T23:33:32.357" UserId="231" />
  <row Id="7907" PostId="5274" Score="1" Text="Note that which faces are hidden depends on the whole camera projection, not just the position and normal. Say you look at one face of a cube, but deform the cube into a frustum where the near face is *slightly* smaller than the far face. The other four faces might be visible in an orthographic projection but not in a perspective projection." CreationDate="2017-06-22T16:03:43.193" UserId="2041" />
  <row Id="7908" PostId="5274" Score="0" Text="@DanHulme OK. Let's assume we have the camera projection. Usually given by the 3D graphics API. Remember, we have points. Not meshes which makes the problem simpler." CreationDate="2017-06-22T16:08:11.617" UserId="4678" />
  <row Id="7909" PostId="5274" Score="1" Text="I don't think it does make the problem simpler. If you don't know the connectivity of the points, you don't know where the faces are, so you can't test for occlusion at all. I think mesh reconstruction needs to be the first step of your algorithm, unless an approximation is good enough (e.g. if a point is within a half-degree of another from the camera's pov, one occludes the other)" CreationDate="2017-06-22T16:12:34.783" UserId="2041" />
  <row Id="7910" PostId="5274" Score="0" Text="@DanHulme Right. Probably we need some mathematical algorithms playing around with the XYZ coordinates and understand if a point is behind others. Or some other approach." CreationDate="2017-06-22T16:18:19.620" UserId="4678" />
  <row Id="7911" PostId="5269" Score="0" Text="The MSE will be a random variable. You can solve for $ \sigma $ by pre-defining the expected value of the PSNR rather than it's exact value. Of course, there are things you can do if you want to prescribe MSE on the nose, for example you can add a constant, but how you do it depends on what properties you need for the noise." CreationDate="2017-06-22T22:43:57.977" UserId="6608" />
  <row Id="7912" PostId="5274" Score="1" Text="Seems like you might want to do a sort of software render. You can create a buffer and initialize it with max z value. For each vertex apply MVP to find the x, y coord in the buffer. perform depth testing and if passes store new z and vertexID. If you have a buffer the size of your screen then you have perfect precision, if you create a buffer less than your screen size you improve performance at accurracy expense. You can probably clusterize and compute with a compute shader." CreationDate="2017-06-22T23:17:17.703" UserId="250" />
  <row Id="7913" PostId="5270" Score="0" Text="Are you more interested in post processing to disguise the noise, or ways of speeding up convergence so that less noise is present?" CreationDate="2017-06-22T23:31:02.323" UserId="231" />
  <row Id="7915" PostId="5277" Score="0" Text="`/*layout (location=1)*/` Why did you comment out important declarations like that?" CreationDate="2017-06-23T01:35:19.970" UserId="2654" />
  <row Id="7916" PostId="5272" Score="0" Text="Is there any image or picture with all of these components working together?" CreationDate="2017-06-23T04:02:54.570" UserId="6868" />
  <row Id="7917" PostId="5278" Score="0" Text="Thank you! Updating it to have one color per vertex worked!" CreationDate="2017-06-23T05:18:09.657" UserId="6874" />
  <row Id="7918" PostId="5278" Score="0" Text="If I may ask for furthur optimization and advice, Currently in order to generate those quadrilateral cells, I am using 8 vertices for each cell. This then means each time I change the colour of a cell, I send 3*4*8 bytes each time. I changed it to send a third of the data by creating the colour in the shader itself, and using an int to determine which colour to use. Is there a more efficient method than the one I just described?" CreationDate="2017-06-23T05:21:01.093" UserId="6874" />
  <row Id="7919" PostId="5277" Score="0" Text="I had a bug where I was unable to obtain attribute indices that I resolved. The indices that I declared (and commented out) just so happened to be the same ones that the compiler chose. Already changed it back to query the attribute index based on the name!" CreationDate="2017-06-23T05:22:18.383" UserId="6874" />
  <row Id="7920" PostId="5263" Score="0" Text="Still need to be more specific—what info about the topology do you want, precisely? &quot;What direction it is flowing&quot; doesn't help much." CreationDate="2017-06-23T06:43:02.260" UserId="48" />
  <row Id="7921" PostId="5278" Score="2" Text="Since the original question was about getting the code to work correctly, you could post a separate question about efficiency." CreationDate="2017-06-23T08:06:29.767" UserId="231" />
  <row Id="7923" PostId="5274" Score="2" Text="@TinaJ A mathematical point can't occlude another because they have zero size. Only the surface can occlude. To take your diagram, what if the orange object were a doughnut? How would we know we can see through the middle of it? You need more information about the nature of this point cloud to decide what surfaces the points represent. You probably have that if you know what the real-world objects are, but you haven't told us anything about that." CreationDate="2017-06-23T08:21:33.280" UserId="2041" />
  <row Id="7924" PostId="5272" Score="0" Text="@mrigendra Every frame displayed on your Android phone shows these components working together! Seriously, if you mean a diagram, I don't know of one." CreationDate="2017-06-23T08:22:41.297" UserId="2041" />
  <row Id="7925" PostId="5270" Score="1" Text="FWIW, Benedikt Bitterli recently released the following https://twitter.com/tunabrain/status/872174108385136640   based on his denoising paper." CreationDate="2017-06-23T08:58:25.933" UserId="209" />
  <row Id="7926" PostId="5270" Score="0" Text="In postprocessing area, there is nice algorithm called bilateral filter https://www.shadertoy.com/view/4dfGDH" CreationDate="2017-06-23T09:10:57.343" UserId="4958" />
  <row Id="7927" PostId="5274" Score="0" Text="@FelipeLira That's interesting. I didn't know it can be as complicated. Is there any reference/source code to that?" CreationDate="2017-06-23T10:08:34.623" UserId="4678" />
  <row Id="7928" PostId="5274" Score="0" Text="@DanHulme Well, you are right in general. But even with my simple mathematical assumption, the middle of donut is empty, no points there. So we should see whatever behind. It is like sending a beam to every single pixel visible from my projection. Then if two points are in the same line, we have to remove whatever behind." CreationDate="2017-06-23T10:11:01.427" UserId="4678" />
  <row Id="7929" PostId="5274" Score="1" Text="@TinaJ The point is, how big does the gap between points have to be before you think it's a doughnut hole? We can't know that (or even if there is a value that makes sense for that) without knowing something about what the points represent in the real world." CreationDate="2017-06-23T10:16:04.993" UserId="2041" />
  <row Id="7930" PostId="5274" Score="0" Text="@DanHulme Yes the size of points in a point cloud matter to know if anything is visible in the behind or not. We can start from the simplest approach and assumptions independent. I've updated the question with a test model." CreationDate="2017-06-23T10:40:41.423" UserId="4678" />
  <row Id="7931" PostId="5277" Score="0" Text="Why query the attribute index when you can just *specify it* in the shader?" CreationDate="2017-06-23T13:35:15.483" UserId="2654" />
  <row Id="7932" PostId="5274" Score="1" Text="I think you're a bit optimistic if you think people will go download your file and run tests on it before they can answer your question usefully. You're most likely to get good answers when all the relevant information is in the question." CreationDate="2017-06-23T13:58:47.280" UserId="2041" />
  <row Id="7933" PostId="5281" Score="0" Text="can you reference an implementation of a contour detector for which you can specify shape?  I was using find_contours (http://scikit-image.org/docs/dev/api/skimage.measure.html#find-contours) which apparently does no support that" CreationDate="2017-06-23T14:48:35.437" UserId="6875" />
  <row Id="7934" PostId="5269" Score="0" Text="I realized that because the expected value of the random noise is 0, I can estimate the sum of the squares of the noise values for each pixel as the sum of $\sigma^2$ itself. That makes the whole thing a lot easier to solve when I have a predefined PSNR" CreationDate="2017-06-23T15:01:41.913" UserId="6865" />
  <row Id="7935" PostId="5274" Score="0" Text="The test file is to show what I mean of point cloud and the format for those who are not familiar. Yes, most important for me is the description of the approach." CreationDate="2017-06-23T15:06:20.720" UserId="4678" />
  <row Id="7936" PostId="5274" Score="1" Text="A point (in the mathematical sense generally used in computer graphics) has zero size, as mentioned in previous comments. If you want an approach that will work with points that *do* have a size, we'd need to know what size the points are and how their size works. For example, does a point have some defined size in 3D space, or does it have a fixed 2D size independent of distance from the camera (maybe 4 pixels per point)? Using a non-standard definition of &quot;point&quot; is fine, but we can't answer without knowing what that new definition is." CreationDate="2017-06-23T17:32:31.623" UserId="231" />
  <row Id="7937" PostId="5284" Score="0" Text="Are you talking about diffuse hemisphere lighting from a point light?" CreationDate="2017-06-23T17:38:31.773" UserId="56" />
  <row Id="7938" PostId="5284" Score="1" Text="By hemisphere lighting I mean that the top and bottom (hemispheres) of the &quot;world&quot; both emit light of a differing color, e.g. the top may emit blue light (sky) while the bottom may emit a dim brown (earth). You can think of it as a simplified environment map." CreationDate="2017-06-23T17:42:48.957" UserId="6876" />
  <row Id="7939" PostId="5281" Score="0" Text="I can't think of an existing one; when I wrote the answer I was thinking you'd have to do it yourself. For an arbitrary shape that might be a lot of work but you're only looking for four lines." CreationDate="2017-06-23T18:43:29.123" UserId="2041" />
  <row Id="7940" PostId="5274" Score="1" Text="@trichoplax yes In point clouds, points have a size. So let's assume they have a size, say 5 pixel radius just for modeling purpose. Updated the question." CreationDate="2017-06-23T21:36:32.440" UserId="4678" />
  <row Id="7941" PostId="5274" Score="0" Text="I see. So just to be explicit, points further away don't look any smaller, still 5 pixels?" CreationDate="2017-06-23T22:27:29.640" UserId="231" />
  <row Id="7942" PostId="5274" Score="1" Text="It might be worth changing the example picture if you are interesting in points occluded by other points, rather than by polygonal faces. A picture showing points might make for less confusion." CreationDate="2017-06-23T22:29:10.057" UserId="231" />
  <row Id="7943" PostId="5274" Score="1" Text="@trichoplax Sure. I updated the figure. It should be clear what I mean." CreationDate="2017-06-23T23:23:41.737" UserId="4678" />
  <row Id="7944" PostId="5263" Score="0" Text="i want information about the shape of the mesh so i can create a new mesh from that information with different geometry (but geometry with specific edges in specific places according to the shape of the mesh)&#xA;&#xA;So in other words, i want geometry that follows the form of the mesh" CreationDate="2017-06-24T01:11:30.323" UserId="6834" />
  <row Id="7945" PostId="5263" Score="3" Text="Sounds like you want to perform curvature-aligned remeshing, see e.g. https://hal.inria.fr/inria-00099624, http://users.cs.cf.ac.uk/Yukun.Lai/papers/remesh_cad.pdf" CreationDate="2017-06-24T06:20:18.920" UserId="106" />
  <row Id="7946" PostId="5281" Score="0" Text="actually after using find_contours I am indeed looking for rectangle myself, but I thought that if I can make find_contours produce better contours, my final result will be more accurate and less prone to lighting conditions." CreationDate="2017-06-24T06:24:04.033" UserId="6875" />
  <row Id="7949" PostId="5267" Score="0" Text="@DanHulme Do you have any answer for this post ?" CreationDate="2017-06-24T11:08:27.120" UserId="6862" />
  <row Id="7950" PostId="5263" Score="0" Text="Thanks Rahul! this is the first reply that has something to do with what i am looking for!" CreationDate="2017-06-24T23:05:13.590" UserId="6834" />
  <row Id="7951" PostId="5288" Score="0" Text="Thank you for the answer, I understand my problem.&#xA;But , in my opinion, the second image should be changed, the direction vector should not be changed as I know." CreationDate="2017-06-25T00:12:08.467" UserId="6806" />
  <row Id="7952" PostId="5288" Score="0" Text="In the second image I swapped the light source and the camera, this means that the incoming light is coming from another direction than previously. It is not like the camera suddenly emits light. That is why I changed Wo and Wi with each other. I was a bit lazy and I did not swap the arrow shapes themselves, but I thought it was enough to convey the point." CreationDate="2017-06-25T08:57:10.393" UserId="4908" />
  <row Id="7953" PostId="5274" Score="1" Text="There is no simple answer for this question simply because it depends on your interpretation of things and as such has many answers and none at the same time. Since you are not describing your qualitative border constraints. So the question might as well be how to create a mesh fom a pointcloud." CreationDate="2017-06-25T09:27:15.360" UserId="38" />
  <row Id="7956" PostId="5267" Score="0" Text="Note that the @username notifications only work for users who have already commented on this particular post. It's just coincidence that the person mentioned happens to have posted an answer..." CreationDate="2017-06-25T18:43:13.147" UserId="231" />
  <row Id="7957" PostId="5294" Score="0" Text="Wow, this is exactly what I've been looking for.  I started reading a Linear Algebra textbook...but what kind of mathematics do I need to know for physically based rendering?" CreationDate="2017-06-25T20:06:11.133" UserId="6025" />
  <row Id="7958" PostId="5294" Score="2" Text="Wait, did you meant &quot;Because $x_{proj}$ doesn't vary from $0 \rightarrow width$&quot;?" CreationDate="2017-06-25T20:42:15.407" UserId="6025" />
  <row Id="7959" PostId="5294" Score="0" Text="For openGL it needs to project to normalised device coordinates (-1 to 1) so it scales from -Width/2 to Width/2" CreationDate="2017-06-26T02:19:54.297" UserId="3073" />
  <row Id="7961" PostId="5287" Score="0" Text="Correct. This is called spectral rendering. The easiest implementation is to assign a wavelength to each starting ray and then reflect, refract, etc. as per normal. The BRDF / fresnel would then take the wavelength as an additional parameter. This would automatically handle Iridescence." CreationDate="2017-06-26T13:18:26.693" UserId="310" />
  <row Id="7964" PostId="5294" Score="0" Text="You should ask another question about PBR, but here is a good resource: https://learnopengl.com/#!PBR/Theory" CreationDate="2017-06-26T17:56:01.663" UserId="56" />
  <row Id="7965" PostId="5294" Score="0" Text="@mikeglaz I did, thank you. I'd say if you know some undergrad linear algebra and high-school optics, you're fine. Any particulars you don't already understand, you'll already have the grounding to understand by reading the appropriate Wikipedia page." CreationDate="2017-06-26T19:22:05.960" UserId="2041" />
  <row Id="7966" PostId="5264" Score="0" Text="Well, yeah, you are quite right. I'd better do it on CPU and if I am not happy with performance, I'll try to find different ways. Thank you!" CreationDate="2017-06-27T08:51:19.307" UserId="6841" />
  <row Id="7967" PostId="5295" Score="1" Text="Part 1 is the sticky point, it is in fact the problem. This makes the question subjective as the mesh creation in fact has same definition problem as the question in general. We can not give qualitative advice because we dont know what the data is supposed to mean. Only the original author can nknow what the broder constraints should be. And in this case it seems OP does not so we can not really help. It affects the choice of disk size in method 2 too." CreationDate="2017-06-27T08:55:30.997" UserId="38" />
  <row Id="7968" PostId="5296" Score="1" Text="It sounds like your triangles are being transformed so they can have Z &lt; 0 when rotated into eye space. Usually you need to clip the triangle against a near plane to avoid this." CreationDate="2017-06-27T12:28:33.567" UserId="3073" />
  <row Id="7969" PostId="5296" Score="2" Text="@PaulHK You should post that as an answer." CreationDate="2017-06-27T12:47:00.513" UserId="2041" />
  <row Id="7970" PostId="5297" Score="0" Text="My suggestion would be a screenshot, or if you want a higher resolution, print directly to PNG. Perhaps with something like: https://sourceforge.net/projects/pdfcreator/ I've never used it before, but it seems to get pretty good reviews. And it's open source." CreationDate="2017-06-27T13:51:24.437" UserId="310" />
  <row Id="7971" PostId="5295" Score="0" Text="Thanks. It is a good description. But we don't know if they work unless we test them. Are you aware of any implementation, source code, or anything?" CreationDate="2017-06-27T14:33:04.553" UserId="4678" />
  <row Id="7972" PostId="5296" Score="0" Text="@PaulHK that should indeed be right. The clipping should happen before the perspective divide and after the projection transformation. So, I would indeed post that as an answer as Dan said." CreationDate="2017-06-27T16:24:03.327" UserId="4908" />
  <row Id="7975" PostId="5295" Score="0" Text="At that point you'd have to start experimenting. _C_ should be trivial to implement, _B_ is simple but a little more involved, and as said, the triangulation for _A_ is really problematic. Not only it is an open problem, but I'd expect existing solutions to be tedious to implement. I would recommend finding a library that does a half decent job at it." CreationDate="2017-06-28T07:32:23.457" UserId="182" />
  <row Id="7979" PostId="5290" Score="0" Text="In your equation, you transform from integration in cartesian coordinates to integration in sphere coordinates. Shouldn't you add *sin(ϕ)* there to account for this coordinate transformation? Alternatively, you might define *p(θ, ϕ)=sin(ϕ)p(sin(ϕ)cos(θ), sin(ϕ)sin(θ), cos(ϕ))*, but I think it will be more clear if the *sin(ϕ)* factor is distinct from *p(θ, ϕ)*." CreationDate="2017-06-28T10:07:11.750" UserId="6876" />
  <row Id="7980" PostId="5267" Score="0" Text="@trichoplax It's not a complete coincidence. I think I did get notified, because I'd previously edited the question, and the comment reminded me that I was intending to come back and post an answer when I got more time." CreationDate="2017-06-28T10:55:16.287" UserId="2041" />
  <row Id="7981" PostId="5290" Score="1" Text="@FlorianR. The question already describes that *sin(θ)* is inside *p(θ, ϕ)*. The questioner didn't seem to have trouble with it and it's a bit of a side issue, so I didn't really pay any attention to it." CreationDate="2017-06-28T11:32:59.357" UserId="2041" />
  <row Id="7983" PostId="5299" Score="1" Text="Wouldn't the cartesian bounds of the rectangle with h=1 and w=2 be equal to the bounds of the rectangle with h=2 and w=1?" CreationDate="2017-06-28T13:15:11.423" UserId="137" />
  <row Id="7984" PostId="5299" Score="0" Text="@ratchetfreak h=2 and w=1 on isometric space?&#xA;&#xA;If so, I think not, the image that I posted has the same w and h on isometric space with 2:1 Cartesian bounds, but we could fit different objects with different isometric dimensions on the same Cartesian bounds." CreationDate="2017-06-28T15:13:22.633" UserId="6908" />
  <row Id="7985" PostId="5299" Score="0" Text="Thinking again, my algorithm should take circles into account." CreationDate="2017-06-28T15:15:41.127" UserId="6908" />
  <row Id="7986" PostId="5267" Score="0" Text="Interesting. I didn't realise that edits did that. It makes sense though... Thanks for letting me know." CreationDate="2017-06-28T21:43:24.287" UserId="231" />
  <row Id="7990" PostId="5300" Score="0" Text="OK, made the edit." CreationDate="2017-06-29T06:24:35.323" UserId="2040" />
  <row Id="7992" PostId="5300" Score="0" Text="Is it understandable now?" CreationDate="2017-06-29T08:58:03.953" UserId="2040" />
  <row Id="7994" PostId="5300" Score="1" Text="Yes, it's way more clear." CreationDate="2017-06-29T14:11:30.473" UserId="182" />
  <row Id="7995" PostId="5301" Score="1" Text="Dan, I've added an image describing my expectation as an &quot;answer&quot;." CreationDate="2017-06-29T16:28:03.307" UserId="6785" />
  <row Id="7996" PostId="5301" Score="1" Text="You probably want to search for CSG (Constructive Solid Geometry) and Stencils.  For example, this https://www.usenix.org/legacy/events/usenix05/tech/freenix/full_papers/kirsch/kirsch.pdf might allow you to achieve the desired results.  Similarly, perhaps look at http://www.opencsg.org/ ?&#xA;&#xA;Actually, I might add this as a possible answer if no other answers appear" CreationDate="2017-06-29T16:43:57.417" UserId="209" />
  <row Id="7997" PostId="5301" Score="1" Text="Thanks! In fact, I was reading that paper last night, but was hoping that OpenGL would have provided a way of accomplishing this with just basic stencil functions. It looks like quite an involved task!" CreationDate="2017-06-29T16:50:45.123" UserId="6785" />
  <row Id="7998" PostId="5301" Score="1" Text="Simon, one of the issues that I'd have with using the EXCELLENT CSG resources is that I'm accessing OpenGL via Visual Basic modules linked to an Excel spreadsheet from which I drive my application. I do not know whether or not the project files for MSVC6 will include extensions for VBA." CreationDate="2017-06-29T17:49:04.300" UserId="6785" />
  <row Id="8000" PostId="5267" Score="0" Text="@trichoplax I just tried my luck" CreationDate="2017-06-29T21:27:39.423" UserId="6862" />
  <row Id="8004" PostId="5290" Score="0" Text="@DanHulme Thanks for your answer.&#xA;Ok it's good to see p(w) as an abstract representation.&#xA;In my question, I didn't understand how to compute $p(\theta, \phi)$. According to your answer, to calculate $p(\theta, \phi)$ you state that _integrals equality_ involves _integrands equality_ so $p(\omega)d\omega$ = $p(\theta, \phi)d\theta d\phi$, am I right ?&#xA;But is it still right if in this case integration domains are not the same for the two integrals?" CreationDate="2017-06-29T21:58:27.563" UserId="6862" />
  <row Id="8005" PostId="5290" Score="0" Text="@FlorianR do you state that the transformation from $p(\omega)$ to $p(\theta, \phi)$ is the transformation from cartesian coordinates to spherical coordinates ?" CreationDate="2017-06-29T22:03:19.413" UserId="6862" />
  <row Id="8007" PostId="5307" Score="4" Text="If you know how diffuse reflection works (ie not a specular mirror like reflection), what that does with reflected rays, you do with refracted rays." CreationDate="2017-06-30T04:40:05.140" UserId="56" />
  <row Id="8008" PostId="5290" Score="1" Text="@Yoo Rather the transformation from integrating over *ω* to integrating over *θ* and *ϕ* is the transformation from integrating in cartesian to integrating in spherical coordinates. While you already define *p(ω) = sin(θ)p(θ,ϕ)*, I'd rather not change *p* at all, using *ω=(sin(ϕ)cos(θ), sin(ϕ)sin(θ), cos(ϕ))* and using *dω = sin(θ)dθdϕ*. I think that should make it clear that *p* stays the same, we are just transforming the coordinate system of the integration. This is, however, subjective and if you redefine *p* to include *sin(θ)* like you did, the resulting equation will be the exact same." CreationDate="2017-06-30T04:41:08.773" UserId="6876" />
  <row Id="8009" PostId="5307" Score="1" Text="Keep the same parameters, but increase the roughness." CreationDate="2017-06-30T04:48:36.567" UserId="182" />
  <row Id="8011" PostId="5309" Score="1" Text="If you compute du/dx etc can you not check if the bounds of the pixel (including required line widths) include the required U value and set the colour accordingly?" CreationDate="2017-06-30T09:02:17.050" UserId="209" />
  <row Id="8012" PostId="5290" Score="0" Text="@FlorianR You are right to precise that p is the same thing no matter its starting set ($\omega$, or {\theta, \phi}). It's the probability density function, used to find the relative probability of a direction, am I right ?&#xA;&#xA;I think you are misunderstanding what I understand and what I don't (my bad, my question can be confusing).&#xA;**Even though I state that $p(\omega) = \sin(\theta)p(\theta, \phi)$, i don't know why it is true.** (That's why I asked the third question in my post), and because I don't know why is true, I can't plainly understand what is the transformation." CreationDate="2017-06-30T11:40:23.130" UserId="6862" />
  <row Id="8013" PostId="5290" Score="0" Text="@Yoo Is my edit helpful? At first I thought I had understood what you understand and what you don't, but I'm not so sure now." CreationDate="2017-06-30T12:21:03.047" UserId="2041" />
  <row Id="8014" PostId="5290" Score="1" Text="@Yoo Ah, I'm sorry. Dan just added an explanation and maybe to help you visualize it, think of [a map using latitude/longitude](https://en.wikipedia.org/wiki/Equirectangular_projection). The south pole appears enormous, and if you were to integrate something along latitude and longitude, the poles would be overrepresented. The sin(θ) factor removes this bias towards the poles. Also, you might want to read up on integration by substitution for the general case." CreationDate="2017-06-30T12:25:31.683" UserId="6876" />
  <row Id="8015" PostId="5290" Score="0" Text="@FlorianR. Oh, that's a good way of describing it. I might use that one in future." CreationDate="2017-06-30T12:29:44.127" UserId="2041" />
  <row Id="8017" PostId="5312" Score="0" Text="Thanks for your answer. I was checking more for the formula to get to know which points are inside my rotated rectangle, based on X, Y position, width and height and also rotation angle (centered). Thx" CreationDate="2017-06-30T14:50:06.800" UserId="6844" />
  <row Id="8019" PostId="5312" Score="0" Text="I've just done it. Thx" CreationDate="2017-06-30T21:00:45.583" UserId="6844" />
  <row Id="8021" PostId="5290" Score="0" Text="@DanHulme yes your edit is helpful, it is always good to restart from scratch to be sure that everything is understood." CreationDate="2017-07-01T14:43:01.353" UserId="6862" />
  <row Id="8022" PostId="5290" Score="0" Text="@FlorianR.Thanks, it's useful to have a &quot;viewable&quot; way of what the integral means." CreationDate="2017-07-01T14:44:09.660" UserId="6862" />
  <row Id="8023" PostId="5290" Score="0" Text="@DanHulme&#xA;&#xA;However, Florian and you explained why $d(\omega) = \sin(\theta)d(\theta)d(\phi)$, am I right ?&#xA;&#xA;Let me ask the question:&#xA;**Why can we state that $p(\omega)d(\omega) = p(\theta,\phi)d(\theta)d(\phi)$ ? An equality that is in the book.** This is the equality I want to prove, because it would infer that $p(\omega) = p(\theta, \phi) \sin(\theta)$, the questions I asked in my post.&#xA;&#xA;**Is it because the _integrals_ (in the Dan's answer) are equals so their _integrands_ are equals ?**&#xA;I'm looking for a concrete, and rigorous proof of the equality." CreationDate="2017-07-01T14:56:50.787" UserId="6862" />
  <row Id="8024" PostId="5290" Score="0" Text="@DanHulme To be clear, in th book they state that:&#xA;&#xA;$$p(\omega)d(\omega) = p(\theta, \phi)d(\theta)d(\phi)$$ so &#xA;$$p(\omega)\sin(\theta)d(\theta)d(\phi) = p(\theta, \phi)d(\theta)d(\phi)$$ so&#xA;$$p(\omega)\sin(\theta) = p(\theta, \phi)$$.&#xA;&#xA;And I can't rigorously demonstrate the first equality." CreationDate="2017-07-01T15:06:02.820" UserId="6862" />
  <row Id="8025" PostId="5312" Score="0" Text="Can you edit your answer please?" CreationDate="2017-07-01T15:16:57.853" UserId="6844" />
  <row Id="8026" PostId="5290" Score="1" Text="That wouldn't generally be true, but it is true here because of the extra constraint that the distribution is uniform. *Any* probability distribution has to integrate to 1, but this one also has the same value everywhere. (Or to put it another way, the two integrals have to be equal for *every* region you integrate over.)" CreationDate="2017-07-01T18:22:54.607" UserId="2041" />
  <row Id="8028" PostId="5311" Score="1" Text="Could you clarify what is meant by &quot;subpixel&quot; in a greyscale context? Is this just where a new section overlaps with a non-integer number of old pixels, and a proportional value is returned?" CreationDate="2017-07-02T08:01:11.137" UserId="231" />
  <row Id="8029" PostId="5311" Score="0" Text="It seems worth checking whether you are only interested in this approach specifically, or whether you are just looking for the best approach to resizing. There are other approaches, and which is best will depend on the purpose of resizing and your priorities." CreationDate="2017-07-02T08:05:26.210" UserId="231" />
  <row Id="8031" PostId="5315" Score="0" Text="That doesn't seem to work for me. In this [example](https://pastebin.com/dKC8xz4V), I tried to use these identifiers but with no success. I also added the source .mtl file and the code I was trying to run with the output for clarification." CreationDate="2017-07-03T06:35:01.510" UserId="2138" />
  <row Id="8032" PostId="5315" Score="1" Text="Unfortunately I didn't test with a .mtl file, the information was from browsing the header files, the 2 macros I posted are the only references to those material parameters I could find so this could possibly be a incomplete feature in Assimp." CreationDate="2017-07-03T07:35:30.067" UserId="3073" />
  <row Id="8033" PostId="5317" Score="0" Text="I've no idea what that tool uses, but one approach might be to use seam carving. A quick internet search, for example, turned up this project report:  https://inst.eecs.berkeley.edu/~cs194-26/fa15/upload/files/projFinalUndergrad/cs194-dm/Alex_Liu_Report.pdf   Another possibility is to build textures via texture synthesis." CreationDate="2017-07-03T07:38:21.600" UserId="209" />
  <row Id="8034" PostId="5315" Score="0" Text="That might very well be the case. I tried to find some document specifying which parameters are supported and which aren't, but no luck so far.&#xA;Would you know of any alternative way to define and load a material in an external file?" CreationDate="2017-07-03T07:43:53.253" UserId="2138" />
  <row Id="8035" PostId="5311" Score="2" Text="Your description sounds like box filtering which while easy is not very good in terms of quality" CreationDate="2017-07-03T08:16:22.590" UserId="38" />
  <row Id="8036" PostId="5318" Score="0" Text="absolutely awesome answer thanks so much!" CreationDate="2017-07-03T09:32:46.317" UserId="6834" />
  <row Id="8037" PostId="5311" Score="0" Text="@trichoplax: Your &quot;subpixel&quot; assumption is correct. *_new is not necessarily an integer quotient of *_old. And it's not just about resizing - I know there are better algorithms for that. Let's say that in my case each pixel value represents an amount of energy, and I want to know how much energy each pixel of a downsized image must emit when the sum of energy shall be constant." CreationDate="2017-07-03T11:57:31.477" UserId="6346" />
  <row Id="8038" PostId="5312" Score="0" Text="Edited following the edit to the question." CreationDate="2017-07-03T12:26:08.583" UserId="231" />
  <row Id="8040" PostId="5311" Score="0" Text="Unless you are only interested in this specific approach, it might help to edit the question to describe your underlying objectives, like the energy conservation you mentioned." CreationDate="2017-07-03T12:29:14.297" UserId="231" />
  <row Id="8041" PostId="5311" Score="0" Text="+1 to joojaa's comment.  Also, if your grey scale data has come from a captured image, there's a good chance it may be non-linear, e.g. sRGB (Search also for Charles Poynton's gamma FAQ).  If it is, you should first linearise it, filter, and then de-linearise again." CreationDate="2017-07-03T12:33:20.120" UserId="209" />
  <row Id="8042" PostId="5312" Score="1" Text="That's an hell of an edit. I wish I could upvote more." CreationDate="2017-07-03T12:34:43.807" UserId="6844" />
  <row Id="8043" PostId="5317" Score="0" Text="That looks like a very interesting report. I'll make sure to check that out." CreationDate="2017-07-03T16:01:55.743" UserId="6838" />
  <row Id="8044" PostId="5311" Score="0" Text="@joojaa: I played a little with box filtering in Gimp, but have no idea what filter matrix could do the job. Actually it would have to sum all (sub)pixel grayscale values in every section, followed by maybe a simple nearest neighbor resize to maintain that values in the final image. The problem is that summing up needs different filter matrix values for each pixel, e.g. with resize factor 1/2,33: Horizontally: p_new[0] = p_old[0] + p_old[1] + 0.33*p_old[2],  p_new[1] = 0.67*p_old[2] + p_old[3] + 0.67*p_old[4],  p_new[3] = 0.33*p_old[4] + p_old[5] + p_old[6]" CreationDate="2017-07-04T07:33:24.757" UserId="6346" />
  <row Id="8045" PostId="5311" Score="0" Text="Just do a usual resizing then divide by the area ratio." CreationDate="2017-07-04T10:24:59.870" UserId="106" />
  <row Id="8046" PostId="5319" Score="0" Text="Can you show us your shader?" CreationDate="2017-07-04T12:02:44.047" UserId="2041" />
  <row Id="8047" PostId="5319" Score="0" Text="Well, since the base colour of all the quads (or triangles) is white upon which I overlay the cloud feature, I didn't think I needed to do anything specific with shading. For the light model I've used GL_SMOOTH, if hat helps." CreationDate="2017-07-04T12:27:28.457" UserId="6785" />
  <row Id="8048" PostId="5319" Score="0" Text="Well, there shouldn't be a light model at all. You're using the obsolete fixed-function shading, aren't you?" CreationDate="2017-07-04T12:52:03.037" UserId="2041" />
  <row Id="8049" PostId="5319" Score="0" Text="I *think* I may have figured out the problem. For common vertices between adjacent quads, I'm using different normals depending on the quad being rendered. I calculate these vertex normals  as the local cross product of the unit tangent vectors that follow from the bilinear parametrisation of each quad. If I average out these normals, then that *should* address the issue. I'll need to do some coding." CreationDate="2017-07-04T12:52:05.600" UserId="6785" />
  <row Id="8050" PostId="5319" Score="0" Text="if you average them out, you'll get something that actually looks like an ellipsoid, but that shouldn't be necessary anyway for your use case." CreationDate="2017-07-04T12:53:30.110" UserId="2041" />
  <row Id="8051" PostId="5319" Score="0" Text="True,  but it would (hopefully!) eliminate the visible boundaries between adjacent quads due to the light model interacting with different normals at the common vertices of adjacent quads. What's encouraging is that when I disabled lighting, I confirmed that the quad boundaries were no longer visible, so back to coding it is for me!" CreationDate="2017-07-04T13:32:46.747" UserId="6785" />
  <row Id="8052" PostId="5319" Score="0" Text="Well yes, disabling lighting is the right answer. It'll fix this problem as well as bias added by the lighting, which you haven't yet noticed. You should post that as an answer and then not worry about the normals at all." CreationDate="2017-07-04T13:35:42.290" UserId="2041" />
  <row Id="8053" PostId="5322" Score="1" Text="This projection is a 2d projection, with the Z axis pointing into your screen. When both vertices with the same X&amp;Y are projected to screen space they will occupy the same screen coordinate so your line will either be 1 pixel or invisible." CreationDate="2017-07-05T03:20:35.283" UserId="3073" />
  <row Id="8055" PostId="70" Score="0" Text="A cheap way to do this is simply use Gaussian blur on your final scene and store two of them. One for foreground and one for background. They can have the same blur level or different and it is up to you. Upon finishing the blur, simply lerp from depth 0 to some determined (and tweak-able) depth (say 0.10). What you are learning is this blurred scene to the perfectly in-focused scene. For the background blur you can simply do the same except you are blurring from perfect in-focus to the blurred image. Leave some room for the middle to create a perfectly in focus region." CreationDate="2017-07-05T03:55:46.300" UserId="6938" />
  <row Id="8056" PostId="5026" Score="0" Text="In the link provided above it's Monotone cubic Hermite interpolation. The idea is to maintain the direction of tangent vectors. Any idea how to extend it for 3D case?" CreationDate="2017-07-05T06:40:47.247" UserId="2687" />
  <row Id="8057" PostId="5026" Score="0" Text="@Bla...  So you dont actually want a monotone curve at all then? As you ar not defining what your going to be monotone about. Try a catmul-rom spline." CreationDate="2017-07-05T06:52:38.407" UserId="38" />
  <row Id="8059" PostId="5311" Score="0" Text="@Rahul: What resize algorithm would you suggest? A &quot;usual&quot; resizing will calculate average values from neighboring pixels, but not sum-up." CreationDate="2017-07-05T07:41:53.563" UserId="6346" />
  <row Id="8060" PostId="5322" Score="0" Text="@PaulHK You should write that as an answer. Please avoid writing answers as comments, because they might get lost. Comments are for temporary messages only." CreationDate="2017-07-05T07:52:22.293" UserId="2041" />
  <row Id="8061" PostId="5324" Score="0" Text="&quot;the dot product of this blur vector&quot; with what? dot product has two operands. For your question, don't forget the possibility that an error in the code itself is what's making the effect too strong." CreationDate="2017-07-05T07:54:32.630" UserId="2041" />
  <row Id="8063" PostId="5297" Score="0" Text="Thank you for your comment. I have installed the PDFcreator virtual printer, as you suggested. I have tried to print the 3D PDF directly to PNG, and it works well. The only drawback is that I have to give the 3D PDF image very large dimensions (height and width) to obtain a high resolution of the print. But it does work, and it allows me to obtain the PNG-images that I want." CreationDate="2017-07-05T09:07:48.270" UserId="6902" />
  <row Id="8064" PostId="5327" Score="0" Text="Yes, 3 points in any dimension above 2 all lie on the same plane unless they are degenerate and lie on a line" CreationDate="2017-07-05T10:04:28.607" UserId="38" />
  <row Id="8065" PostId="5328" Score="0" Text="Please check my maths, people. I haven't done this since I was an undergrad. :-)" CreationDate="2017-07-05T10:33:35.887" UserId="2041" />
  <row Id="8066" PostId="5328" Score="0" Text="Well yes i would just construct a affine matrix with a crossed and normalized 3 axis does much the same but with less thinking, and then just matrix multiply. Anyway your missing one normalize." CreationDate="2017-07-05T10:37:05.617" UserId="38" />
  <row Id="8067" PostId="5328" Score="0" Text="Yeah, doing it by a matrix would have been simpler but maybe harder to understand. Thinking about each vector separately lets you approach the problem a piece at a time instead of all at once." CreationDate="2017-07-05T10:44:14.790" UserId="2041" />
  <row Id="8068" PostId="5327" Score="0" Text="Or even an infinite number of lines for the most degenerate case. (I'll show myself out)" CreationDate="2017-07-05T10:50:31.123" UserId="209" />
  <row Id="8069" PostId="5328" Score="0" Text="I would say its easier to understand as it has a more straightforward visual interpretation. Assuming you know how affine matrices work" CreationDate="2017-07-05T10:57:24.770" UserId="38" />
  <row Id="8070" PostId="5327" Score="1" Text="@SimonF and planes ;)" CreationDate="2017-07-05T11:00:05.600" UserId="38" />
  <row Id="8071" PostId="5329" Score="0" Text="Then, how do you get the point A?" CreationDate="2017-07-05T11:14:45.263" UserId="2687" />
  <row Id="8072" PostId="5328" Score="0" Text="$a$ is the point? I haven't fully understand it. So, what's next after I know the three basis vectors?" CreationDate="2017-07-05T11:20:49.957" UserId="2687" />
  <row Id="8073" PostId="5328" Score="0" Text="Just to elaborate, initially I have the info for all four points. My goal is to find where point $A$ is when the other three points changing position and orientation." CreationDate="2017-07-05T11:23:34.937" UserId="2687" />
  <row Id="8074" PostId="5329" Score="0" Text="Just to elaborate, initially I have the info for all four points. My goal is to find where point $A$ is when the other three points changing position and orientation." CreationDate="2017-07-05T11:23:40.433" UserId="2687" />
  <row Id="8075" PostId="5324" Score="0" Text="Are you happy to show your code?" CreationDate="2017-07-05T11:38:47.997" UserId="231" />
  <row Id="8076" PostId="5328" Score="0" Text="Ah, I think I understand your solution.." CreationDate="2017-07-05T11:39:59.103" UserId="2687" />
  <row Id="8077" PostId="5329" Score="0" Text="Ah I think I understand your point." CreationDate="2017-07-05T11:40:22.087" UserId="2687" />
  <row Id="8078" PostId="5332" Score="0" Text="Thank you! Both your answer and Michal's answer explained it to me very well. I'll have to remember this next time a code a shader." CreationDate="2017-07-05T16:23:00.687" UserId="6838" />
  <row Id="8079" PostId="5331" Score="1" Text="Thank you! Your answer as well as Dan's answer covered my question very well. I'll make sure to use this new piece of knowledge." CreationDate="2017-07-05T16:24:00.770" UserId="6838" />
  <row Id="8080" PostId="5324" Score="0" Text="@DanHulme, I'm using the dot product of the blur vector, so basically dot(blurVector, blurVector). This is basically the length squared to see if the blur vector is large enough." CreationDate="2017-07-05T17:09:16.700" UserId="6938" />
  <row Id="8081" PostId="5330" Score="0" Text="Note that later in the article, wikipedia says &quot;When the color is represented as RGB values, as often is the case in computer graphics, this equation is typically modeled separately for R, G and B intensities, allowing different reflections constants k a , {\displaystyle k_{\text{a}},} k_{\text{a}}, k d {\displaystyle k_{\text{d}}} k_{\text{d}} and k s {\displaystyle k_{\text{s}}} k_{\text{s}} for the different color channels.&quot;" CreationDate="2017-07-06T00:46:04.340" UserId="6947" />
  <row Id="8082" PostId="5334" Score="0" Text="Can you show what you've tried so far, what the results were, and what you were expecting? Would the above image be three or four lines, for example? (Or something else?)" CreationDate="2017-07-06T04:05:07.850" UserId="3003" />
  <row Id="8083" PostId="5335" Score="2" Text="Does it have to be a procedural texture? A photo of a shrink-wrapped black box would be the obvious solution." CreationDate="2017-07-06T08:48:18.147" UserId="2041" />
  <row Id="8084" PostId="5335" Score="0" Text="Yes, because I would like to generate thousands of unique textures. Thanks for the remark, I updated my question accordingly." CreationDate="2017-07-06T08:51:28.177" UserId="6951" />
  <row Id="8085" PostId="5334" Score="0" Text="Points 1 and 2 together imply that no line will have a slope greater than 45 degrees above or below horizontal. Is this correct or do the definitions need to be adjusted?" CreationDate="2017-07-06T09:02:16.327" UserId="231" />
  <row Id="8086" PostId="5334" Score="0" Text="What kind of noise does point 3 refer to? Does this only add points, or can points be removed/moved (which would then sometimes leave gaps between points in the line)?" CreationDate="2017-07-06T09:03:38.063" UserId="231" />
  <row Id="8087" PostId="5334" Score="0" Text="We won't know for certain until you have fully specified what you are looking for, but it sounds likely that this is a far from trivial task, rather than you overcomplicating it." CreationDate="2017-07-06T09:09:27.053" UserId="231" />
  <row Id="8088" PostId="5335" Score="1" Text="You'll want to generate a bump map/normal and use that for the reflective highlights. Getting the tension streaks to look realistic on the geometry is going to be challenging though." CreationDate="2017-07-06T09:24:07.760" UserId="137" />
  <row Id="8089" PostId="5335" Score="1" Text="I wonder if you took something like your original plasma texture and stretched it along a random axis and used the result as ratchet freak suggests above, if that would look decent?" CreationDate="2017-07-07T01:19:34.250" UserId="3003" />
  <row Id="8090" PostId="5200" Score="0" Text="From your additional description, it sounds like you just want `r = 1-c`, `g = 1-m` and `b = 1-y`, then work in RGB." CreationDate="2017-07-07T01:24:35.750" UserId="3003" />
  <row Id="8091" PostId="5324" Score="0" Text="Is the 'oval like artefact' because the blur is sampled in device coordinate space?" CreationDate="2017-07-07T06:01:10.907" UserId="3073" />
  <row Id="8093" PostId="5336" Score="1" Text="I notice that the example output is not minimal according to the definition (there exists an arrangement of 5 convex polygons that covers the mesh). Is this because it's just an example, or are there further requirements that prevent the 5 polygon output being valid? For instance, are the output polygons required to be in vertical strips of fixed width or is that just for the example image?" CreationDate="2017-07-07T11:18:53.900" UserId="231" />
  <row Id="8095" PostId="5336" Score="0" Text="Thanks trichoplax. I've added the requested additional information to the question. Hope they clarify your questions." CreationDate="2017-07-07T11:41:53.843" UserId="4558" />
  <row Id="8096" PostId="5336" Score="0" Text="I've identified the difference between the example output and the minimal one I was thinking of: Mine covers the area using convex polygons that use only the existing vertices, but do not use the existing triangles. The example output shows the minimal number of convex polygons if they are required to be constructed from the existing triangles. Is this a requirement?" CreationDate="2017-07-07T15:13:02.573" UserId="231" />
  <row Id="8097" PostId="5336" Score="1" Text="No this is no requirement. The resulting polygons can be generated with any means available. The only requirement is, that the resulting shape is exactly the same as with triangles." CreationDate="2017-07-07T19:23:04.757" UserId="4558" />
  <row Id="8098" PostId="5324" Score="0" Text="@PaulHK, what do you mean? NDC? I'm drawing the screen as a post-processing effect, meaning I'm drawing a square that's the size of the screen and with texture coordinate (0,0) being top left and (1, 1) being bottom right. Therefore the UV will be interpolated from top left to bottom right." CreationDate="2017-07-07T20:30:36.610" UserId="6938" />
  <row Id="8101" PostId="5340" Score="0" Text="I have tried glFinish or glTextureBarrier, but still can't fix my problem.I think I should post this into AMD's community. Anyway, Thanks.@Nicol Bolas" CreationDate="2017-07-08T01:15:54.817" UserId="6660" />
  <row Id="8102" PostId="5336" Score="0" Text="This problem is known as *convex decomposition*. See https://gamedev.stackexchange.com/q/53142, http://doc.cgal.org/latest/Partition_2/index.html#title2, http://masc.cs.gmu.edu/wiki/ACD" CreationDate="2017-07-08T05:02:54.800" UserId="106" />
  <row Id="8104" PostId="5324" Score="0" Text="Yes that's what I was thinking. The problem is that if you go, say, 0.01 in any direction, it will be stretched horizontally because it is not corrected for aspect. When sampling for a blur, you want to be in pixel space, or at least a aspect corrected version of NDC, e.g. vec2 samplePos = ndcPos + blurOffset * vec2(aspectRatio, 1);" CreationDate="2017-07-09T08:03:40.343" UserId="3073" />
  <row Id="8105" PostId="5307" Score="0" Text="Thanks for the comments. it shed light on how I got it wrong first." CreationDate="2017-07-09T09:22:15.863" UserId="6041" />
  <row Id="8106" PostId="5071" Score="0" Text="The same question was asked on Math. SE, and answered, here https://math.stackexchange.com/questions/2269849/join-two-bezier-curves-so-that-the-result-is-two-times-continuously-differentiab/2271518#2271518" CreationDate="2017-07-09T11:06:57.020" UserId="6968" />
  <row Id="8109" PostId="5335" Score="1" Text="I would suggest using the &quot;ridged&quot; version of FBM/Perlin noise to mimic the crests and branchiness you get with cling film. This can do done using something like 1-abs(FBM). visualisation of ridge FBM here :  http://accidentalintricacy.blogspot.hk/2009/04/base-landmass.html" CreationDate="2017-07-10T03:23:30.473" UserId="3073" />
  <row Id="8111" PostId="5348" Score="0" Text="Nice survey. Alexei Efros has some nice [slides on image blending](http://graphics.cs.cmu.edu/courses/15-463/2005_fall/www/Lectures/Pyramids.pdf) as well from his computational photography course. He mentions classical Burt-Adelson laplacian pyramid blending as well as gradient domain methods and graph cuts." CreationDate="2017-07-10T06:02:46.910" UserId="106" />
  <row Id="8115" PostId="5348" Score="0" Text="This is a very good answer, and I'm glad you took the time to go over multiple algorithms, rather than just one. I'll make sure to look at those links." CreationDate="2017-07-10T14:40:03.727" UserId="6838" />
  <row Id="8117" PostId="5343" Score="0" Text="I am only interested in description in analytic solutions though. I dont really care how complex they become i just need to see how pthers have approached the problem. It is easy to say it becomes hard but thats just waving hands." CreationDate="2017-07-10T17:13:35.593" UserId="38" />
  <row Id="8118" PostId="5324" Score="0" Text="@PaulHK I will try that but I doubt it will change much. I'm already calculating everything in pixel space ... so I reconstruct the 3D position from uv and depth texture, then multiply by inverse view-projection matrix. I then do perspective divide myself after multiplying by previous view-projection matrix. This coordinate is then converted to texture space [0, 1]. I then use this to subtract uv in fragment shader to get the delta uv." CreationDate="2017-07-10T17:37:35.277" UserId="6938" />
  <row Id="8119" PostId="5349" Score="0" Text="What are you requirements? Gaussian kernels are designed to blur without artifacts. A different kernel that blurs but isn't Gaussian is the box filter. It will produce artifacts however. I suppose more importantly, why isn't Gaussian good enough?" CreationDate="2017-07-10T20:26:09.630" UserId="6938" />
  <row Id="8120" PostId="5309" Score="0" Text="Can't you simply convert the thickness into texture coordinates? I assume you know the dimension of your planar polygon and you know your starting point in world space? If so, simply offset from that point given your thickness, than transform it into UV coordinate. You can either multiple view-projection matrix and then divide by w (perspective divide) and then convert [-1, 1] to [0, 1] by times 0.5 and then add 0.5. This should get you a UV offset from the line start. You now have your thickness. You might want to do half the thickness and offset from center (the start)." CreationDate="2017-07-10T20:50:04.140" UserId="6938" />
  <row Id="8121" PostId="5343" Score="0" Text="The hand-waving was meant to convince you that seeking analytic solutions is hopeless without making you read a lot of justification. But, I added some justification, anyway." CreationDate="2017-07-11T00:22:59.217" UserId="6968" />
  <row Id="8122" PostId="5343" Score="0" Text="Yes well now it is better however I was thinking one could solve this from the derivate." CreationDate="2017-07-11T06:37:20.880" UserId="38" />
  <row Id="8123" PostId="5343" Score="0" Text="See a torus is much more difficult than what i am asking here. As its perfectly fine to eliminate all curves that self occlude." CreationDate="2017-07-11T07:07:57.153" UserId="38" />
  <row Id="8124" PostId="5349" Score="1" Text="What kind of aliasing is present? Is it a synthetic image with staircasing/jaggies? Is it a texture with spurious low-frequency components (like a high-frequency grating or checkerboard)? Is it a photo that's been badly upscaled?" CreationDate="2017-07-11T11:07:24.513" UserId="2041" />
  <row Id="8125" PostId="5349" Score="0" Text="The image consist of Jaggies mostly ... like for instance, i have an image filled with Grass and I want to make sure that the grass blades are not too blurred." CreationDate="2017-07-11T11:22:44.190" UserId="6974" />
  <row Id="8126" PostId="5343" Score="0" Text="Your example could easily be the inside region of a torus. Occluding or not occluding doesn't change the basic silhouette equation." CreationDate="2017-07-11T12:19:51.173" UserId="6968" />
  <row Id="8127" PostId="1984" Score="0" Text="I'm sure SolidWorks (or any system based on the Parasolid kernel) can do this correctly." CreationDate="2017-07-11T12:24:25.863" UserId="6968" />
  <row Id="8128" PostId="1983" Score="0" Text="@linguisticturn Have you tried contacting Hartley &amp;/or Zisserman for the additional details you require? A quick search brought up contact info for both." CreationDate="2017-07-11T18:24:20.763" UserId="5349" />
  <row Id="8129" PostId="5354" Score="1" Text="You dont actually need a rotation at all, it is practical but not needed. Also singularities in euler angles is only a problem if you intend to interpolate stuff." CreationDate="2017-07-12T13:44:37.653" UserId="38" />
  <row Id="8130" PostId="5354" Score="0" Text="Apparently we have different understandings of the word &quot;rotation&quot;. So I changed to use &quot;orientation&quot;, instead. Clearly you **do** need an orientation if you want to orient objects in 3D." CreationDate="2017-07-13T00:26:26.117" UserId="6968" />
  <row Id="8131" PostId="5356" Score="0" Text="I thought of edge detection, but when observing the same effect ingame in the terrain (which has rather large triangles), it is possible to see the same transition between a &quot;wide&quot; rim and the constant-width as the camera turns to a narrower angle. Also, what kind of edge detection could be used here? I actually tried using Sobel on depth, but then I have no idea how to adjust the &quot;width&quot; without blurring internal lines. (Though I imagine this should be a separate question)" CreationDate="2017-07-13T12:31:43.727" UserId="6986" />
  <row Id="8132" PostId="5356" Score="0" Text="They probably distinguish characters and terrain (I thought the effect was absent on terrain, but maybe I just missed it). For Sobel, just scale the kernel to the thickness you want: instead of comparing the.adjacent pixels, compare the ones a bit further." CreationDate="2017-07-13T12:58:54.767" UserId="182" />
  <row Id="8133" PostId="5356" Score="0" Text="Hmm I'll take another dab at Sobel, and report back" CreationDate="2017-07-13T23:07:59.447" UserId="6986" />
  <row Id="8134" PostId="5355" Score="0" Text="Frankly, I like the look you've achieved better!" CreationDate="2017-07-14T01:16:29.610" UserId="3003" />
  <row Id="8135" PostId="5355" Score="0" Text="@user1118321 I would be satisfied too, if not for the fact that it silhouettes perfectly every minor problem with the models it is applied to." CreationDate="2017-07-14T01:21:51.130" UserId="6986" />
  <row Id="8137" PostId="5358" Score="1" Text="I think you might get a better answer on [physics.se]. They have people who actually work with carbon nanotubes, and probably understand the optics better." CreationDate="2017-07-14T11:13:17.473" UserId="2041" />
  <row Id="8138" PostId="5358" Score="0" Text="That is possibly right, in fact, I initially wanted to post it there, but thought it was too CG specific. You are however welcome to migrate the question there." CreationDate="2017-07-14T11:15:21.050" UserId="6991" />
  <row Id="8139" PostId="5358" Score="1" Text="I think it's actually on-topic for us, so I'm not keen to migrate it, but if other people vote to close it I will definitely migrate rather than leave it closed here." CreationDate="2017-07-14T12:40:15.320" UserId="2041" />
  <row Id="8140" PostId="5359" Score="1" Text="There isn't necessarily a conversion factor afterwards. If your `M` is a projection or a non-uniform scale, then the distance between a pair of points depends on where it was in the original space." CreationDate="2017-07-14T14:02:39.433" UserId="2041" />
  <row Id="8141" PostId="5361" Score="0" Text="Thank you Dan.. is there any link that you can suggest for me to look further in to this (like a code or paper ) ?" CreationDate="2017-07-14T18:18:29.553" UserId="6974" />
  <row Id="8142" PostId="5358" Score="2" Text="I think &quot;what it looks like&quot; would depend highly on how the nanotubes are put together into a macroscopic material—like how long are the nanotubes? Are they aligned with each other, or tangled/braided, or just randomly oriented? What treatments have been done to the surface of the material, like polishing or coating, etc.?" CreationDate="2017-07-14T21:01:09.227" UserId="48" />
  <row Id="8143" PostId="5361" Score="0" Text="This link explains how to get the frequency information of an image, which you could use to know how much aliasing there would be if you downsized the image: https://blog.demofox.org/2016/07/28/fourier-transform-and-inverse-of-images/" CreationDate="2017-07-14T22:44:27.483" UserId="56" />
  <row Id="8144" PostId="5349" Score="0" Text="a sinc filter can downsample &quot;perfectly&quot; such that any details that would cause aliasing are removed, while other details are left perfectly alone.&#xA;https://en.wikipedia.org/wiki/Sinc_filter" CreationDate="2017-07-15T00:34:15.260" UserId="56" />
  <row Id="8145" PostId="5362" Score="0" Text="Sounds like marketing mumbo-jumbo to me. I would assume &quot;slope-based&quot; means based on the gradient. Usually the gradient is generated from the luminance, though, so who knows?" CreationDate="2017-07-15T01:29:25.130" UserId="3003" />
  <row Id="8146" PostId="4099" Score="0" Text="Opinion based?  Puzzling. I don't think any of the items in my answer are &quot;opinions&quot;, except perhaps the statement in the last line." CreationDate="2017-07-15T03:17:03.337" UserId="6968" />
  <row Id="8147" PostId="5363" Score="0" Text="Yes, but the best way depends on what the particular constraint is, so you need to be more specific. What's the actual problem you're trying to solve?" CreationDate="2017-07-15T12:27:07.363" UserId="2041" />
  <row Id="8148" PostId="5363" Score="0" Text="Thanks @DanHulme, let's say, I would like to have it for first constrain. I am a novice in this area so probably, not even sure if I am looking in the right direction." CreationDate="2017-07-15T12:47:52.530" UserId="6994" />
  <row Id="8149" PostId="5362" Score="0" Text="You make a good point. I wouldn't be surprised if what you said is true. I'll keep this question open just to see if anybody else has an idea of what it means." CreationDate="2017-07-15T14:13:38.683" UserId="6838" />
  <row Id="8150" PostId="5367" Score="0" Text="This question might be better on the [graphic design stack exchange](https://graphicdesign.stackexchange.com) (not positive though, so be sure to read their help section to see)." CreationDate="2017-07-16T05:04:01.300" UserId="3003" />
  <row Id="8151" PostId="5367" Score="0" Text="@user1118321 yeah it would fit on GD but im sure it can be here too." CreationDate="2017-07-16T10:43:18.293" UserId="38" />
  <row Id="8152" PostId="5363" Score="0" Text="If you edit the question to show just a single question, it will be better received and it will be easier for people to write good answers. You can then post the other points as separate questions later." CreationDate="2017-07-16T16:12:35.407" UserId="231" />
  <row Id="8154" PostId="5369" Score="0" Text="Size of file can be also quite expensive imagine your image gets downloaded a 10 million times due to getting popular then the difference between a 2 mega file and a 400kb file is 1700 \$ vs 340 \$ (using aws pricing)" CreationDate="2017-07-16T17:44:39.957" UserId="38" />
  <row Id="8156" PostId="5369" Score="0" Text="S3 is currently ~\$0.023/GB, so \$450 vs \$90.. Not that I would still want to pay \$360 for nothing (:" CreationDate="2017-07-16T19:49:56.567" UserId="1952" />
  <row Id="8157" PostId="5358" Score="0" Text="In some configuration CNT is used for Vantablack, so it would be quite a simple shader (:" CreationDate="2017-07-16T20:36:52.697" UserId="1952" />
  <row Id="8158" PostId="5371" Score="0" Text="Great to see you active on this new Stack Exchange site dedicated to graphics." CreationDate="2017-07-16T20:37:42.823" UserId="7000" />
  <row Id="8161" PostId="4099" Score="0" Text="@bubba The first question about places to learn is very much opinion-based as it can differ per person. The second question about open problems is very broad." CreationDate="2017-07-17T03:42:47.867" UserId="2707" />
  <row Id="8162" PostId="5359" Score="0" Text="Just to clarify the original data is 3d imaging data in imager coordinates and the transformed data is in world coordinates. If there were no rotation, only scaling and translation I could work out the new pix/vox-&gt;mm conversion but because the volume is also rotated it makes it more difficult..." CreationDate="2017-07-17T09:36:08.893" UserId="6990" />
  <row Id="8163" PostId="5375" Score="0" Text="That helped. Thanks a lot. :-)" CreationDate="2017-07-18T03:23:54.030" UserId="2096" />
  <row Id="8166" PostId="5378" Score="0" Text="Thanks for the answer and reference paper. I will try to implement the method proposed." CreationDate="2017-07-18T12:55:37.787" UserId="7013" />
  <row Id="8167" PostId="5377" Score="2" Text="Assuming that barycentric approach does what I think it does it would be quite slow with large sets. Imagine a set of 9 million vertices with only 9 vertices in the desired set. Why iterate the entire set when v1, v2, and v3 give you all the information you need. The flood fill answer would be the fastest flexible solution. Although unflexible, if you can assume you have lines like you do now in the geometry then scanline would be the fastest approach." CreationDate="2017-07-19T05:25:46.620" UserId="113" />
  <row Id="8168" PostId="5380" Score="0" Text="I'm not familiar with flood filling algorithm. Your explanation seems a little complicated to me. Could you please provide a decent reference to look at? Thanks." CreationDate="2017-07-19T05:47:27.480" UserId="7013" />
  <row Id="8169" PostId="5377" Score="0" Text="You're absolutely right about performance issues. I'd like to use this approach in large meshes, so what I'm looking for is an efficient method. Actually I'm not familiar with neither flood filling nor scan fill algorithms, I'll take a look at them. Thanks." CreationDate="2017-07-19T05:52:26.147" UserId="7013" />
  <row Id="8170" PostId="5377" Score="3" Text="A flood fill with a graph would start at a node, visit every neighbor node if boundary condition is met and not visited, mark it as visited, and repeat (recursion). Alteration: mark every node on path as visited and start from a node inside the set. Then simply use the visitation check as the boundary condition." CreationDate="2017-07-19T07:00:57.967" UserId="113" />
  <row Id="8172" PostId="5377" Score="0" Text="Thanks for the detailed explanation. I find flood filling algo more reasonable, but I want to implement both flood fill and scan line, then compare the performances." CreationDate="2017-07-19T08:09:43.763" UserId="7013" />
  <row Id="8173" PostId="5380" Score="0" Text="I got the solution by reading some. Thanks." CreationDate="2017-07-19T08:12:40.590" UserId="7013" />
  <row Id="8178" PostId="5389" Score="0" Text="How are you currently accessing memory? Can you share code for that?" CreationDate="2017-07-20T03:58:55.177" UserId="2707" />
  <row Id="8179" PostId="5389" Score="0" Text="This topic is big enough that books have been written on it. To get a good answer, you need to [edit] your question and make it more specific. But your software rasterizer will always be orders of magnitude slower than using OpenGL or DirectX." CreationDate="2017-07-20T08:21:47.120" UserId="2041" />
  <row Id="8181" PostId="5390" Score="2" Text="Sorry, how are you getting these values? Are you simply reading the depth buffer (which is a linear function of the inverse value of Z) value for each pixel?  If so, then for a plane, all the points should still lie in a plane if you map them (correctly) back into 3D.  (Yes the distance from the camera for each point will vary and you would get a curve, but you shouldn't be using that)" CreationDate="2017-07-20T08:36:33.043" UserId="209" />
  <row Id="8182" PostId="5390" Score="1" Text="I'm exporting custom render passes out of the Sequencer. The specific pass is 'Scene Depth World Units'. The standard depth pass ('Scene Depth') doesn't give actual world Z position in correct units." CreationDate="2017-07-20T09:17:22.253" UserId="7023" />
  <row Id="8184" PostId="5392" Score="3" Text="Mip mapping is also a good strategy to improve cache coherence in texture mapping." CreationDate="2017-07-20T13:26:09.640" UserId="4546" />
  <row Id="8186" PostId="5389" Score="0" Text="@aces The backbuffer I write to is the buffer of the screen. It is represented as 3 chars for RGB in sequential order. I call it &quot;ibuffer&quot;. The texture RGB values are stored in an integer array where each integer is the chars RGBA.&#xA;&#xA;[code]&#xA;*ibuffer = mesh-&gt;texture-&gt;intbuffer[(int)textureX + mesh-&gt;texture-&gt;width * (int)textureY];&#xA;[\code]&#xA;&#xA;The integer I copy from the texture to the backbuffer overwrites the R value of the next pixel in the back buffer. I do this so as to not use bitwise operations used to extract the RGB values from texture memory and store in backbuffer." CreationDate="2017-07-20T16:05:47.067" UserId="7016" />
  <row Id="8187" PostId="5393" Score="0" Text="Everything I've seen online about Raspberry Pi and OpenGL ES 3.0 is that it is not supported. So I have no idea how glxinfo returned that." CreationDate="2017-07-20T23:41:47.837" UserId="2654" />
  <row Id="8188" PostId="5389" Score="0" Text="A minor optimisation would be to use powers of 2 for your texture size, so you can compute the offset using a logical shift. E.g. if you have a 256 pix wide texture you can compute the texel offset with &quot;U + V &lt;&lt; 8&quot;. Also pad your RGB structure to 4 bytes instead of 3 so the compiler can apply a similar optimisation to the array indirection. You can also apply texture wrapping trivially if power of 2 size e.g.  (U &amp; 255) + (V &amp; 255) &lt;&lt; 8" CreationDate="2017-07-21T03:55:40.950" UserId="3073" />
  <row Id="8189" PostId="5392" Score="0" Text="@Stefan Indeed. Texture aliasing is painful in multiple ways :-).&#xA;&#xA;There's a bit more on the subject here: https://computergraphics.stackexchange.com/questions/357/is-using-many-texture-maps-bad-for-caching/419#419" CreationDate="2017-07-21T08:42:05.620" UserId="209" />
  <row Id="8190" PostId="5396" Score="0" Text="Can you not just replace the &quot;Standard&quot; shader a copy in which the hack is fixed?" CreationDate="2017-07-21T10:35:33.103" UserId="2041" />
  <row Id="8191" PostId="5397" Score="0" Text="What OpenGL version are you using, and is it WebGL or a real program?" CreationDate="2017-07-21T14:13:34.777" UserId="2041" />
  <row Id="8192" PostId="5397" Score="1" Text="I'm using SDL2 to create a context for OpenGL 4.1. It's the regular version of OpenGL, not ES or WebGL" CreationDate="2017-07-21T15:00:07.990" UserId="7029" />
  <row Id="8193" PostId="5389" Score="0" Text="@PaulHK I will try that Paul. Compared to my original statement, I found if I access the same texel for all interpolated pixels (*ibuffer = mesh-&gt;texture-&gt;intbuffer[10000]), the program framerate increases by 3 to 4 times. Do you have any idea why?" CreationDate="2017-07-21T16:42:22.820" UserId="7016" />
  <row Id="8194" PostId="5398" Score="2" Text="Considering (0, 0) as the center of the screen. If we divide any point we want to place on that screen by z it will get infinitely closer to (0, 0). This creates that vanishing effect. Farther back it goes, the larger the z, the more it vanishes. This is essentially what perspective projection is doing despite all the confusing notation, matrices, and what not. It also handles all the other transformations. Certain scalings and translations must be applied to get a point into screen coordinates." CreationDate="2017-07-21T21:29:26.703" UserId="113" />
  <row Id="8195" PostId="5397" Score="2" Text="In your `kernel` array you divide `float` by `int`. I suspect that for some reason your compiler cast this to `int`instead of `float`  so information is lost and then to `float`." CreationDate="2017-07-21T23:21:30.023" UserId="4958" />
  <row Id="8196" PostId="5399" Score="0" Text="Indeed.  A simple operator to make HDR colors to LDR is a simplified reinhard operator which is this:  colorOut = colorIn / (colorIn + 1.0).  It's not the best, but it's easy.  It makes it so the color will never quite reach 1.0 no matter how large a number it is.  For more information, check this out: https://imdoingitwrong.wordpress.com/2010/08/19/why-reinhard-desaturates-my-blacks-3/" CreationDate="2017-07-22T00:34:48.260" UserId="56" />
  <row Id="8197" PostId="5397" Score="0" Text="Why don't you use the numerators from the kernel and divide the whole thing by 16.0 at the end just to see if it works?" CreationDate="2017-07-22T01:56:57.510" UserId="3003" />
  <row Id="8198" PostId="5396" Score="0" Text="It seems like they multiply the BRDF by pi to maintain backwards compatibility with existing shaders using Blinn-Phong and their existing spherical harmonics system. With a bit more research I'm not sure this is the problem, more as Stefan says below, highlights will realistically be much brighter than diffuse light, and LDR just doesn't have the bandwidth for it. Looking at GGX renders with various material values on Google images, it seems to be an issue that goes beyond unity." CreationDate="2017-07-22T07:11:37.453" UserId="1937" />
  <row Id="8199" PostId="5399" Score="0" Text="Thanks for the suggestions guys. I guess what I need is something like a soft-knee compressor as used in audio processing - linear below a threshold and with a strong reinhard-like rolloff above it. Not sure what such a function looks like mathematically though. I'm gonna play around with it over the weekend and see what I can come up with." CreationDate="2017-07-22T07:16:09.427" UserId="1937" />
  <row Id="8200" PostId="5397" Score="0" Text="I've tried both adding `.0` to the divisions by 16 and removing the division altogether, and even changing the kernel to another post-proc effect, but it still gives me the same results.&#xA;For now, my solution is to just unravel the loop and write all the steps manually. Would that be a good approach or a bad practice in a professional shader?" CreationDate="2017-07-22T08:55:11.543" UserId="7029" />
  <row Id="8201" PostId="5389" Score="0" Text="Using a constant index makes the address calculation a lot simpler, the compiler will calculate 1 pointer outside of your loops as opposed to recalculating it each pixel because it is using a none-constant index. Also accessing the same memory location will ensure it's in the CPU cache so you will get massive performance boost from that." CreationDate="2017-07-22T10:12:09.687" UserId="3073" />
  <row Id="8202" PostId="5400" Score="0" Text="Thanks for the answer. tell me If I am wrong here that we map frustum ranges to [-1,1] because OpenGL expects them to be in that range(because OpenGL performs clipping) so that clipping can occur properly. Am I correct?" CreationDate="2017-07-22T11:19:35.910" UserId="2096" />
  <row Id="8203" PostId="5400" Score="1" Text="That's about right. The clipping occurs in automatic hardware on the GPU,  so it's broadly the same across different graphics APIs. Some specify -1 for the near plane and some specify 0. 0 is actually better for depth precision, for reasons Nathan Reed explains at http://reedbeta.com/blog/depth-precision-visualized/" CreationDate="2017-07-22T13:37:08.827" UserId="1937" />
  <row Id="8204" PostId="5403" Score="0" Text="Excellent answer.  After commenting out all of the enabled attributes, I am able to call `glDrawArrays` with no buffer / enabled attributes.  However, I can see that I *do* still need a VAO bound when I call glDrawArrays. Why is this? My current VAO has no attributes enabled at all so it seems like it wouldn't be necessary." CreationDate="2017-07-23T20:55:40.293" UserId="7000" />
  <row Id="8205" PostId="5403" Score="2" Text="@Startec: Because that's the rule; you need a VAO bound for that state to exist. Even if the state is the empty default state, you still need vertex array state in order to render." CreationDate="2017-07-23T21:19:10.353" UserId="2654" />
  <row Id="8207" PostId="5404" Score="0" Text="https://math.stackexchange.com/questions/187107/calculate-coordinates-of-3rd-point-vertex-of-a-scalene-triangle-if-angles-and?rq=1" CreationDate="2017-07-24T09:12:59.793" UserId="457" />
  <row Id="8208" PostId="5404" Score="0" Text="I tried to implement this to my inputs but not succeed." CreationDate="2017-07-24T10:40:51.883" UserId="7039" />
  <row Id="8209" PostId="5408" Score="1" Text="I think this question might do better on Stack Overflow than here, as your questions are very specific to using VBA in Excel and not much to do with graphics algorithms." CreationDate="2017-07-24T13:48:41.613" UserId="2041" />
  <row Id="8210" PostId="5408" Score="0" Text="Ah, I see you asked an earlier, less detailed version of the question there. You should probably edit *that* question to give as much detail as this one. But you should brace yourself for the possibility that Excel just can't do what you're asking." CreationDate="2017-07-24T13:51:07.137" UserId="2041" />
  <row Id="8211" PostId="5408" Score="0" Text="Yes, and in fact I've almost given up on a simple solution from within glut as I was hoping. The workarounds suggested all involve things like &quot;VB script worker threads&quot;  and the like that will take me on a gargantuan sidetrack from my key objectives, so very well, I suppose ..." CreationDate="2017-07-24T13:55:45.150" UserId="6785" />
  <row Id="8212" PostId="5389" Score="0" Text="@PaulHK In my innermost for loop I am iterating over the pixels in the triangle and fetching the appropriate texels in a 1D array and storing in the back buffer:&#xA;for all pixels in triangle&#xA;     *ibuffer = mesh.texture.intbuffer[texcoord];&#xA;&#xA;If my texture is allocated on the heap, and the texture is being accessed vertically... I do not understand how the cache is filling up with unecessary values. I suppose if I tiled my texture, the cache would guess the next values and store them in registers and since it's tiled, those values would more likely be the correct ones to use." CreationDate="2017-07-24T22:41:16.973" UserId="7016" />
  <row Id="8213" PostId="5397" Score="0" Text="I would find that odd, but if it works where a loop doesn't, go for it! It could be a bug in the glsl compiler or something. I'd add a comment explaining why you're doing." CreationDate="2017-07-25T04:23:04.013" UserId="3003" />
  <row Id="8214" PostId="5409" Score="1" Text="From a quick glance it looks like your application of Beer's law is correct. The image also looks reasonable to me. Can you clarify what you think is wrong, and why? Have you compared the same scene in another path tracer so you can tell if there are subtle differences?" CreationDate="2017-07-25T18:26:07.953" UserId="48" />
  <row Id="8215" PostId="2450" Score="0" Text="joojaa do you have any more information on this technique? What is done in case of double curved free form surfaces? I'm also not sure what happens when you have a cone or a cylinder that is being cut/trimmed by something free form. https://stackoverflow.com/questions/43795262/silhouette-rendering-with-webgl-opengl" CreationDate="2017-07-25T20:35:13.490" UserId="5005" />
  <row Id="8216" PostId="5411" Score="2" Text="I don't think there's a &quot;right&quot; way to &quot;strengthen&quot; normals. It's not clear what your intent is behind it. What kind of visual effect are you trying to accomplish here?" CreationDate="2017-07-26T14:24:40.757" UserId="2654" />
  <row Id="8217" PostId="5411" Score="0" Text="The desired  'effect' is a stronger bump.What?There are no standard techniques in CG to do that?" CreationDate="2017-07-26T14:26:41.970" UserId="213" />
  <row Id="8218" PostId="5411" Score="0" Text="What do you mean by a &quot;stronger&quot; bump? Again, what do you want it to look like? Be specific. Generally speaking, bump mapping only effectively works for short bumps (relative to the viewing distance). For taller bumps, you need geometry to reproduce it convincingly. Or at the very least, parallax normal mapping." CreationDate="2017-07-26T14:31:58.400" UserId="2654" />
  <row Id="8219" PostId="5411" Score="0" Text="@NicolBolas I need an ability to adjust the strength similar to how it is done in any typical game engine.If you want an example, open Unity,drop a normal map on a surface and tweak its strength in the inspector.That's the effect. My code currently seems to produce similar effect. But,I wanted to know if there is a standard technique to do that because mine is based on trials&amp;errors." CreationDate="2017-07-26T14:35:09.953" UserId="213" />
  <row Id="8220" PostId="5404" Score="0" Text="Please show what you tried to implement. Are you familiar with all the necessary mathematics used here? Can you solve this problem by hand?" CreationDate="2017-07-26T17:04:18.477" UserId="7055" />
  <row Id="8221" PostId="5413" Score="0" Text="If you solved the problem yourself, please undelete the question and explain what fixed it. That will help anyone else with a similar problem." CreationDate="2017-07-27T08:58:46.333" UserId="2041" />
  <row Id="8222" PostId="5415" Score="0" Text="You probably need to use a separate pass to do this kind of effect." CreationDate="2017-07-27T09:07:36.637" UserId="2041" />
  <row Id="8227" PostId="5412" Score="2" Text="BTW, you don't need to normalize twice. :)" CreationDate="2017-07-27T20:27:41.710" UserId="48" />
  <row Id="8228" PostId="5412" Score="0" Text="@Nathan: sure, but that's what the OP did. I would use a signed RG texture too, or a quternion based pipeline." CreationDate="2017-07-27T20:43:53.617" UserId="7057" />
  <row Id="8229" PostId="5422" Score="0" Text="You went exactly to the point! I did not realize that the decision was being made on top of a different probability (i.e. 0.25 + 0.5 * Re). Now the scaling makes total sense. Actually, this framework seems to be quite flexible since the decision of which path to follow can be made from a probability that can be completely detached from the original one (e.g. P could be a fixed value) since RP and TP are recomputed to account for it. Obviously, some choices for P may have better justifications and may work better than others (as the one you have discussed above). Thank you Stefan!" CreationDate="2017-07-27T23:16:55.510" UserId="5681" />
  <row Id="8231" PostId="5412" Score="0" Text="@ybungalobill but can you point out if there is a problem with my approach because it seems to me like that's pretty similar to what I do.Also,performance wise,I would prefer doing less ALU operations in the fragment shader(the target is mobile platform)" CreationDate="2017-07-28T10:15:26.930" UserId="213" />
  <row Id="8232" PostId="5412" Score="1" Text="Oh, I missed that part. See, in principle it's the same. However, to get correct results you need to reorthogonalize the TBN per-fragment. Even without doing it, you can still improve the code performance wise. First, scale the normal maps so that z=1 and use `GL_RG8_SNORM` to store only the xy components. This already saves you half the memory bandwidth and the `n=2n-1` calculation in the shader. Then your function becomes simply `vec3 GetNormal(){ return v_TBN*normalize(vec3(texture(normalMap, uv).xy, C)); }`, and that's already with correct scaling." CreationDate="2017-07-28T10:27:01.087" UserId="7057" />
  <row Id="8233" PostId="5429" Score="0" Text="There is a lack of `360 panorama`, `cubemap` (!), `codec` and other relevant tags on computergraphics.stackexchange.com." CreationDate="2017-07-28T13:40:33.397" UserId="5430" />
  <row Id="8234" PostId="3928" Score="0" Text="Using  trigonometric functions has some costs. You could avoid using  trigonometric functions by directing multiplying complex numbers." CreationDate="2017-07-28T14:33:15.843" UserId="7066" />
  <row Id="8235" PostId="5429" Score="0" Text="If you do not use the mobile interface then you can make new tags if you wish." CreationDate="2017-07-28T14:43:01.220" UserId="38" />
  <row Id="8236" PostId="5432" Score="2" Text="How are you currently storing your mesh data? For example do you have a  half-edge data structure, or just vertex/index buffers, or something else?" CreationDate="2017-07-28T15:28:12.563" UserId="48" />
  <row Id="8238" PostId="5430" Score="1" Text="Non-pyramid view frustum is not supported by rasterisators, but we use ray-tracer and it may have sense in our case. Also pixel density can be adjusted to be one-to-one on resulting flat video, but still pixel density is not uniform in case of octamap." CreationDate="2017-07-28T17:11:07.130" UserId="5430" />
  <row Id="8239" PostId="5430" Score="0" Text="Currently &quot;3x2-unfolded&quot; video is used. The video looks seamless. No edges, no distortion on the first (and second) glance. Only four edges are glued and ten are onto the outer boundary. Currently used codec is randomly chosen and it is a luck, that there is no notable seams. (OpenGL supported) cubemap is a way to avoid geometry and filtering on glued and outer edges at all. It is interesting to try octamap, but presumably it have more distortion on the one hand and less glued and outer edges OTOH. Pixel to pixel mapping is impossible in case of non-cube Platonic solids." CreationDate="2017-07-28T17:17:19.707" UserId="5430" />
  <row Id="8240" PostId="3928" Score="0" Text="@CroCo I don't think there are any cases where complex numbers save you from trig. You still need trig functions to get a unit complex number corresponding to a given angle, for instance." CreationDate="2017-07-28T17:54:24.460" UserId="48" />
  <row Id="8241" PostId="5434" Score="0" Text="So, when writing to the texture, WebGL internally does something like `floor(x*255)` to map the [0,1] float value _x_ to [0,255]? And then later, when reading from that texture again, _x_ will probably deviate from the original value. Is that correct?&#xA;And in case of a `gl.FLOAT` texture, no mapping to [0,255] occurs?" CreationDate="2017-07-28T20:14:28.357" UserId="5406" />
  <row Id="8242" PostId="5434" Score="1" Text="@Muad It would be `round(x*255)` instead of `floor`, but yes. And yeah, when you read it back, you'll likely get a slightly different value due to the rounding. In a float texture, it just stores and retrieves the exact value you write, no remapping (and no clamping to [0, 1])." CreationDate="2017-07-28T22:08:37.137" UserId="48" />
  <row Id="8243" PostId="5431" Score="1" Text="You can also make rotation matrices for special angles like these by plugging values directly into the matrix. For example the matrix $\begin{bmatrix}1 &amp; -1 \\ 1 &amp; 1\end{bmatrix}$ implements the same rotation and scaling as the complex number $1 + 1i$. For general angles you still need trig functions, with either matrices or with complex numbers." CreationDate="2017-07-29T04:28:44.200" UserId="48" />
  <row Id="8244" PostId="5434" Score="0" Text="Gotcha, thank you!" CreationDate="2017-07-29T08:38:18.517" UserId="5406" />
  <row Id="8245" PostId="5436" Score="0" Text="What about when 2 vertices are outside the clipping area and the other 2 are inside? You can end up with a pentagon or hexagon inside. Or when 2 are outside one corner and the other 2 are outside the opposite corner?" CreationDate="2017-07-29T14:33:41.383" UserId="3003" />
  <row Id="8246" PostId="5436" Score="0" Text="Good point... I guess I have to cover these edge cases." CreationDate="2017-07-29T20:42:38.913" UserId="7068" />
  <row Id="8247" PostId="5439" Score="0" Text="Thanks Clabe45 for the excellent suggestion. I'll give this a shot and revert if I run into issues or have more questions" CreationDate="2017-07-30T00:12:54.133" UserId="6785" />
  <row Id="8248" PostId="5439" Score="0" Text="No problem. I apologize if I didn't answer directly since I don't really understand list objects." CreationDate="2017-07-30T00:37:55.227" UserId="6883" />
  <row Id="8250" PostId="5443" Score="1" Text="You don't need GLU for that. You can just use straight OpenGL. According to [wikipedia](https://en.wikipedia.org/wiki/OpenGL_Utility_Library): &quot;The GLU specification was last updated in 1998, and it depends on features which were deprecated with the release of OpenGL 3.1 in 2009.&quot;" CreationDate="2017-07-31T05:24:23.717" UserId="3003" />
  <row Id="8251" PostId="5432" Score="0" Text="There is a similar question in stackoverflow which could help you: https://stackoverflow.com/questions/14108553/get-border-edges-of-mesh-in-winding-order" CreationDate="2017-07-31T07:04:36.717" UserId="7059" />
  <row Id="8252" PostId="5436" Score="0" Text="@user1118321 although if you do the planes one by one you will just split the original quad and have a standard solution again." CreationDate="2017-08-01T13:45:28.613" UserId="38" />
  <row Id="8253" PostId="5451" Score="0" Text="Alright, that makes sense. So post processing should usually be applied before gamma correction, so that the operations happen in linear space. Got it." CreationDate="2017-08-02T01:52:35.867" UserId="6838" />
  <row Id="8254" PostId="5440" Score="0" Text="I think it's because if you look at just the error it looks like noise, whereas error could be just a positive bias for instance." CreationDate="2017-08-02T02:42:21.190" UserId="56" />
  <row Id="8255" PostId="5442" Score="0" Text="Could you maybe add an illustration or a reference picture of the effect you are trying to achieve?" CreationDate="2017-08-02T02:55:58.363" UserId="182" />
  <row Id="8256" PostId="5295" Score="0" Text="@JulienGuertault Hi Julien. On A.2, what do you mean of &quot;render the mesh busing a depth buffer but not color buffer&quot;? Is that a known technique? Any references?" CreationDate="2017-08-02T17:11:06.080" UserId="4678" />
  <row Id="8257" PostId="5436" Score="0" Text="@joojaa I'm not sure what you mean. What do you mean by standard solution?" CreationDate="2017-08-03T01:12:35.420" UserId="7068" />
  <row Id="8258" PostId="5459" Score="0" Text="For instancing would that mean to use `glDrawElementsInstanced`?" CreationDate="2017-08-03T16:08:40.673" UserId="6883" />
  <row Id="8259" PostId="5459" Score="0" Text="@clabe45 yeah or glDrawArraysInstanced" CreationDate="2017-08-03T16:10:56.333" UserId="137" />
  <row Id="8260" PostId="5451" Score="0" Text="@DanielKareh it depends on how you developed the tone map. If you developed it say in Photoshop from a  screen capture then it will obviously go last" CreationDate="2017-08-03T19:00:17.437" UserId="38" />
  <row Id="8261" PostId="5295" Score="1" Text="@TinaJ Before rendering the mesh, you just disable writing to the color buffer. The depth will be written but the image will be left untouched. It's pretty standard, although the details will depend on the API or software you are using." CreationDate="2017-08-04T12:14:51.710" UserId="182" />
  <row Id="8262" PostId="5462" Score="4" Text="Hi and welcome. Why would you use ray casting? Why not just dump the triangles to opengl and let it render with a rasterizer? Its not like you need reflections or have complex transparency sorting. Even your geometry is not very complex." CreationDate="2017-08-04T18:39:55.130" UserId="38" />
  <row Id="8263" PostId="5462" Score="0" Text="Hi joojaa. I'm not very experienced with coding (I'm a mechanical engineer by trade); I know the basics to Java coding, but outside of using Java swing, I have no experience with computer graphics. What I'm getting about opengl is that it's to do with writing graphics code that takes better of advantage of the screen's hardware. If you could point me in the write direction, I'd really appreciate that. :) (I've been getting by using Java with Eclipse)" CreationDate="2017-08-04T19:42:56.700" UserId="7108" />
  <row Id="8264" PostId="5462" Score="3" Text="So am I, a mechanical engineer that is. You should really take your time to go through some modern OpenGL (but not legacy opengl) tutorial to understand the concepts." CreationDate="2017-08-04T21:18:23.617" UserId="38" />
  <row Id="8265" PostId="5456" Score="1" Text="Thank you for the long explanation. &quot;try it in your scene and if you then think it's too bright, lower the values until you're satisfied&quot;: The whole purpose of PBR is to not just look good, but follow the laws of physic to get the result. I am looking for (linear) RGB values and intensities, which I can use to display a scene with daylight, or with lamps. But at the moment I can not put lights in that scene, because I have no idea, which color and intensity values would be realistic in the `radiance = lightColor * intensity * attenuation` equation" CreationDate="2017-08-04T21:42:26.160" UserId="7028" />
  <row Id="8267" PostId="5462" Score="3" Text="I second joojaa's comments. Give triangles to OpenGL. You're right that ray casting would be way to computationally expensive." CreationDate="2017-08-04T21:49:33.047" UserId="2941" />
  <row Id="8268" PostId="5465" Score="0" Text="A triangle is simpler math wise. All 3 points are coplanar so all the points inside the are in the same plane which makes interpolation simpler. For quads I wouldn't know why to prefer that, unless it's to do with LoD." CreationDate="2017-08-04T22:12:19.133" UserId="137" />
  <row Id="8269" PostId="5457" Score="0" Text="Please don't cross post. If necessary, request migration instead." CreationDate="2017-08-04T22:28:37.243" UserId="5349" />
  <row Id="8270" PostId="5464" Score="1" Text="To be clear—is your question how to _compute the coordinates_ of the intersection box without an if statement?" CreationDate="2017-08-05T00:10:06.090" UserId="48" />
  <row Id="8271" PostId="5464" Score="0" Text="Also are you on cpu or gpu? What language? It can vary but min and max functions are probably the key component to answering your question, so long as they are branchless where you are using them. They are branchless in hlsl/glsl for what it's worth." CreationDate="2017-08-05T00:21:54.560" UserId="56" />
  <row Id="8272" PostId="5467" Score="0" Text="You lost me somewhere. In the first paragraph, you mention finding the left-most, right-most, top and bottom *squares*. Aren't there only 2 squares? Then, in the second paragraph, you're talking about `(right.x, bottom.y)`, which sound like points, not rectangles. Can you clarify? I'm not understanding how the top-left position is `(right.x, bottom.y)` given what you've written above that." CreationDate="2017-08-05T04:05:18.163" UserId="3003" />
  <row Id="8273" PostId="5467" Score="0" Text="Okay, `top`, `bottom`, `left`, and `right` are references; `top` just references whatever one of the two is higher, and `bottom` the lower one. `left` references whatever one is to the left, and `right` the one to the right. So, for instance, `top` could be rectangle A, and `right` could be rectangle A also. I'll edit the answer to clarify about the coordinates." CreationDate="2017-08-05T12:02:56.740" UserId="6883" />
  <row Id="8274" PostId="5471" Score="0" Text="I believe you're looking for the [Nyquist rate](https://en.wikipedia.org/wiki/Nyquist_rate). Also, he is very much attempting to rebuild a signal. That signal being a radial sine wave. This is very much a case of aliasing: the digital sampling of an analog signal with fewer samples than required to reconstruct it." CreationDate="2017-08-05T15:09:50.207" UserId="2654" />
  <row Id="8275" PostId="5471" Score="0" Text="Wow I never realized the nyquist limit applied to graphics (I'm an ex-sound engineer turned programmer) :) Can I ask more details (or a link) when you refer to shifting points toward the gradient maximum?" CreationDate="2017-08-05T15:35:13.690" UserId="5978" />
  <row Id="8276" PostId="5471" Score="0" Text="@NicolBolas I meant he is just linear interpolating points not recosntructing the signal with a higher order fuction." CreationDate="2017-08-05T17:19:52.283" UserId="38" />
  <row Id="8277" PostId="5471" Score="2" Text="@scx points in space are signals. Shader shards are signals. Antialiasing is exactly the same in sound as in images... etc. A lot of signal processing applies to images. Scaling, rotating etc. same things." CreationDate="2017-08-05T17:23:59.407" UserId="38" />
  <row Id="8278" PostId="5460" Score="1" Text="Could you either include a link to the source of this formula, or define what the terms in it refer to?" CreationDate="2017-08-05T21:05:01.283" UserId="231" />
  <row Id="8279" PostId="4917" Score="0" Text="As this question is being closed as a duplicate of the [first half](https://computergraphics.stackexchange.com/questions/4737/building-view-transform-matrices), I've copied and pasted the wording from here to there, with minor adjustment to make it fit as a second half. You may need to make minor edits if I've accidentally changed your intention." CreationDate="2017-08-05T21:33:42.117" UserId="231" />
  <row Id="8280" PostId="4926" Score="0" Text="As this question is being closed as a duplicate of the [first half](https://computergraphics.stackexchange.com/questions/4737/building-view-transform-matrices), I've copied and pasted this answer from here to there, with minor adjustment to make it fit as a second half of your original answer there. You may need to make minor edits if I've accidentally changed your intention." CreationDate="2017-08-05T21:34:31.123" UserId="231" />
  <row Id="8281" PostId="5464" Score="0" Text="Yes, how to compute (/find) the coordinates." CreationDate="2017-08-06T06:18:59.177" UserId="7111" />
  <row Id="8282" PostId="5451" Score="0" Text="@DanielKareh Note though that even in programs like Photoshop you can change the color space" CreationDate="2017-08-06T08:30:24.187" UserId="7008" />
  <row Id="8283" PostId="5456" Score="0" Text="@IterAtor, sorry, I assumed you were looking to understand the rendering method rather than a translation of the real world to simulation (or as close as possible).&#xA;&#xA;As far as I can see it, the color (in PBR) is usually determined by the luminous intensity and the temperature. Chapter 4 in the Frostbite notes explains this. Note that this is perception based.&#xA;Another way to get the color is with CIE color matching and the spectral emission of the light.&#xA;&#xA;I really advice to look into the Frostbite notes, I don't think I can give you a better explanation than they delivered already." CreationDate="2017-08-06T08:45:06.650" UserId="7008" />
  <row Id="8284" PostId="5464" Score="0" Text="@AlanWolfe I'm using JavaScript in the web browser, so CPU. I had not heard of branchless min/max functions, but that seems to be the right direction to go in. Thanks." CreationDate="2017-08-06T09:27:17.337" UserId="7111" />
  <row Id="8285" PostId="5467" Score="0" Text="Thanks for the breakdown, @clabe45. I'm using JavaScript, so now I'm researching if JavaScript's Math.max and Math.min functions are branchless." CreationDate="2017-08-06T09:29:59.600" UserId="7111" />
  <row Id="8286" PostId="5470" Score="0" Text="glad loads function pointers. GLU includes utility functions for drawing. They're not comparable." CreationDate="2017-08-06T10:19:16.740" UserId="67" />
  <row Id="8288" PostId="5465" Score="0" Text="@ratchetfreak if you begin form a parametric curve and then extend it into a planar definition you end up with something that has a four sided topology, Now as time has gone forward the same problem is still there." CreationDate="2017-08-06T14:00:40.690" UserId="38" />
  <row Id="8289" PostId="5467" Score="2" Text="I'm curious why you care about it being branchless in JavaScript.  If you were doing this in a shader I can see why you'd want that (for efficiency), but branching is less of a problem on the CPU and JavaScript has much bigger perf issues than worrying about your code being branchless." CreationDate="2017-08-07T15:47:49.010" UserId="56" />
  <row Id="8290" PostId="5479" Score="0" Text="Can you give some more context for what kind of correlation you're asking about? To define correlation you need two variables (eg &quot;correlation between $x$ and $y$&quot;). So—Monte Carlo samples with correlation between what and what?" CreationDate="2017-08-07T20:26:40.733" UserId="48" />
  <row Id="8291" PostId="5479" Score="0" Text="For example in BDPT, I've read that there is path correlation between the different paths which are generated by connecting eye and light paths at different points, since the paths share vertices." CreationDate="2017-08-07T20:51:08.173" UserId="6367" />
  <row Id="8292" PostId="5479" Score="0" Text="I'm curious about this too. I *think* it's when you reuse the same random number for two different things, or derive a new &quot;random number&quot; from an old one. It starts to make patterns that using independent random numbers wouldn't have. I'd like a more precise / formal explanation though too." CreationDate="2017-08-08T17:51:37.950" UserId="56" />
  <row Id="8293" PostId="5480" Score="1" Text="Can you clarify your question? Just saying &quot;I'm confused&quot; doesn't give us much to go on. What do you understand already, what don't you understand, what specific questions do you have?" CreationDate="2017-08-08T22:22:48.520" UserId="48" />
  <row Id="8294" PostId="5480" Score="0" Text="I don't understand what the question is." CreationDate="2017-08-09T01:11:22.820" UserId="182" />
  <row Id="8295" PostId="5480" Score="0" Text="@NathanReed Question updated." CreationDate="2017-08-09T13:46:44.183" UserId="5944" />
  <row Id="8296" PostId="5480" Score="1" Text="@JulienGuertault Question updated." CreationDate="2017-08-09T13:48:20.757" UserId="5944" />
  <row Id="8297" PostId="5485" Score="0" Text="I think CGPP uses &quot;specular&quot; for sharp, mirror-like reflection due to a perfectly smooth surface, and &quot;glossy&quot; for blurred reflection due to a rough surface. I don't think that distinction has anything to do with the specular color or energy conservation. It sounds like you might be mixing this up with metallic vs dielectric materials (metallic = high specular with no diffuse; dielectric = low specular + diffuse)." CreationDate="2017-08-09T17:13:25.293" UserId="48" />
  <row Id="8298" PostId="5485" Score="0" Text="no i know that, i wrote all that on the basis of CGPP's assumption. That's one way to think of it. what you said &quot;dielectric = low specular + diffuse&quot; that's what cgpp is referring to as glossy." CreationDate="2017-08-09T18:14:20.863" UserId="6046" />
  <row Id="8299" PostId="5485" Score="2" Text="No, dielectric != glossy. Glossy just means a blurry reflection due to a rough surface. Metals can have glossy or sharp reflection, and dielectrics can have glossy or sharp reflection. They're two independent axes of variation." CreationDate="2017-08-09T18:20:32.777" UserId="48" />
  <row Id="8300" PostId="5485" Score="0" Text="I think i am bad at explaining and you are getting me wrong. I never said dielectric == glossy. What i mean is, dielectrics can be glossy if they have a specular componenet like you mentioned. I don't understand what do you mean by metals having &quot;sharp or glossy&quot; reflections. Reflection is either diffuse or specular or atleast that's what i have read in all different places. Glossiness or specular highlights is an effect of specular reflection." CreationDate="2017-08-09T19:29:56.147" UserId="6046" />
  <row Id="8301" PostId="5484" Score="0" Text="What's the difference between the two different phenomena - &quot;glossy reflection&quot; and &quot;specular reflection&quot;?" CreationDate="2017-08-10T03:56:54.830" UserId="5944" />
  <row Id="8302" PostId="5484" Score="0" Text="I found another image." CreationDate="2017-08-10T03:58:40.957" UserId="5944" />
  <row Id="8303" PostId="5485" Score="0" Text="https://en.wikipedia.org/wiki/Gloss_(optics)" CreationDate="2017-08-10T06:14:19.213" UserId="5944" />
  <row Id="8304" PostId="5485" Score="0" Text="@chaosink - ? like i am saying there is no such thing as glossy reflection. Gloss is an effect due to specular reflection. Specular reflection is the correct word. The other type is the diffuse reflection. You can think of gloss as how much the light is reflected specularly. The higher the amount the more glossy the surface." CreationDate="2017-08-10T06:54:17.083" UserId="6046" />
  <row Id="8305" PostId="5484" Score="0" Text="There **aren't** two different phenomena. That's exactly what I just explained. Your third image is no different from the original two: it's just showing what effect the glossiness parameter has on the specular highlights." CreationDate="2017-08-10T08:42:29.773" UserId="2041" />
  <row Id="8306" PostId="5489" Score="2" Text="I've only seen something like this when someone has interpreted the bottom 8 bits of a 16- or 24-bit depth buffer as an image channel. How did you generate this image?" CreationDate="2017-08-10T10:38:25.367" UserId="2041" />
  <row Id="8307" PostId="5489" Score="0" Text="@DanHulme With the buffer visualiser: https://docs.unrealengine.com/latest/INT/Engine/UI/LevelEditor/Viewports/ViewModes/#buffervisualization" CreationDate="2017-08-10T10:44:15.077" UserId="7107" />
  <row Id="8308" PostId="5484" Score="0" Text="I misunderstood your words..." CreationDate="2017-08-10T13:02:48.353" UserId="5944" />
  <row Id="8309" PostId="5489" Score="0" Text="It could be a visualisation aid, instead of showing the depth as 1 gradient you can scale it to fit inside 3 steeper gradients. This is especially helpful for objects near the camera which have less variation in terms of depth." CreationDate="2017-08-11T02:43:03.517" UserId="3073" />
  <row Id="8310" PostId="5487" Score="0" Text="I haven't looked at it in years, but it might be worth looking through [ILM's source for OpenEXR](http://openexr.org/)." CreationDate="2017-08-11T05:12:00.790" UserId="3003" />
  <row Id="8311" PostId="5491" Score="0" Text="Are you doing a first-person-shooter style camera? Most games that implement this kind of camera restrict the x-axis rotation so you can never look straight up or down (clamped at maybe 85 degrees) to avoid exactly this problem." CreationDate="2017-08-11T07:06:21.977" UserId="1937" />
  <row Id="8312" PostId="5491" Score="0" Text="@russ its a CAD camera in my case, so it does need every view angle." CreationDate="2017-08-11T07:09:15.890" UserId="5646" />
  <row Id="8313" PostId="5491" Score="0" Text="hmmm ok, then i guess storing a right vector to handle the edge cases would be one way to do it. either that or use a quaternion for the camera orientation and extract the matrix from that." CreationDate="2017-08-11T07:19:16.197" UserId="1937" />
  <row Id="8314" PostId="5491" Score="0" Text="@russ yeah i think quaternion is a nice solution. If i get how to do euler rotation on quaternion done, i will answer this question myself. THX" CreationDate="2017-08-11T07:24:06.873" UserId="5646" />
  <row Id="8315" PostId="5489" Score="0" Text="@PaulHK yeah it could be, I mean, it actually repeats more than 3 times, it repeats indefinitely!" CreationDate="2017-08-11T08:32:20.533" UserId="7107" />
  <row Id="8316" PostId="5489" Score="0" Text="the upper 2 bytes would have a resolution of 256*256 = 65536. If the depth buffer is 32 bits then that increases to 16777216. That may seem indefinite at first glance." CreationDate="2017-08-11T11:12:28.520" UserId="137" />
  <row Id="8317" PostId="5491" Score="0" Text="An alternate to quaternions is to describe the camera's rotation in spherical coordinates: one angle represents rotation around the vertical axis (0 to 360 degrees) and another angle represents how far you are looking down or up (-90 to +90).  If going that route you may be interested in this which talks about how to transform between spherical and cartesian coordinates so you can get vectors out: https://blog.demofox.org/2013/10/12/converting-to-and-from-polar-spherical-coordinates-made-easy/" CreationDate="2017-08-11T18:15:39.527" UserId="56" />
  <row Id="8321" PostId="5494" Score="1" Text="&quot;*Since were close to hitting photorealism now it seems like the only main challenge for the next decade is performance.*&quot; Because God forbid that you would want a graphical effect that photorealism doesn't cover." CreationDate="2017-08-13T17:19:26.413" UserId="2654" />
  <row Id="8322" PostId="5496" Score="1" Text="Generally our customers decide what needs to be improved. They write up bugs or blog posts, or buy competitors products when ours don't meet their needs. Marketing figures out what the problems are from user feedback and passes it on to design. Design then decides how that should translate into product features. They discuss it with engineering who decides how it will be implemented." CreationDate="2017-08-13T18:52:28.907" UserId="3003" />
  <row Id="8323" PostId="5496" Score="0" Text="At this point in time what are some of the common issues if you don't mind me asking?" CreationDate="2017-08-13T19:14:53.880" UserId="5256" />
  <row Id="8324" PostId="5496" Score="0" Text="Unfortunately, I can't go into details about not-yet-released features. But as mentioned above, we are in the process of moving from OpenGL to Metal, and we're always trying to take advantage of new hardware features." CreationDate="2017-08-13T22:17:46.917" UserId="3003" />
  <row Id="8325" PostId="5495" Score="0" Text="i can't get why this formula will change the bad situation. If w equals l, then c will still be zero vector? Because when these two vectors are parallel, there will only parallel part, and no orthogonal part." CreationDate="2017-08-14T06:41:19.960" UserId="5646" />
  <row Id="8326" PostId="5495" Score="0" Text="Yes that's why i said, you have to check for this. If that's the case then you'll have to take another arbitrary vector. For example if &quot;j&quot; is the world up and it's parallel with look at, take &quot;i' or &quot;k&quot;. What i wanted to point out is, `world up` and `front` vector won't always be orthogonal. To make an orthogonal vector close to `world up` you have to use that formula" CreationDate="2017-08-14T07:23:56.220" UserId="6046" />
  <row Id="8327" PostId="5495" Score="0" Text="if just switch j to i, the the RIGHT vector will probably change a lot. And cross(front, worldUp) is used to compute right vector, the worldUp don't need to be orthogonal to front, just not parallel." CreationDate="2017-08-14T07:29:23.927" UserId="5646" />
  <row Id="8328" PostId="5495" Score="0" Text="i think the world up needs to be orthogonal to the front, else you won't have an orthogonal basis for the camera?" CreationDate="2017-08-14T07:41:52.023" UserId="6046" />
  <row Id="8329" PostId="5495" Score="0" Text="for usual, figure out RIGHT by cross(front, woldUp) first, and then get real up by cross(front, right). That's why it's called world up but not camera up. Your idea make sense maybe, but i think it's not related to the problem i met here." CreationDate="2017-08-14T07:45:21.660" UserId="5646" />
  <row Id="8330" PostId="5495" Score="0" Text="Well from what i have read in books either you choose an arbitrary vector to solve this issue or store the starting orientation in a matrix then apply a rotation matrix to change the orientation." CreationDate="2017-08-14T08:10:41.437" UserId="6046" />
  <row Id="8331" PostId="5497" Score="0" Text="Without even a stack trace of the crash it's unlikely that anyone will be able to help you." CreationDate="2017-08-14T10:34:56.990" UserId="2041" />
  <row Id="8332" PostId="5497" Score="0" Text="You are right, see edit." CreationDate="2017-08-14T11:28:22.650" UserId="7151" />
  <row Id="8333" PostId="5497" Score="0" Text="Just checking: do you have the latest graphics drivers installed? Some googling suggests that this error is common with very long-running GPU compute jobs, and it may be (speculating) that there's some other timeout or bug inside the driver. Breaking up your job into smaller pieces may help (and would avoid the TDR problem too)." CreationDate="2017-08-14T21:13:46.577" UserId="48" />
  <row Id="8334" PostId="5498" Score="0" Text="Would like to add that normal map is about bump mapping, it wouldn't be &quot;lighting&quot; the surface. Lighting equation does that and so check that if you aren't getting them lit." CreationDate="2017-08-15T06:29:18.053" UserId="6938" />
  <row Id="8335" PostId="5500" Score="0" Text="Think of it like this which one looks more sharp one that is blurred more versus one that is blurred less." CreationDate="2017-08-15T07:47:49.067" UserId="38" />
  <row Id="8336" PostId="5497" Score="0" Text="Yes, I have the latest driver installed. I thought about breaking it up into smaller parts but it cannot be easily done. Also, I would really like to solve this problem for its own sake. I have it on two computers using two different Nvidia cards. TDR is deactivated on both and I always get error code 2. I did a Google search myself and tried everything I found - nothing works (setting energy options to maximum, playing around with the other TDR registry keys). I would like to have the computation run for months in the end." CreationDate="2017-08-15T07:59:47.700" UserId="7151" />
  <row Id="8337" PostId="5499" Score="0" Text="Thank you! I knew derivatives were the answer, just couldn't figure it out by myself. The vector way is really elegant and clever." CreationDate="2017-08-15T14:26:26.903" UserId="2064" />
  <row Id="8338" PostId="5503" Score="0" Text="Yes, I thought of this after posting the question. And I'm trying to calculate $k_s$." CreationDate="2017-08-16T09:20:58.933" UserId="5944" />
  <row Id="8339" PostId="5503" Score="0" Text="Wow, the link you give is amazing!" CreationDate="2017-08-16T09:22:03.830" UserId="5944" />
  <row Id="8340" PostId="5501" Score="0" Text="Which filters do most monitors use?" CreationDate="2017-08-17T01:09:59.227" UserId="7154" />
  <row Id="8341" PostId="5496" Score="0" Text="Which software are you making?" CreationDate="2017-08-17T12:48:22.863" UserId="110" />
  <row Id="8342" PostId="5496" Score="1" Text="I'd rather not say publicly, as I like to keep my personal and work lives a little separate." CreationDate="2017-08-18T01:00:02.323" UserId="3003" />
  <row Id="8343" PostId="5506" Score="2" Text="What kind of laptop does not have a GPU?" CreationDate="2017-08-18T04:33:08.273" UserId="38" />
  <row Id="8344" PostId="5506" Score="3" Text="You need to profile your code to see where the problem is. Drawing 10,000 points should be no problem for even a low-end GPU. Are you actually uploading the data on every draw call? If so, don't do that. You're using `GL_STATIC_DRAW`, so it looks like your data is not changing. If that's the case, upload it once and draw it on every frame." CreationDate="2017-08-18T05:15:08.030" UserId="3003" />
  <row Id="8345" PostId="5506" Score="2" Text="Are you creating a new vertex buffer each frame? If so it would be more efficient to create it only once, then update its content when it changes. Nowadays, even a low end laptop with an integrated chipset can draw hundreds of thousands of points at a good framerate." CreationDate="2017-08-18T07:49:07.503" UserId="182" />
  <row Id="8346" PostId="5506" Score="0" Text="@joojaa I assume he means it has an Intel integrated GPU instead of a beefier off-chip one." CreationDate="2017-08-18T08:27:09.030" UserId="2041" />
  <row Id="8347" PostId="5498" Score="0" Text="Couldn't you get the tangent by just doing a cross product between the normal and the main axis of your spherical coordinate system?" CreationDate="2017-08-18T08:57:32.860" UserId="1937" />
  <row Id="8348" PostId="5494" Score="0" Text="Disney's rendering team recently built a brand-new path tracing renderer from the ground up to make 'Big Hero 6' while the artists were already well underway animating the movie. See here https://www.disneyanimation.com/technology/innovations/hyperion" CreationDate="2017-08-18T09:19:43.433" UserId="1937" />
  <row Id="8349" PostId="5506" Score="1" Text="@DanHulme Thats my point its still a GPU! You will have hard timefinding laptops with really no GPU." CreationDate="2017-08-18T09:56:26.003" UserId="38" />
  <row Id="8350" PostId="5498" Score="0" Text="@russ Yes, Nathan proposed exactly that on the accepted answer." CreationDate="2017-08-18T11:47:54.863" UserId="2064" />
  <row Id="8353" PostId="5510" Score="0" Text="The image is loaded with Java's ImageIO.read, and it'll correctly load the transparency. I've attached pictures." CreationDate="2017-08-19T11:46:28.083" UserId="7177" />
  <row Id="8355" PostId="5484" Score="0" Text="_The amount or brightness of the specular reflection_ - this is also called metalness, right?" CreationDate="2017-08-21T14:27:55.467" UserId="4958" />
  <row Id="8356" PostId="5484" Score="0" Text="@narthex No, that's something different. You should ask a new question." CreationDate="2017-08-22T07:36:59.323" UserId="2041" />
  <row Id="8357" PostId="5498" Score="0" Text="whoops so he did, i didn't read that far down ;)" CreationDate="2017-08-23T05:13:09.937" UserId="1937" />
  <row Id="8358" PostId="5520" Score="0" Text="Without reading the code and just from the picture, my intuition would be to double check the integration. It looks like too much light is absorbed along the ray. I would first check that in the first implementation the color stays the same when changing the number of steps (just to make sure it's a good reference image), then I would double check the absorption equation used in the MT implementation." CreationDate="2017-08-23T07:22:37.037" UserId="182" />
  <row Id="8361" PostId="5523" Score="0" Text="Nevermind. all 3d patch solutions are done by sphere tracking." CreationDate="2017-08-24T15:53:26.563" UserId="7199" />
  <row Id="8362" PostId="5523" Score="0" Text="it just significantly increases performance and precision when the patch is simpler. to a point where a patch of seamess heavensines gets you very far in a few iterations:" CreationDate="2017-08-24T15:56:34.310" UserId="7199" />
  <row Id="8363" PostId="5523" Score="0" Text="https://www.shadertoy.com/view/MtsXzl" CreationDate="2017-08-24T15:56:48.017" UserId="7199" />
  <row Id="8365" PostId="5528" Score="3" Text="[Bilinear interpolation](http://reedbeta.com/blog/quadrilateral-interpolation-part-2/)?" CreationDate="2017-08-25T15:52:36.583" UserId="106" />
  <row Id="8366" PostId="5528" Score="0" Text="It does look like it from that link. In fact it's the first term that popped into my head but the initial Google hits seemed to be mostly about something that looked different to me." CreationDate="2017-08-25T16:11:49.413" UserId="1647" />
  <row Id="8367" PostId="5506" Score="0" Text="@joojaa &quot;GPU&quot; is very common colloquial nomenclature for a dedicated graphics card, excluding integrated graphics processors. Don't be pedantic. :P" CreationDate="2017-08-25T17:28:24.590" UserId="6145" />
  <row Id="8368" PostId="5506" Score="0" Text="@Dan well, most GPUs on laptops tend to be integrated to the motherboard. So does that mean laptops do not have them?" CreationDate="2017-08-25T18:01:34.797" UserId="38" />
  <row Id="8369" PostId="5506" Score="0" Text="@joojaa It's vague, depends on the model of the GPU whether or not I would consider it worthy of being called a &quot;GPU&quot; in such a colloquial way as to imply that it e.g. has drivers which support OpenGL &gt; 1.1." CreationDate="2017-08-25T18:17:31.340" UserId="6145" />
  <row Id="8370" PostId="5529" Score="0" Text="I'm not sure if anti-aliasing techniques would work for me. As far as I understood AA it aims to smooth these so called jaggies, so it works small-scale. But that's not my concern. My goal is to eliminate this 'large-scale' blocky appearance so that a great amount of neighboring pixels will change their color in order to manipulate the big picture." CreationDate="2017-08-25T21:58:52.317" UserId="5406" />
  <row Id="8371" PostId="5524" Score="0" Text="Would blurring the image do what you want? It would produce a gradual transition between colors. Or do you want to retain the sharp edge between the colors, but remove the noisy small-scale details?" CreationDate="2017-08-26T00:19:39.417" UserId="48" />
  <row Id="8372" PostId="5520" Score="0" Text="Thanks for your comment. Still not working and I do not understand why. I sample a point uniformly along the ray, instead of dividing it to segment, and the pdf is simply 1/dist. But for some reason it doesn't converge to same result." CreationDate="2017-08-26T09:21:48.233" UserId="6041" />
  <row Id="8373" PostId="5524" Score="0" Text="It's more the latter that I want. E.g., a highest 'degree of smoothing' could produce proper circles. So what I want is the boundaries between the different colored zones to be more straight or linear, respectively. Hope this helps clarify. If not, I will probably have to sketch it..." CreationDate="2017-08-26T10:04:15.380" UserId="5406" />
  <row Id="8375" PostId="5524" Score="0" Text="If you want to preserve the discrete zone values while smoothing their boundaries, you could look into morphological opening and closing (faster and easier to implement) or level set curvature flow (more expensive but will converge to circles)." CreationDate="2017-08-27T18:23:24.117" UserId="106" />
  <row Id="8376" PostId="5529" Score="0" Text="@Muad Gaussian blur?" CreationDate="2017-08-27T19:01:48.067" UserId="38" />
  <row Id="8379" PostId="5536" Score="0" Text="Welcome to the Computer Graphics Stack Exchange site! I am not familiar with Vulkan but it seems using an UBO would be more efficient in your case (you can try to compare the performances of both approaches in a sample program). Which graphic APIs are you familiar with? Also you might be interested by this GDC 2012 presentation: [Don't Throw it all Away: Efficient Buffer Management](http://gamedevs.org/uploads/efficient-buffer-management.pdf)" CreationDate="2017-08-28T13:29:55.270" UserId="110" />
  <row Id="8380" PostId="5536" Score="0" Text="From [Vulkan documentation](https://www.khronos.org/registry/vulkan/specs/1.0/html/vkspec.html#vkCmdPushConstants): while an UBO allocates a block of video memory on the GPU (that you can update at a later timing), the Push Constant does not use video memory (which is why it must be provided during every draw/compute call, or else the shader would not know which value to use). I guess it is stored in an other short-term, fast storage area on the GPU, though the details might depend on your GPU vendor." CreationDate="2017-08-28T13:44:31.957" UserId="110" />
  <row Id="8384" PostId="5538" Score="0" Text="You might get better answers if you cut down your code to the minimum necessary to understand the problem. There's a lot here to do with rotating and &quot;falling&quot; that seems peripheral to the question, and nobody is going to read this much source code. What do you mean by the &quot;three closest sides&quot;? Do you mean the three visible faces? These aren't necessarily closer because the projection biases to +z." CreationDate="2017-08-29T13:36:19.227" UserId="2041" />
  <row Id="8385" PostId="5538" Score="0" Text="Cropped unnecessary code. Yes, I mean 3 visible faces. How to determine them?" CreationDate="2017-08-29T13:52:35.583" UserId="7220" />
  <row Id="8387" PostId="5524" Score="0" Text="a reasonable approach would be blurring and then limit the amount of colours using k-means or a pallete" CreationDate="2017-08-30T06:21:11.183" UserId="5703" />
  <row Id="8388" PostId="5518" Score="0" Text="Did you produce the black dot from the yellow dot image using thresholding? Looks pretty spot on to me? Or is that what you want to achieve? You could consider some sort of region growing technique? Like active contours?" CreationDate="2017-08-30T06:34:34.017" UserId="113" />
  <row Id="8389" PostId="5518" Score="0" Text="I achieved this by manual thresholding-median filtering-noise reduction on ImageJ...I have thousands of these images and would like to get a generalized image processing procedure to implement a batch algorithm" CreationDate="2017-08-30T06:54:03.630" UserId="7185" />
  <row Id="8390" PostId="5538" Score="0" Text="The visible faces are the faces that have cos(theta) &lt; 0 where theta is the angle between the faces normal and cameras direction. The dot product. You can use the cross product to get the faces normal. That doesn't tell you anything about how close the faces are to the cameras plane. You can use the GJK algorithm to find the minimum distance between the plane and cube." CreationDate="2017-08-30T07:08:09.553" UserId="113" />
  <row Id="8391" PostId="5538" Score="0" Text="What is your goal? Do you want to draw the visible faces only?" CreationDate="2017-08-30T07:26:15.263" UserId="7028" />
  <row Id="8393" PostId="5538" Score="0" Text="Yes. That is the goal. Why to draw invisible faces with no transparancy?" CreationDate="2017-08-30T07:45:38.703" UserId="7220" />
  <row Id="8394" PostId="5542" Score="0" Text="Thanks! I had seen that along the way, but thought it was for something else. One question though, is a stencil buffer all or nothing, or would it be possible to have some parts be more/less visible based on ie: distance from light source, overlapping light sources." CreationDate="2017-08-30T07:50:13.947" UserId="7202" />
  <row Id="8395" PostId="5518" Score="0" Text="Theres a lot of papers on automatically thresholding images. A quick look at the images histogram should make it obvious where you should threshold." CreationDate="2017-08-30T08:14:14.963" UserId="113" />
  <row Id="8396" PostId="5542" Score="1" Text="The value (in case of a 8bit stencil) can be anything between 0-255. There are also glStencilOp's like `GL_INCR` and `GL_DECR`. If 0=hidden, 1=half-visible, 2=visible, then you can draw a black quad on the screen with `GL_EQUAL` and `0`, and a half-transparent gray quad with `GL_EQUAL` and `1`. If you would like to draw continuous shadow gradients, then you have to use a shadow texture. But stencil buffers are still really useful even if you have a shadow texture, because the stencil test is much cheaper than a discard in the fragment shader" CreationDate="2017-08-30T09:00:48.577" UserId="7028" />
  <row Id="8397" PostId="5544" Score="0" Text="Thanks for your answer. But this solution is suitable for perspective projection. I'am using parallel projection." CreationDate="2017-08-30T12:59:11.527" UserId="7220" />
  <row Id="8406" PostId="5544" Score="0" Text="If I change this line `var P = mat4.perspective(mat4.create(), Math.PI/2.0, width/height, 0.1, 100);` in your code snippet to this line: `var P = mat4.ortho(mat4.create(), -5,5,-5,5,-5,5);` so I have the same result which I originally had - faces blink." CreationDate="2017-08-30T14:26:05.240" UserId="7220" />
  <row Id="8409" PostId="5453" Score="0" Text="The Colour of a 60w light source can be any colour, it depends on its wavelengths. For a white colour light source &quot;it cannot consist solely of the green light to which the eye's image-forming visual photoreceptors are most sensitive, but must include a generous mixture of red and blue wavelengths, to which they are much less sensitive.&quot;" CreationDate="2017-08-30T20:56:05.283" UserId="6041" />
  <row Id="8410" PostId="5542" Score="0" Text="Hm, so sounds like I definitely want a stencil buffer to mark off everything that's not lit up at all. However I would definitely like to be able to have gradient shadows or possibly even tinted shadows for various effects. So then I'd need a texture buffer for that? Or a frame buffer? Or are those the same thing?" CreationDate="2017-08-31T00:27:05.500" UserId="7202" />
  <row Id="8412" PostId="5542" Score="0" Text="If you render to texture, you will use a frame buffer with only one color attachment. A framebuffer is an object, which contains depth+stencil buffer and color attachment(s). You already use the default framebuffer to render to the screen. But if you would like to render offscreen (to a texture) you have to create a new one for it. The texture is the color attachment in that case. I advise you to implement it with stencil buffers only, and if everything looks as intended, start experimenting with textures" CreationDate="2017-08-31T09:10:01.070" UserId="7028" />
  <row Id="8414" PostId="5520" Score="0" Text="@JulienGuertault Thanks. I edited the question to explain the general issue." CreationDate="2017-08-31T13:13:44.663" UserId="6041" />
  <row Id="8415" PostId="5540" Score="0" Text="this is somewhat the same [question or atleast the answer is the same as this](https://computergraphics.stackexchange.com/questions/1887/how-to-implement-a-realtime-2d-light-renderer-with-fog-colored-light-on-the-gpu). the approach is essentially the inverse of the answer you got. Apparently the examples have undergone link rot so i need to see if i can find an alternate solution for this." CreationDate="2017-08-31T17:49:49.197" UserId="38" />
  <row Id="8416" PostId="5520" Score="0" Text="The comment was directed toward people willing to propose answers. But thanks, it makes the question more interesting too. :)" CreationDate="2017-09-01T00:29:41.157" UserId="182" />
  <row Id="8417" PostId="5539" Score="2" Text="It sounds vaguely like using [signed distance fields](https://www.youtube.com/watch?v=s8nFqwOho-s) to generate geometry. However I've never heard of &quot;folding&quot; in this context. I would guess it's a specific technique invented by the guy who did the talk, and if he hasn't published any more details then it may not be feasible to reproduce his results closely." CreationDate="2017-09-01T04:37:30.143" UserId="48" />
  <row Id="8418" PostId="5547" Score="0" Text="Are you able to share your code?" CreationDate="2017-09-01T06:43:21.457" UserId="231" />
  <row Id="8419" PostId="5547" Score="0" Text="I will try to make it readable later and then share it (or at least the important parts). It is (or should be) what they do in Physically Based Rendering Third (p.324), if by chance you have that book." CreationDate="2017-09-01T07:57:55.250" UserId="7008" />
  <row Id="8421" PostId="5547" Score="0" Text="No the result is probably not grayscale. What happens is there is a very sophisticated white balance in the system. I mean put tinted glasses on and pretty soon you nolonger see the tint." CreationDate="2017-09-01T17:19:56.747" UserId="38" />
  <row Id="8422" PostId="5540" Score="0" Text="With one lightsource they may be similar but that approach wouldn't work for multiple light sources, which is something I want to be able to do." CreationDate="2017-09-01T18:22:14.967" UserId="7202" />
  <row Id="8423" PostId="5549" Score="0" Text="But what exactly is the smoothstep doing? How will it change the outcome?" CreationDate="2017-09-01T19:25:41.983" UserId="6546" />
  <row Id="8424" PostId="5549" Score="1" Text="Smoothstep is just a hermite curve (a 3rd degree polynomial) that smoothly varies between 0 and 1. Rather than a straight line, it ramps up slowly at first, then faster, then eases up at the end. So it looks sort of like an integral symbol if you map it from 0 to 1. Like this: ∫. You can read [the wikipedia article](https://en.wikipedia.org/wiki/Smoothstep), which has a graph." CreationDate="2017-09-01T20:38:00.733" UserId="3003" />
  <row Id="8425" PostId="5540" Score="0" Text="It would work with as many light sources as you like. Just render multiple times." CreationDate="2017-09-01T21:57:46.740" UserId="38" />
  <row Id="8426" PostId="5557" Score="4" Text="It looks to me like the RGB data from the first image is being interpreted as Y'CbCr data in the second image. Since the Y' value goes from -0.5 to +0.5, 0.0 is a greenish color rather than black." CreationDate="2017-09-02T01:59:57.313" UserId="3003" />
  <row Id="8427" PostId="5542" Score="0" Text="Manged to get the stencil version up and running, everything looks pretty good. Added checks for winding on the line segments so you can see the object casting the shadow as well, which makes for a nice effect. How do I tell OpenGL which frame buffer to draw to? Do I need a different shader program for that, or do I just use the same one like for stencil drawing? And for stencil drawing, would it be more efficient to use a specific program that doesn't take color data for that draw call, or does that not matter?" CreationDate="2017-09-02T03:59:07.033" UserId="7202" />
  <row Id="8428" PostId="5542" Score="0" Text="You don't have to change the shaders if you render to texture instead of &quot;directly&quot; to the screen. Just tell OpenGL before drawing that instead of the default framebuffer, use the one you created. Read a tutorial about rendering to textures, and If you have further questions about that, please open a new question." CreationDate="2017-09-02T08:58:02.757" UserId="7028" />
  <row Id="8429" PostId="5547" Score="0" Text="@joojaa thanks but I was looking more for a proof, i.e. someone who has a working spectrum-&gt;rgb converter who would calculate the rgb response you get for using an $ior = 1.52 \iff reflection intensity = 0.04257999496$ on the wavelengths $390-780nm$" CreationDate="2017-09-02T13:46:49.693" UserId="7008" />
  <row Id="8432" PostId="5513" Score="0" Text="Why AB instead of BA?  Presumably because the polygon is being defined in anticlockwise order and so all edges are labelled accordingly. In fact, it is important to keep things consistent, especially when creating the edge equations. This is to avoid 'holes' or 'double filling' of pixels when handling &quot;tie-break&quot; cases when the centre of a pixel lies 'exactly' on an edge." CreationDate="2017-09-04T08:31:27.517" UserId="209" />
  <row Id="8434" PostId="5567" Score="0" Text="That's a good suggestion for clarity. Even so, if they misuse terms or write obscurely, it'll hurt their credibility in the paper, so it's wise to check even if they include the diagram." CreationDate="2017-09-05T07:46:21.357" UserId="2041" />
  <row Id="8435" PostId="5567" Score="0" Text="I see your point" CreationDate="2017-09-05T08:18:29.610" UserId="7250" />
  <row Id="8436" PostId="5539" Score="0" Text="I have seen the TED talk now, and I really want to know how it is done. This must be a known techinque?" CreationDate="2017-09-05T08:20:13.873" UserId="7250" />
  <row Id="8437" PostId="5571" Score="2" Text="I guess the author actually computes *inverse* orientation in `orientation` because he calls this `orientation` in `Camera::view` and then `Camera::matrix` without transpose or inversion of it. And because `glfwGetCursorPos` will give you flipped Y coordinates, the `orientation` does compute the inverse of orientation coincidentally. However X coordinates are not flipped, which should cause problem but I cannot explain why it does not." CreationDate="2017-09-05T17:02:46.930" UserId="120" />
  <row Id="8438" PostId="5572" Score="0" Text="Great answer, thank you. I work on games but using Unity so no source code :( I was curious about what goes on under the hood. Sounds like just as much of a PITA as I was imagining with the older APIs" CreationDate="2017-09-06T01:06:21.503" UserId="1937" />
  <row Id="8439" PostId="5476" Score="0" Text="This does not provide an answer to the question. Once you have sufficient [reputation](https://computergraphics.stackexchange.com/help/whats-reputation) you will be able to [comment on any post](https://computergraphics.stackexchange.com/help/privileges/comment); instead, [provide answers that don't require clarification from the asker](https://meta.stackexchange.com/questions/214173/why-do-i-need-50-reputation-to-comment-what-can-i-do-instead). - [From Review](/review/low-quality-posts/3830)" CreationDate="2017-09-06T01:14:51.180" UserId="310" />
  <row Id="8441" PostId="5568" Score="0" Text="Using a transform matrix for UVs is super clever! I'll have to refactor my system a little, since I was (happily) using only 1 uv, but your answer seems like a good solution. Thank you!" CreationDate="2017-09-06T01:33:06.573" UserId="5978" />
  <row Id="8445" PostId="5575" Score="2" Text="The code make a lot of sense now that I've read your answer. I had a hunch that this is it but couldn't quite figure it out. Thank you!&#xA;&#xA;While looking into the issue I have also stumbled across this article that explains in greater detail what you wrote: https://www.3dgep.com/understanding-the-view-matrix/. Maybe someone else will find it useful as well." CreationDate="2017-09-06T16:02:37.553" UserId="7255" />
  <row Id="8448" PostId="5578" Score="1" Text="The incoming direction woW(coming from eye) is filipped as in pbrt &quot;the incoming and outgoing vectors direction are outward facing, after being transformed into local coordinate system, at the surface&quot;. It's just pbrt convention to ease the calculation." CreationDate="2017-09-07T03:54:43.683" UserId="6041" />
  <row Id="8449" PostId="5578" Score="0" Text="@ali Please post that as an answer. Comments are for seeking clarification, not for answering the question." CreationDate="2017-09-07T08:44:10.873" UserId="2041" />
  <row Id="8456" PostId="5585" Score="0" Text="I hadn't heard the term light tracing before.  For other folks who haven't either, it seems to just be path tracing from the light towards the screen, vs the normal screen towards light.  Similar to photon mapping, but without the photon?" CreationDate="2017-09-09T00:24:43.157" UserId="56" />
  <row Id="8457" PostId="5585" Score="0" Text="Yes it is forward ray tracing as opposed to backward ray tracing. It traces importance, rather than radiance, from the light source, bouncing it around the scene and connecting each vertex to camera." CreationDate="2017-09-09T00:28:23.307" UserId="6041" />
  <row Id="8458" PostId="5585" Score="2" Text="Have you considered using fresnel to figure out how much light is reflected vs transmitted, then using that value as a percent chance to do diffuse or specular lighting?  If specular, since you are asking about mirror like specular only, you just reflect the vector against the surface normal.  If that is making sense, if wanting to go beyond mirror like specular reflection, you could use eg a microfacet specular BRDF instead later." CreationDate="2017-09-09T00:36:18.123" UserId="56" />
  <row Id="8459" PostId="5585" Score="0" Text="Thanks but I guess it's impossible to simulate this sort of path in forward ray tracing; since an incoming ray from light to a specular surface point it has zero probability of going out towards the camera as it has a delta function." CreationDate="2017-09-09T01:43:43.203" UserId="6041" />
  <row Id="8460" PostId="5585" Score="2" Text="Darn, hrm. Maybe the problem is you can't have pure mirror specular reflection, but need a specular lobe that isn't a delta function spike." CreationDate="2017-09-09T03:42:22.727" UserId="56" />
  <row Id="8461" PostId="5587" Score="0" Text="How did you make the diagram?" CreationDate="2017-09-09T18:34:01.570" UserId="7272" />
  <row Id="8462" PostId="5587" Score="0" Text="I used Processing, good for quick and dirty sketches like this :)" CreationDate="2017-09-10T10:33:54.303" UserId="1937" />
  <row Id="8463" PostId="5587" Score="0" Text="Athough nearly anything could have worked. You could have used almost anykind of toolset to do this in less than 5 minutes. You could have used a editor like inkscape or illustrator, you could have written postscript, svg, TeX or javascript in a browser even webGL in that time." CreationDate="2017-09-10T13:31:55.677" UserId="38" />
  <row Id="8464" PostId="5585" Score="2" Text="This will only work if your camera is not a pinhole but simulates a non-zero sized aperture and a film. Then you can continue the reflect light path through the aperture and record it once it hits the film plane." CreationDate="2017-09-11T06:30:36.190" UserId="4546" />
  <row Id="8466" PostId="5594" Score="1" Text="So  what is your question?" CreationDate="2017-09-11T15:20:39.057" UserId="209" />
  <row Id="8467" PostId="5594" Score="0" Text="Mainly, I'm wondering if I'm missing something obvious in this scenario. Also, I'm quite surprised that cheating way looks much better than proper way." CreationDate="2017-09-11T16:10:43.727" UserId="7282" />
  <row Id="8470" PostId="5600" Score="0" Text="You should probably include what your using as reference as its hard to tell." CreationDate="2017-09-13T04:42:12.373" UserId="38" />
  <row Id="8474" PostId="5600" Score="0" Text="In my experience, i,j,k typically imply mutually orthogonal unit *vectors*." CreationDate="2017-09-13T12:38:58.807" UserId="209" />
  <row Id="8475" PostId="5600" Score="0" Text="@SimonF they do not have to be though there areplenty of reasons to transform them." CreationDate="2017-09-13T13:09:51.340" UserId="38" />
  <row Id="8477" PostId="2003" Score="0" Text="I'm looking for the same thing. Did you get what you wanted?" CreationDate="2017-09-13T14:47:50.217" UserId="7299" />
  <row Id="8478" PostId="5301" Score="1" Text="I´d treat the cylinder as a &quot;light source&quot;, putting its geometry as a shader uniform and test fragment position against the cylinder, clipping fragments inside. Not sure if it is feasible for you or even interesting but in case you or others are stuck I´m just saying there is a way :-)" CreationDate="2017-09-13T18:08:29.897" UserId="3041" />
  <row Id="8479" PostId="5301" Score="1" Text="FYI: you have not enabled depth test" CreationDate="2017-09-13T18:10:15.700" UserId="3041" />
  <row Id="8485" PostId="5607" Score="0" Text="That could work. IMO it will require too much extra work and maybe the performance won't be that great! Reversed depth FTW :3" CreationDate="2017-09-14T13:53:26.893" UserId="7107" />
  <row Id="8486" PostId="5600" Score="0" Text="I am thinking mainly about 3d space for games. I have the feeling that the basis vectors are the vectors which show the orientation of the space. On wikipedia, the 2 images on the right seems to show this. But still, I am not sure that it is the main purpose of basis vectors. Also, does orthogonal mean at 90 degrees? As an example, in 2d, does (1,0) and (0,1) are orthogonals? And (0.7, 0) and (0, 1) are not? The wiki page : https://en.wikipedia.org/wiki/Basis_(linear_algebra)" CreationDate="2017-09-14T23:18:29.780" UserId="7291" />
  <row Id="8490" PostId="5619" Score="0" Text="Also you can get near n, or atleast less than n log n, with bucket sort and it is trivial to do. Even better if you begin with a BSP/occ-tree you can even get it to be less than N as you get cameara occlusion culling for free." CreationDate="2017-09-15T13:03:41.230" UserId="38" />
  <row Id="8491" PostId="5619" Score="0" Text="BSP and octrees are kinda overrated for real-time graphics work. All the pointer chasing required is detrimental for cache performance." CreationDate="2017-09-15T13:13:26.673" UserId="137" />
  <row Id="8492" PostId="5619" Score="0" Text="Not necceserily if your buckets are bigish then you divide the work, but still have the benfits of lists with very few jumps" CreationDate="2017-09-15T13:25:01.780" UserId="38" />
  <row Id="8493" PostId="5619" Score="0" Text="For me, the &quot;ubershader&quot; is the magic sauce here and is a good point. Thanks!" CreationDate="2017-09-15T13:42:43.687" UserId="56" />
  <row Id="8495" PostId="5619" Score="1" Text="If you have a huge amount of particles you can also sort them on the GPU with a compute shader using bitonic sort. It's n log n (i think) but parallelized so super fast. There's a working implementation in the old DirectX 2010 SDK." CreationDate="2017-09-16T11:54:40.797" UserId="1937" />
  <row Id="8496" PostId="5625" Score="1" Text="Personally I don't think you need a better explanation for Raytracing, you need one for rasterization because it's somewhat opposite to what happens naturally. Realistically speaking Raytracing is how we see objects, Just instead of rays coming into our eyes we shoot them from the eyes outward ( if we are talking about backward raytracing) and apply all the physics of light etc. Hence one of the reason we get so realistic images" CreationDate="2017-09-16T16:36:02.807" UserId="6046" />
  <row Id="8497" PostId="5625" Score="6" Text="Use pictures. Thing is raytracing may be one of the simplest concepts to get. With good enough sequence of pictures it becomes clear enough." CreationDate="2017-09-16T17:42:29.927" UserId="38" />
  <row Id="8498" PostId="5630" Score="0" Text="My problem solved by disabling texture2d before drawing colored triangles. Also I did use GL.Finish() after reading pixels." CreationDate="2017-09-17T10:21:43.073" UserId="7298" />
  <row Id="8499" PostId="5628" Score="0" Text="If I understand correctly what you are trying to do, then, it is possible to do with FBO. Just create rendertexture instead of renderbuffer for color attachment (and call appropriate bind function). Then you'll be able to use this render texture as shader input." CreationDate="2017-09-18T10:47:51.960" UserId="43" />
  <row Id="8500" PostId="5594" Score="0" Text="What if you try to normalize also before passing your calculated normals to glNormalXXX(). As documentation says: &quot;normal vectors are normalized to unit length AFTER transformation and before lighting&quot;. So in cases when interpolated normal has really small length (may happen during interpolation if C and K normals look in opposite directions) it may be distorted even more after transformation. This may happen in both interpolation schemes, but could be that effect is more visible if using quadratic interpolation" CreationDate="2017-09-18T11:02:10.743" UserId="43" />
  <row Id="8501" PostId="5628" Score="0" Text="No i was thinking that i could use the FBO to display the texture image on the window. Like i was thinking if the texture image is already set in the compute shader, then the FBO it's attached to would just display it in the window if i make it default, but i don't think there's any command for that." CreationDate="2017-09-18T13:10:48.140" UserId="6046" />
  <row Id="8503" PostId="5628" Score="0" Text="I guess internally opengl keeps a texture buffer for the default framebuffer. It's just the API does not allow you to consider it as a texture, so a draw/copy operation is needed." CreationDate="2017-09-19T07:28:56.473" UserId="3073" />
  <row Id="8504" PostId="5632" Score="2" Text="Hello, welcome to CGSE. It's difficult to infer the context of your question when it is just referring to someone's answer. Could you please quote and/or link the relevant discussion in your question?" CreationDate="2017-09-19T08:39:26.140" UserId="182" />
  <row Id="8505" PostId="5632" Score="0" Text="please kindly click this link:  https://computergraphics.stackexchange.com/questions/1718/what-is-the-simplest-way-to-compute-principal-curvature-for-a-mesh-triangle/1719#1719?newreg=1059615fd16d40dcbcbc73980ae075de" CreationDate="2017-09-19T10:52:46.210" UserId="7335" />
  <row Id="8506" PostId="5633" Score="0" Text="Because the question is old  I´ll ask you instead :-) For what I understand simply having adjencency information for each vertex was not good enough for russ?" CreationDate="2017-09-19T15:34:17.740" UserId="3041" />
  <row Id="8509" PostId="5628" Score="0" Text="Yes that's what i was thinking. I thought opengl provided the functionality to use the FBO as a default one to display the texture directly attached but it doesn't i guess. They provided FBO for offscreen rendering only i guess for help with things like getting the reflection texture etc." CreationDate="2017-09-19T19:07:02.510" UserId="6046" />
  <row Id="8510" PostId="5634" Score="0" Text="Thank you! (I solved this a while ago and posted the solution just now when I saw your comment.)" CreationDate="2017-09-20T11:41:34.707" UserId="4714" />
  <row Id="8511" PostId="5632" Score="1" Text="I've extended my original answer with more explanation about the derivation of the formula. I think this question can be closed." CreationDate="2017-09-21T05:34:08.210" UserId="48" />
  <row Id="8513" PostId="5639" Score="0" Text="Thanks this is very helpful! Great point about the non-object state values too! I do agree that giving the client more responsibility over the rendering process might be beneficial as they would be able to adapt to each situation more properly. But how would I handle things like depth sorting without having one system that manages/contains all the renderable instances? (the throw-all-things-at-a-wall approach)" CreationDate="2017-09-21T09:14:00.470" UserId="5960" />
  <row Id="8515" PostId="5638" Score="0" Text="I'll make an answer with details specific to this question after i get it working, but from what i've heard, the answer is to use a high pass filter.  More info regarding high pass filters in graphical applications here: https://www.gamasutra.com/view/feature/3073/the_power_of_the_high_pass_filter.php?print=1" CreationDate="2017-09-22T20:07:46.780" UserId="56" />
  <row Id="8516" PostId="5640" Score="0" Text="Can you explain a few things? What's your color scale? Are you using the full hue range? (For example 0° = red, 60° = yellow, 120° = green, 180° = cyan, 360° = red?) Also, what makes you think what you're seeing isn't correct or on the right track? Do you know how many particles were used in the &quot;goal&quot; picture? Given what you said about the colors for small `N`, it seems feasible that by using a larger `N` you might get coloring more like the goal. But I'm just speculating." CreationDate="2017-09-23T16:18:26.253" UserId="3003" />
  <row Id="8517" PostId="5640" Score="0" Text="I am using HSV colors where each parameter is between 0 and 1. I fix S and V to be 1 and H corresponds to the magnitude of the force. Then I convert HSV colors to RGB and multiply each parameter by 255 because pygame display only accepts RGB colors in this format." CreationDate="2017-09-23T19:12:45.643" UserId="7363" />
  <row Id="8518" PostId="5640" Score="0" Text="What I mean is are you using the full Hue range from 0° to 360°? If so, you might want to limit it to either 0-180° or 0-270°. Otherwise, things with very large force look the same as things with very little force, since the hues wrap around at 360°." CreationDate="2017-09-23T19:15:19.577" UserId="3003" />
  <row Id="8519" PostId="5640" Score="0" Text="@user1118321: you might be right that the colors in my sim look weird because I don't have the same conditions as in the &quot;goal&quot; sim. We are both using 10,000 particles, but my initial conditions are randomized positions and velocities, which will never produce a galaxy like the one shown above. We need to give the system some initial angular momentum to get a rotating galaxy. So I will see what happens by using proper initial conditions. I was just wondering if there was anything else I should be considering w.r.t the color scaling, or anything I'm doing that is obviously wrong." CreationDate="2017-09-23T19:15:21.003" UserId="7363" />
  <row Id="8520" PostId="5638" Score="0" Text="two related links:&#xA;http://hhoppe.com/proj/hatching/&#xA;and&#xA;http://www.floored.com/blog/2014sketch-rendering/" CreationDate="2017-09-24T14:38:01.087" UserId="56" />
  <row Id="8521" PostId="5640" Score="0" Text="@user1118321: as I said, I use HSV colors where each parameter is from 0-1 instead of 0-360. If I set S and V to 1, and let H vary between 0 and 1, there is not this problem of wrapping around. H=0 gives red while H=1 gives green." CreationDate="2017-09-24T17:53:39.803" UserId="7363" />
  <row Id="8522" PostId="5640" Score="0" Text="OK cool. I'm used to positive Hue going from red through yellow, to green, but I saw you had cyan, blue, and magenta in your image, so I was confused. But I think I get it now." CreationDate="2017-09-24T18:24:25.960" UserId="3003" />
  <row Id="8523" PostId="5643" Score="2" Text="&quot;I think these are NaNs, will double check, but if you know common scenarios when this happens it'd be appreciated.&quot; The common scenario is that you're using floating-point and you have a bug in your code :-) You have to expect that you'll get NaNs during development, so you should either (a) make sure they get filtered out instead of propagated by the filter, or (b) show all NaNs as bright pink so it's immediately obvious when they're there." CreationDate="2017-09-25T15:26:35.457" UserId="2041" />
  <row Id="8524" PostId="5633" Score="0" Text="This was for my thesis project last year, I ended up just going with the dumb  way and using integer atomic adds, scaled way up to maximize precision, then normalizing to float vectors. Couldn't figure out a way to list faces round each vertex without allocating worst-case space and using atomic counters to build the lists anyway. It's probably inefficient as hell but I still got a couple of orders of magnitude speedup from the CPU version and a first-class mark so I was happy enough with it :)" CreationDate="2017-09-26T07:23:48.047" UserId="1937" />
  <row Id="8525" PostId="5647" Score="2" Text="TBH, the time needed to save out an image file is insignificant compared to 5 seconds. So, I'd just save out the images and stitch them together into a video later, if that's the easiest thing to do. Another possibility is to [stream the frames out to ffmpeg](https://computergraphics.stackexchange.com/a/5627/48) (answer is for C++, but the same idea should be adaptable to Python)." CreationDate="2017-09-27T01:19:36.940" UserId="48" />
  <row Id="8527" PostId="5645" Score="0" Text="Unfortunately in my case, there is no ground truth image available. We cannot get these images due to some issues related to Field of View of the images. &#xA;**Is there any other way we can use the results and do the comparisons against each other ?**" CreationDate="2017-09-27T08:45:25.103" UserId="6974" />
  <row Id="8528" PostId="5645" Score="0" Text="WRT to generating the Ground Truth, make sure you don't down sample with a box filter. You could to 10k samples per pixel and still have aliasing." CreationDate="2017-09-27T09:33:39.870" UserId="209" />
  <row Id="8529" PostId="5640" Score="0" Text="What do you mean by &quot;properly&quot; or &quot;realistic&quot;? It's not at all realistic to colour things according to the force acting on them. There are many choices of colour map and the answer depends on what your goal is for using colour." CreationDate="2017-09-27T10:12:09.300" UserId="2041" />
  <row Id="8530" PostId="5647" Score="0" Text="@NathanReed You should post that as an answer." CreationDate="2017-09-27T10:14:08.267" UserId="2041" />
  <row Id="8531" PostId="5640" Score="0" Text="@DanHulme: By realistic I mean a realistic representation of the forces acting on the bodies. I want to be able to see some sort of gradation where the force gradually increases as we move closer to the center of the galaxy, like the second image shown." CreationDate="2017-09-27T10:54:45.613" UserId="7363" />
  <row Id="8532" PostId="5645" Score="0" Text="There may be some way but I don't know of one. The problem with aliasing is that information is lost, which is hard to detect after the fact." CreationDate="2017-09-27T14:51:14.017" UserId="56" />
  <row Id="8533" PostId="5645" Score="0" Text="I'm surprised you can't get a ground truth due to a field of view setting.  Can you excplain?  Anyways, another idea that may or may not work out is to use the image you have as a super sampled ground truth image: shrink it down to like 1/4 size or smaller using good filtering to minimize aliasing.  Next, take the source image and shrink it down to the same size using bad filtering that causes aliasing. next use your AA techniques and compare them against the ground truth you have.  Another option that may work is to use a different image to test AA on, one you can get a ground truth for." CreationDate="2017-09-27T18:05:07.690" UserId="56" />
  <row Id="8534" PostId="5647" Score="0" Text="Possible duplicate of [OpenGL animation - turn into mp4 movie](https://computergraphics.stackexchange.com/questions/5606/opengl-animation-turn-into-mp4-movie)" CreationDate="2017-09-27T18:55:02.617" UserId="48" />
  <row Id="8535" PostId="5647" Score="0" Text="@DanHulme TBH, I think this question can probably be marked as a dupe of the one I linked. :)" CreationDate="2017-09-27T18:55:25.780" UserId="48" />
  <row Id="8536" PostId="5647" Score="1" Text="@NathanReed I'm inclined not. The other question seems to be about turning an apitrace recording into a video. That doesn't have the same concern about keeping a fixed time-step. Also, this questioner has some misconceptions about performance, which need addressing directly." CreationDate="2017-09-28T07:28:53.187" UserId="2041" />
  <row Id="8537" PostId="5653" Score="0" Text="Can you elaborate on this a bit? As far as i can tell, the GL methods for loading data to buffers (glBufferData and glBufferSubData) just copy a contiguous block of bytes across. Is it possible to somehow get a pointer into GPU memory and write your own copy routine?" CreationDate="2017-09-28T07:48:35.363" UserId="1937" />
  <row Id="8538" PostId="5653" Score="0" Text="Ahh of course, you can map the buffer then copy directly. Derp :-/" CreationDate="2017-09-28T08:54:21.313" UserId="1937" />
  <row Id="8539" PostId="5599" Score="0" Text="You could try using the latest version of DirectXMath available at [Github](https://github.com/Microsoft/DirectXMath) and see if the problem persists." CreationDate="2017-09-29T16:37:51.593" UserId="2287" />
  <row Id="8540" PostId="5599" Score="0" Text="Is there actually some info at compile or runtime about the crash?" CreationDate="2017-09-29T16:39:27.067" UserId="2287" />
  <row Id="8541" PostId="5658" Score="0" Text="Initially, I thought the same. However, for images larger than 2x2 this doesn't hold. This is because of the condition −1≤xsi,ysi≤1−1≤xis,yis≤1. I have added an example at the end of my post to clarify this." CreationDate="2017-09-30T11:45:22.007" UserId="7390" />
  <row Id="8543" PostId="5397" Score="0" Text="Minor optimization unrelated to your problem (but also circumventing it): your specific kernel can as well be achieved with just 4 texture accesses and an average of those 4 values. Just use a linearly filtered sampler and move the four texture coordinates by half a texel into the four corners of the current texel. You'll find that with normal linear filtering this will give you exactly the weighting specified in `kernel`." CreationDate="2017-09-30T15:52:44.333" UserId="6" />
  <row Id="8546" PostId="5596" Score="0" Text="Cross x and y and you will get your z axis assuming x and y are perpendicular" CreationDate="2017-09-30T18:56:15.260" UserId="6938" />
  <row Id="8551" PostId="5657" Score="0" Text="Where does this -0.833 come from in your example? Obviously if you sample outside the original image only the two border pixels will be closest. You have to express the sample points in the same co-ordinate system you're using for the pixels in the old image." CreationDate="2017-10-01T10:42:52.390" UserId="2041" />
  <row Id="8553" PostId="5412" Score="0" Text="Great answer, thanks!" CreationDate="2017-10-01T11:45:01.830" UserId="213" />
  <row Id="8554" PostId="369" Score="0" Text="As it happens I just recently implemented this algorithm (in C++) and it's really not that difficult, I found a much better short explanation than the one on Wikipedia. I'll see if I can come up with a pseudocode answer that hopefully adresses your concerns. But then again, I also had the luxury of one of the polygons being a triangle." CreationDate="2017-10-01T14:25:48.753" UserId="6" />
  <row Id="8555" PostId="1995" Score="1" Text="If you figured it out yourself, you might want to answer you own question then." CreationDate="2017-10-01T16:16:01.787" UserId="6" />
  <row Id="8556" PostId="5303" Score="0" Text="Are you sure that the algorithm *needs* a tight exact AABB rather than just being satisfied with an approximation of triangles by boxes? I only gave it a short glimpse, but it seem that function is only used for finding the optimal splitting plane. It might not necessarily require a perfectly tight triangle box for that when the worst that could happen would just be a less than optimal splitting plane." CreationDate="2017-10-01T16:33:47.813" UserId="6" />
  <row Id="8557" PostId="5059" Score="0" Text="@Dolda2000 But don't they in turn (theoretically) lack all the other things *apart* from compute shaders, like all the features related to rasterization and whatever graphics stuff?" CreationDate="2017-10-01T18:06:38.220" UserId="6" />
  <row Id="8558" PostId="5010" Score="0" Text="Why do you *need* voxels to begin with? Can't you just render the points themselves (possibly with some adaptive point size)?" CreationDate="2017-10-01T20:00:14.843" UserId="6" />
  <row Id="8559" PostId="4345" Score="0" Text="If you solved your problem, feel free to answer your own question." CreationDate="2017-10-01T20:09:36.783" UserId="6" />
  <row Id="8560" PostId="5659" Score="1" Text="You can, however, make smart pointers work by specializing `std::allocator` for any aligned data type. This will also allow you to put your stuff into standard library containers, which is not a particularly rare use-case. Basically whenever you overload `new/delete` you should always remember to make your own allocator (or better, specialize the standard allocator). But Solution 2 might still be the most headache-free anyway." CreationDate="2017-10-01T20:37:04.210" UserId="6" />
  <row Id="8562" PostId="5059" Score="0" Text="@ChristianRau: I don't know, that's why I'm asking. :) That being said, though, I'm not aware of any rasterization features that Vulkan requires which haven't been in OpenGL for a long time." CreationDate="2017-10-02T00:54:25.097" UserId="6567" />
  <row Id="8563" PostId="5657" Score="0" Text="I am sorry for the incomplete addition to my question. Given that I am new, I cannot add another image to explain the values. You can view the image I was planning on putting in the following link https://www.dropbox.com/s/4jmcangptz1lftu/bilint_derivation11.png?dl=0. I will add this image to the question when my reputation is sufficient to include it." CreationDate="2017-10-02T07:14:33.637" UserId="7390" />
  <row Id="8564" PostId="5657" Score="0" Text="The $x_s$ and $y_s$ in that image are clearly in [-1,1] co-ordinates, not [0,W) co-ordinates. Obviously to make the subtraction work they both have to be in the same co-ordinate system." CreationDate="2017-10-02T09:21:43.913" UserId="2041" />
  <row Id="8565" PostId="5672" Score="0" Text="Hmm, upon further inspection, the [original ARB_occlusion_query extension](https://www.khronos.org/registry/OpenGL/extensions/ARB/ARB_occlusion_query.txt) does *explicitly* include the stencil test but also says *&quot;Note that the depth test comes after the stencil test, so to say that it is the number that pass the depth test is sufficient; though it is often conceptually helpful to think of the depth and stencil tests as being combined&quot;*, which might already strongly point to the 1st solution." CreationDate="2017-10-02T14:42:15.780" UserId="6" />
  <row Id="8566" PostId="5657" Score="0" Text="In both cases the equation doesn't add up." CreationDate="2017-10-02T15:10:12.387" UserId="7390" />
  <row Id="8567" PostId="5672" Score="0" Text="Note that the behavior of a discarded fragment is not clear, with regard to processing in further stages. That is, if a fragment is discarded, will it still be processed by later stages. [I've asked for clarification on this point,](https://github.com/KhronosGroup/OpenGL-API/issues/19) and I've updated the Wiki to suggest that this is in question." CreationDate="2017-10-02T15:54:51.083" UserId="2654" />
  <row Id="8568" PostId="5672" Score="0" Text="@NicolBolas Hmm, but I thought it's guaranteed that it will at least not *write* anything (be that colour, depth or whatever) if discarded (which...prety much is the point of discarding, isn't it?)." CreationDate="2017-10-02T16:02:52.300" UserId="6" />
  <row Id="8569" PostId="5672" Score="0" Text="[Discarded fragments *can* write things](https://www.khronos.org/opengl/wiki/Fragment#Fragment_discard). They can write to the stencil buffer, if the stencil function so chooses. And if you use early fragment tests, you can still discard from the fragment shader, which will prevent color writes and other things. But it *won't* prevent depth/stencil updates. The standard is *very* clear about these things. So the idea that a discarded fragment could still update the sample count is hardly unreasonable." CreationDate="2017-10-02T16:04:33.230" UserId="2654" />
  <row Id="8570" PostId="5672" Score="0" Text="Well, I wasn't really considering explicit early tests and more concentrating on the *theoretical* late tests the standard requires. But sure, when doing explcit early tests and discrading it messes things up. I *did* however not know that stencil writes don't care about discarding. That's interesting to know." CreationDate="2017-10-02T16:07:46.000" UserId="6" />
  <row Id="8571" PostId="5672" Score="0" Text="But seeing how Vulkan *does* account for all tests, that might be another hint to the current wording just being not entirely accurate. I'd doubt this to be a major feature (and ultimately hardware) change between APIs. But it still doesn't exclude the possibility that the spec really leaves this open in GL. It's odd that noone ever stumbled over this then, though." CreationDate="2017-10-02T16:20:48.857" UserId="6" />
  <row Id="8572" PostId="1502" Score="0" Text="If the model matrix is made of translation, rotation and scale, you don't need to do inverse transpose to calculate normal matrix. Simply divide the normal by squared scale and multiply by model matrix and we are done. You can extend that to any matrix with perpendicular axes, just calculate squared scale for each axes of the matrix you are using instead. I wrote the details in my blog: https://lxjk.github.io/2017/10/01/Stop-Using-Normal-Matrix.html" CreationDate="2017-10-01T23:52:53.853" UserId="7400" />
  <row Id="8585" PostId="4345" Score="0" Text="Thinking it over, doing this correctly would have required a full 3D model and a physical model of the weight distribution. Doing it manually by rotating the model in 3D-space until it looked straight was easier." CreationDate="2017-10-03T07:41:26.037" UserId="5603" />
  <row Id="8586" PostId="5675" Score="1" Text="What do you mean by diffuse bounds? Light has a quadratic falloff, i.e. it decays proportionally to 1/dist^2 . This applies to both diffuse and specular response. Obviously this will lead to infinite bounds, so often the bounds of radius r are set so that `LightIntensity/r^2 &lt;= threshold`&#xA;&#xA;A light has no concept of diffuse and specular in itself, those are material properties, so you shouldn't really be concerned about it when culling lights." CreationDate="2017-10-03T08:35:21.023" UserId="100" />
  <row Id="8587" PostId="5675" Score="1" Text="What's the problem you're trying to solve? What kind of renderer are you talking about?" CreationDate="2017-10-03T14:05:36.630" UserId="2041" />
  <row Id="8589" PostId="5665" Score="1" Text="Here's an interesting extension to storing look up tables in textures. It (ab)uses N-linear texture interpolation to get higher order interpolation (aka better than linear) of data points, surfaces, volumes, and hypervolumes.&#xA;https://blog.demofox.org/2016/02/22/gpu-texture-sampler-bezier-curve-evaluation/" CreationDate="2017-10-03T17:14:18.673" UserId="56" />
  <row Id="8590" PostId="5675" Score="0" Text="I'm betting the issue is that diffuse component becomes less noticeable at a different range (closer) than the specular component for shiny objects?" CreationDate="2017-10-03T18:51:41.057" UserId="56" />
  <row Id="8591" PostId="5676" Score="0" Text="So you want to place a bunch of small shapes of one type (same size?) inside a larger shape? Its not possible to fill the shapes in the images with circles completely with no overlap." CreationDate="2017-10-04T03:18:11.033" UserId="113" />
  <row Id="8592" PostId="5676" Score="0" Text="Same sized polygon/circle, there'd be a fixed distance between them, and it's fine to have partial shapes near the boundary." CreationDate="2017-10-04T03:49:54.190" UserId="7413" />
  <row Id="8593" PostId="5676" Score="1" Text="For circles you'd hexagonally pack them to get them as tight as can be. Then clip ones that lie on boundaries. This wouldn't vary with regions or circle size. If all you want to do is fill these shapes with color then forget the rectangles/circles and just scanline them in." CreationDate="2017-10-04T03:57:30.467" UserId="113" />
  <row Id="8594" PostId="1503" Score="0" Text="Normals are also so-called pseudovectors. As a generalization and rule of thumb, everything resulting from a cross product (e.g. planes) will be transformed in a similar fashion." CreationDate="2017-10-04T17:03:53.123" UserId="2287" />
  <row Id="8595" PostId="2246" Score="0" Text="Related: [Image downscale algorithm](https://stackoverflow.com/questions/31873215/image-downscale-algorithm)" CreationDate="2017-10-04T20:10:50.367" UserId="7421" />
  <row Id="8596" PostId="3576" Score="0" Text="@MobyDisk - actually, the C++ analogy is right on, though &quot;C vs Java&quot; or &quot;C vs C#&quot; might be a better comparison.  Vulkan is a **lower-level** API than OpenGL, not a next-gen replacement. According to Khronos, they are &quot;complementary&quot;.  Eventually I expect we'll see an OpenGL-inspired layer on top of Vulkan. **That** would be a next-gen successor, if it successfully combined performance-gains with higher-level API calls for programmer convenience." CreationDate="2017-10-04T20:52:22.547" UserId="7422" />
  <row Id="8597" PostId="3579" Score="0" Text="@DanHulme - I'm surprised by your comment &quot;GL encourages you to think in an immediate-mode style&quot;. I thought that was only true of *early* OpenGL versions?" CreationDate="2017-10-04T21:07:04.937" UserId="7422" />
  <row Id="8598" PostId="5675" Score="0" Text="Ok, let's assume that the point light has range, so we calculate the diffuse light with this range in mind. To make better performance and not to draw thousands of lights on a scene when few of them are actually visible, we cull them by the view frustum. This works well for diffusion, but does not with specular. Specular range is different from diffuse range. What I'm thinking of is that calculate light strength from range and use it to calculate range for specular so I will be able to cull it the right way." CreationDate="2017-10-05T01:00:16.117" UserId="226" />
  <row Id="8599" PostId="5675" Score="0" Text="What I'm trying to do is to optimize renderer. I need to find out when light is doing any noticeable contribution to the lighting or not." CreationDate="2017-10-05T01:03:11.940" UserId="226" />
  <row Id="8600" PostId="5681" Score="1" Text="That could be a nice approach, I think we don't have inconsistencies on that aspect, but as I said, there are quite a few allocs and deallocs. And yeah, I have to start doing some profiling but I really think that this could be a potential improvement :). Thanks!" CreationDate="2017-10-05T08:40:44.750" UserId="7107" />
  <row Id="8601" PostId="3579" Score="0" Text="@ToolmakerSteve The OpenGL WG has put a lot of work into adding concurrency and batched writes, so you can express your program in a way that's suitable for tiling/deferred implementations. But it's still a foundation that was set in the immediate-mode age, which is why those people decided a fresh start in Vulkan was needed. And I still see a lot of new users starting with GL and forming an immediate-mode mental model of how implementations work." CreationDate="2017-10-05T09:06:28.180" UserId="2041" />
  <row Id="8602" PostId="5687" Score="0" Text="How does this technique compare to the very easy to generate [Halton sequence](https://en.wikipedia.org/wiki/Halton_sequence)?" CreationDate="2017-10-05T22:05:30.717" UserId="3470" />
  <row Id="8603" PostId="5687" Score="1" Text="It's a lot closer to ideal blue noise. Worth another question probably!" CreationDate="2017-10-05T22:52:08.610" UserId="56" />
  <row Id="8604" PostId="5684" Score="1" Text="It's not entirely clear from your description and image what you're asking. Can you clarify? What does the image show? The sun's rays will hit every surface of (the outside of) the house on any given day, most likely. (Unless I suppose if a side is exactly parallel to the path of the sun, or the building is at an extreme latitude.) What do you want to color?" CreationDate="2017-10-06T02:57:12.357" UserId="3003" />
  <row Id="8606" PostId="5675" Score="0" Text="&quot;Specular range is different from diffuse range&quot;, you are thinking on the wrong terms. You should think about range w.r.t intensity of the light not the material response to the light" CreationDate="2017-10-06T10:00:28.943" UserId="100" />
  <row Id="8607" PostId="5675" Score="0" Text="@cifz nope. I'm thinking about the light, not about taking into account the material parameters. I've just mentioned it to make clear that if I had no reflective materials there would be no need in culling lights by specular range." CreationDate="2017-10-06T11:10:43.273" UserId="226" />
  <row Id="8611" PostId="5684" Score="2" Text="There's two ways i can think of going about this. The first is that this case is basically a ray vs tube test where the tube follows the 3d path of the sun. Maybe you can look for (or ask for) ray vs &quot;tube&quot; intersections. The second idea is to calculate the closest point on the ray to the path of the sun (the line), and if that is closer than the radius of the sun, count it as a hit, and do whatever else you need to do from there." CreationDate="2017-10-06T16:35:59.667" UserId="56" />
  <row Id="8612" PostId="5684" Score="0" Text="Thanks for your comment. Could you please elaborate the second idea. Couldn't grasb it. Bear in mind that there are not one sun but many. Also regarding the first solution, do you have any idea what the equation would be for the tube like surface?" CreationDate="2017-10-06T20:34:32.773" UserId="6041" />
  <row Id="8613" PostId="5684" Score="1" Text="I don't have any equations in mind for the first thing, no sorry. The second thing is this: the sun is traveling on a path. Imagine that path as a line (it bends around and curves, but it doesn't have any width).  If you can find a way to find the closest point on that curve to a line (the line that the ray is part of), you can get the distance from the ray to that path. If it gets closer than the radius of the sun, that means it MUST hit the sun. Then from there need to figure out when / where etc, so more work needed, but it's a start." CreationDate="2017-10-06T21:02:02.397" UserId="56" />
  <row Id="8614" PostId="5692" Score="0" Text="Have you tried out_s = col_s and out_v = in_v?" CreationDate="2017-10-07T15:15:37.360" UserId="5703" />
  <row Id="8615" PostId="5692" Score="0" Text="@SebastiánMestre Yes, this was my first thought. Unfortunately it does not replicate the Colorize behavior." CreationDate="2017-10-07T16:53:14.073" UserId="7434" />
  <row Id="8616" PostId="5692" Score="0" Text="Isn't the colorize function kinda operating on the HSL space instead of HSV? What if you convert directly into HSL then what the above person said would work?" CreationDate="2017-10-07T23:04:59.933" UserId="6046" />
  <row Id="8617" PostId="5695" Score="0" Text="I'm getting close. Using the source I was able to write a shader that approximates the intended results. See here:  &#xA;  &#xA;https://imgur.com/a/kWGce  &#xA;  &#xA;Here is my GLSL fragment shader code:  &#xA;  &#xA;https://gist.github.com/anonymous/aacbe1aafe2fac75216b113bacbb8a0f&#xA;    &#xA;And here is the GimpColorizeOperation source code I'm drawing from:  &#xA;  &#xA;https://github.com/GNOME/gimp/blob/master/app/operations/gimpoperationcolorize.c  &#xA;  &#xA;Still some ways off.." CreationDate="2017-10-09T01:29:59.570" UserId="7434" />
  <row Id="8619" PostId="5701" Score="0" Text="it makes sense and thanks for the info.  Are you able to speak to what for instance &quot;uniform&quot; would look like in the progressive (non binary) texture setup by chance?" CreationDate="2017-10-10T00:24:48.670" UserId="56" />
  <row Id="8622" PostId="5704" Score="6" Text="Yes if each kernel is just a convolution. Sharpening algorithms may or may not be, median also usually is not. Read [this](https://math.stackexchange.com/questions/1023984/combining-two-convolution-kernels)" CreationDate="2017-10-10T15:27:06.027" UserId="38" />
  <row Id="8628" PostId="5704" Score="0" Text="It might depend on the actual *kernel*. Can you include the specific kernels you're talking about in the question?" CreationDate="2017-10-11T18:05:31.253" UserId="6" />
  <row Id="8635" PostId="5704" Score="0" Text="&quot;Kernel&quot; is not a synonym for &quot;filter&quot;. There is no such thing as a median kernel." CreationDate="2017-10-12T18:23:34.120" UserId="106" />
  <row Id="8637" PostId="5680" Score="3" Text="Remember that disney's diffuse BRDF is not a physical correct one. They are fully aware that their BRDF is not energy conserving, but they found that it looked better (probably because it makes up for interreflections) and their artists liked it. &#xA;Also note, that it is based on a physical BRDF model, which takes an integral over all microfacet normals (for this, see [Earl Hammon Jr's Diffuse GGX Lighting Slides][1]). That integral is not solvable, thus **it is  only an approximation**.&#xA;&#xA;&#xA;  [1]: https://twvideo01.ubm-us.net/o1/vault/gdc2017/Presentations/Hammon_Earl_PBR_Diffuse_Lighting.pdf" CreationDate="2017-10-13T08:41:49.670" UserId="7008" />
  <row Id="8639" PostId="2498" Score="0" Text="You have to bother about the cosine Term. The thing with $E(\omega_i)$ is that it is defined as $E = \frac{\Phi}{A}$ and $A$ is in this case orthogonal to the light direction, i.e. $A$ is projected from the original surface onto a plane orthogonal to the light direction.&#xA;&#xA;In short, you can't ignore $cos$, but in the equation it is implicitly handled." CreationDate="2017-10-13T09:51:21.800" UserId="7008" />
  <row Id="8640" PostId="5718" Score="0" Text="The name of the guy is Hooke threfore it is Hooke's law" CreationDate="2017-10-13T09:54:58.607" UserId="38" />
  <row Id="8641" PostId="5656" Score="0" Text="Blinn-Phong does not use $\alpha \equiv roughness^2$. it has an arbitrary &quot;roughness&quot; parameter. Also, the $\alpha$ in Beckmann is not the same as the $\alpha$ in GGX. In Beckmann, $\alpha \in [0, \infty)$ and in GGX $\alpha \in [0, 1]$ (although both describe the RMS Slope)." CreationDate="2017-10-13T12:46:27.970" UserId="7008" />
  <row Id="8642" PostId="5656" Score="1" Text="@Tare For Blinn-Phong you need to use a derived version which derives alpha from the specular exponent. See http://graphicrants.blogspot.be/2013/08/specular-brdf-reference.html" CreationDate="2017-10-13T13:21:11.003" UserId="2287" />
  <row Id="8643" PostId="5656" Score="1" Text="Okay, you didn't mention that in your post, so I assumed you were using the original form." CreationDate="2017-10-13T14:16:40.890" UserId="7008" />
  <row Id="8644" PostId="5656" Score="0" Text="@Tare I added the link to the answer itself for clarity :)" CreationDate="2017-10-13T14:27:02.377" UserId="2287" />
  <row Id="8645" PostId="5720" Score="0" Text="I don't understand how having other types of translucent objects makes it impossible to keep using sorted alpha blending. Could you elaborate?" CreationDate="2017-10-13T14:53:56.987" UserId="182" />
  <row Id="8648" PostId="5718" Score="0" Text="Oh, sorry about that! I edited my typos." CreationDate="2017-10-13T16:28:04.613" UserId="6806" />
  <row Id="8649" PostId="5719" Score="0" Text="I add an example could you please see that?" CreationDate="2017-10-13T17:04:32.547" UserId="6806" />
  <row Id="8650" PostId="5720" Score="0" Text="If you have 10,000 particles in a single draw call, you can sort them and do a single draw call.  If you then add 5 other transparent objects, you then have up to something like 12 draw calls since you need to draw all the objects in the correct sort order.  It's possible but painful and likely costly!" CreationDate="2017-10-13T17:14:06.493" UserId="56" />
  <row Id="8651" PostId="5720" Score="0" Text="related question: https://computergraphics.stackexchange.com/questions/5618/are-there-tricks-for-getting-proper-sort-ordering-on-particle-systems" CreationDate="2017-10-13T17:14:43.160" UserId="56" />
  <row Id="8652" PostId="5720" Score="0" Text="Ah I see what you mean. I suppose you could partition Z space to reduce the error due to local incorrect order." CreationDate="2017-10-13T17:21:47.443" UserId="182" />
  <row Id="8653" PostId="5720" Score="0" Text="Vylsain you might be interested in &quot;Order Independent Transparency&quot; or OIT.  It's an active area of research with a few different leading solutions that may or may not be appropriate for your specific usage case.  It allows you to render transparency without sorting, but get the correct results anyways (or close enough to correct, or correct most of the time)." CreationDate="2017-10-13T17:51:12.667" UserId="56" />
  <row Id="8654" PostId="5719" Score="0" Text="@shashack obviously they model a  different thing. The other models springs that can be any size but equally elastic and the other only works for springs that you haev separately tested. All is as i explained and expected." CreationDate="2017-10-13T17:56:46.187" UserId="38" />
  <row Id="8656" PostId="5233" Score="0" Text="ORB means ORB features and BF means Brute Force Matching. In step 2, for each candidate region, calculate the ORB features and use bruteforce matching to compare it with the template pattern. If there are enough(by number and ratio) matches(corresponding feature points in the template and current region), then this candidate region is considered a target." CreationDate="2017-10-11T01:55:57.957" UserId="2131" />
  <row Id="8657" PostId="5233" Score="0" Text="I cannot give you raw data because I'm off this case now. Kalman filter was once an option, but we solved the problem with an easier solution. Use opencv subpixel. That simply solves it. I have also tried upsampling and then downsampling. Now I cannot remember the result of the experiments." CreationDate="2017-10-11T02:02:00.447" UserId="2131" />
  <row Id="8670" PostId="5722" Score="0" Text="I think I know what flip you're talking about. Most people just clamp it and stop the camera. You want your camera to be upside down after going over +-90 degrees? If not then there'd have to be a twist to the camera. Like once one of your spherical angles hits +-90 degrees the other angle does a 180." CreationDate="2017-10-14T01:18:31.690" UserId="113" />
  <row Id="8671" PostId="5233" Score="0" Text="There are two reasons we didn't do the smoothing. First, we were processing the images on embedded systems. Resources are really limited and something like Gaussian filter costs a lot. The situation is, even with image resolution 640*480, the numbers of frames processed can drop a lot with Gaussian smoothing. Second, with smoothing, there is a chance that  the target zone being connected with outer zone." CreationDate="2017-10-14T03:41:16.393" UserId="2131" />
  <row Id="8672" PostId="5722" Score="0" Text="The OP is having a different problem. Their code protects the pitch from getting to high or low, but they are having problems with yaw rotation." CreationDate="2017-10-14T15:39:04.530" UserId="56" />
  <row Id="8673" PostId="5722" Score="1" Text="@AlanWolfe I've just uploaded a video of explaining the issue.&#xA;&#xA;I think the yaw(rotation around y axis) works, but pitch(around x) doesn't, if I'm not mistaking." CreationDate="2017-10-14T16:19:06.053" UserId="6908" />
  <row Id="8674" PostId="5722" Score="1" Text="@AlanWolfe Also, sorry I think I've uploaded the wrong code on the original post, just updated it. I'm simply trying to rotate the camera around the center. Around y axis works, but around x it acts strange, and I think it's because the rotation on x axis is limited to 180 degrees and if we go over it, it probably has to rotate the y axis first and then x to achieve the desired rotation point?" CreationDate="2017-10-14T16:25:12.277" UserId="6908" />
  <row Id="8675" PostId="5724" Score="0" Text="It is important to note that the Rasterization stage before the optional Fragment Shader performs fragment attribute interpolation **and** viewport transformation for the `gl_FragCoord` attribute." CreationDate="2017-10-14T16:29:17.517" UserId="2287" />
  <row Id="8677" PostId="2332" Score="0" Text="[Gratis Photoshop CS5 Download](http://fileparrots.blogspot.com/2017/08/download-adobe-photoshop-cs5-free.html) it is easy to change in this tool." CreationDate="2017-10-14T20:17:17.427" UserId="7476" />
  <row Id="8678" PostId="5732" Score="0" Text="How to calculate angle and phi when rotate around all 3 axes? I've read wikipedia you linked, and try to calculate it... but still don't know how to calculate those with rotation angle used on glRotate(angle, rotate_x, rotate_y, rotate_z);" CreationDate="2017-10-16T05:22:54.443" UserId="7482" />
  <row Id="8679" PostId="5720" Score="0" Text="Hi and thanks for your help. That's right, sorted alpha blending is not efficient if I constantly have to bind different shaders and split my particle system draw call. About Order Independant Transparency, that's what I've tried as I said in the first post. But it seems there's way too many particles and overdraw involved in my case to have correct or efficient results. I wonder how this is handled with a ray-marching approach..." CreationDate="2017-10-16T07:18:12.930" UserId="7473" />
  <row Id="8681" PostId="5720" Score="1" Text="Have you thought of using volume rendering for this? It seems to me like a more sensible approach than your current one" CreationDate="2017-10-16T12:57:39.703" UserId="5703" />
  <row Id="8682" PostId="5716" Score="0" Text="I'm not familiar with the Lane Resienfield algorithm but I think you need to be more specific as to what you don't understand." CreationDate="2017-10-16T13:57:06.903" UserId="209" />
  <row Id="8683" PostId="5720" Score="0" Text="Yes, I'm thinking about it but I don't know what to expect with transparency using this kind of technique... If I render my clouds using ray marching, what happens when a ray encounters a semi transparent object ?" CreationDate="2017-10-16T14:10:41.890" UserId="7473" />
  <row Id="8684" PostId="5732" Score="0" Text="The easiest way is to take a unit vector, like `&lt;0, 0, 1&gt;`, run it through the Euler rotation matrix, then convert the point at the end of the resulting vector to spherical coordinates. You can use [this formula](https://en.wikipedia.org/wiki/Rotation_matrix#Rotation_matrix_from_axis_and_angle) to get the cartesian rotation matrix from your Euler angles." CreationDate="2017-10-16T14:55:12.863" UserId="3003" />
  <row Id="8685" PostId="5734" Score="0" Text="While it is not common in games it does exist in applications" CreationDate="2017-10-16T15:28:56.423" UserId="38" />
  <row Id="8686" PostId="5735" Score="0" Text="Does this need to be fast or can it be done offline in advance?" CreationDate="2017-10-16T19:59:20.483" UserId="56" />
  <row Id="8687" PostId="5735" Score="0" Text="It doesn't need to be especially fast." CreationDate="2017-10-16T20:00:43.267" UserId="7490" />
  <row Id="8688" PostId="5735" Score="0" Text="Do the segments need to be exactly equal length (within limitations of floating point etc) or can they be approximately equal? Like equal within some epsilon?" CreationDate="2017-10-16T20:02:08.053" UserId="56" />
  <row Id="8689" PostId="5735" Score="0" Text="Interesting point. Some approximation is probably okay, but I'd prefer to maximise precision." CreationDate="2017-10-16T20:04:26.210" UserId="7490" />
  <row Id="8690" PostId="5716" Score="0" Text="The actual algorithm in a geometrical sense. Like, with Castlejeaus you can explain it by taking fractions of the straight lines (1/3 of the line between A and B for example), recursively.&#xA;&#xA;I need something similar but for lane resienfield" CreationDate="2017-10-16T22:28:31.423" UserId="7462" />
  <row Id="8691" PostId="5735" Score="0" Text="Do you want equal length segments or segments that represent equal arc lengths?" CreationDate="2017-10-17T00:44:23.520" UserId="5703" />
  <row Id="8692" PostId="5695" Score="0" Text="Maybe use hsl instead of hsv/hsb?" CreationDate="2017-10-17T00:47:47.027" UserId="5703" />
  <row Id="8693" PostId="5732" Score="0" Text="Thx. This works perfectly good. However, I'm still curious that how to calculate 'angle' and 'phi' that for spherical coordinates with Euler angle." CreationDate="2017-10-17T07:32:22.290" UserId="7482" />
  <row Id="8694" PostId="5738" Score="1" Text="What's the context?" CreationDate="2017-10-17T09:40:51.630" UserId="2041" />
  <row Id="8696" PostId="5735" Score="0" Text="That makes sufficiently little difference to me that I'm happy with whichever is easiest." CreationDate="2017-10-17T10:30:03.067" UserId="7490" />
  <row Id="8697" PostId="5737" Score="0" Text="That being said, this approach does mean that the value of a influences both line length and turn rate. I really need those to be separate terms." CreationDate="2017-10-17T11:01:00.420" UserId="7490" />
  <row Id="8698" PostId="5732" Score="0" Text="A quick search didn't turn up any obvious answers, and my linear algebra is somewhat limited, so unfortunately, I don't know of another way." CreationDate="2017-10-17T16:37:05.477" UserId="3003" />
  <row Id="8699" PostId="5741" Score="0" Text="thank you for your quick answer. I didn't get the results I hoped, but you answered the question." CreationDate="2017-10-17T22:31:49.533" UserId="7492" />
  <row Id="8700" PostId="5737" Score="0" Text="@SolarGranulation I just added that, and updated the JSFiddle and Desmos examples to reflect that." CreationDate="2017-10-18T00:10:01.723" UserId="7431" />
  <row Id="8702" PostId="5720" Score="0" Text="What types of other objects are you expecting to draw besides clouds? I would think you could partition the objects by elevation." CreationDate="2017-10-18T03:52:41.317" UserId="2707" />
  <row Id="8703" PostId="5737" Score="0" Text="Oh wow, thank you!" CreationDate="2017-10-18T05:56:39.407" UserId="7490" />
  <row Id="8704" PostId="5743" Score="0" Text="You could have a backup dynamic viewport pipeline object for use during resize until the finalized pipeline object with fixed viewport is ready." CreationDate="2017-10-18T11:14:24.923" UserId="137" />
  <row Id="8705" PostId="5720" Score="0" Text="Other particles drawn with different shader and blending mode (additive), buildings with windows, aircrafts... The cloud ceiling altitude can be adjusted and I have case where a transparent object is in the middle of a cloud. For example, my test scene is a wind turbine equipped with lights in the middle of a cloud." CreationDate="2017-10-18T12:36:28.030" UserId="7473" />
  <row Id="8711" PostId="5748" Score="0" Text="Explicitly thanks for the clarification &quot;Some information is lost, though: the sign of $w$, which corresponds to whether a point was behind or in front of the camera.&quot;" CreationDate="2017-10-19T08:17:07.697" UserId="2287" />
  <row Id="8712" PostId="4948" Score="1" Text="This is not right. OP wants to build a 3d model from mri scans. Not from photographs" CreationDate="2017-10-19T10:52:46.910" UserId="5703" />
  <row Id="8713" PostId="4948" Score="0" Text="I'm pretty sure photogrammetry is not tied to photographs. If the source is multiple 2-dimensional images of any kind it should apply. But perhaps MRI scans are fundamentally different from things like X-ray images and are not 2 dimensional images as I expect." CreationDate="2017-10-20T02:36:19.480" UserId="1647" />
  <row Id="8714" PostId="5754" Score="0" Text="What do you expect to happen for #1? Your assumption is correct. If you're scaling the accumulation texture but not the particles, then it's going to get larger (or smaller) than the particle. What is the problem with #3? I don't see anything odd about it." CreationDate="2017-10-20T06:01:17.083" UserId="3003" />
  <row Id="8715" PostId="5754" Score="0" Text="in #3 the original particle is not moving. So, when panning translation is applied, I expect both textures to have synchronized transformation, and no fading trail should appear. Yet you can see when translation is very small at the end - there is a fading trail around particle, as if transformations slightly misaligned" CreationDate="2017-10-20T06:27:20.710" UserId="7512" />
  <row Id="8720" PostId="5757" Score="0" Text="Thank you! I'm actually computing position of a particle on a GPU and encode it into texture. The formulas to find next particle position are quite involved, and I want to support 100k - 1,000k particles. If I'd have to recompute positions for last N steps, I'm worried that would impact frame rate a lot." CreationDate="2017-10-20T16:42:36.333" UserId="7512" />
  <row Id="8721" PostId="5760" Score="0" Text="See [What is glVertexAttrib (vs. glVertexAttribPointer) used for?](https://stackoverflow.com/questions/7718976/what-is-glvertexattrib-versus-glvertexattribpointer-used-for#7719060). Essentially, they're the same thing, but `glVertexAttrib` is for the older immediate mode." CreationDate="2017-10-21T05:17:51.297" UserId="3003" />
  <row Id="8722" PostId="5763" Score="0" Text="Thanks. I suspected such thing because I recently experimented with cloud raymarchind sample code form here: https://www.gamedev.net/forums/topic/680832-horizonzero-dawn-cloud-system/?page=3&amp;tab=comments#comment-5319203 It probably has corrected approach. I don't exacly understand how to implement this formula though. It looks like when putting _S_ before the formula, what was left in between parenthesis is transmittance? I need to put it in `output.w` for further reconstruction." CreationDate="2017-10-22T09:12:09.553" UserId="4958" />
  <row Id="8723" PostId="5760" Score="0" Text="Thanks for the response! I think I understand the difference now. One sets a constant value, and the other links the values to a buffer." CreationDate="2017-10-22T20:21:24.750" UserId="6838" />
  <row Id="8724" PostId="5763" Score="0" Text="@narthex I added to the answer with my guess as to what the code should look like with the modified integration." CreationDate="2017-10-22T22:11:14.583" UserId="48" />
  <row Id="8731" PostId="5757" Score="0" Text="I was assuming you'd save the positions each frame rather than recomputing. An array of particle positions is much less state to pass around than a framebuffer." CreationDate="2017-10-24T08:18:04.140" UserId="2041" />
  <row Id="8732" PostId="5773" Score="1" Text="Well, I am not getting a compiler error at all, all that happens is that OpenGL throws an INVALID_OPERATION error.&#xA;&#xA;So the syntax is correct, but some operation is wrong" CreationDate="2017-10-24T10:10:35.750" UserId="7462" />
  <row Id="8733" PostId="5773" Score="1" Text="The error comes from my face type array being too big, but now I don't know how to pass the face type information into the shader, since I know I need 2048 elements at most" CreationDate="2017-10-24T10:27:24.317" UserId="7462" />
  <row Id="8734" PostId="2532" Score="0" Text="In my experience using an integrated graphics card should be good enough for most OpenGL development on Windows, just make sure it supports a recent OpenGL version." CreationDate="2017-10-24T14:13:05.860" UserId="5462" />
  <row Id="8739" PostId="5778" Score="0" Text="Note: I've asked the same question in the comp sci Stack Exchange, but received no answers: https://cs.stackexchange.com/questions/81169/how-to-prevent-moire-artifacts-in-this-light-casting-algorithm" CreationDate="2017-10-25T20:34:04.183" UserId="2437" />
  <row Id="8740" PostId="5779" Score="1" Text="If you solved your problem yourself, you can answer your own question. Deleting it just means the next person with a similar problem won't get the benefit of your experience." CreationDate="2017-10-25T21:55:34.467" UserId="2041" />
  <row Id="8741" PostId="5781" Score="0" Text="Check out the answer to [this question](https://computergraphics.stackexchange.com/questions/3742/how-to-unproject-quadrilateral-into-rectangle). You can take any 4 points and transform the quadrilateral between them into a unit square. From there you can simply scale in x and y to the size and shape you want." CreationDate="2017-10-26T04:57:37.453" UserId="3003" />
  <row Id="8742" PostId="5781" Score="0" Text="Does the transformation need to be perspective correct ?" CreationDate="2017-10-26T12:27:22.553" UserId="3073" />
  <row Id="8743" PostId="5778" Score="0" Text="I don't understand, never heard of light casting before. Isn't what you are trying to do similar to ray-tracing? Just shoot rays from the origin, for every single pixel. That way there won't be any uncolored pixel." CreationDate="2017-10-26T13:02:25.377" UserId="6046" />
  <row Id="8744" PostId="5782" Score="0" Text="Got any links how to do this in a 2D engine? I've tried to write it in a &quot;gather&quot; fashion in a fragment shader - its unbearably slow since it does so many duplicate calculations." CreationDate="2017-10-26T13:45:47.913" UserId="2437" />
  <row Id="8745" PostId="5781" Score="0" Text="@user1118321 thanks for the link, that's perfect!" CreationDate="2017-10-26T15:38:06.850" UserId="7541" />
  <row Id="8746" PostId="5781" Score="0" Text="@PaulHK what do you mean by perspective correct?" CreationDate="2017-10-26T15:38:09.833" UserId="7541" />
  <row Id="8748" PostId="5757" Score="0" Text="Dan, but I don't think I'm passing a framebuffer. I just use two textures and constantly swapping them. Does this mean I'm passing a framebuffer? Also I tried to re-render last 10 -15 steps and take into account transforms - performance was not acceptable.. Maybe I'm doing something wrong" CreationDate="2017-10-26T21:44:18.610" UserId="7512" />
  <row Id="8749" PostId="5781" Score="0" Text="For example, if the floor was a huge chess board, we would see squares vanish into the distance. When transforming back to 2d we need to take that perspective into account." CreationDate="2017-10-27T02:40:05.170" UserId="3073" />
  <row Id="8750" PostId="2004" Score="0" Text="I haven't read the paper, but this &quot;maximum mipmaps&quot; sounds very similar to the technique used for _Cone Step Mapping_ (which improves over parallax occlusion mapping by skipping large areas thanks to cones)." CreationDate="2017-10-27T03:35:22.803" UserId="182" />
  <row Id="8751" PostId="5780" Score="0" Text="You should also write to them the same way you write to `gl_Position`: before calling `EmitVertex()`." CreationDate="2017-10-27T03:51:01.957" UserId="2654" />
  <row Id="8752" PostId="5763" Score="0" Text="This is my comparison - standard vs corrected (above code): https://imgur.com/a/POTaD. But differences only get visible with very high scattering. Also correction leads to division by zero so small epsilon is needed." CreationDate="2017-10-27T14:02:14.517" UserId="4958" />
  <row Id="8753" PostId="5784" Score="0" Text="what about CMYK?" CreationDate="2017-10-27T17:17:54.027" UserId="7551" />
  <row Id="8755" PostId="5784" Score="0" Text="CMYK is a process by which we can print colors onto a substrate (and a much smaller selection of colors than we can see) but the way we see is RGB." CreationDate="2017-10-27T23:12:42.657" UserId="7555" />
  <row Id="8756" PostId="5784" Score="12" Text="They don't make up *all* the colors. They just make up a sufficient range of them that most scenes can be represented with acceptable fidelty." CreationDate="2017-10-28T00:08:27.377" UserId="2653" />
  <row Id="8758" PostId="5786" Score="5" Text="&quot;Thus, with only three colors, we can reconstruct all the colors we can see.&quot; This sentence is incorrect. Starting from three primaries, you can only reconstruct certain colors. The range of colors that can be reconstructed is called the &quot;gamut&quot;. You can search for &quot;sRGB gamut&quot; and find pictures that show a triangle inside a larger parabola. The triangle represents the colors that we can make from the sRGB primaries, and the parabola is all the colors we can see. From this it's clear that *any* triangle inside the parabola will be smaller than it." CreationDate="2017-10-28T02:20:25.410" UserId="7557" />
  <row Id="8759" PostId="5784" Score="6" Text="Because humans have red, green and blue receivers in their eyes." CreationDate="2017-10-28T02:43:44.590" UserId="2316" />
  <row Id="8760" PostId="5786" Score="0" Text="woops, you're right. I've replaced &quot;all&quot; with &quot;most&quot; and will try to think of an explanation for the remaining visible colors." CreationDate="2017-10-28T07:38:27.817" UserId="182" />
  <row Id="8761" PostId="5786" Score="4" Text="Also the concept of white light is governed by our really fancy  white balance system it dont mich matter what the color is it will be precieved as white. Incandescent lightbulbs are orange but if we are inside the house we precieve them as white. As for the extra colors, if you integrate the energies of your color distribution multiplied by curves ratchet freaks show you youll notice that sometimes  you get unique signals because the overlap is different." CreationDate="2017-10-28T07:45:14.310" UserId="38" />
  <row Id="8762" PostId="5789" Score="0" Text="_The region outside of the triangle cannot be shown on your screen._ Well, I can see this region on screen with colors. So how can it be more precisely stated?" CreationDate="2017-10-28T12:14:57.297" UserId="4958" />
  <row Id="8763" PostId="5789" Score="2" Text="@narthex: Thanks for the comment. I updated the answer. Is it any better now?" CreationDate="2017-10-28T12:28:30.443" UserId="7560" />
  <row Id="8764" PostId="5789" Score="0" Text="Ok, thats fine. But what acually the values on each axis mean? This is never explained." CreationDate="2017-10-28T12:39:43.350" UserId="4958" />
  <row Id="8765" PostId="5789" Score="0" Text="@narthex see https://en.wikipedia.org/wiki/CIE_1931_color_space#CIE_xy_chromaticity_diagram_and_the_CIE_xyY_color_space" CreationDate="2017-10-28T12:57:57.177" UserId="7560" />
  <row Id="8766" PostId="5599" Score="0" Text="Did my answer solved your question?" CreationDate="2017-10-28T13:26:26.260" UserId="2287" />
  <row Id="8767" PostId="5789" Score="1" Text="And also, (staring at that last image), the red circle dances around. Fun" CreationDate="2017-10-28T16:47:20.703" UserDisplayName="user7566" />
  <row Id="8768" PostId="5789" Score="4" Text="The problem with CIE colorspace plots is that they are very hard to understand, hell we dont even know if some of the areas in the graph happen to make metamers.  Also the reason why you simply can not make a bigger triangle is not apparent (hint there is nothing outside the shape)." CreationDate="2017-10-28T18:33:18.013" UserId="38" />
  <row Id="8769" PostId="5784" Score="2" Text="This would be better for biology stack exchange (if there is one) because it is more a question about the human visual system than one of computer graphics." CreationDate="2017-10-28T18:57:16.603" UserId="7567" />
  <row Id="8770" PostId="5784" Score="2" Text="@mathreadler https://biology.stackexchange.com" CreationDate="2017-10-28T19:06:34.587" UserId="7568" />
  <row Id="8771" PostId="5784" Score="0" Text="@briantist wow cool, who knew." CreationDate="2017-10-28T19:09:28.190" UserId="7567" />
  <row Id="8772" PostId="5784" Score="0" Text="Good answers here and good answers on Biology.SE will give different perspectives, so I'm glad the question has been asked on both sites." CreationDate="2017-10-28T19:18:57.623" UserId="231" />
  <row Id="8774" PostId="5789" Score="2" Text="@joojaa: https://xkcd.com/1882/" CreationDate="2017-10-28T19:24:20.387" UserId="7560" />
  <row Id="8775" PostId="5784" Score="0" Text="[Similar/related questions on Biology.SE for comparison](https://biology.stackexchange.com/search?tab=votes&amp;q=red%20green%20blue%20is%3aq)" CreationDate="2017-10-28T19:29:09.067" UserId="231" />
  <row Id="8776" PostId="5789" Score="1" Text="@narthex The screen can show a _diagram_ that _represents_ the CIE color space.  You don't even need a color screen for that:  You can just have text labels &quot;red&quot;, &quot;orange&quot;', &quot;yellow&quot;,...  What the screen can't show is all of the actual colors that exist in that space." CreationDate="2017-10-28T19:33:24.810" UserId="7569" />
  <row Id="8777" PostId="5790" Score="0" Text="Depends what you mean by &quot;color.&quot;  In many contexts, it makes a lot of sense to say that if nobody can see the difference between two different patches on a surface, then both patches must be the same &quot;color.&quot;  On the other hand, when a painter says &quot;color,&quot; she or he is talking about the physical substance into which he/she dips a brush.  In that case, see https://en.wikipedia.org/wiki/Metamerism_(color)#Metameric_failure" CreationDate="2017-10-28T19:38:32.483" UserId="7569" />
  <row Id="8778" PostId="5784" Score="0" Text="@mathreadler Actually this is better for [cognitive sciences.se](https://cogsci.stackexchange.com/). More accurately the place that mixes all that's needed. But its fine here as well." CreationDate="2017-10-28T19:53:42.347" UserId="38" />
  <row Id="8779" PostId="5790" Score="1" Text="@jameslarge: It really doesn't. Just because they look the same under one light source doesn't mean they'll look the same under a different one, even if both light sources look identical on a white surface." CreationDate="2017-10-28T21:02:00.480" UserId="3411" />
  <row Id="8780" PostId="5789" Score="4" Text="Great, now I have a cyan dot in the middle of my vision :-(" CreationDate="2017-10-29T00:32:15.310" UserId="7576" />
  <row Id="8781" PostId="5784" Score="4" Text="Apparently there is at least one tetrachromat woman (see https://en.wikipedia.org/wiki/Tetrachromacy) that is able to distinguish more colours than those of us who are trichromat." CreationDate="2017-10-29T03:53:59.353" UserId="7579" />
  <row Id="8782" PostId="5793" Score="0" Text="What are &quot;original lines&quot;?" CreationDate="2017-10-29T04:08:29.697" UserId="113" />
  <row Id="8783" PostId="5793" Score="0" Text="Sorry for not being clear, I fixed that now. I hope its clear now" CreationDate="2017-10-29T04:35:41.817" UserId="7578" />
  <row Id="8784" PostId="5793" Score="3" Text="This diagram simply shows how in perspective projection the parallel lines are not parallel in the resulting image (image plane). They would only be parallel in orthogonal projection. The author of the diagram could project any lines he wanted to on the image plane. They don't have to be parallel with the line going through the vanishing point." CreationDate="2017-10-29T04:47:28.343" UserId="113" />
  <row Id="8785" PostId="5784" Score="0" Text="@immibis that is a gross over-simplification and really quite inaccurate." CreationDate="2017-10-29T05:54:20.360" UserId="7581" />
  <row Id="8786" PostId="5790" Score="0" Text="I don't think this answers the question in any way.  It also applies to any colours - not just violet and purple.  Monochromatic light of any hue from red through to violet won't get split by a prism, and any mixed light will get split." CreationDate="2017-10-29T05:59:17.800" UserId="7581" />
  <row Id="8787" PostId="5784" Score="1" Text="@BillBell and probably its digital counterpart: [Quattron](https://en.wikipedia.org/wiki/Quattron), adding yellow color subpixels on Sharp Aquos LCD TV." CreationDate="2017-10-29T07:51:34.757" UserId="2170" />
  <row Id="8788" PostId="5791" Score="1" Text="One common example for that is the quasi-monochromatic sodium vapor lamps, which are commonly used for city lamps and look always different in reality than on photos." CreationDate="2017-10-29T09:03:15.753" UserId="182" />
  <row Id="8789" PostId="5795" Score="0" Text="Yeah i looked at the later code now after giving up and it was definitly a missprint. It works out with (A-C) = y as you said." CreationDate="2017-10-29T14:20:49.803" UserId="7577" />
  <row Id="8790" PostId="5784" Score="1" Text="@AndrewT.: I expect that a lot of this is lost on people like me that can't see the difference between 'Saddle Brown' and 'Sienna'. :)" CreationDate="2017-10-29T16:16:11.667" UserId="7579" />
  <row Id="8791" PostId="5782" Score="0" Text="2D is no different than 3D. check out https://youtu.be/0YdPHNJjSxI. In convential CPU-style thinking you'll tell yourself `for all lights: modify affected pixels`. In GPU thinking your start point is already inside the `for all pixel` loop. So you need the `all lights` loop &quot;nested&quot; in your fragment code. You must have a structured buffer (or texture, or constant buffer) that tells you lights information so you access `light[index].position` and `SV_Position` being the fragment pos, you have the light vector. from this  you can do a usual lambert &amp; attenuation calculation" CreationDate="2017-10-30T02:09:31.583" UserId="1614" />
  <row Id="8792" PostId="5791" Score="0" Text="but those are fringe issues, I would consider very advanced. The issue doesn't materialize in most cases, RGB is just a fourier encoding with 3 harmonics of some signal that happens to be enough for most cases." CreationDate="2017-10-30T02:14:26.250" UserId="1614" />
  <row Id="8793" PostId="5793" Score="1" Text="@AndrewWilson, absolutely. And the OP phrase `to the parallel lines of the image` is invalid, because there is no such thing that &quot;THE&quot; parallel lines. It's only meant to mean that some cubic looking, well aligned geometry, like architecture and streets, will tend to display lots of parallel lines that &quot;engenders&quot; the presence of a virtual vanishing point. Do anybody speaks of vanishing points in image of trees and ponds ?" CreationDate="2017-10-30T02:23:11.547" UserId="1614" />
  <row Id="8794" PostId="2325" Score="0" Text="honestly I'm not completely sure about `to be completely unbiased it must be random`. I think you can still get math-ok results by using fractional weigthing of samples, rather than the binary pass/drop that the russian roulette imposes, it's just that the roulette will converge faster because it's operating a perfect importance sampling." CreationDate="2017-10-30T02:36:58.093" UserId="1614" />
  <row Id="8795" PostId="5791" Score="0" Text="@JulienGuertault: While that's a nice example, I don't think it's quite an example of what my answer is pointing out - as long as your photosensor's/film's responses to the light in its 3 components matches the human eye's well enough, it should faithfully represent what a human would see. Where RGB (or any other model that lumps whole ranges of the frequency spectrum together) is insufficient is for actually modelling surfaces and light sources in a way that you can predict the perceived color of a light on a surface." CreationDate="2017-10-30T02:40:58.363" UserId="3411" />
  <row Id="8796" PostId="2004" Score="0" Text="@JulienGuertault I would say this is plain HiZ tracing. it's a safe method to be sure of what you hit. But not very fast compared to unsafe methods like binary search." CreationDate="2017-10-30T02:46:17.023" UserId="1614" />
  <row Id="8797" PostId="5791" Score="0" Text="@v.oddou: &quot;I don't care, it looks good enough&quot; is a reasonable position to take, but there really is a difference. You won't be able to model the way the color of a wall looks different under daylight vs incandescent light vs led light that's supposed to be the same color temp as one or the other." CreationDate="2017-10-30T02:52:23.420" UserId="3411" />
  <row Id="8799" PostId="5793" Score="0" Text="This question seems nonsensical as it currently stands. Perhaps a photo showing which parallel lines you mean would clarify?" CreationDate="2017-10-30T03:28:54.167" UserId="3003" />
  <row Id="8800" PostId="5791" Score="0" Text="hmm, I might have misunderstood. Do you have a concrete example of the limitation you are referring to?" CreationDate="2017-10-30T03:42:51.897" UserId="182" />
  <row Id="8802" PostId="5789" Score="0" Text="It's worth noting that the CIE 1931 diagrams are somewhat misleading in the sense that the distance between two arbitrary points within it does not correspond to the perceived difference in colour. In other words, the area within and without the sRGB triangle is not representative of the perceived gamut: the difference is not that dramatic. (On the 1931 diagram, sRGB occupies less than half of the area). The 1976 version is much better in this regard." CreationDate="2017-10-30T05:01:06.800" UserId="7594" />
  <row Id="8806" PostId="5791" Score="0" Text="I've seen plenty of visual phenomena that *seem to be* examples of what I'm saying, but haven't done experiments or anything to confirm. The canonical example would be something like a narrow-band yellow light source on a material that reflects most light but absorbs a band around the particular yellow of the light source. In an RGB model, the material's color would be close to white (1.0,1.0,1.0), the light's color would be something near (1.0,1.0,0.0), and a rendering would show a yellow-lit surface whereas in reality the surface would be barely-lit." CreationDate="2017-10-30T16:22:51.063" UserId="3411" />
  <row Id="8807" PostId="5782" Score="0" Text="You mean something like this? https://github.com/matyasf/sparrow-game/blob/master/SparrowGame.Shared/CustomMesh/FragShader.frag This is sadly dog slow, I can render ~5-10 lights. The compute shader above can render ~200 with the same FPS." CreationDate="2017-10-30T23:16:17.257" UserId="2437" />
  <row Id="8809" PostId="5782" Score="0" Text="It's slow because it is doing step marching. marching is necessary only for volumetric effects of complex intersection routines. why do you need this ?" CreationDate="2017-10-31T02:59:19.233" UserId="1614" />
  <row Id="8810" PostId="5599" Score="0" Text="Yeah thanks, I've got it working now. I upgraded to the latest version of directx math and copied the source directly from Rastertek (second series). Still confused about what the bug actually was... Not alignment, AFAICT, since compiling 64-bit forces 16-byte heap alignment. And the other matrix methods worked fine and produced valid results. Possibly a subtle bug in my own copy of the code. Going forward, I think I'll work with Microsoft's own DirectX tutorials since these seem to work well out of the box with a minimum of hassle." CreationDate="2017-10-31T04:06:20.703" UserId="1937" />
  <row Id="8811" PostId="5782" Score="0" Text="Because I want volumetric effects, like my original algorithm :) This is not just a simple shadow cast, I want something new. It would be able to render nice god rays, light trough smoke, fog, coloured glass, etc. My aim is to make something that looks a bit different than everything else out there." CreationDate="2017-10-31T08:25:02.957" UserId="2437" />
  <row Id="8812" PostId="360" Score="0" Text="&quot;adjust the knots so that they lie on top of each other. This is essentially a bit like having several control points on top of each other&quot; -- Well maybe it's a bit similar, but not much. Making control points coincident will cause derivative vectors to be zero, which leads to all sorts of problems. Making knots coincident is benign." CreationDate="2017-10-31T12:24:18.713" UserId="6968" />
  <row Id="8813" PostId="360" Score="0" Text="&gt; you end up with a cusp -- you have the *potential* for forming a cusp, but you won't necessarily get a cusp." CreationDate="2017-10-31T12:28:55.533" UserId="6968" />
  <row Id="8816" PostId="360" Score="0" Text="@bubba yes its possible to fit on the other side, and yes if you have multiple knots on top you loose some of the interpolant to no movement, anything outside this area is same." CreationDate="2017-10-31T14:21:26.733" UserId="38" />
  <row Id="8817" PostId="5782" Score="0" Text="Ok, then in my book there are only 2 ways : either you pay the simulation price, either you find an analytical solution (or a bunch of separated solutions, like Antoine Bouthors did in his 2008 paper for cloud rendering). I can't help you much beyond that" CreationDate="2017-11-01T01:50:09.557" UserId="1614" />
  <row Id="8818" PostId="5778" Score="0" Text="@wandering-warrior That would be waay too slow, this is for a game. To save GPU time I am shooting rays only to the edge of the circle, these colour the path they trace. Just they don't do it evenly, so the artifacts." CreationDate="2017-11-01T01:56:59.837" UserId="2437" />
  <row Id="8819" PostId="5802" Score="0" Text="Powerpoint is actually quite expensive considering things. Its ok, after all drawing skill comes from you. But atleast tikz, inkscape, blrnder and postscript are way cheaper than powerpoint." CreationDate="2017-11-01T05:38:45.533" UserId="38" />
  <row Id="8820" PostId="5803" Score="0" Text="If you can pass the coordinates of the original line to the fragment shader, you can try to find the distance from the fragment to the original relevant line segment, and then discard any fragments which are further away than your line thickness." CreationDate="2017-11-01T09:03:18.037" UserId="457" />
  <row Id="8821" PostId="5802" Score="0" Text="I'm old, so time is more important to me than money. Inkscape is good, but there's no way I'd spend time writing tikz or Postscript code to produce pictures." CreationDate="2017-11-01T12:47:00.937" UserId="6968" />
  <row Id="8822" PostId="5803" Score="0" Text="&quot;*I believe is the job for the fragment shader as I dont have enough processing resources to calculate more geometry.*&quot; Considering that most line systems of this sort use only the CPU to rasterize them, I rather suspect you have plenty of processing resources to calculate more geometry." CreationDate="2017-11-01T14:32:53.923" UserId="2654" />
  <row Id="8823" PostId="5801" Score="1" Text="&quot;Links to external resources are encouraged, but please add context around the link so your fellow users will have some idea what it is and why it’s there. Always quote the most relevant part of an important link, in case the target site is unreachable or goes permanently offline.&quot; - [Computer Graphics SE Help Center](https://computergraphics.stackexchange.com/help/how-to-answer)" CreationDate="2017-11-01T15:56:32.663" UserId="7431" />
  <row Id="8824" PostId="5803" Score="0" Text="@NicolBolas If I were just rendering lines I certainly would but the simulation that creates the lines is quite expensive." CreationDate="2017-11-01T16:36:32.640" UserId="2308" />
  <row Id="8825" PostId="5804" Score="0" Text="Could you expand a bit more on the 2nd idea? I have used a simple stencil buffer as a mask before but never for something so complicated as telling the GPU not to draw fragments where the same object has already drawn fragments. How exactly would I fill up the stencil buffer?" CreationDate="2017-11-01T16:38:23.830" UserId="2308" />
  <row Id="8826" PostId="5804" Score="0" Text="It's really just as simple as that. If you used the stencil buffer for masking before, that's exactly what you do. You clear it to all 0s, then configure the test to succeed only where there's 0 and write a 1 (or whatever) for each fragment that passes. This way every pixel gets written only once." CreationDate="2017-11-01T16:41:50.493" UserId="6" />
  <row Id="8827" PostId="5804" Score="0" Text="When using alpha-blending for antialiasing together with the seperated version described last, you should not perform testing for the line middle sections but only fill the stencil buffer, then test against that when drawing the connection points." CreationDate="2017-11-01T16:43:52.150" UserId="6" />
  <row Id="8828" PostId="5810" Score="2" Text="Can you show your shader code—how are you declaring and using the SSBOs in the shader?" CreationDate="2017-11-02T05:47:51.173" UserId="48" />
  <row Id="8829" PostId="5757" Score="0" Text="Dan, here is the project that I was working on: https://anvaka.github.io/fieldplay/ I can finally share it. The source code is here https://github.com/anvaka/fieldplay - this is vector field explorer/simulation. Maybe playing with it would give you better ideas of what I'm trying to do? If you don't have time for this - no worries!" CreationDate="2017-11-02T05:50:51.910" UserId="7512" />
  <row Id="8830" PostId="5805" Score="1" Text="From the question title I thought you were asking about bilinear interpolation.  For other folks who came here looking for information on that, here's a link (but feel free to explicitly ask a question too) https://blog.demofox.org/2015/04/30/bilinear-filtering-bilinear-interpolation/" CreationDate="2017-11-02T16:28:33.570" UserId="56" />
  <row Id="8831" PostId="5811" Score="1" Text="The arrows are there to point to the line which is the problem you are trying to solve right?" CreationDate="2017-11-02T16:30:42.870" UserId="56" />
  <row Id="8832" PostId="5811" Score="0" Text="So... what exactly are those lines? Are they drawn via `GL_LINES`? Are they textures?" CreationDate="2017-11-02T22:52:28.457" UserId="2654" />
  <row Id="8833" PostId="5806" Score="0" Text="Good answer. But it's mostly about using barycentric coordinates, which work best with triangles. So, do GPUs just break a quad into two triangles? If they do, aren't there some issues related to continuity across the junction?" CreationDate="2017-11-03T00:41:18.097" UserId="6968" />
  <row Id="8834" PostId="5811" Score="0" Text="Sorry about thedelay, the black lines are not the issue, the issue  are the red line and red dots." CreationDate="2017-11-03T01:10:55.770" UserId="7462" />
  <row Id="8836" PostId="5815" Score="0" Text="I *imagine* it's been borrowed from audio where, I guess, it implies you can have accurate reproduction at both high and low volumes (within the same piece of music)" CreationDate="2017-11-03T08:39:24.033" UserId="209" />
  <row Id="8837" PostId="2519" Score="0" Text="@ivokabel How is the term $-1+ \sqrt{1+\alpha^2tan^2\theta}\over 2$ derived?" CreationDate="2017-11-03T09:37:25.893" UserId="5256" />
  <row Id="8838" PostId="5811" Score="0" Text="Is it a HW renderer or software? One possible reason for this could be a lack of Z precision in that a red surface (though further away) is resulting in the same Z-buffer depth (due to limited precision) at a problematic pixel as the neighbouring cube's blue surface.  Things you could try: 1) Move the front plane closer to the object to try to get more precision or 2) turn off &quot;equals&quot; in the Z test and draw all blue, then red &amp; yellow surfaces." CreationDate="2017-11-03T12:33:06.073" UserId="209" />
  <row Id="8839" PostId="5816" Score="0" Text="*&quot;plot these seamns in a 3D matrix&quot;*? Can you give a short explanation what you understand as a &quot;3D matrix&quot;?" CreationDate="2017-11-03T12:33:58.207" UserId="6" />
  <row Id="8840" PostId="5806" Score="0" Text="Yes, they do. And yes, there are (or can be), but those are relatively minor depending on your geometry. In fact modern APIs for hardware accelerated rasterization have dropped the concept of a quad altogether, since it has always been just a convenience feature without any special treatment compared to just two triangles. So nowadays even the breaking into two triangles isn't done by the GPU but has to be done by you." CreationDate="2017-11-03T13:32:09.157" UserId="6" />
  <row Id="8841" PostId="5811" Score="0" Text="Just to be clearer by &quot;Move the front plane closer&quot; I meant the front clipping plane." CreationDate="2017-11-03T16:42:17.637" UserId="209" />
  <row Id="8842" PostId="5816" Score="0" Text="Yup! Essentially as voxels. So I'm imagining a 100x100x100 3D space. I would plot the meshes in that space. Then I would plot the seams in that same space, on top of the meshes. Is that clearer?" CreationDate="2017-11-03T17:47:29.923" UserId="7622" />
  <row Id="8844" PostId="5817" Score="0" Text="I unwrapped a cube in Blender into six islands to test out what you're using. I got 8 vertices but each was linked to 3 different textures, like so: f 1/1/1 2/2/1 3/3/1 4/4/1 f 5/5/2 8/6/2 7/7/2 6/8/2 f 1/9/3 5/5/3 6/10/3 2/11/3 f 2/12/4 6/13/4 7/14/4 3/15/4 f 3/16/5 7/17/5 8/18/5 4/19/5 f 5/5/6 1/9/6 4/20/6 8/6/6 Is this an example of the vertices having been deduplicated? (Because there is only one instance of each vertex but each is linked to several textures)" CreationDate="2017-11-03T20:29:52.190" UserId="7622" />
  <row Id="8845" PostId="5817" Score="0" Text="@ganesha123 yeah, I dislike the obj format but this is the one thing it's decent for." CreationDate="2017-11-03T20:40:54.587" UserId="137" />
  <row Id="8846" PostId="5811" Score="0" Text="Sounds like a Z-fighting issue? I don’t think OpenGL guarantees precise polygon rasterization." CreationDate="2017-11-03T23:47:18.773" UserId="7624" />
  <row Id="8849" PostId="5827" Score="0" Text="thanks for the answer. i use a quad tree, so the removal depends on the quad tree level that is displayed.&#xA;my idea would be to set a threshold based on the LOD level and remove trees up to that threshold (although first fading them is probably a visually good idea). my problem however is the randomization of tree removal, since i don't want to accidentally remove a cluster of trees and produce big holes in my forest that way" CreationDate="2017-11-06T15:04:13.337" UserId="7008" />
  <row Id="8850" PostId="5827" Score="0" Text="This depends on your data set. If it's small, nothing beats manual tagging for removal. Otherwise, randomly assign an LOD level to trees in each &quot;region&quot; based on desired coverage. How you define those regions largely depends on how your data is stored. You can also probably get fancy and refine the tagging process somehow. I would however expect the simple random tagging to suffice on average. It's also much easier to implement than any other alternative!" CreationDate="2017-11-06T15:13:29.487" UserId="7644" />
  <row Id="8852" PostId="5827" Score="0" Text="the quadtree is being introduced because if very large data sets. also, the trees are randomly placed based upon settings the customer will use for the forest, so manual tagging is not an option.&#xA;&#xA;okay, thank you. for now i will try the random removal then, otherwise i will experiment with marking nearest neighbors of removed trees to prevent removal of close-by trees." CreationDate="2017-11-06T15:20:33.593" UserId="7008" />
  <row Id="8853" PostId="5825" Score="0" Text="Are these guaranteed to be the exact color you've shown above, or might they vary a bit from region to region?" CreationDate="2017-11-07T03:21:39.920" UserId="3003" />
  <row Id="8855" PostId="5825" Score="0" Text="@user1118321 Let’s assume they’re exact for now." CreationDate="2017-11-07T08:53:22.393" UserId="7641" />
  <row Id="8856" PostId="5824" Score="3" Text="That's a nice explanation, but seems to miss answering the actual question, which didn't ask for what HDR is but just what the D in there derives from etymologically." CreationDate="2017-11-07T09:07:56.210" UserId="6" />
  <row Id="8857" PostId="4654" Score="0" Text="&quot;Vulkan also exists but I don't know whether this API is mature enough yet.&quot; It's mature but Apple doesn't support it, so you won't meet your goal of running on any OS." CreationDate="2017-11-07T14:28:38.273" UserId="2041" />
  <row Id="8858" PostId="5835" Score="0" Text="Although this is a clear answer i would really like to see the process animated. But hey we are graphics programmers that shouldn't be too hard. Ill try tomorrow." CreationDate="2017-11-07T20:49:34.520" UserId="38" />
  <row Id="8859" PostId="5835" Score="0" Text="Perfect, thank you very much." CreationDate="2017-11-08T00:04:31.160" UserId="7462" />
  <row Id="8860" PostId="5835" Score="1" Text="Also, what software did you use to draw/generate this?" CreationDate="2017-11-08T01:22:29.563" UserId="7462" />
  <row Id="8861" PostId="5836" Score="2" Text="What kind of algorithm are you thinking of for mesh conversion? Would it be an edge detection type algorithm, just putting one point for every pixel and connecting them together, a single textured plane, voxel meshing, or something else?" CreationDate="2017-11-08T01:27:24.900" UserId="7431" />
  <row Id="8863" PostId="5835" Score="0" Text="@Makogan you may want to look at [this post](https://computergraphics.stackexchange.com/questions/405/how-to-produce-simple-2d-illustrations-to-accompany-geometry-answers/) for ideas on how to draw. Though the choice is a highly personal one." CreationDate="2017-11-08T07:32:45.383" UserId="38" />
  <row Id="8864" PostId="5836" Score="1" Text="Are these slices of the same object? So are you trying to in fact conver a voxel image to a mesh." CreationDate="2017-11-08T07:39:42.997" UserId="38" />
  <row Id="8865" PostId="5835" Score="0" Text="Most of the software recommended there is payed software, I have a thing for free software :p" CreationDate="2017-11-08T08:32:39.480" UserId="7462" />
  <row Id="8866" PostId="5836" Score="0" Text="@ScottMilner: thanks for your comment. I'm new to computer graphics and I need any simple algorithm that give me a meshed image representing the pictures like above. So, a meshed image of the apples (both the white boundary and interior, but not the black exterior) with specifically the vertices. The (geometric) framework I've will take the vertices of these meshes as a point cloud. &#xA;&#xA;I don't think it's a voxel meshing though-as voxels are for 3D, but my pictures are 2D images lying in the plane R².&#xA;&#xA;You can see more of the dataset at http://www.dabi.temple.edu/~shape/MPEG7/dataset.html" CreationDate="2017-11-08T10:03:30.070" UserId="7648" />
  <row Id="8867" PostId="5836" Score="0" Text="@joojaa: thanks for your comment. No, the images are slices of different objects. So it's not like I get these pictures when I take pictures of the same 3D apple from different angles. They're indeed 2D projection of possibly different apples. You can see more of the dataset at http://www.dabi.temple.edu/~shape/MPEG7/dataset.html" CreationDate="2017-11-08T10:07:16.313" UserId="7648" />
  <row Id="8868" PostId="5836" Score="0" Text="@Mathmath then how do you propose i make them 3D? Extrusion?" CreationDate="2017-11-08T11:06:13.383" UserId="38" />
  <row Id="8869" PostId="5836" Score="1" Text="Something like [this image](https://i.stack.imgur.com/wHihD.png)?" CreationDate="2017-11-08T13:23:42.723" UserId="38" />
  <row Id="8870" PostId="5836" Score="0" Text="@joojaa: Thanks for the picture-except ideally a 2D projection of the 3D volumetric mesh, so ignoring all the z-co-ordinates of the mesh and building the corresponding 2D mesh. Is it possible? Thanks again!" CreationDate="2017-11-08T13:46:32.293" UserId="7648" />
  <row Id="8871" PostId="5787" Score="1" Text="There was a derivation done by Earl Hammon Jr in his talk, I hope that helps. He starts a derivation via Ray Tracing on slide 71. There is more information about it in the Appendix (slide 148ff.) I think.&#xA;&#xA;https://twvideo01.ubm-us.net/o1/vault/gdc2017/Presentations/Hammon_Earl_PBR_Diffuse_Lighting.pdf" CreationDate="2017-11-08T15:53:58.183" UserId="7008" />
  <row Id="8872" PostId="5787" Score="0" Text="@Tare Thanks so much, this is exactly what I needed! I'm gonna post an answer with the derivation soon." CreationDate="2017-11-08T18:26:06.650" UserId="5256" />
  <row Id="8874" PostId="5386" Score="0" Text="So, the only reason not to include all extensions, is that you have a smaller header file generated from glad?" CreationDate="2017-11-09T09:04:13.073" UserId="7000" />
  <row Id="8875" PostId="5835" Score="1" Text="@Makogan I made the diagrams in [asymptote](http://asymptote.sourceforge.net/), which is open-source." CreationDate="2017-11-09T09:09:45.870" UserId="7647" />
  <row Id="8876" PostId="5835" Score="1" Text="@joojaa I've added animations of quadratic and cubic B-spline generation." CreationDate="2017-11-09T12:37:33.663" UserId="7647" />
  <row Id="8877" PostId="5386" Score="1" Text="@Startec: It's not *just* a smaller header. It means that you can't *accidentally* use extensions that you don't want to. It also means that you get better IntelliSense, since you're only going to see functions and enums that you want to be able to use." CreationDate="2017-11-09T14:25:06.013" UserId="2654" />
  <row Id="8878" PostId="5835" Score="0" Text="@gilgamec yeah that meskes it easier to follow" CreationDate="2017-11-09T14:56:45.187" UserId="38" />
  <row Id="8879" PostId="5835" Score="0" Text="@Makogan atleast half of all the suggested tools are free, its just that if you want nice sofdtware that do all you need quickly you need to pay. But obviously a researcher has access to even nonfree software for free." CreationDate="2017-11-09T14:58:41.900" UserId="38" />
  <row Id="8881" PostId="5845" Score="0" Text="+1 - just not need break the font into the parts, because the font itself is stored in the `ttf` as a pairs of x-y coordinates. (the connections between the points could be straight or some are quadratic beziers). Also see my answer above - it is _too complicated_ for my use-case. **Thanks** anyway for the answer it kicked me to the right direction - e.g. start studying the ttf font structure. :)" CreationDate="2017-11-10T09:16:48.450" UserId="7661" />
  <row Id="8884" PostId="5847" Score="1" Text="See also Giliam de Carpentier's series of articles on terrain on [his blog](http://www.decarpentier.nl/)—look for the &quot;Scape&quot; articles." CreationDate="2017-11-10T23:23:13.840" UserId="48" />
  <row Id="8885" PostId="5855" Score="0" Text="So to sum up *never* (even when talking to OpenGL) use `GLvoid` and only use GLtypes when talking to OpenGL.  If you need fixed bitdepth integers in your code that does not talk to OpenGL use `cstdint`?" CreationDate="2017-11-12T02:32:27.357" UserId="7000" />
  <row Id="8886" PostId="5855" Score="0" Text="And why never use `GLvoid`?" CreationDate="2017-11-12T02:37:37.610" UserId="7000" />
  <row Id="8887" PostId="5855" Score="0" Text="`GLuint` has a meaning that is distinct from the fundamental type `unsigned int`. `GLvoid` is no different from `void`." CreationDate="2017-11-12T02:47:59.233" UserId="2654" />
  <row Id="8889" PostId="5859" Score="0" Text="Ah, that makes sense. So, I'm guessing from your statement, that the remapped roughness is just to make the artists happy, right? It's not required." CreationDate="2017-11-13T00:12:01.873" UserId="6838" />
  <row Id="8890" PostId="5850" Score="0" Text="Can you explain more clearly what you're trying to do, which information is given and which you are trying to solve for?" CreationDate="2017-11-13T03:53:08.377" UserId="48" />
  <row Id="8892" PostId="5859" Score="0" Text="Yes, it's unnecessary. If you look into Scientific Papers (like the original GGX Paper by Walter et. al), there is no remapping done.&#xA;&#xA;Also from a logical viewpoint, if you use $0.25$ as your roughness or $0.5^2$ as your roughness shouldn't make a difference." CreationDate="2017-11-13T07:18:39.050" UserId="7008" />
  <row Id="8895" PostId="5864" Score="0" Text="Thank you for the answer. but what is the algorithm if I one wants to implement it and not using 3rd parties? I use c# language." CreationDate="2017-11-13T10:18:02.697" UserId="6041" />
  <row Id="8896" PostId="5864" Score="0" Text="Just to note that my question referred to how the black and white stripped image is created given the sine or whatever signal." CreationDate="2017-11-13T10:28:14.170" UserId="6041" />
  <row Id="8897" PostId="5864" Score="0" Text="In that case, could you edit your question and make it clearer? Regardless of language, the short answer is &quot;make an array of values from your function and then convert it into the image type you're using&quot;." CreationDate="2017-11-13T10:40:48.127" UserId="2041" />
  <row Id="8898" PostId="5864" Score="0" Text="Sorry about the question not being clear enough. I have updated it. I still don't understand how to do it could you give an example please." CreationDate="2017-11-13T12:04:35.277" UserId="6041" />
  <row Id="8899" PostId="5862" Score="0" Text="You need to explain where you're stuck. Your first example even includes all the Matlab code needed to make the image!" CreationDate="2017-11-13T14:15:18.120" UserId="2041" />
  <row Id="8900" PostId="5862" Score="0" Text="Thanks. I was not clear as to how get pixel colour based on the function. It works now" CreationDate="2017-11-13T15:11:27.000" UserId="6041" />
  <row Id="8901" PostId="4968" Score="1" Text="@metalim it seems that, for many implementations as of this writing, the maximum time that the client is allowed to wait is 0. You may check the value with `gl.getParameter(gl.MAX_CLIENT_WAIT_TIMEOUT_WEBGL)`. That MAX_CLIENT_WAIT_TIMEOUT_WEBGL is an enum and not a value is not well presented in the documentation which was (for me at least) a source of confusion." CreationDate="2017-11-13T16:59:00.170" UserId="7679" />
  <row Id="8902" PostId="5867" Score="0" Text="A simple lerp (linear interpolation) between a light and dark blue would work." CreationDate="2017-11-13T19:39:19.787" UserId="113" />
  <row Id="8903" PostId="5855" Score="0" Text="On my system I see `typedef unsigned int    GLuint;` and `typedef void        GLvoid;` so how is `GLvoid` no different but `GLuint` is?" CreationDate="2017-11-13T20:16:25.147" UserId="7000" />
  <row Id="8904" PostId="5855" Score="1" Text="@Startec: Headers will redefine `GLuint` based on the platform(s) they work on. `GLvoid` will *always* be defined as `void`." CreationDate="2017-11-13T20:18:25.890" UserId="2654" />
  <row Id="8905" PostId="5867" Score="5" Text="Even for a given time of day (and a fixed atmospheric turbidity), the color of the sky is different in different directions. [Mitsuba](https://www.mitsuba-renderer.org/) uses the model of [Hosek and Wilkie](http://cgg.mff.cuni.cz/projects/SkylightModelling/), which I assume is the state of the art." CreationDate="2017-11-13T21:49:10.187" UserId="106" />
  <row Id="8906" PostId="5866" Score="0" Text="Yes, that's exactly what I'm looking for, triangulated 2D polygons. Thanks for the steps!" CreationDate="2017-11-13T22:16:20.447" UserId="7648" />
  <row Id="8907" PostId="5859" Score="0" Text="@Matthias: That looks like an interesting read. I'll make sure to read through it, thanks for the link." CreationDate="2017-11-14T00:43:57.673" UserId="6838" />
  <row Id="8908" PostId="5863" Score="0" Text="I noticed while learning basic Direct3D recently, you generally specify a single framebuffer when creating your swapchain. I'm guessing this simply gets blitted to the compositor when 'swapping' buffers? How about when switching to fullscreen mode, does a separate front buffer get created for you in that case?" CreationDate="2017-11-14T04:05:09.740" UserId="1937" />
  <row Id="8909" PostId="5867" Score="1" Text="When you say &quot;sky&quot; do you include clouds and the sun? Just earlier tonight part of the sky was bright pink and peach for a few minutes just before the sun went down." CreationDate="2017-11-14T05:30:16.500" UserId="3003" />
  <row Id="8910" PostId="5868" Score="0" Text="What about debugging tools? Do you have any experience with Nsight vs CodeXL? Which has the quickest/easiest edit -&gt; run -&gt; debug cycle?" CreationDate="2017-11-14T06:49:34.853" UserId="7674" />
  <row Id="8911" PostId="5859" Score="0" Text="Edit: I removed my comments to merge them in an answer." CreationDate="2017-11-14T09:01:30.220" UserId="2287" />
  <row Id="8912" PostId="5863" Score="0" Text="I don't know the specifics about Direct3D, but both those guesses are plausible. Typically the application framebuffer looks like a texture to the compositor, which it'll put on a quad of the appropriate size and position. I have no idea what fullscreen on MS Windows does: it may be something more complicated." CreationDate="2017-11-14T10:47:03.277" UserId="2041" />
  <row Id="8913" PostId="5863" Score="0" Text="If you ask a new question specifically about D3D fullscreen, it's more likely to be seen by someone who knows." CreationDate="2017-11-14T10:48:05.200" UserId="2041" />
  <row Id="8914" PostId="5863" Score="0" Text="OK, was just curious about how this worked in general, didn't want to open a duplicate-ish question. I don't think it's API-specific anyway, since OpenGL under Windows uses the same DXGI backend as D3D does." CreationDate="2017-11-14T12:09:42.933" UserId="1937" />
  <row Id="8915" PostId="5868" Score="0" Text="Although CodeXL is currently being developed by AMD, you can debug NVidia cards with it as well. I think just OpenCL won't work on Nvidia cards.&#xA;I can't say which is better though, and there are more tools (like RenderDoc) out there." CreationDate="2017-11-14T13:52:27.007" UserId="7008" />
  <row Id="8916" PostId="5868" Score="0" Text="@Tare [OpenCL support is included in the latest NVIDIA GPU drivers](https://developer.nvidia.com/opencl)." CreationDate="2017-11-14T14:33:22.750" UserId="5349" />
  <row Id="8917" PostId="5868" Score="0" Text="@Pikalek Okay, but is the debugging of OpenCL automatically supported in CodeXL? I haven't used it for a while, but I always got a warning about that." CreationDate="2017-11-14T14:47:34.943" UserId="7008" />
  <row Id="8918" PostId="5868" Score="0" Text="@Tare Offhand, I don't know either. My general point is to focus on the big picture, not the small details. To me, casual hobbyist dev means low stakes, casual research." CreationDate="2017-11-14T15:17:25.537" UserId="5349" />
  <row Id="8919" PostId="5871" Score="0" Text="Thank you for your help, I can understand your answer, but I don’t know which function should I choice to calculate the density?" CreationDate="2017-11-15T04:17:39.013" UserId="7675" />
  <row Id="8920" PostId="5732" Score="1" Text="note that $angle$ here is called the polar angle and $phi$ here is called the azimuth angle.&#xA;how you translate between spherical coordinates and euler angles depends on the way your euler angles are defined, since there are multiple ways to use them.&#xA;if you have access to these books, they have information about coordinate conversions and euler angles:&#xA;Real-Time Rendering 3rd Edition (in the fourth chapter there is information about Euler Angles)&#xA;Mathematics for 3D Game Programming and Computer Graphics 3rd Edition (in the appendix there is a lot about cartesian and spherical coordinates)" CreationDate="2017-11-15T07:52:02.800" UserId="7008" />
  <row Id="8924" PostId="5855" Score="3" Text="@Startec You can as well look at it with the logic you use in your question. `GLuint` makes sense, because you communicate that value to the GL, you either put it into a GL function or it's returned from a GL function. But that `GLvoid` function is **your** function, a C++ function that doesn't return anything anyway, let alone a GL value, you don't go `return glWhatever(...);`, since you don't return anything. `void` is just not a meaningful *data* type, whose representation would have any relevance, because it has none." CreationDate="2017-11-15T10:58:36.497" UserId="6" />
  <row Id="8925" PostId="5867" Score="2" Text="You probably need the altitude, time of year, latitude, longitude, etc. to achieve a perfect result. For instance, there are days on the Arctic, when it's always black" CreationDate="2017-11-15T16:56:37.910" UserId="7699" />
  <row Id="8926" PostId="5867" Score="0" Text="https://www.scratchapixel.com/lessons/procedural-generation-virtual-worlds/simulating-sky/simulating-colors-of-the-sky" CreationDate="2017-11-15T17:56:06.767" UserId="7699" />
  <row Id="8927" PostId="5872" Score="0" Text="Thanks - that does look relevant. I'll give that a try." CreationDate="2017-11-15T21:28:41.727" UserId="7635" />
  <row Id="8929" PostId="5881" Score="0" Text="I only have the smart alecky comment to make, that nothing in nature lines up perfectly, so I'm sure it isn't exactly 100hz, or the same every frame. But, I have no idea how close it is.  There is probably a bit of inaccuracy in whatever is measuring the hz as well." CreationDate="2017-11-15T23:32:05.740" UserId="56" />
  <row Id="8930" PostId="5881" Score="2" Text="100 Hz is _far_ too low for any PC graphics card clock speed from the last couple of decades. Even if the chip is completely idle and clocked down as far as it goes, it should still be running at some tens or hundreds of MHz. I guess 100 Hz could conceivably be the video refresh rate, not the clock speed. But that would still be strange, as refresh rates are almost always multiples of 60 Hz these days." CreationDate="2017-11-16T00:03:26.527" UserId="48" />
  <row Id="8931" PostId="5881" Score="1" Text="100Hz is far too low for any PC graphics card clock, ever. Formerly PAL countries continue to broadcast at 50Hz though, for the record. So multiples of 50 aren't too surprising." CreationDate="2017-11-16T03:27:56.423" UserId="7704" />
  <row Id="8932" PostId="5877" Score="0" Text="So it’s more the physical hardware make up (rgb filter, pixel size, backlight) that determines the gamut than say software like a color profile?" CreationDate="2017-11-16T11:49:20.447" UserId="7693" />
  <row Id="8933" PostId="5884" Score="0" Text="Need some help with the tags." CreationDate="2017-11-16T13:52:59.227" UserId="5846" />
  <row Id="8934" PostId="5884" Score="1" Text="The two images you have show completely unrelated techniques. The first is placing 3D objects on a surface. The second just downsamples a 2D image to 1-bit using [dithering](https://en.wikipedia.org/wiki/Dither). Which are you interested in?" CreationDate="2017-11-16T14:25:16.117" UserId="2041" />
  <row Id="8935" PostId="5884" Score="0" Text="@DanHulme ya OK I see what you mean - there is no effort to show 3D in the 2nd one, it's just coming from shading (original lighting) of the model. I've removed it." CreationDate="2017-11-16T14:27:14.963" UserId="5846" />
  <row Id="8936" PostId="5871" Score="0" Text="That's already in the answer: use the count." CreationDate="2017-11-16T14:31:44.680" UserId="5349" />
  <row Id="8937" PostId="5883" Score="0" Text="I'm not seeing the part where you use a member barrier to ensure visibility." CreationDate="2017-11-16T16:15:47.317" UserId="2654" />
  <row Id="8938" PostId="5885" Score="0" Text="The distance impostor concept is somehow related to the classical &quot;impostor&quot; definition, but I do not get how. The concept should be more clear in this paper http://sirkan.iit.bme.hu/~szirmay/ibl3.pdf" CreationDate="2017-11-16T19:07:07.547" UserId="7686" />
  <row Id="8939" PostId="5884" Score="1" Text="The kinect can produce depth images for you. The depth image essentially gives you a set of points in 3d space. A point cloud much like what we see above. You'll have to downsample, the resolution will be much too high. Then to get the shade of them (steeper areas should have darker colors) you can take the gradient of the image for your normals. Not sure how his lights are positioned or what kind of lights he's using exactly though. There does seem to be some sort of blur going on or maybe that's the fog." CreationDate="2017-11-17T00:24:22.590" UserId="113" />
  <row Id="8940" PostId="5877" Score="0" Text="@ohmmy  a color profile is a measurement of  a physical devices capabilities. You can emulate another device, and much of the standardisation has gone to define what to do when you can not reach the colors. But yes your gamut is fixed by the hardware. Obviously though if you have a wide gamut device it can emulate a device that is not so wide. So you can go down and you can use a intent to describe how to emulate when you can nlt." CreationDate="2017-11-17T00:46:54.953" UserId="38" />
  <row Id="8941" PostId="5877" Score="0" Text="Thanks. Makes sense now that it’s hardware and can only emulate a gamut below what’s been designed originally" CreationDate="2017-11-17T01:07:29.023" UserId="7693" />
  <row Id="8942" PostId="5883" Score="0" Text="@Nicol Bolas - Do we still need to use a memory barrier when &#xA;1) - We aren't reading/writing from/to a same image.&#xA;2) - We aren't overwriting any pixel of a image by subsequent `ImageStore`&#xA;&#xA;So there wouldn't be any need for that since every invocation is writing to a separate pixel?" CreationDate="2017-11-17T07:24:21.763" UserId="6046" />
  <row Id="8944" PostId="5883" Score="0" Text="I think you only need the barrier on the C/C++ side, after the compute shader has run but before passing the texture to the rendering command. Something like glMemoryBarrier(GL_SHADER_IMAGE_ACCESS_BARRIER_BIT);" CreationDate="2017-11-17T08:13:36.553" UserId="3073" />
  <row Id="8945" PostId="5883" Score="2" Text="If you found a solution, post it as an answer below." CreationDate="2017-11-17T12:42:24.917" UserId="4958" />
  <row Id="8947" PostId="5888" Score="0" Text="With regard to the idleness of threads: for MSAA $T$x, $T \times w \times h$ threads are used for retrieving data of which $w \times h$ threads perform the averaging and store the final results; for SSAA $T$x, $T \times T \times w \times h$ threads are used for retrieving data of which $w \times h$ threads perform the averaging and store the final results. Here, $w$ and $h$ represent the number of ouput texels for the width and height." CreationDate="2017-11-17T17:14:15.997" UserId="2287" />
  <row Id="8949" PostId="5888" Score="0" Text="Agree with this answer. Using a thread per texture read isn't likely to get your data out of memory any faster than issuing $T$ texture reads per thread—in either case you're almost certainly going to be memory-bandwidth-bound. So by using all those extra threads you're just reducing your occupancy and taking longer to get through all the threads in the dispatch." CreationDate="2017-11-18T00:05:06.693" UserId="48" />
  <row Id="8950" PostId="5887" Score="0" Text="FYI it's generally considered more useful to report timings in milliseconds rather than in FPS, especially when comparing deltas between different techniques." CreationDate="2017-11-18T00:06:31.663" UserId="48" />
  <row Id="8952" PostId="5869" Score="1" Text="Great job at summarizing everything! I'll also make sure to check out the BRDF Reference and test out some of them." CreationDate="2017-11-18T14:54:55.693" UserId="6838" />
  <row Id="8953" PostId="5869" Score="0" Text="@DanielKareh you can also take a look at my [brdf.hlsl](https://github.com/matt77hias/MAGE/blob/master/MAGE/MAGE/shaders/brdf.hlsli) which includes even more BRDF components :)" CreationDate="2017-11-18T15:02:09.103" UserId="2287" />
  <row Id="8954" PostId="5805" Score="0" Text="I'm on the &quot;split the quad into two triangles&quot; camp. I've written about how interpolation works in a triangle in some detail here: http://www.gabrielgambetta.com/computer-graphics-from-scratch/shaded-triangles.html It doesn't use barycentric coordinates at all, so it complements the other answer nicely :)" CreationDate="2017-11-14T18:33:51.950" UserId="7692" />
  <row Id="8955" PostId="5792" Score="0" Text="I've written a detailed, step by step derivation of the math here: http://www.gabrielgambetta.com/computer-graphics-from-scratch/basic-ray-tracing.html#ray-meets-sphere Hope this helps :)" CreationDate="2017-11-14T18:31:31.283" UserId="7692" />
  <row Id="8958" PostId="5895" Score="0" Text="_you MUST bind the VBO's under a VAO to get rendering output, if you just bind the VAO you won't get outpt._ That's not true. You can bind dummy VAO without attached VBOs and still dispatch drawcall." CreationDate="2017-11-20T10:06:30.390" UserId="4958" />
  <row Id="8959" PostId="5896" Score="0" Text="Thanks, I'm aware of the Sobel operator, but it slows my algorithm down too much. The MO answer assumes I have the isoline as a set of points. I don't, I have a 2D array of grayscale values." CreationDate="2017-11-20T12:29:04.790" UserId="3085" />
  <row Id="8960" PostId="5896" Score="0" Text="In that case, you might consider posting your code to the [CodeReview StackExchange](https://codereview.stackexchange.com/) and ask for performance help." CreationDate="2017-11-20T15:50:55.973" UserId="3003" />
  <row Id="8961" PostId="54" Score="0" Text="I think the answer is &quot;always&quot; *if* the compute shader is done properly. This is not trivial to achieve. A compute shader is also a better match than a pixel shader conceptually for image processing algorithms. A pixel shader however provides less leeway with which to write poorly performing filters." CreationDate="2017-11-20T18:32:48.447" UserId="7644" />
  <row Id="8962" PostId="54" Score="0" Text="@bernie Can you clarify what's needed for the compute shader to be &quot;done properly&quot;? Maybe write an answer? Always good to get more perspectives on the subject. :)" CreationDate="2017-11-20T18:36:11.990" UserId="48" />
  <row Id="8963" PostId="5896" Score="0" Text="I could, but the folks over there are less likely to have the specialized expertise for doing this *graphics* thing efficiently. Thanks again." CreationDate="2017-11-20T18:58:40.073" UserId="3085" />
  <row Id="8964" PostId="54" Score="1" Text="Now look at what you made me do! :)" CreationDate="2017-11-20T21:01:12.340" UserId="7644" />
  <row Id="8965" PostId="5907" Score="0" Text="That's cool. I'll need to give Vulkan a try one of these days, I had a look at it when it came out but got scared off by the 1000-line hello triangle program!" CreationDate="2017-11-21T12:21:58.317" UserId="1937" />
  <row Id="8966" PostId="5907" Score="0" Text="TBH most of those 1000 lines busywork is filling in structs that 80% of the time will not need to change, but are there for the remaining 20% to have a good path. Once you understand the general architecture of the api and can place why each struct is important it's not that horrible." CreationDate="2017-11-21T12:25:33.937" UserId="137" />
  <row Id="8971" PostId="5913" Score="0" Text="yes the only data I have is matrix itself. Only matrices (world, view, proj, projView) are cached inside camera." CreationDate="2017-11-22T13:00:10.290" UserId="6929" />
  <row Id="8972" PostId="5913" Score="0" Text="See my edited answer" CreationDate="2017-11-22T13:36:21.773" UserId="7725" />
  <row Id="8973" PostId="5913" Score="0" Text="but this  [Decompose the OpenGL projection matrix](http://lektiondestages.blogspot.com.tr/2013/11/decompose-opengl-projection-matrix.html) works for glFrustum, maybe some similar formula can work for glPerspective, no way? I will try your steps (thanks), it also seems interesting and matching to my goal (get and convert view frustum coordinates to another view space (light's point of view)" CreationDate="2017-11-22T13:52:55.327" UserId="6929" />
  <row Id="8974" PostId="5913" Score="0" Text="That formula also works for getting near and far from glPerspective, only left, right, top and bottom need to be calculated. Maybe m00 (f / aspect) and m11 (f) relation could help to get aspectRatio. Then we will have aspectRatio, near and far, then maybe we could get the remain values, I will give a try" CreationDate="2017-11-22T14:10:10.110" UserId="6929" />
  <row Id="8975" PostId="5913" Score="0" Text="Well, I think it does not matter whether the matrix was created with glFrustum or gluPerspective. If the formula from the link you provided works &quot;for glFrustum&quot; it should equally well work &quot;for gluPerspective&quot;. In fact, gluPerspective is built on top of glFrustum. gluPerspective simply &quot;remaps&quot; fov and aspect on left/right/top/bottom and calls glFrustum." CreationDate="2017-11-22T14:19:40.970" UserId="7725" />
  <row Id="8976" PostId="5913" Score="0" Text="My confusion was that some items used to decompose always is zero in glPerspective, maybe this doesn't change the result. I will compare same view frustum for both" CreationDate="2017-11-22T14:31:58.370" UserId="6929" />
  <row Id="8977" PostId="5869" Score="0" Text="Heh, thanks :)The main problem I find is that lots of the lighting models I find on Google or in papers are, I guess, scientific and don't include roughness, so your collection will be pretty useful." CreationDate="2017-11-22T15:56:24.877" UserId="6838" />
  <row Id="8978" PostId="5911" Score="0" Text="on point! Thank you so much :)" CreationDate="2017-11-22T17:06:06.577" UserId="7412" />
  <row Id="8980" PostId="5915" Score="4" Text="Nvidia Flex uses positional based fluids (PBF) for simulations and a variation of screen space fluids (SSF) for rendering fluids. Rendering and simulating fluids are two very separate tasks. Just start looking at some fluid simulation code on github and connect the ideas from books/papers/ppts to the code. In particular, the video shows PBF and SSF. But there are many other fluid simulation techniques like FLIP and SPH. http://mmacklin.com/pbf_sig_preprint.pdf" CreationDate="2017-11-23T02:14:48.397" UserId="113" />
  <row Id="8981" PostId="5915" Score="0" Text="Thank you for the information, Mr. Wilson. I will take a look at all these techniques and try them all." CreationDate="2017-11-23T02:50:39.207" UserId="4839" />
  <row Id="8986" PostId="5867" Score="1" Text="Are you asking about the Earth's sky?" CreationDate="2017-11-23T14:24:44.097" UserId="110" />
  <row Id="8987" PostId="5904" Score="0" Text="For some reason the shadertoy did not run on my browser. Could you post a/some simple image(s) to demonstrate what you mean?" CreationDate="2017-11-23T15:20:25.960" UserId="209" />
  <row Id="8989" PostId="5904" Score="0" Text="Shouldn't it be more like n=]-1, 1[?" CreationDate="2017-11-24T06:35:25.330" UserId="1952" />
  <row Id="8996" PostId="5913" Score="0" Text="I tried to implement what you said to get frustum vertices in world space, is it correct? Here: https://gist.github.com/recp/8d9a3f65d3573bd2775b658ffab46212" CreationDate="2017-11-25T15:42:01.100" UserId="6929" />
  <row Id="8997" PostId="5627" Score="0" Text="`fwrite(buffer, sizeof(int)*width*height, 1, ffmpeg);` is throwing a Seg fault" CreationDate="2017-11-25T16:06:28.200" UserId="6025" />
  <row Id="8998" PostId="54" Score="0" Text="In addition to sharing work across threads, ability to use async compute is one big reason to use compute shaders." CreationDate="2017-11-25T17:57:45.973" UserId="1952" />
  <row Id="8999" PostId="5924" Score="3" Text="Also, W &lt; 0 for points behind the camera. So if you divide by W first, you then cannot tell whether a point/primitive was behind the camera or in front of it." CreationDate="2017-11-26T03:05:57.547" UserId="48" />
  <row Id="9001" PostId="5925" Score="0" Text="Yes, no, possibly. Not as a normal physically based 3D but still traditional animation has done this for ages. But its a trick that relys on a nonphysically correct image an stylized look and feel. Hard to say anything useful as the question is a bit open ended" CreationDate="2017-11-26T15:14:03.567" UserId="38" />
  <row Id="9005" PostId="5925" Score="0" Text="Sure, you can pre-render the entire sequence of images along some dense sampling of the path, what's the problem?" CreationDate="2017-11-26T19:36:34.470" UserId="106" />
  <row Id="9006" PostId="5925" Score="0" Text="@Rahul, is it any faster than Render Time x number of samples?" CreationDate="2017-11-26T19:43:45.050" UserId="7743" />
  <row Id="9007" PostId="5925" Score="0" Text="What &quot;efficiency&quot; are you worried about? Pre-rendering image sequences has a big up-front cost per scene but a tiny cost for the client. Are you trying to reduce the amount of data you need to give the client, or the amount of up-front work (e.g. because the scene is generated afresh each time)?" CreationDate="2017-11-27T10:17:01.393" UserId="2041" />
  <row Id="9008" PostId="5924" Score="1" Text="Last fact seems most important to me" CreationDate="2017-11-28T12:46:25.150" UserId="7737" />
  <row Id="9009" PostId="5625" Score="0" Text="Nicest introduction by far: [Disney's Practical Guide to Path Tracing](https://www.youtube.com/watch?v=frLwRLS_ZR0)" CreationDate="2017-11-28T13:29:45.853" UserId="18" />
  <row Id="9010" PostId="5013" Score="1" Text="As a hint for the people wanting to learn this, the delaunay triangulation can be build with voronoi diagrams. A vornoi diagram takes an unordered set of points and calculates vornoi regions such that each region contains exactly one point and the border of a region is set such that the two points of the bordering fields have exactly the same distance to the border.&#xA;&#xA;Once the voronoi diagram is build, the delaunay triangulation is a simple matter of connecting each point with the points of the neighbouring voronoi regions." CreationDate="2017-11-28T14:03:21.553" UserId="7008" />
  <row Id="9011" PostId="5924" Score="0" Text="But I doubt it's true. Could you prove it or provide an example?" CreationDate="2017-11-28T14:44:29.060" UserId="7737" />
  <row Id="9012" PostId="5924" Score="0" Text="@Nolan: The point (0, 0, 0, -1). Divide by -1, and it's in the [-1, 1] range. But it can't be in the range of [1, -1], since that is an empty set." CreationDate="2017-11-28T15:10:57.973" UserId="2654" />
  <row Id="9013" PostId="5924" Score="0" Text="You can't get point (0, 0, 0, -1) after multiplying by the projection matrix. It requires initial point have w coordinate equal 0 or have weird projection planes like negative near or far behind near." CreationDate="2017-11-28T16:50:23.430" UserId="7737" />
  <row Id="9014" PostId="5924" Score="0" Text="@Nolan: Nowhere in OpenGL is there an assumption that `gl_Position` can only arise by &quot;multiplying by a projection matrix&quot;. The specification must deal with *all* possible values, not just the ones that come about through the usual means." CreationDate="2017-11-28T16:53:26.857" UserId="2654" />
  <row Id="9015" PostId="5924" Score="0" Text="Also I would argue this point must be clipped. You can multiply (0,0,0,-1) by (-1) and get (0,0,0,1) which denotes same point in 3D but shouldn't be clipped according to its w coordinate." CreationDate="2017-11-28T17:04:57.883" UserId="7737" />
  <row Id="9016" PostId="5924" Score="0" Text="@Nolan: But it cannot be clipped in NDC space, because in NDC space, the point is just (0, 0, 0). Remember: NDC is 3D space, not 4D. So it cannot tell the difference between a clip-space (0, 0, 0, 1) and (0, 0, 0, -1). The former should not be clipped; the latter should. NDC space can't tell the difference. This is why clipping is done in clip-space; because it has sufficient information to do it correctly." CreationDate="2017-11-28T17:06:22.437" UserId="2654" />
  <row Id="9017" PostId="5924" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/69391/discussion-between-nolan-and-nicol-bolas)." CreationDate="2017-11-28T17:08:10.540" UserId="7737" />
  <row Id="9019" PostId="5932" Score="1" Text="It should also be noted that, while OpenGL only requires support for combined depth/stencil, the API still technically allows it for hardware that could handle it. Vulkan, by contrast, makes it *impossible* to separate them. In a Vulkan renderpass, there is exactly one depth/stencil attachment, in a render pass, and that image can provide depth, stencil, or both. But there is only one image allowed there." CreationDate="2017-11-29T03:34:59.240" UserId="2654" />
  <row Id="9020" PostId="5919" Score="0" Text="There are many ways of texture mapping a sphere with a rectangle. It's pretty well researched under the term [map projections](https://en.wikipedia.org/wiki/Map_projection)" CreationDate="2017-11-29T09:56:29.853" UserId="137" />
  <row Id="9021" PostId="5886" Score="0" Text="OK I'll give your suggestions a try, but the BTLNews article [I've linked to in the question](http://www.btlnews.com/crafts/visual-fx/level-256-vfx-produces-the-final-moments-of-karl-brant-for-nerdist-com/) explains that they've started with a 3D representation or possibly a point cloud from a hacked Kinect box. So they start with an actual heigh of the surface, rather than generate one from a color or brightness. Possibly they re-interpolate it to a regular x-y grid of points with continuous z (z being &quot;out of the page&quot;)." CreationDate="2017-11-30T05:53:25.160" UserId="5846" />
  <row Id="9022" PostId="3806" Score="0" Text="I've just asked [Algorithms to “anti-alias” (or somehow improve) binary 1-bit drawings and fonts](https://computergraphics.stackexchange.com/q/5934/5846), any thoughts?" CreationDate="2017-11-30T06:21:48.943" UserId="5846" />
  <row Id="9024" PostId="4814" Score="0" Text="I can't imagine how this works in 3d, can you enlighten on that? Should you simply connect the vertex for each cube to the adjacent cubes that shared a crossing?" CreationDate="2017-11-30T06:52:32.053" UserId="7778" />
  <row Id="9025" PostId="5935" Score="1" Text="Ah! for ellipses (and presumably arbitrary angle lines) PIL does draw fairly &quot;OK-looking&quot; 1-bit curved lines without standard antialiasing, https://i.stack.imgur.com/HGDox.png and in fact, [this answer](https://stackoverflow.com/a/2292690/3904031) shows how to do more arbitrary curves. So combining those with a search for historical bitmapped fonts, I should be good to go! If you'd like to include this info into your answer I'll delete this comment. Thanks!" CreationDate="2017-11-30T07:08:20.040" UserId="5846" />
  <row Id="9026" PostId="4814" Score="0" Text="Also, how do you form a quad/triangle?" CreationDate="2017-11-30T07:42:27.557" UserId="7778" />
  <row Id="9027" PostId="5935" Score="2" Text="The problem is, all those techniques trade off spatial resolution, and it's not like these displays have pixels to spare." CreationDate="2017-11-30T10:34:04.010" UserId="2041" />
  <row Id="9028" PostId="5936" Score="0" Text="windows has a comptr class for managing the handles" CreationDate="2017-11-30T16:23:54.247" UserId="137" />
  <row Id="9029" PostId="5936" Score="0" Text="using comptr gives same problem.No idea why." CreationDate="2017-11-30T17:11:52.357" UserId="6064" />
  <row Id="9030" PostId="5934" Score="1" Text="I don't know all the details of your specific usage case, but if you have a decent refresh rate, you could get anti aliasing by faking greyscale through rapidly turning on and off individual pixels.&#xA;&#xA;There's more to it than just that though, so you might find these two write ups interesting:&#xA;&#xA;https://blog.demofox.org/2017/10/31/animating-noise-for-integration-over-time/&#xA;&#xA;https://blog.demofox.org/2017/11/03/animating-noise-for-integration-over-time-2-uniform-over-time/" CreationDate="2017-11-30T17:11:54.367" UserId="56" />
  <row Id="9031" PostId="5940" Score="0" Text="altough to be honest if you touchup a few pixels you can get a good compromize between the two." CreationDate="2017-11-30T21:17:42.623" UserId="38" />
  <row Id="9032" PostId="5940" Score="0" Text="Excellent! This is exactly what I needed, thank you." CreationDate="2017-12-01T00:33:43.687" UserId="5846" />
  <row Id="9033" PostId="5938" Score="0" Text="Thanks for the link. Since I'm new to this having something like this to read is very helpful." CreationDate="2017-12-01T00:37:00.790" UserId="5846" />
  <row Id="9034" PostId="5934" Score="1" Text="@AlanWolfe that's a great idea! For this particular class of units the data rate is slow and there are no extra data buffers to toggle between, but I will keep this in mind, and if I have a unit where this would be possible I'll give it a go. Thanks!" CreationDate="2017-12-01T00:45:54.533" UserId="5846" />
  <row Id="9035" PostId="5904" Score="0" Text="@JarkkoL Well the formula for converting floating point to integer is output=floor(input * intmax + n), where n=0.5 without noise, because you want (for example) &gt;=0.5 to round up, but &lt;0.5 down. That's why the noise is &quot;centered&quot; at 0.5." CreationDate="2017-12-01T17:12:47.560" UserId="2306" />
  <row Id="9036" PostId="5919" Score="0" Text="Asking about how to write an algorithm to do this would be on topic, but software recommendation questions are not on topic on any Stack Exchange site, apart from [softwarerecs.se]." CreationDate="2017-12-01T19:57:42.160" UserId="231" />
  <row Id="9037" PostId="5925" Score="0" Text="I'd look into making BSP trees that were fourth dimensional - the fourth dimension being time." CreationDate="2017-12-01T20:40:59.173" UserId="56" />
  <row Id="9038" PostId="5904" Score="0" Text="@SimonF added image of the shadertoy" CreationDate="2017-12-01T20:41:26.943" UserId="2306" />
  <row Id="9039" PostId="5904" Score="1" Text="It seems you were truncating the output rather than rounding it (like GPUs do) - rounding instead, at least you get proper whites: https://www.shadertoy.com/view/MlsfD7&#xA;(image: https://i.stack.imgur.com/kxQWl.png )" CreationDate="2017-12-01T20:49:07.197" UserId="2463" />
  <row Id="9041" PostId="5943" Score="0" Text="(sorry I hit enter accidentally, and edit time limit expired) The usecase in the example is one those situations where it would matter: rendering a floating point grayscale image on a 3-bit display device. Here the intensity changes greatly by just changing the LSB. I'm trying to understand if there is the any &quot;correct way&quot; to have end values map to solid colors, like compressing the value range and have the end values saturate. And what is the mathematical explanation of that? In the example formula, input value of 1.0 doesn't produce output that averages to 7, and that's what bothers me." CreationDate="2017-12-01T23:03:02.153" UserId="2306" />
  <row Id="9042" PostId="5904" Score="0" Text="You could try to apply triangular weight function to the noise, e.g. replace     float n = n1+n2 - 0.5;&#xA;with:&#xA;float w = 1.5-2.25*abs(ci-0.5);&#xA;float n = (n1+n2-1.0)*w+0.5;" CreationDate="2017-12-02T03:45:32.257" UserId="1952" />
  <row Id="9043" PostId="5904" Score="0" Text="Sorry, you were actually right to begin with :) If we round on output, the noise of course needs to be symmetric -1;1, which then results in your original image." CreationDate="2017-12-02T07:18:38.340" UserId="2463" />
  <row Id="9044" PostId="5939" Score="0" Text="I'm not actually sure what the usual way to do this is, but if you could somehow know the size of a pixel at this intersection, you could add epsilons to the hit position to get numeric derivatives and use that pixel size to know what mip level to use." CreationDate="2017-12-02T15:59:41.820" UserId="56" />
  <row Id="9045" PostId="5939" Score="2" Text="This question is related and has some good info in the answers: https://stackoverflow.com/q/1813303/2817105" CreationDate="2017-12-02T16:01:10.993" UserId="56" />
  <row Id="9046" PostId="5936" Score="0" Text="Does the amount of &quot;leaked memory&quot; grow each time you do this or only the first? if only the first it might be something like, even though you released an object (ref count decriment) it's deciding to keep it around until your app closes or something." CreationDate="2017-12-02T16:03:42.947" UserId="56" />
  <row Id="9047" PostId="5936" Score="0" Text="Each time that too the same amount by which it was started" CreationDate="2017-12-02T16:10:17.293" UserId="6064" />
  <row Id="9048" PostId="5936" Score="0" Text="Around 18 mb to be exact" CreationDate="2017-12-02T16:10:48.837" UserId="6064" />
  <row Id="9049" PostId="5942" Score="0" Text="You mean instead of directly sampling whatever combined noise again at the new scale value per mip?  And the goal is to speed-up mip generation?" CreationDate="2017-12-02T20:21:29.843" UserId="2831" />
  <row Id="9050" PostId="5926" Score="0" Text="Did you know that you can path trace without doing direct light sampling? You rely on emissive terms to add the lighting energy to the scene.  You would use fresnel to calculate how much reflects (specular) vs is absorbed (diffuse / refraction / subsurface scattering) and roll a random number to choose which of those to do, based on that reflectance amount." CreationDate="2017-12-04T00:15:55.733" UserId="56" />
  <row Id="9051" PostId="5952" Score="0" Text="It's amazing that the lerp at the edges gives a perfect looking result. I would expect at least a little deviation :P" CreationDate="2017-12-04T03:15:18.843" UserId="56" />
  <row Id="9052" PostId="5946" Score="0" Text="Thanks, this is just perfect. :)." CreationDate="2017-12-04T16:22:39.950" UserId="7814" />
  <row Id="9053" PostId="5949" Score="0" Text="I used the debug macro with the other other answer. Thanks." CreationDate="2017-12-04T16:23:33.083" UserId="7814" />
  <row Id="9054" PostId="5950" Score="0" Text="Thanks. I will surely look into RenderDoc." CreationDate="2017-12-04T16:24:35.077" UserId="7814" />
  <row Id="9055" PostId="5953" Score="0" Text="I dont think a analytic solution is even possible." CreationDate="2017-12-04T16:39:08.807" UserId="38" />
  <row Id="9057" PostId="5957" Score="1" Text="As a first debugging step, you should replace the `Parallel.for` with a normal loop, to make sure a concurrency problem isn't the source of your error." CreationDate="2017-12-05T10:02:41.213" UserId="2041" />
  <row Id="9058" PostId="5957" Score="0" Text="It was normal loop first. It is changed just for speed. The results are identical." CreationDate="2017-12-05T10:36:34.413" UserId="6041" />
  <row Id="9059" PostId="4979" Score="0" Text="This is link a good read that explains what a PDF is. TL;DR a PDF is a function that describes the probability of (continuous aka floating point) random numbers.  Generating random numbers from a specific PDF can be challenging and there are a few techniques for doing so.  This talks about one of them.  The article after this one talks about another way.&#xA;https://blog.demofox.org/2017/08/05/generating-random-numbers-from-a-specific-distribution-by-inverting-the-cdf/" CreationDate="2017-12-05T21:25:07.703" UserId="56" />
  <row Id="9060" PostId="5955" Score="0" Text="BTW, the Blinn–Phong NDF should have $n + 2$, not $n + 1$, in the numerator (see equation 30 in the paper)." CreationDate="2017-12-05T21:49:17.520" UserId="48" />
  <row Id="9061" PostId="5957" Score="2" Text="I wonder if your colors are clipping? They may be going above 1.0 or below 0.0.  Might be worth checking out as part of debugging.  It could be that things are smooth in at least part of the image, but that they are out of range." CreationDate="2017-12-05T23:40:20.683" UserId="56" />
  <row Id="9062" PostId="5945" Score="0" Text="I partially answered to a similar question recently. See https://computergraphics.stackexchange.com/a/5773/182" CreationDate="2017-12-06T02:45:01.223" UserId="182" />
  <row Id="9063" PostId="5949" Score="1" Text="You can go even further with the macro and do something like this: https://github.com/bkaradzic/bgfx/blob/8d471959eb3cbc16c0e7fdac25efcf842abd2ad1/src/renderer_gl.h#L994 so your code just becomes: `GL_CHECK(glGenBuffers(1, &amp;buffer));` and reports everything you need." CreationDate="2017-12-06T02:47:15.983" UserId="182" />
  <row Id="9064" PostId="5958" Score="0" Text="Ahh i see. that makes sense. thank you very much. EDIT: Awesome blog post as well!" CreationDate="2017-12-06T06:14:12.077" UserId="5703" />
  <row Id="9065" PostId="5955" Score="0" Text="Yup, thank you. now I also understand that the exponent in the PDF should be n+1" CreationDate="2017-12-06T06:37:48.110" UserId="5703" />
  <row Id="9066" PostId="5957" Score="2" Text="&quot; It is changed just for speed.&quot; &#xA;If you want speed, why don't you do it as a separable filter, e.g. do Y operations first into an intermediate buffer and then do the X?&#xA;&#xA;Also, since you aren't doing a true integral, are you normalising your sinc filter taps? i.e. does Sum(Helper.Sinc(Ws * (x - n))) over all n == 1?" CreationDate="2017-12-06T10:03:25.923" UserId="209" />
  <row Id="9067" PostId="5925" Score="0" Text="Bleeding edge question :) There is work currently going on with Google Seurat, but unfortunately not too much info out yet. See e.g. https://www.roadtovr.com/preview-google-seurat-ilm-xlab-mobile-vr-rendering/" CreationDate="2017-12-06T13:32:42.047" UserId="2463" />
  <row Id="9068" PostId="5957" Score="0" Text="The sinc method wasn't normalised. I modified the code and the image looks more like it but it is very blurry. Is this expected? I have updated the question with the new code and image." CreationDate="2017-12-07T10:03:19.497" UserId="6041" />
  <row Id="9069" PostId="5957" Score="0" Text="The colour wasn't clamped between 0-255 too. Thanks." CreationDate="2017-12-07T10:03:45.343" UserId="6041" />
  <row Id="9070" PostId="5948" Score="0" Text="*&quot;In general, non-linear would mean that an increase in a value does not produce a similar increase in perception.&quot;*&#xA;&#xA;That's a very confusing opening line. Perception, IMO, implies human vision, which, as you state, is non-linear." CreationDate="2017-12-07T11:07:39.667" UserId="209" />
  <row Id="9071" PostId="5957" Score="1" Text="Yes the normalisation looks about right..though you probably don't need to repeatedly compute the sum of the weights as, in your test, they shouldn't be changing from pixel to pixel.&#xA;&#xA;Having said all this, I'm not sure sinc is the right choice for graphics. Can I suggest you try looking at Mitchell &amp; Netravali's https://www.cs.utexas.edu/~fussell/courses/cs384g-fall2013/lectures/mitchell/Mitchell.pdf ?" CreationDate="2017-12-07T16:55:26.927" UserId="209" />
  <row Id="9072" PostId="5962" Score="1" Text="Please expand a bit on how to achieve the desired result. Link only answers are not encouraged here. If you link goes down or the page gets modified your answer becomes empty and irrelevant for any future visitors( here is a (admittedly fairly old) meta discussion on the topic : https://computergraphics.meta.stackexchange.com/questions/98 )" CreationDate="2017-12-07T20:00:47.513" UserId="5703" />
  <row Id="9073" PostId="5962" Score="1" Text="@SebastiánMestre: This is not a link-only answer, as it gives the key term: *Boolean union operation*. If the link goes down, future visitors can look up Boolean unions in any computational geometry textbook. It would be a link-only answer if what it said was, &quot;Check this link, the answer to your question is in there&quot;." CreationDate="2017-12-08T03:04:22.257" UserId="106" />
  <row Id="9074" PostId="5948" Score="1" Text="I've edited to clarify. Thanks for the feedback." CreationDate="2017-12-08T06:05:36.660" UserId="3003" />
  <row Id="9076" PostId="5966" Score="0" Text="Note that DoF corresponds to combining pinhole cameras each having their own view frustum. So it still corresponds to a view frustum, but not &quot;the&quot; view frustum." CreationDate="2017-12-08T12:39:22.667" UserId="2287" />
  <row Id="9077" PostId="5966" Score="0" Text="Yes, that's a nice way of looking at it." CreationDate="2017-12-08T14:49:23.107" UserId="2041" />
  <row Id="9078" PostId="5966" Score="0" Text="Thank you! So the &quot;random&quot; part of the algorithm only comes into effect when rays interact with diffuse surfaces?" CreationDate="2017-12-08T21:06:54.220" UserId="7868" />
  <row Id="9080" PostId="5968" Score="1" Text="the cdf's are (as far as i know) always computed from the pdf's of of each of the sperical coordinates, not from a probability function of solid angle. besides, a &quot;bigger&quot; solid angle is not well defined since you could choose any two shapes over the hemisphere with equal solid angle but vastly diferent probability" CreationDate="2017-12-09T03:04:25.760" UserId="5703" />
  <row Id="9082" PostId="5966" Score="0" Text="It's up to you really. If you purely sample on a space-filling curve, you don't need any randomness, but you can still jitter if you're worried about structured artefacts. If you jitter a regular grid, you'd definitely want to use random numbers for that. I forgot to mention that using Poisson discs is another possibility for structured random sampling; I'll add that to my answer." CreationDate="2017-12-09T10:15:47.263" UserId="2041" />
  <row Id="9083" PostId="5966" Score="0" Text="To be explicit: if you shoot all your primary rays from the same location in the same direction each sample, you'll have aliasing. Randomly jittering the start position by amounts smaller than the size of a pixel will give you antialiasing through &quot;super sampling&quot;" CreationDate="2017-12-09T14:21:12.747" UserId="56" />
  <row Id="9084" PostId="5968" Score="0" Text="Because the differential variable in your integral is dw and so the pdf would be a function of w. Hence the pdf = 1/2pi. If you instead write the differential variable dw as Sin(tetha)d(tetha)d(phi) as in fact dw equals this, then your pdf I think will be sin(tetha)/2pi. But it doesn't make difference as in this case the sin is cancelled out with the sin in the numinator and you will be left with 1/2pi." CreationDate="2017-12-09T20:39:46.007" UserId="6041" />
  <row Id="9085" PostId="5967" Score="1" Text="I ran some test, and found that ka is 1 for all blending functions with a floating point texture, not 2^(mc)-1 as in reference page. Floating point values are not clamped and negative values are taken as it is. So you were right :)" CreationDate="2017-12-09T21:31:14.340" UserId="3024" />
  <row Id="9086" PostId="5968" Score="0" Text="which sin(theta) in the numerator cancels out with the sin in the pdf?" CreationDate="2017-12-10T12:42:09.443" UserId="7871" />
  <row Id="9091" PostId="5889" Score="0" Text="I don't entirely follow. The edges of a triangle are not parallel, so how would they have a vanishing point?" CreationDate="2017-12-11T10:13:54.157" UserId="2041" />
  <row Id="9092" PostId="5972" Score="0" Text="as far as i know a half edge is a directed edge. That is, an edge with a defined begining and end rather than two points where order is irrelevant. I don't know how that would fit into a triangulation algorithm but i hope this helps." CreationDate="2017-12-12T00:54:59.300" UserId="5703" />
  <row Id="9093" PostId="5867" Score="0" Text="As the other comments show, there is a very wide range of possible answers here, depending on your requirements. Could you [edit] to explain what you will be using these for, and how accurate they need to be, and whether you just want shades of blue or other sky objects like clouds, stars, sun, moon?" CreationDate="2017-12-12T20:03:23.183" UserId="231" />
  <row Id="9094" PostId="5974" Score="1" Text="Yes that's a good point but I still think it might make a cool poster or something if you had the &quot;standard observer&quot; calibrated diagram." CreationDate="2017-12-12T20:45:20.737" UserId="7740" />
  <row Id="9095" PostId="5978" Score="0" Text="Ok, how about something more specific: A surface with reflectance properties such that if you shine D65 on it a CIE standard observer would get XYZ tristimulus values which could be projected onto a maxwell triangle to build a diagram in x and y. It would be insanely difficult but should it not be possible in theory?" CreationDate="2017-12-12T20:52:00.900" UserId="7740" />
  <row Id="9096" PostId="5968" Score="0" Text="Sin(tetha) in the numerator comes from the solid angle definition which equals dw = sin(tetha) x d(phi) x d(tetha). And dw is what you have as differential variable in the main integral." CreationDate="2017-12-12T21:30:24.217" UserId="6041" />
  <row Id="9097" PostId="5968" Score="0" Text="This is exactly the reason why the final PDF should be sin(theta) / 2pi the sin term doesn't cancel out" CreationDate="2017-12-12T22:51:30.467" UserId="7871" />
  <row Id="9098" PostId="5973" Score="0" Text="Thank you for your answer, I think I was partly thrown by the dissimilarity between the &quot;half-edges&quot; that the library provided (simply an array of indices) and the more fully fledged structures which came up when searching for half-edges." CreationDate="2017-12-12T23:46:58.187" UserId="6406" />
  <row Id="9099" PostId="5982" Score="1" Text="An isolated implementation, outside of a full renderer, was posted here: https://groups.google.com/forum/?hl=en#!topic/comp.graphics.algorithms/e11sjI-nzpM" CreationDate="2017-12-13T09:35:29.633" UserId="4546" />
  <row Id="9100" PostId="5979" Score="0" Text="There are several open-source libraries for rendering OSM data. Couldn't you use one of those?" CreationDate="2017-12-13T10:12:03.283" UserId="2041" />
  <row Id="9102" PostId="5979" Score="0" Text="Well I found a few of those but they all dont seem customizable enough. Like what if I have my own data and e.g. would want to render a &quot;hut&quot; on the road side. The way I see it I have to either use the existing renderers and then insert the hut in the front end engine I am using, or render everything completely myself which is what I was going for." CreationDate="2017-12-13T15:26:50.580" UserId="7888" />
  <row Id="9103" PostId="5972" Score="0" Text="Here is a good practical introduction on the topic: https://fgiesen.wordpress.com/2012/02/21/half-edge-based-mesh-representations-theory/" CreationDate="2017-12-13T16:08:26.687" UserId="182" />
  <row Id="9104" PostId="5968" Score="0" Text="The illumination integral is L = integral E cos(tetha)sin(tetha) d(phi) d(tetha). Which has sin(tetha) as part of dw." CreationDate="2017-12-13T23:44:04.880" UserId="6041" />
  <row Id="9108" PostId="5986" Score="0" Text="Although I just realized that you were asking about the midpoint algorithm so I haven't really answered the question at all but I hope my post is helpful." CreationDate="2017-12-14T03:34:27.413" UserId="7740" />
  <row Id="9109" PostId="5968" Score="0" Text="Sorry the above illumination integral should be L(out) = integral L brdf cos(tetha) sin(tetha) d(phi) d(tetha). And so its MC estimator will be &lt;I&gt; = L brdf cos(tetha) sin(tetha) / pdf. Here pdf will be in sin form. If you had your integral in terms of dw then your estimator would be &lt;I&gt; = L brdf cos(tetha) / pdf. The pdf here is 1/2pi." CreationDate="2017-12-14T06:56:29.370" UserId="6041" />
  <row Id="9111" PostId="5978" Score="0" Text="@Chuck No, it would require each element in the poster to have different wavelength charactersitics, white of the paper would aslo have to be perfect. Its not pssible to do this atleast untill we can print stuff that behaves like butterfly wings. Printing simply does not work this way. Its not just insanely expensive it requires tech we do not have." CreationDate="2017-12-14T10:01:45.817" UserId="38" />
  <row Id="9112" PostId="5986" Score="0" Text="Your post is really helpful. I see where I went wrong. So I guess I either need to split up my curve, or do what you said and consider the derivatives. Thanks for the link, by the way, I'll make sure to read it when I have some time." CreationDate="2017-12-14T13:57:47.453" UserId="6838" />
  <row Id="9113" PostId="5986" Score="0" Text="Its not really a derivative in the formal sense but it is very similar to the idea. Essentially you want to keep finding higher order differences until you get one that does not depend on t and so you would just be adding a constant every time. When you apply forward differencing you get a series of unconnected points. What you should really be doing is drawing straight lines between them. Here are some possibly helpful links on parametric equations: [1](http://tutorial.math.lamar.edu/Classes/CalcII/ParametricEqn.aspx) [2](http://jccc-mpg.wikidot.com/parametric-equations)" CreationDate="2017-12-14T14:41:14.690" UserId="7740" />
  <row Id="9114" PostId="5977" Score="0" Text="Thanks for the enlightenment! Never thought about it this way." CreationDate="2017-12-14T15:34:11.940" UserId="7884" />
  <row Id="9115" PostId="5986" Score="0" Text="Okay, so that code example definitely helped me understand it. My question now is what if you can never differentiate the function into a constant, such as the sine function?" CreationDate="2017-12-14T23:30:41.177" UserId="6838" />
  <row Id="9116" PostId="5986" Score="0" Text="Yes that's a good catch, I was thinking about that as well. It seems that this might not work so well when functions involve things like sin, log, etc. One thing to keep in mind though is that this not a derivative. Its `f(x + d) - f(x)` So  you might possibly be able to simplify with some [identities](http://www.sosmath.com/trig/Trig5/trig5/trig5.html) while remembering that d is a constant (however I was not able to). I got as far as `sin(x + d) - sin(x) = sin(x)cos(d) + cos(x)sin(d) - sin(x)` and gave up." CreationDate="2017-12-15T05:00:56.863" UserId="7740" />
  <row Id="9117" PostId="5986" Score="0" Text="In certain cases you may be able to come up with some sort of approximation or clever trick, see [here](https://math.stackexchange.com/questions/2063396/cubic-spline-between-circles) and [here](https://mathnow.wordpress.com/2009/11/06/a-rational-parameterization-of-the-unit-circle/)." CreationDate="2017-12-15T05:01:03.713" UserId="7740" />
  <row Id="9118" PostId="5989" Score="0" Text="I tried it, but the result was same. &#xA;You mentioned fs_in.TexCoords is == vec2(0.0,0.0)&#xA;but as you can see, the glass color is brown and there are some shading, not perfect black.&#xA;Maybe it meant that the entire color is darkened by some reason." CreationDate="2017-12-15T11:46:20.380" UserId="6806" />
  <row Id="9119" PostId="5989" Score="0" Text="To me it looks that there are multiple sub meshes (each one with a set of uvs) Is that right? If that is the case, then you are getting 0,0 but for other reason then." CreationDate="2017-12-15T11:56:11.983" UserId="7107" />
  <row Id="9120" PostId="5989" Score="0" Text="You're right. but I don't know the reason. The two pair code are same? no?" CreationDate="2017-12-15T12:09:46.700" UserId="6806" />
  <row Id="9121" PostId="5989" Score="0" Text="Yeah both should work with gl version 330. You could launch your project with RenderDoc or Nvidia Nsight maybe that gives you a clew." CreationDate="2017-12-15T12:16:05.247" UserId="7107" />
  <row Id="9122" PostId="5988" Score="1" Text="if you use (2), do you comment out the first line of your vertex shader (in the main function, the TexCoords assignement)?&#xA;because if you do, i'm not sure this is legal - you'd have an out variable that never had a value assignment." CreationDate="2017-12-15T12:23:05.730" UserId="7008" />
  <row Id="9123" PostId="5988" Score="0" Text="I tried both of them. And all result were same." CreationDate="2017-12-15T12:30:37.447" UserId="6806" />
  <row Id="9125" PostId="5989" Score="0" Text="Oh my god, I found my mistake. I duplicate the same name file other folder, and executed the code. the pairs are same. Sorry to bother you.." CreationDate="2017-12-15T20:57:17.513" UserId="6806" />
  <row Id="9126" PostId="5988" Score="0" Text="I added comment the below answer. I'm really sorry about that" CreationDate="2017-12-15T20:58:28.127" UserId="6806" />
  <row Id="9127" PostId="5989" Score="0" Text="Good you found it!" CreationDate="2017-12-16T20:21:07.167" UserId="7107" />
  <row Id="9128" PostId="5995" Score="0" Text="it is probably due to LoD and mipmap levels not being changed when zooming in, therefore having the same texels stretched over a polygon that takes up more of the screen" CreationDate="2017-12-16T21:46:12.863" UserId="5703" />
  <row Id="9129" PostId="5966" Score="0" Text="So the jitter is an AA technique, not something strictly related to lighting?" CreationDate="2017-12-17T00:37:27.640" UserId="7868" />
  <row Id="9130" PostId="5999" Score="0" Text="Thank you for the clarification! I shall now to avoid extra work in my shaders :)" CreationDate="2017-12-17T10:09:24.723" UserId="6938" />
  <row Id="9131" PostId="5966" Score="0" Text="@PaulFerris That's right, but *especially* in a full MC path tracer, you need to consider AA in your sampling strategy. If you're just getting started, you can ignore it for now but you'll have to come back to it later." CreationDate="2017-12-17T10:10:42.463" UserId="2041" />
  <row Id="9132" PostId="5952" Score="0" Text="Yea, I was positively surprised too :) I believe it works because we are scaling down the dither-magnitude linearly, at the same rate the signal is decreasing. So then at least the scale matches... but I agree that it is interesting that directly blending the distributions appears to have no negative side-effects." CreationDate="2017-12-17T13:16:16.777" UserId="2463" />
  <row Id="9133" PostId="5998" Score="0" Text="Thanks for the good answer +1" CreationDate="2017-12-17T14:58:08.197" UserId="6202" />
  <row Id="9134" PostId="5966" Score="0" Text="Good to know.&#xA;My whole renderer runs through DirectCompute so supersampling is trivial (render with n-times more thread groups into an n-times larger buffer, average pixel neighbourhoods in post-processing); I was mostly just very confused about the &quot;correct&quot; way to organise my ray directions when they're emitted from the camera. Thanks for all the help :)" CreationDate="2017-12-17T20:08:07.200" UserId="7868" />
  <row Id="9135" PostId="6000" Score="0" Text="The clipping library returns a list of points that make up the above figure. I cant distinguish between points that make up the arc and points that make up the lines." CreationDate="2017-12-18T00:51:02.303" UserId="7412" />
  <row Id="9136" PostId="6000" Score="0" Text="So the dotted purple lines are part of the returned object? Well if the points in the arc are closely packed as you say, you can look at the distance between consecutive points. If that that distance is above some threshold then you can say that you've hit an edge of the arc." CreationDate="2017-12-18T04:17:53.030" UserId="7740" />
  <row Id="9137" PostId="5995" Score="0" Text="Can you post a screen shot of what you mean by &quot;washed out&quot;?" CreationDate="2017-12-18T09:37:23.923" UserId="209" />
  <row Id="9138" PostId="6004" Score="0" Text="This works perfectly and is extremely fast, even though the GL driver of my Pi does not advertise ARB_viewport_array. Thank you!" CreationDate="2017-12-18T10:12:09.403" UserId="7915" />
  <row Id="9139" PostId="6008" Score="0" Text="is your lighting turned on for rendering? if so, i suggest trying to turn it off" CreationDate="2017-12-18T10:51:22.750" UserId="7008" />
  <row Id="9140" PostId="6008" Score="0" Text="No, not using any lighting." CreationDate="2017-12-18T10:59:21.103" UserId="7915" />
  <row Id="9141" PostId="6008" Score="0" Text="What kind of alpha blending are you using?" CreationDate="2017-12-18T11:20:55.923" UserId="2041" />
  <row Id="9142" PostId="6008" Score="0" Text="I use glEnable(GL_ALPHA_TEST) and glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA) at the initialization of my program." CreationDate="2017-12-18T11:21:53.513" UserId="7915" />
  <row Id="9143" PostId="6008" Score="0" Text="Is the RGB channel of the font texture all the same colour? It should not contain any anti-aliasing in the RGB channel, only the A channel." CreationDate="2017-12-18T11:48:25.007" UserId="3073" />
  <row Id="9144" PostId="6003" Score="0" Text="What do you mean by &quot;consider the directions&quot;? Are you trying to integrate over some solid angle instead of a single ray? Are you trying to extend the volume to be heterogeneous or anisotropic?" CreationDate="2017-12-18T12:11:37.010" UserId="2041" />
  <row Id="9145" PostId="6008" Score="0" Text="@PaulHK, that the 100% the right lead! While the PNGs Cairo produce look normal (only alpha changes), the internal storage format that I use to create textures uses pre-multiplied alpha. I noticed that there were slight color differences in antialiased regions of the PNG (presumably because the premultiplied alpha is divided back before writing the PNG). I.e., a pixel that would be white with 0xaa alpha does not appear as 0xffffffaa but as 0xaaaaaaaa in the binary data. The only question is: how do I blend that data in OpenGL efficiently? Thanks already so much for your help!" CreationDate="2017-12-18T12:38:35.103" UserId="7915" />
  <row Id="9146" PostId="6008" Score="1" Text="Figured it out! GL_ONE, GL_ONE_MINUS_SRC_ALPHA -- and it works perfectly. Since you brought me on the right track, if you want to write an answer I'll accept it! Thanks again so much." CreationDate="2017-12-18T12:43:23.797" UserId="7915" />
  <row Id="9147" PostId="5998" Score="0" Text="Also, in a streaming-world game, it might not even be possible to show higher detail in a distant area because the models/textures may not even be loaded at the moment. One could try to stream them in when you zoom the camera, but as mentioned that could produce a lot of popping." CreationDate="2017-12-18T17:12:21.397" UserId="48" />
  <row Id="9148" PostId="6003" Score="0" Text="I am integrating over some solid angle." CreationDate="2017-12-18T19:59:35.697" UserId="5183" />
  <row Id="9149" PostId="6010" Score="0" Text="Thanks. If I understand correctly, this will give a gradually changing specular highlight across the tri (because each pixel is in a slightly different location). The discontinuity between tris will vanish, because adjacent pixels in the different tris are very close in location. Is that what you mean? [BTW I've gotten a similar effect by letting the specular result from the vertex shader interpolate in the fragment shader]. I agree this is more natural - but I want uniform shading across the quad (so the discontinuity is between quads). A bit like the flashing facets of a jewel." CreationDate="2017-12-18T23:16:24.020" UserId="5545" />
  <row Id="9150" PostId="6011" Score="0" Text="Thanks. Yes, &quot;flat shading&quot; as [here](https://wikipedia.org/wiki/File:Phong-shading-sample.jpg). And yes, the diffuse + specular reflection of [Phong shading](https://wikipedia.org/wiki/Phong_reflection_model), but without the interpolation (TIL the interpolation is also called &quot;Phong shading&quot;). I believe the specular component increases the angle between the angle of reflection and the viewing direction decreases (brightest when aligned).... specular highlights move as you move your head. I needed a diagram too! I'll look for an online diagram tool (do you know of one?)" CreationDate="2017-12-19T03:12:59.217" UserId="5545" />
  <row Id="9152" PostId="6009" Score="3" Text="As a minor optimisation, you should be able to compute the average of 4 texels using a single texture2D operation. If you sample from a half-pixel offset the sampling area will cover 4 texels equally." CreationDate="2017-12-19T04:24:26.217" UserId="3073" />
  <row Id="9153" PostId="6014" Score="0" Text="Thanks, that's really clever! I think it almost doubles the number of 16bit indices, but isn't there some caching advantage to indexing too? TIL ES 3.1 has `gl_VertexID` and `primitive restart` (thought degenerate tris was the only way)... pity there's no `gl_PrimitiveID`. I see it's more flexible, but I think same memory overhead as the provoking-trick." CreationDate="2017-12-19T04:25:47.427" UserId="5545" />
  <row Id="9154" PostId="6011" Score="0" Text="typo, should be: &quot;specular component *increases* **as** the angle between reflected ray and the direction of the viewer *decreases*&quot;" CreationDate="2017-12-19T04:33:37.173" UserId="5545" />
  <row Id="9155" PostId="6012" Score="1" Text="Didn't you ask this on SO as well? https://stackoverflow.com/questions/47857995/transforming-screen-position-into-image-space&#xA;&#xA;Anyways as the guy mentioned I don't understand your transformation matrix. You said perspective projection isn't involved but you name it projection anyways? what is the space of world position after you apply the matrix? Camera space? Projection space?" CreationDate="2017-12-19T04:54:13.143" UserId="6046" />
  <row Id="9156" PostId="6009" Score="2" Text="Your results look exactly like what I'd expect, and what I see from the output of popular video editing applications by the major manufacturers. Due to the low resolution of the Cb and Cr channels, I don't think you're likely to get it any sharper. That's just how 420 subsampling is. It takes up less space because it throws out data, and it isn't careful about it *at all*!" CreationDate="2017-12-19T05:06:15.573" UserId="3003" />
  <row Id="9157" PostId="6014" Score="0" Text="@hyperpallium Yeah, more indices. They're usually a pretty small memory overhead compared to vertex data, but YMMV. As for cache, a well optimized indexed mesh can be as good or better than strips for general meshes, but I'm not sure if that's true for pure grid meshes." CreationDate="2017-12-19T05:41:55.680" UserId="48" />
  <row Id="9158" PostId="6014" Score="0" Text="I can see it's much less than vertex data; but better to have no extra at all (*if* possible...). I probably shouldn't worry too much over efficiency at this stage - it seems a bit hard to predict. BTW I was thinking the way to cache-optimize a (plane) mesh is to split the rows, making it short enough so the vertices shared with the above tris are still in cache when needed (I don't think there's benefit to it being any shorter; and a small detriment with losing the horizontally shared vertices where the row is split.)." CreationDate="2017-12-19T06:24:57.647" UserId="5545" />
  <row Id="9159" PostId="6014" Score="0" Text="@hyperpallium You might be interested in this article: [Optimal grid rendering is not optimal](https://zeuxcg.org/2017/07/31/optimal-grid-rendering-is-not-optimal/)" CreationDate="2017-12-19T06:40:49.413" UserId="48" />
  <row Id="9160" PostId="6014" Score="0" Text="Spot on. Definitely best to not worry about optimizing yet! I wonder if different on mobile GPUs, which probably do fewer clever things under the hood(?)" CreationDate="2017-12-19T06:57:36.677" UserId="5545" />
  <row Id="9161" PostId="5995" Score="0" Text="@SimonF There is not need to bother with uploading. Have you ever played a game and the textures just popped in to full detail (regardless of zoom or anything, you just running around). If you set &quot;Detail distance&quot; to max then you'll see high resolution textures to a certain extent and beyond that are low-poly, low res textures." CreationDate="2017-12-19T08:40:36.120" UserId="6202" />
  <row Id="9162" PostId="5995" Score="0" Text="_&quot;Have you ever played a game and the textures just popped in to full detail (regardless of zoom or anything, you just running around)&quot;_ &#xA;No, I haven't and that's *why* I asked if you could post a picture. :-/" CreationDate="2017-12-19T09:47:19.520" UserId="209" />
  <row Id="9163" PostId="6013" Score="0" Text="the radius is pixel buffer space, screenPosition.x is screenspace, and screenPositionX is in actual pixel buffer. I still gets wrong cropped rectangle. Thanks" CreationDate="2017-12-19T11:52:28.217" UserId="7918" />
  <row Id="9164" PostId="6013" Score="0" Text="I have edited the post.. still I get wrong cropped image, out of bounds exception" CreationDate="2017-12-19T11:59:41.200" UserId="7918" />
  <row Id="9166" PostId="6015" Score="2" Text="Regarding the background mixing with your objects during the blur, my recommendation would be to not render on white, but to render on transparent black. When doing the blur, you should blur using pre-multiplied alpha.  That will make the result such that you can apply it to any background color (using standard alpha blending) and it will look correct.  More information here: https://stackoverflow.com/questions/4854839/how-to-use-pre-multiplied-during-image-convolution-to-solve-alpha-bleed-problem" CreationDate="2017-12-19T17:01:07.867" UserId="56" />
  <row Id="9167" PostId="5995" Score="0" Text="@SimonF This post would be a good reference.&#xA;https://www.quora.com/Fallout-4-PC-How-do-I-make-settlement-structures-visible-from-really-far-away&#xA;Anyway, the thing I'm asking is not bug related or anything like that. It's more of a technical nature. The answer below got it right." CreationDate="2017-12-19T17:13:20.943" UserId="6202" />
  <row Id="9168" PostId="6013" Score="0" Text="Which piece of code gives the out of bounds exception?" CreationDate="2017-12-19T18:13:12.990" UserId="7740" />
  <row Id="9169" PostId="6013" Score="0" Text="Ogre::Image *out = cropImage(img,  min_x, min_y,  max_x,  max_y);" CreationDate="2017-12-19T20:35:16.427" UserId="7918" />
  <row Id="9171" PostId="6009" Score="1" Text="@PaulHK, do you mean the case where the texture has linear filtering performed by the hardware?" CreationDate="2017-12-19T22:33:12.147" UserId="213" />
  <row Id="9177" PostId="6010" Score="0" Text="a common way to get that discontinuity is to interpolate position but not interpolate normals. maybe this works better for you" CreationDate="2017-12-20T02:32:01.397" UserId="5703" />
  <row Id="9178" PostId="6009" Score="0" Text="Yes, it should be the equivalent to the 4 texture reads in the code above." CreationDate="2017-12-20T04:09:01.543" UserId="3073" />
  <row Id="9179" PostId="6009" Score="1" Text="To answer OP's original question: I've seen implementations of this which use seperate CrCb and Y textures (called semiplanar on some platforms). By doing this you not only simplify texel coordinate computation but you also get independent filtering on both the Chroma and Luma channels." CreationDate="2017-12-20T04:10:32.813" UserId="3073" />
  <row Id="9180" PostId="6009" Score="1" Text="I have also seen that. Done via fragments shader. This approach is suboptimal in terms of performance, due to higher bandwidth consumption." CreationDate="2017-12-20T08:02:43.660" UserId="213" />
  <row Id="9181" PostId="6019" Score="0" Text="Is it rendered in linear colour space ?" CreationDate="2017-12-20T12:46:38.250" UserId="3073" />
  <row Id="9182" PostId="6019" Score="0" Text="Do you actually have two lights 180 degrees apart? It *should* get dark at the light terminator but it wouldn't normally get light again on the other side unless you have two-sided lighting turned on and shadows turned off, or two lights on opposite sides." CreationDate="2017-12-20T13:28:43.523" UserId="2041" />
  <row Id="9183" PostId="6019" Score="0" Text="PaulHK, tried it with both and the effect persists, although it's a little less pronounced in linear." CreationDate="2017-12-20T13:32:58.507" UserId="1937" />
  <row Id="9184" PostId="6019" Score="0" Text="Dan, check out the pics. The lights are more like 60 degrees apart, the problem is when one light's contribution reaches zero, it appears to have a negative contribution to the other light - probably just a perceptual artifact but still annoying." CreationDate="2017-12-20T13:35:01.193" UserId="1937" />
  <row Id="9185" PostId="6020" Score="0" Text="What kind of output are you looking for? Just a wireframe like this but without the shaded solid behind it? A wireframe with the hidden lines also visible? Or a line around the silhouette of the object, without a wireframe?" CreationDate="2017-12-20T14:34:58.850" UserId="2041" />
  <row Id="9187" PostId="6020" Score="0" Text="@DanHulme I'm sorry, I should've made clear what result I was looking for. This is an example: https://gyazo.com/000673d8afda399013a1ad72e71b7a62" CreationDate="2017-12-20T14:47:11.383" UserId="7925" />
  <row Id="9188" PostId="6020" Score="0" Text="I assume you rerender your object after your stencil pass. have you reset your primitive type to triangles rather than lines?" CreationDate="2017-12-20T14:55:11.040" UserId="7008" />
  <row Id="9189" PostId="6020" Score="0" Text="@Tare By re-render you mean in a different frame or still the same frame?&#xA;The primitive type is reset to triangles before rendering the object (line 9: node_to_display.leaves[i].primitive.primitiveType = this.scene.gl.TRIANGLES;)" CreationDate="2017-12-20T14:58:18.963" UserId="7925" />
  <row Id="9190" PostId="6020" Score="0" Text="I assume your rendering to be somewhat like this:&#xA;1.) render one pass to fill the stencil buffer&#xA;2.) render one pass (or in the same pass) to render red wherever the stencil pass has written to&#xA;3.) rerender the original image to have it display over the red stencil image&#xA;&#xA;so before step three you'd need to reset your primitive type to triangles rather than lines" CreationDate="2017-12-20T15:04:02.413" UserId="7008" />
  <row Id="9191" PostId="6020" Score="1" Text="It's been some time since I worked with stenciling, but I think you might be wrong in your way of going about this.&#xA;What I think you are doing is rendering the original image, and then rerendering it's wireframe (by setting the primitive type to line), just that you're rendering red wherever the stencil buffer is set. this would of course lead to the red wireframe you get right now.&#xA;try rendering your second time (the &quot;red pass&quot;) with primitive type triangles, I guess that would lead to the whole object being red." CreationDate="2017-12-20T15:13:59.833" UserId="7008" />
  <row Id="9192" PostId="6022" Score="1" Text="30x30x1 will cover 900 points. It is up to the shader what to do with the global invocation ID, in your case it would be an index into the particles[] array. If you have a case were the ID is greater than the size of the particles array you can just skip doing any work." CreationDate="2017-12-21T02:55:36.183" UserId="3073" />
  <row Id="9193" PostId="6013" Score="0" Text="This is a longshot but maybe you need to clamp the max value to cameraRes - 1 because accessing cameraRes on a zero indexed array would be out of bounds." CreationDate="2017-12-21T07:09:05.123" UserId="7740" />
  <row Id="9195" PostId="6014" Score="0" Text="With using the same provoking vertex, the location will be for that vertex, not the center of the square. But because I have the normal, I can work out the gradient and because I have a 2D height map (i.e. heights over a uniform 2D grid), I can calculate the exact 3D location of the center. There'a  bit of calculation there, but probably faster than the bandwidth cost of explicitly storing the center." CreationDate="2017-12-22T01:30:47.483" UserId="5545" />
  <row Id="9196" PostId="6014" Score="0" Text="I'm not sure of the meaning of `gl_VertexID`. With the &quot;un-sharing&quot;, can you use the same indexing as for the first method? i.e. `gl_VertexID` is not the index of the vertex, but the index in the list of indices? So if the same vertex is used twice, that vertex has two different `gl_VertexID`'s?" CreationDate="2017-12-22T01:39:36.030" UserId="5545" />
  <row Id="9197" PostId="6019" Score="1" Text="Unfortunately, this is the correct result of the Lambertian lighting model. The physically valid way of getting rid of it is to replace your point light sources with area lights. Alternatively, if you don't care about correctness you could replace the $\max(n\cdot\ell,0)$ step with a different function that smooths over the kink at $n\cdot\ell=0$." CreationDate="2017-12-22T02:11:28.923" UserId="106" />
  <row Id="9198" PostId="6023" Score="0" Text="Yet they are choosing samples from a join pdf of theta and azimuth (as you said, double integral over a rectangle) but the resulting radiance is instead multiplied by 1 / 2pi  instead of sin(theta) / 2pi, why is that?" CreationDate="2017-12-22T02:44:55.153" UserId="7871" />
  <row Id="9199" PostId="6023" Score="0" Text="If you are referring to Monte Carlo integration, there's no weighting required because the samples are evenly distributed over the hemisphere by uniformSampleHemisphere() function, so each sample has the same the same solid angle. In the case of double integral the samples close to sphere poles have smaller solid angle which is compensated with the sin-term" CreationDate="2017-12-22T04:06:47.043" UserId="1952" />
  <row Id="9200" PostId="6023" Score="0" Text="Btw, if you would implement the integration with two nested sums over theta &amp; phi, you would need to add the sin-term because the samples near poles would represent smaller solid angle than the samples at the equator." CreationDate="2017-12-22T04:17:31.323" UserId="1952" />
  <row Id="9202" PostId="6029" Score="2" Text="Are you sure that the code you've written results in branches? Most simple `if` statements are compiled to parallel evaluation with a multiply-add to select the desired output. You should check with the profiling tool that goes with your target GPU." CreationDate="2017-12-22T10:28:15.673" UserId="2041" />
  <row Id="9204" PostId="6023" Score="0" Text="So essentially since the integration is a single recursive sum, each  received differential radiance from a pixel represents the radiance along a single steradian and we multiply that by 2pi to estimate the irradiance arriving at point x, to finally multiply that value with the BRDF to see how much of that irradiance arrives to the camera?" CreationDate="2017-12-22T14:30:35.360" UserId="7871" />
  <row Id="9205" PostId="6023" Score="0" Text="For hemispherical integration with even sample distribution (no biasing towards any region), each sample covers 2pi/n sr (hemisphere=2pi sr). So for the rendering equation you just add up all the samples multiplied by the solid angle for each sample (sample=incident radiance * brdf * cos(theta)) to calculate the total radiance from a pixel towards the eye." CreationDate="2017-12-22T16:55:41.717" UserId="1952" />
  <row Id="9207" PostId="6031" Score="1" Text="To put it another way, you want to write code to &quot;bake&quot; the texture into vertex colors, so that the vertex colors approximate the original texture?" CreationDate="2017-12-22T19:04:22.700" UserId="48" />
  <row Id="9208" PostId="6014" Score="0" Text="@hyperpallium It's the index into the vertex buffer (ie the value from the index buffer). If a vertex is re-used, it has the same `gl_VertexID` every time. By &quot;un-sharing&quot; I mean that, e.g.: vertices 0–3 are the first quad, 4–7 are the second quad, etc. So if a vertex is used in multiple quads, it's duplicated in the vertex buffer for each quad." CreationDate="2017-12-22T19:20:42.617" UserId="48" />
  <row Id="9210" PostId="6014" Score="0" Text="Thanks, so that's much more buffer-expensive than the first (&quot;provoking&quot;)  method. (I see what you mean: but at least you can reuse/share two vertices within the one quad.) Another approach to the heightmap might interact: my heights change, so 2D plane in a &quot;static&quot;-hinted buffer, and heights in a &quot;dynamically&quot;-hinted buffer. But, one could also calculate the 2D location, based on knowing which quad it's in (alternative: send in x,y coords, in just 16bit (or 8bit) Uints). But maybe caching won't catch the duplicates?" CreationDate="2017-12-23T02:47:29.907" UserId="5545" />
  <row Id="9211" PostId="6031" Score="0" Text="I've reopened your question now that you've clarified it a little, but to make sure you're not disappointed later, I'll say now: if you're looking for an existing piece of software that you can run your mesh through, we won't be able to help. If you're looking for an algorithm that you can use to write some software, we might be able to help." CreationDate="2017-12-23T09:39:40.143" UserId="2041" />
  <row Id="9212" PostId="6032" Score="0" Text="Thank you. I have added a figure to show that the PRP is the 3D point that corresponds to the 2D vanishing point" CreationDate="2017-12-24T19:37:08.103" UserId="7929" />
  <row Id="9213" PostId="6040" Score="1" Text="Is this just a way of drawing a lego technic or a format for storing the design of one? If is is the latter, are you looking for something compact/compressed which takes up little storage or something that is convenient to work with and easy to edit?" CreationDate="2017-12-25T03:53:29.420" UserId="7740" />
  <row Id="9214" PostId="6033" Score="2" Text="The answer is very unclear. You should elaborate on each point." CreationDate="2017-12-25T06:14:17.310" UserId="457" />
  <row Id="9215" PostId="6040" Score="0" Text="@chuck easy to edit/create, and its drawing lego technic sections which create a full model, a full model is impossible to interpret on small phone screens." CreationDate="2017-12-25T06:42:42.190" UserId="7945" />
  <row Id="9216" PostId="6039" Score="0" Text="Can you clarify? Do you not understand how piecewise Bézier cubic splines work? Or do you understand that, but not how to use them in this particular application?" CreationDate="2017-12-25T06:50:30.843" UserId="3003" />
  <row Id="9220" PostId="5991" Score="0" Text="here you are asking three different questions. one about generation of quasi-random number generation, one about importance sampling, and one about mapping from a 3d direction to a coordinate on an env map. Also, are your questions exclusively about implementation? do you understand those concepts and algorithms fully? What have you tried already? what worked and what didn't? It would help if you made what you're asking more clear." CreationDate="2017-12-25T16:28:38.913" UserId="5703" />
  <row Id="9222" PostId="5998" Score="0" Text="I remember a really long time ago in the roblox game engine you were unable to edit the camera FOV (not sure if you can nowadays) and somebody had built a sniper game where you could click somewhere on the map and your camera would move there. You had full 360 pivot controls at the new camera location and you could click to take shots which were raycast from your player position to where you clicked. It was absolutely ridiculous but still lots of fun trying to out camera manoeuver people." CreationDate="2017-12-25T23:08:04.673" UserId="7740" />
  <row Id="9223" PostId="210" Score="1" Text="I found a real photo that proves you are missing some occlusion https://imgur.com/a/qcxmK" CreationDate="2017-12-26T03:31:54.170" UserId="1614" />
  <row Id="9224" PostId="6039" Score="0" Text="@ user1118321 If I understand how Bezier cubic splines works, what I do not understand is the following. I'm trying to make a hill, what I do not know if it should be if it must have some points to make a curve, for example Q (16,137), S (64,198), T (112,137) and later add the parameters theta = Pi/4 and Phi = Pi/4 to build a feature curve of the hill. Could you tell me how steps 3.1 and 3.2 work, please, since I am very confused" CreationDate="2017-12-26T03:44:02.373" UserId="7943" />
  <row Id="9225" PostId="6045" Score="0" Text="What do you mean by `ComputeShader.use()` ? And if you are passing SSBO to vertex shader you will have to define interface blocks, afaik you can't use them like uniform variables. Check here&#xA;https://www.khronos.org/opengl/wiki/Shader_Storage_Buffer_Object" CreationDate="2017-12-26T12:01:14.767" UserId="6046" />
  <row Id="9226" PostId="6045" Score="0" Text="@wandering-warrior Oh, sorry to confuse you. ComputeShader.use() is just glUseProgram(compute.glsl), so we can ingnore it. And I wonder your meaning 'you can't use them like uniform variables'. I don't want to use the data like uniform variables, instead, I just want to pass the whole arrays to vertex shader, probably it means that when I call glDrawElements or glDrawArrays(triangle), the vertex shader would get the array stream." CreationDate="2017-12-26T13:21:21.607" UserId="6806" />
  <row Id="9228" PostId="6046" Score="0" Text="I think the source of confusion is `target` parameter in `glBufferData`. If buffer object is just data, when why we need to specify this to actually upload raw data?" CreationDate="2017-12-26T15:49:55.953" UserId="4958" />
  <row Id="9229" PostId="6046" Score="0" Text="@narthex: Because that's how [OpenGL objects work](https://www.khronos.org/opengl/wiki/OpenGL_Object). Unless you're [using DSA](https://www.khronos.org/opengl/wiki/Direct_State_Access), you have to bind them somewhere to the context to be able to edit them. This is one of the most *consistent* parts of the OpenGL API, and it's basically the first thing anyone should learn about OpenGL." CreationDate="2017-12-26T15:56:16.357" UserId="2654" />
  <row Id="9232" PostId="6031" Score="0" Text="@DanHulme YEs, I was looking for an algorithm" CreationDate="2017-12-26T21:00:48.330" UserId="7936" />
  <row Id="9233" PostId="6036" Score="0" Text="Thank you for your response. Since I never worked with openGl, I would need some clarifications about your code. The parameter texture that you've passed to the texture2DRect function is the 2D image associated to texture right ? Can you please give a more complet code having the lines for reading the 3D textured mesh, transferring the color texture to 3D vertices, and saving it to a .ply file ?" CreationDate="2017-12-26T21:03:41.933" UserId="7936" />
  <row Id="9234" PostId="6036" Score="1" Text="That's beyond the scope of an answer here, I'm afraid. I suggest you look up a modern beginning OpenGL tutorial. Once you've learned how to put geometry on the screen and add a texture to it, it's pretty much the same for a 3D texture. For 3D Textures you'll use a `sampler3D` instead of a `sampler2DRect` and call `texture3D()` instead of `texture2DRect()`. Good luck!" CreationDate="2017-12-26T21:53:18.553" UserId="3003" />
  <row Id="9235" PostId="6016" Score="1" Text="To blur the different sphere separately, I decided to do it the same way is done in screen space subsurface scattering. I am using depth to calculate the difference between the basic depth and the blurred depth. If the difference is to high, I just don't apply blur." CreationDate="2017-12-26T23:15:24.340" UserId="2372" />
  <row Id="9236" PostId="6048" Score="0" Text="basically, you should divide the offset by the depth of the current fragment. this is akin to the transform that geometry goes through during perspective projection" CreationDate="2017-12-27T03:32:06.507" UserId="5703" />
  <row Id="9237" PostId="6042" Score="0" Text="For raytracing you can just cast multiple rays per pixel to collect multiple samples per pixel. Then average out the results. Now you can end up getting pretty sophisticated in terms of the sampling algorithm itself especially when you get into indirect lighting, but that's the gist of it: just multiple rays/samples per pixel. I've actually seem some raytracers just fudge the ray by a random subpixel value each time it is being cast to produce an antialiased image and the results actually look quite decent. The randomness of the sampling can help pick up razor thin geometry, e.g...." CreationDate="2017-12-27T04:47:09.523" UserId="2247" />
  <row Id="9238" PostId="6042" Score="0" Text="... which a uniform sampling method might miss. That's for offline raytracers that are intended to run for long amounts of time to converge towards a decent production image. If your aim is more realtime approximation with interactive framerates, then maybe uniform sampling would work better, like one that is fixed to project 5 rays per pixel. There are also dirt cheap post-processing techniques on the rendered image like FXAA, but those don't look so great but maybe acceptable if your goal isn't to render production-quality images." CreationDate="2017-12-27T04:50:02.390" UserId="2247" />
  <row Id="9239" PostId="6036" Score="0" Text="@javier What you're requesting is not an algorithm, it's an entire application. We have a finite amount of space in our answers, so it's best to keep your question limited to something specific. Loading a 3D mesh and save a .ply file are totally different problems and we can't possibly answer that without knowing more about the original format and what specific problems you're having with your mesh loading code. You should post separate questions once you've tried to load / save a mesh if you still need help." CreationDate="2017-12-27T23:35:56.337" UserId="6145" />
  <row Id="9240" PostId="1540" Score="0" Text="@YvesDaoust Could you provide an example of `plain Cartesian formulation` or link to a resource that describes its use in 3D graphics?" CreationDate="2017-12-27T23:41:08.670" UserId="6145" />
  <row Id="9241" PostId="1540" Score="0" Text="@Dan: use y = A.x + b where A is a 3x3 matrix and b a 3x1 vector, instead of y' = A.x' where y', x' are augmented vectors and A a 4x4 matrix." CreationDate="2017-12-28T08:45:17.400" UserId="1703" />
  <row Id="9245" PostId="1540" Score="0" Text="@YvesDaoust So you're passing a 3x3 matrix and a 3x1 vector to your shaders instead of a 4x4 matrix? Where do you calculate and store `w`?" CreationDate="2017-12-28T18:51:04.843" UserId="6145" />
  <row Id="9252" PostId="5925" Score="0" Text="I'd think rendering images with depth buffers and using real time height field raymarching for reconstruction would be a decent solution. of course, you might miss some information between &quot;samples&quot;" CreationDate="2017-12-29T15:54:32.293" UserId="5703" />
  <row Id="9253" PostId="6052" Score="0" Text="Thanks, but the blog post doesn't explain perspective clipping." CreationDate="2017-12-29T18:26:48.860" UserId="5944" />
  <row Id="9254" PostId="6052" Score="0" Text="I think it does. What is perspective clipping for you?" CreationDate="2017-12-29T18:47:53.493" UserId="7957" />
  <row Id="9255" PostId="5557" Score="0" Text="You write that you want a _specific_ answer, but I think that is going to be difficult if you don't give any information about the tools you used to convert it, where you find the image, etc." CreationDate="2017-12-31T02:02:28.940" UserId="7969" />
  <row Id="9256" PostId="6060" Score="0" Text="I received a message that the project can be done in 2d, any suggestions? Physics does not have to be perfect." CreationDate="2017-12-31T07:00:59.657" UserId="7966" />
  <row Id="9257" PostId="6059" Score="0" Text="Would I be right in thinking your &quot;raw RGB triplets&quot; are floating-point values, and the problem comes when you clamp them to [0,1]?" CreationDate="2017-12-31T10:53:07.487" UserId="2041" />
  <row Id="9259" PostId="6059" Score="0" Text="@DanHulme That's right! The same problem could also occur if I work with another colorspace internally and then want to convert to something the display or file format can handle. But in this particular case it is because I use floats with light sources brighter than &quot;one&quot;." CreationDate="2017-12-31T23:23:33.040" UserId="7969" />
  <row Id="9260" PostId="6032" Score="0" Text="Thanks for the figure. It seems as if I have understand you correctly. Use a combination of gluLookAt() and  gluPerspective() to achieve the desired projection and position change of the camera. 2D vanishing points are something else: Parallel lines in 3D do not intersect - but after projection in the camera plane the projected lines might intersect. The intersection point in 2D is called &quot;vanishing point&quot;. It is possible to infer the projection properties and rotation of a camera from vanishing points and when I first have read your question I thought you were maybe interested in this." CreationDate="2018-01-02T14:56:09.033" UserId="4652" />
  <row Id="9261" PostId="6036" Score="0" Text="Ok, I'm trying this. Thank's" CreationDate="2018-01-02T18:24:00.503" UserId="7936" />
  <row Id="9262" PostId="6040" Score="0" Text="I'm not sure I understand what the actual question is here." CreationDate="2018-01-03T09:24:44.237" UserId="2041" />
  <row Id="9263" PostId="6039" Score="0" Text="Could you [edit] your question to add the title and author of the paper you're linking? That'll make it easier for interested people to find your question, and it will help out anyone who can't access your link, or if the link dies." CreationDate="2018-01-03T09:27:07.353" UserId="2041" />
  <row Id="9264" PostId="6070" Score="0" Text="For further information, you might like to consult my answer to [*What are the practical differences when working with colors in a linear vs. a non-linear RGB space?* on Stack Overflow](https://stackoverflow.com/q/12524623/967945)" CreationDate="2018-01-04T11:44:42.567" UserId="2041" />
  <row Id="9265" PostId="6072" Score="0" Text="*&quot;The reasons for not doing it the 'right' way, is probably that it still looks fine&quot;*&#xA;I suspect it's more that when graphics moved from work stations (such as SGI machines) that had, say, 10bit per channel support and perhaps did linear colour directly, to lower end devices with only 8bpc, that either doing the conversion was too expensive or simply forgotten about." CreationDate="2018-01-04T14:50:22.963" UserId="209" />
  <row Id="9266" PostId="6073" Score="0" Text="Where did you find the above image?" CreationDate="2018-01-05T07:05:28.647" UserId="3003" />
  <row Id="9268" PostId="6073" Score="0" Text="@user1118321 [google](http://c8.alamy.com/comp/H9348Y/chaotic-pointillist-half-tone-circle-pattern-random-dots-H9348Y.jpg) search :)" CreationDate="2018-01-05T08:59:08.467" UserId="6541" />
  <row Id="9269" PostId="6044" Score="0" Text="I have added another figure indicating that the eye (origin of the view coordinate) does not change, but the position of the PRP." CreationDate="2018-01-05T11:20:35.753" UserId="7929" />
  <row Id="9270" PostId="6032" Score="0" Text="Thank you for answering. I don't want to change the vanishing point of parallel lines in the 2D projection, but the PRP of the perspective projection matrix. I have added another figure indicating that the eye (origin of the view coordinate) does not change, but the position of the PRP. glLookAt() enables to move the eye, but not the PRP (https://www.khronos.org/registry/OpenGL-Refpages/gl2.1/xhtml/gluLookAt.xml), which is the parameter that I want to change. You can find this general model in &quot;Computer Graphics with OpenGL 4th ed&quot;, page 322." CreationDate="2018-01-05T11:21:37.330" UserId="7929" />
  <row Id="9272" PostId="6073" Score="0" Text="Did the page it came from have any information about it? Just curious because sometimes the artist will discuss how they created it. I have an idea of one way it might work, but am not sure how close it would be." CreationDate="2018-01-05T16:54:51.663" UserId="3003" />
  <row Id="9273" PostId="6073" Score="0" Text="@user1118321 feel free tell me your solution" CreationDate="2018-01-05T17:00:04.617" UserId="6541" />
  <row Id="9274" PostId="6075" Score="0" Text="Are you asking &quot;Why is the inverse of a rotation matrix the same as its transpose&quot;?" CreationDate="2018-01-05T17:08:05.990" UserId="209" />
  <row Id="9275" PostId="6078" Score="1" Text="Blender questions belong on [blender.se]. If you could edit out the &quot;how to use Blender&quot; parts of your question, we should be able to help you with the programming part." CreationDate="2018-01-05T18:23:27.127" UserId="2041" />
  <row Id="9276" PostId="6078" Score="1" Text="First thing to check is that the indices in the file you've shown are 1-based, while in GL they should be 0-based. That is, you need to subtract one from all the indices in the file." CreationDate="2018-01-05T18:24:53.857" UserId="2041" />
  <row Id="9277" PostId="6073" Score="0" Text="I'm looking some stuff up about it and will write something when I've got something concrete." CreationDate="2018-01-05T19:35:01.107" UserId="3003" />
  <row Id="9278" PostId="6073" Score="0" Text="You are probably keen on shadertoy fragment shader code with some magic numbers, but I think this could be solved doing some instanced draw of `GL_POINTS` on screen quad. These would have variable size of point as function of distance from screen center and would be textured as circle. Their screen position would be precomputed using [Hammersley Point Set](http://holger.dammertz.org/stuff/notes_HammersleyOnHemisphere.html)" CreationDate="2018-01-05T20:22:05.687" UserId="4958" />
  <row Id="9279" PostId="6075" Score="1" Text="That is a rotation or reflection matrix. You can check that the four columns are orthogonal to each other (the dot product of the columns is zero). Also they are of unit length, so the determinant is $\pm$1. A well known property of orthonormal square matrices is that transpose is equal to the inverse." CreationDate="2018-01-05T21:26:36.693" UserId="7957" />
  <row Id="9280" PostId="6032" Score="0" Text="Okay, I had a look at the book and now it is clear what you want. I have edited my answer accordingly. Sorry, I did not get it earlier." CreationDate="2018-01-05T21:57:00.857" UserId="4652" />
  <row Id="9281" PostId="6079" Score="0" Text="thanks good explanation but I need your help to implementing this. I tried cellular noise [see this shader](https://www.shadertoy.com/view/4tBfWR) if you help me I accept your question :)" CreationDate="2018-01-06T04:47:52.297" UserId="6541" />
  <row Id="9282" PostId="6075" Score="0" Text="@MauricioCeleLopezBelon your explanation was truly helpful. Thanks." CreationDate="2018-01-06T06:51:13.260" UserId="7993" />
  <row Id="9283" PostId="6078" Score="1" Text="@DanHulme Done, I edited the question part to ask for help to modify the code in order to interpret the `wavefront` data correctly. I also subtracted 1 from every index in the file. This did change the rendered mesh, it is still not a cube though. Any help would be appreciated." CreationDate="2018-01-06T10:56:27.283" UserId="2485" />
  <row Id="9284" PostId="6086" Score="0" Text="Thank you so much for your answer!" CreationDate="2018-01-07T05:24:56.420" UserId="7999" />
  <row Id="9285" PostId="6088" Score="0" Text="have you tried jittering by +- half a pixel width? this should prevent the 'sample zone' belonging to a given pixel intersecting with that of its neighbors" CreationDate="2018-01-07T08:16:25.210" UserId="5703" />
  <row Id="9286" PostId="4804" Score="0" Text="@StinkySkunk I couldn't have said it better myself. I wrote a Unity3D implementation of Naive Surface Nets in C# and I tried to document my code as thoroughly as I could. Maybe something in there will help: https://github.com/TomaszFoster/NaiveSurfaceNets" CreationDate="2018-01-06T17:19:23.753" UserId="7997" />
  <row Id="9287" PostId="6088" Score="0" Text="It appears to be working as designed. If you want smooth edges in that situation, you have to use supersampling." CreationDate="2018-01-07T10:37:24.693" UserId="2041" />
  <row Id="9288" PostId="6088" Score="0" Text="@Sebastian Mmmm...the [image](https://imgur.com/a/mL9vh) still ends up tattered, just with marginally smoother edges. I'm hoping for actual visual noise around the sphere's border." CreationDate="2018-01-07T10:51:13.147" UserId="7868" />
  <row Id="9289" PostId="6088" Score="0" Text="@Dan I'm not looking for smoothness anymore; my supersampler does that after enough samples already. I was more thinking about some sort of edge fuzziness (rather than the raggedness I have at the moment) that'd gradually fade out as I caught more samples. It sounds like that isn't possible, though?" CreationDate="2018-01-07T10:53:41.570" UserId="7868" />
  <row Id="9290" PostId="6087" Score="0" Text="The OpenGL wiki has a great explanation of the process: https://www.khronos.org/opengl/wiki/Compute_eye_space_from_window_space" CreationDate="2018-01-08T17:07:27.233" UserId="7644" />
  <row Id="9292" PostId="6094" Score="0" Text="The second question you link to has a good analysis. What happens when you look at just 1 octave of the noise? Also, what interpolation function are you using? Linear interp will look more blocky than using a smooth step, for example." CreationDate="2018-01-09T06:45:43.723" UserId="3003" />
  <row Id="9293" PostId="6094" Score="0" Text="I don’t really use interpolation I think" CreationDate="2018-01-09T08:21:14.793" UserId="5965" />
  <row Id="9294" PostId="6094" Score="0" Text="@user1118321 so multiplying with another octave might work?" CreationDate="2018-01-09T08:22:13.710" UserId="5965" />
  <row Id="9295" PostId="6094" Score="2" Text="A heightmap generated from Perlin noise won't give you a plausible-looking river system anyway. Rivers &quot;cut&quot; their own valleys through terrain." CreationDate="2018-01-09T13:44:31.723" UserId="2041" />
  <row Id="9296" PostId="6094" Score="0" Text="@DanHulme I’m talking about a flat land mass so it wouldn’t look like normal rivers anyway" CreationDate="2018-01-09T14:59:49.273" UserId="5965" />
  <row Id="9297" PostId="6095" Score="0" Text="Documentation don't say anything about when data is sent to GPU (or I don't see it).&#xA;If `Map` doesn't allocate memory why `memcpy` works on returned pointer?" CreationDate="2018-01-09T15:25:54.890" UserId="3123" />
  <row Id="9298" PostId="6094" Score="2" Text="No, I'm not suggesting a solution. I'm suggesting a way of determining what the problem is. If you show just your lowest octave, what does it look like? Is it blocky? I don't enough about how the `simplexnoise()` function you're using works to tell you. Is that a function you wrote or something supplied by a library?" CreationDate="2018-01-09T16:56:24.843" UserId="3003" />
  <row Id="9299" PostId="6093" Score="0" Text="Just want to point out that you *sometimes* see projection matrices that are singular, but the OP says this isn't case in his example." CreationDate="2018-01-09T17:19:57.720" UserId="209" />
  <row Id="9300" PostId="6095" Score="0" Text="You must call Map on a resource that has been created (for example after calling CreateCommittedResource())" CreationDate="2018-01-09T18:08:41.923" UserId="7107" />
  <row Id="9301" PostId="6095" Score="0" Text="Check this: https://github.com/Microsoft/DirectX-Graphics-Samples/blob/master/Samples/Desktop/D3D12HelloWorld/src/HelloTriangle/D3D12HelloTriangle.cpp#L214" CreationDate="2018-01-09T18:09:03.730" UserId="7107" />
  <row Id="9302" PostId="6095" Score="0" Text="Ok, I get what you mean now. If I get it right: `Map` basically return virtual pointer to GPU memory and when you access it OS/driver send data directly to resource?" CreationDate="2018-01-09T18:20:40.570" UserId="3123" />
  <row Id="9303" PostId="6093" Score="0" Text="True, @SimonF, such is the case for an orthographic projection matrix." CreationDate="2018-01-09T18:21:45.163" UserId="8009" />
  <row Id="9304" PostId="2355" Score="0" Text="Actually @ratchetfreak, There will be a line line of maximal length that __passes through__ two points.  In your case it will likely either hug one wall of the corridor, or connects the adjacent corners.  If neither of those are maximal then it will extend into a far corner.  So I think that the O(n^2) approach is valid, it's just that the two choices of vertices for the line to consider do not necessarily define the start and end of the line; they are merely ON the line." CreationDate="2018-01-09T18:45:44.260" UserId="8009" />
  <row Id="9305" PostId="2355" Score="0" Text="Also, I'm concerned about a C shape that has very narrow thickness, but a large an open interior; and so a large radius of curvature.  Its diameter (as you define it) would be very small because it would only be a short that follows the curvature of the C.  Yet a cancer nodule the size of the interior size would be quite concerning.  So perhaps it is the convex hull that you want to compute the diameter of." CreationDate="2018-01-09T18:50:07.150" UserId="8009" />
  <row Id="9306" PostId="4151" Score="0" Text="Just want to make sure that it's not the sampler.  Make sure you are using nearest neighbour sampling only.  The sampler will attempt to produce a linear combination of a pixel and its neighbours, so don't do that.  If you are just slightly off centre then you might be weighing in a value from a neighbouring pixel." CreationDate="2018-01-09T18:59:42.373" UserId="8009" />
  <row Id="9307" PostId="5002" Score="0" Text="The first thing I think of is that you immediately lose the benefits of ClearType, which renders text with sub-pixel spatial resolution (sacrificing color fidelity) thanks to knowing the micro-geometry of the individual RGB elements of a typical LCD display.  Sampling the ClearType-rendered text and re-rendering it in 3D will exaggerate the poor color and furthermore lose the benefits of the sub-pixel micro-geometry.  Provided you know the geometry of the RGB elements within the VR display, maybe you could employ the same technique?" CreationDate="2018-01-09T20:00:07.540" UserId="8009" />
  <row Id="9308" PostId="6093" Score="0" Text="thank you for your answer! great help!" CreationDate="2018-01-09T20:05:04.420" UserId="7999" />
  <row Id="9309" PostId="6092" Score="0" Text="thank you for your answer!" CreationDate="2018-01-09T20:05:21.757" UserId="7999" />
  <row Id="9310" PostId="4151" Score="0" Text="@Wyck Yes, I was using nearest neighbor sampling: https://github.com/Strilanc/Quirk/blob/master/src/webgl/WglTexture.js#L168" CreationDate="2018-01-09T21:27:27.127" UserId="5328" />
  <row Id="9316" PostId="6099" Score="0" Text="&quot;Dimitar&quot; refers to the [presentation by Dimitar Lazarov](http://blog.selfshadow.com/publications/s2013-shading-course/lazarov/s2013_pbs_black_ops_2_slides_v2.pdf), earlier in the same SIGGRAPH course. See page 21 there for the &quot;split approximation&quot;." CreationDate="2018-01-11T22:30:30.317" UserId="48" />
  <row Id="9317" PostId="6100" Score="0" Text="I'm missing the extra 1/N in your explanation." CreationDate="2018-01-12T00:23:22.747" UserId="228" />
  <row Id="9319" PostId="1770" Score="0" Text="@NathanReed Why would you need the &quot;repeat&quot; mode if you have a quasy unlimited amount of texture space?" CreationDate="2018-01-12T03:13:27.813" UserId="5909" />
  <row Id="9320" PostId="6100" Score="0" Text="@user8469759 the 1/N is implicit in the integrals. it comes from &quot;averaging&quot; samples in numerical integration" CreationDate="2018-01-12T03:14:44.627" UserId="5703" />
  <row Id="9321" PostId="6100" Score="0" Text="Still, there's one more factor 1/N. Also I agree the BRDF is almost constant in realistic cases, but I wouldn't be sure of the $cos$ factor and the probability $p$ due to the importance sampling process." CreationDate="2018-01-12T10:30:21.793" UserId="228" />
  <row Id="9322" PostId="6104" Score="0" Text="I don't thing you understood what I was trying to do, I am basically trying to make the cubes themselves smooth, i.e instead of sharp cubes I should have a smooth cube like &quot;sphere&quot;" CreationDate="2018-01-12T19:01:43.050" UserId="7462" />
  <row Id="9323" PostId="6104" Score="0" Text="ahh i see, i misunderstood" CreationDate="2018-01-12T19:46:14.750" UserId="5703" />
  <row Id="9324" PostId="6104" Score="2" Text="you have finite and relatively small amount of ways that cubes can be arranged around a vertex. you could pregenerate meshes for all different configurations an replace each visible vertex with a mesh with the corresponding smooth shape... keep in mind that you will need C1 continuity so having those meshes align with the grid at their &quot;endpoint&quot; would be an easy way of doing it" CreationDate="2018-01-12T19:50:01.847" UserId="5703" />
  <row Id="9325" PostId="6104" Score="1" Text="&quot;you have finite and relatively small amount of ways that cubes can be arranged around a vertex&quot; And they have already been compactly enumerated in the marching cubes algorithm :)" CreationDate="2018-01-13T01:58:26.883" UserId="106" />
  <row Id="9326" PostId="6106" Score="1" Text="It's possible to have a surface that's both reflective and emissive, eg the glass surface of a dim light bulb. (Dim, so you can actually _see_ some reflection on top of the emission.) So this maybe isn't as much of a mistake as you thought. Although if the material on the light source is _purely_ emissive then certainly stop tracing there." CreationDate="2018-01-13T06:00:00.003" UserId="48" />
  <row Id="9327" PostId="6106" Score="0" Text="Ahh...no, sorry. :)&#xA;My light sources are stars (slowly building a space exploration game), and I haven't made any hybrid surfaces yet :P.&#xA;Wouldn't it be more correct to construct a lightbulb as an emissive filament inside a reflective/refractive volume?" CreationDate="2018-01-13T07:23:00.843" UserId="7868" />
  <row Id="9328" PostId="6112" Score="2" Text="*&quot;...Some authors even call a C0 composite Bézier curve a &quot;Bézier spline&quot;;[5] the latter term is however used by other authors as a synonym for the (non-composite) Bézier curve, and they add &quot;composite&quot; in front of &quot;Bézier spline&quot; to denote the composite case...&quot;* - Seems up to personal taste." CreationDate="2018-01-13T19:40:49.453" UserId="6" />
  <row Id="9329" PostId="6112" Score="0" Text="What would a non-composite Bézier curve be? Just a regular Bézier curve? So, is that statement saying that some people call Bézier curves Bézier splines? Btw, you should at least link us to that statement." CreationDate="2018-01-13T20:13:00.720" UserId="4718" />
  <row Id="9330" PostId="6112" Score="2" Text="It's right from the Wiki page *you* linked to, just a few sentences down." CreationDate="2018-01-13T20:13:36.163" UserId="6" />
  <row Id="9331" PostId="6100" Score="0" Text="those are meant to be &quot;distributed&quot; into the sigma notation sums, that is why there are parentheses. they should really be interpreted as integrals." CreationDate="2018-01-14T02:00:32.393" UserId="5703" />
  <row Id="9332" PostId="6113" Score="0" Text="Matrices can be row or column major which flips the order that you would apply them in. There's no need to inverse any rotations to accomplish a rotation." CreationDate="2018-01-14T04:30:04.793" UserId="113" />
  <row Id="9334" PostId="6113" Score="2" Text="Whoever told you the first case is confused and you are too. The overall transformation matrix, assuming it will be applied to the left of a column vector, should be $M=TR_2R_1S$." CreationDate="2018-01-14T11:48:42.113" UserId="106" />
  <row Id="9335" PostId="6111" Score="0" Text="Thanks Nathan, I was thinking along those lines myself. Unfortunately the cubemaps are engine-supplied per renderer, and Unity's graphics engine is a bit of a black box in a lot of ways. I may just have to bake out my own cubemap per-scene somehow and fall back to that on GLES2." CreationDate="2018-01-15T04:46:39.700" UserId="1937" />
  <row Id="9336" PostId="6118" Score="0" Text="Was hoping to hear from you! Fantastic answer. Regarding your warning: If I am making an application where a user can upload images (.jpgs) -  if I want to store them compressed I really have no choice but to compress the data &quot;online&quot; right? (I mean other than converting each uploaded image &quot;offline&quot; then reading it back)?" CreationDate="2018-01-15T18:19:49.380" UserId="7000" />
  <row Id="9337" PostId="6118" Score="0" Text="Furthermore, I read somewhere that OpenGL is going away from the generic `GL_COMPRESSED*` formats. Is there any situation where they are useful / a good idea?" CreationDate="2018-01-15T18:22:03.633" UserId="7000" />
  <row Id="9338" PostId="6118" Score="1" Text="@Startec: What do you mean by &quot;going away from&quot;? It's a part of the specification. And since it wasn't removed in 3.1, the only way to &quot;go away from&quot; it would be to remove it in a newer version of the spec. And that's pretty much not gonna happen. As for when to use it? Pretty much never." CreationDate="2018-01-15T18:28:53.160" UserId="2654" />
  <row Id="9339" PostId="6118" Score="0" Text="By that I meant: Is a more preferred way  to use non-generic formats? It so it sounds like it. I read this somewhere, although I can't find where. So where can a generic compressed type even legally be used?" CreationDate="2018-01-15T18:32:46.723" UserId="7000" />
  <row Id="9340" PostId="6118" Score="1" Text="@Startec: They were never considered a good way to get compressed images, so the status quo hasn't changed. As for where they can be legally used, the standard spells that out adequately. They're internal formats, so they can be used in most places that take internal formats." CreationDate="2018-01-15T18:34:21.047" UserId="2654" />
  <row Id="9341" PostId="6120" Score="0" Text="What's best depends a lot on your application and what operations you want to do on these spectra. Could you [edit] your question to add some more about that?" CreationDate="2018-01-15T22:17:36.030" UserId="2041" />
  <row Id="9342" PostId="6120" Score="0" Text="Just edited my question to add a bit of clarification." CreationDate="2018-01-15T22:40:25.303" UserId="6838" />
  <row Id="9347" PostId="6125" Score="1" Text="Comforting to know that I'm doing this well. I also like your little quote ;) Well said, your answer is accepted." CreationDate="2018-01-16T21:43:06.743" UserId="6838" />
  <row Id="9348" PostId="6127" Score="2" Text="Textures should be stored on GPU VRAM in their compressed form, AFAIK they are decompressed by texture samplers at rendering time rather than at load time, as that would defeat the purpose of compression if you were to unpack them to VRAM. You should do something like render the texture via a quad into an RGB buffer if you want the decompressed version." CreationDate="2018-01-17T02:10:36.773" UserId="3073" />
  <row Id="9349" PostId="6126" Score="0" Text="So convergence rests on the BRDF being zero/nonzero. How does that work? I've been assuming that the BRDF is just a generator function for ray directions..." CreationDate="2018-01-17T04:53:52.907" UserId="7868" />
  <row Id="9350" PostId="6126" Score="0" Text="Also, why shouldn't light sampling accelerate convergence for large, close light sources? If every ray that isn't obstructed is guaranteed to reach the light (which they *should* be for area lights), then shouldn't light sampling always give a better result for diffuse surfaces?" CreationDate="2018-01-17T04:55:27.003" UserId="7868" />
  <row Id="9351" PostId="6127" Score="0" Text="Why can you not simply have compressed and uncompressed versions of the images on disk?" CreationDate="2018-01-17T06:12:04.833" UserId="2654" />
  <row Id="9352" PostId="6127" Score="0" Text="@NicolBolas My guess is it's a demo-scene program where the total size of assets and program is limited and the questioner is trying to push the limit." CreationDate="2018-01-17T09:44:07.407" UserId="2041" />
  <row Id="9353" PostId="6127" Score="0" Text="If you *really* want the decompressed texels, why not render a 1:1 quad with the texture on it and read back the resulting pixels in the framebuffer." CreationDate="2018-01-17T09:55:55.993" UserId="209" />
  <row Id="9354" PostId="6126" Score="0" Text="@PaulFerris the BRDF is a function which says how much light is reflected in any given direction. The ray generator function can be any distribution which throws rays everywhere the BRDF is non-zero (eg. a uniform generator over the entire sphere). You get ideal sampling when the generator exactly matches the BRDF but that is not a requirement of MIS. Some BRDFs are very hard to generate rays for perfectly so a different function is often used. I'll edit the answer for your other comment." CreationDate="2018-01-17T16:54:24.780" UserId="5717" />
  <row Id="9359" PostId="6138" Score="1" Text="The question might be a little bit broad." CreationDate="2018-01-21T13:52:29.547" UserId="6" />
  <row Id="9360" PostId="6138" Score="0" Text="@ChristianRau Can you give slight ideas at least.basic theoretical aspects ffor quick learn stuff,it would be really helpful.Thank for the editing to.. I had spelled some words wrongly my bad :)" CreationDate="2018-01-21T13:55:44.810" UserId="8043" />
  <row Id="9361" PostId="6138" Score="0" Text="good question, but still flagging as off-topic. I wish you best of luck finding out what you are after" CreationDate="2018-01-21T15:08:09.327" UserId="3041" />
  <row Id="9362" PostId="6138" Score="0" Text="@Andreas :( off topic? I couldn't find nice resource about this topic other Graphic Programming Black Book" CreationDate="2018-01-21T16:25:41.753" UserId="8043" />
  <row Id="9363" PostId="6138" Score="0" Text="...because I think you will find better answers in forums about general driver implementation. No offense meant. Really trying to help :) Maybe you should ask yourself: what is a 'low level' graphics API? how is efficiency measured? what is optimized, or optimal?" CreationDate="2018-01-21T17:25:10.370" UserId="3041" />
  <row Id="9364" PostId="6140" Score="1" Text="Thank you for answer Dan... :)&#xA;So,Learning to implement our own Graphic API is not worthy right ? We cannot optimized our system since we don't have an agreement or understanding with GPU vendors or their GPU Internals." CreationDate="2018-01-21T19:12:45.933" UserId="8043" />
  <row Id="9366" PostId="6126" Score="0" Text="That makes sense, thank you. So my misunderstanding was treating the BRDF and the ray generator as the same thing when the BRDF really just gives reflectance along the output rays provided by the generator? W.r.t light sampling, would this (https://imgur.com/a/BC8HV) be an example of the sort of situation you're referring to?" CreationDate="2018-01-22T05:39:48.833" UserId="7868" />
  <row Id="9367" PostId="6138" Score="0" Text="@Andreas I was getting deviated that's why,Lost between HDL and HAL" CreationDate="2018-01-22T06:12:05.280" UserId="8043" />
  <row Id="9368" PostId="6139" Score="0" Text="seems to me like it is cuadratic, since you scale the amount of pixel samples both horizontally and vertically by some constant k. $O(width \times height \times k^2)$" CreationDate="2018-01-22T10:27:43.473" UserId="5703" />
  <row Id="9369" PostId="6140" Score="2" Text="Unless you're going to reverse-engineer the register and data structure interface of a GPU, you can't implement a GPU acceleration API without getting that information from a GPU vendor." CreationDate="2018-01-22T11:39:29.043" UserId="2041" />
  <row Id="9370" PostId="6137" Score="0" Text="What 'paper' are you referring to?  Where is this derivation coming from?" CreationDate="2018-01-22T11:59:40.660" UserId="7647" />
  <row Id="9371" PostId="6140" Score="0" Text="Got it thank you Dan,Which means whatever tech comes,we have to depend on set of standard/platform specific APIs." CreationDate="2018-01-22T12:52:04.277" UserId="8043" />
  <row Id="9372" PostId="6126" Score="1" Text="@PaulFerris Yes. Although be aware that some older BRDF papers will present only a generator function and a sample weighting function to compensate for the difference between the generator and the BRDF. And yes, the tip of that &quot;cube&quot; nearest the sphere is the kind of thing I had in mind. It would likely have more noise when sampling only the light (sphere) vs MIS." CreationDate="2018-01-22T14:45:21.340" UserId="5717" />
  <row Id="9373" PostId="6145" Score="0" Text="I suspect that this is where &quot;a very small amount&quot; comes in, you'd still want triple buffering for larger data." CreationDate="2018-01-22T18:40:10.633" UserId="174" />
  <row Id="9374" PostId="6142" Score="0" Text="Can you include the actual equation you're asking about, and maybe some more background in your question for the benefit of people who may not have those references handy?" CreationDate="2018-01-22T18:52:36.170" UserId="48" />
  <row Id="9375" PostId="6145" Score="0" Text="Not sure I understand the question. Presumably your render targets are still double or triple buffered, regardless of how you manage uniform data...?" CreationDate="2018-01-22T18:55:12.750" UserId="48" />
  <row Id="9376" PostId="6126" Score="0" Text="Excellent, thanks for all the help :)" CreationDate="2018-01-22T19:05:54.683" UserId="7868" />
  <row Id="9380" PostId="6146" Score="0" Text="Assume I have a `data store` of size 2MB then I use `glBufferSubData` to add new data which is 1.5MB. Wouldn't there be an extra 0.5MB of data in the `data store` which I don't potentially want to draw?" CreationDate="2018-01-23T00:25:19.650" UserId="6546" />
  <row Id="9381" PostId="6146" Score="0" Text="or I suppose that where I'd just draw less `primitives` with `glDraw*`. Is that correct?" CreationDate="2018-01-23T00:43:22.493" UserId="6546" />
  <row Id="9382" PostId="6146" Score="1" Text="@Archmede: Why would you draw using data that you don't want to draw from?" CreationDate="2018-01-23T01:04:24.803" UserId="2654" />
  <row Id="9383" PostId="6146" Score="0" Text="My specific scenario is I fill data with terrain (like minecraft) but when I move somewhere else on the map I'll refill the `data store`  with new terrain but this new terrain might contain less data than the old terrain. So wouldn't there be some data from the old terrain left in the `data store` if the new terrain contained less data? Make sense?" CreationDate="2018-01-23T01:37:57.903" UserId="6546" />
  <row Id="9384" PostId="6146" Score="0" Text="So what if there's old data there? Why would you tell OpenGL to read from the old data? As I said, &quot;You tell OpenGL exactly how many vertices to read from a vertex buffer.&quot; So why would you tell OpenGL to read from part of a buffer that doesn't contain relevant data?" CreationDate="2018-01-23T01:42:09.520" UserId="2654" />
  <row Id="9385" PostId="6145" Score="0" Text="This might be a particularity of Metal, I'm not sure. But I think in case of Metal you just manually manage your render targets, with a semaphore. I'm just guessing, but if I remove the semaphore it behaves like double-buffering (GPU displays one frame while CPU is encoding the next)." CreationDate="2018-01-23T02:03:54.623" UserId="8068" />
  <row Id="9386" PostId="6146" Score="0" Text="I understand that. I misread the difference between `glBufferStorage` and `glBufferData`. I assumed `glBufferSubData` can reallocate the `data store` but I was mistaken. Thanks for your help though, it's greatly appreciated." CreationDate="2018-01-23T02:12:53.143" UserId="6546" />
  <row Id="9387" PostId="6137" Score="0" Text="https://www.pixar.com/assets/pbm2001/pdf/notesc.pdf (Page C10) $//$&#xA;http://run.usc.edu/cs599-s10/cloth/baraff-witkin98.pdf (is also using the equation)" CreationDate="2018-01-23T03:29:49.097" UserId="6806" />
  <row Id="9388" PostId="6149" Score="0" Text="Thanks! I'm guessing Blackman-Harris is blurry (no negative lobes), but not as much as the Gaussian?" CreationDate="2018-01-23T08:27:28.580" UserId="7868" />
  <row Id="9389" PostId="6148" Score="1" Text="What makes you think it is necessarily slower? If you can [edit] your question to add that extra detail, you'll get better answers." CreationDate="2018-01-23T09:30:06.543" UserId="2041" />
  <row Id="9391" PostId="6150" Score="1" Text="I would also add that SH (or the related [H-basis](https://www.cg.tuwien.ac.at/research/publications/2010/Habel-2010-EIN/)) is useful for normal maps on static objects. A bumpy surface will reflect light from different directions, so you want a directional representation of incoming light at the surface." CreationDate="2018-01-23T18:26:23.997" UserId="48" />
  <row Id="9392" PostId="6149" Score="1" Text="@PaulFerris I think that's right, although the two are quite similar in shape, so there's probably not a lot of difference. The filter width can be tuned to trade-off between blurring and aliasing, as well." CreationDate="2018-01-23T19:24:45.653" UserId="48" />
  <row Id="9393" PostId="6150" Score="0" Text="Thanks for the link, I hadn't heard of that basis before. Just been reading about spherical Gaussians which can apparently fake rough specular as well, but the math is pretty gnarly!" CreationDate="2018-01-23T20:15:09.983" UserId="1937" />
  <row Id="9394" PostId="6149" Score="0" Text="Good to know. Thanks for the answer :)" CreationDate="2018-01-24T00:19:30.233" UserId="7868" />
  <row Id="9395" PostId="6156" Score="1" Text="Are you requesting a 4D visualisation (which would be projected back to 2D on the screen)? I don't think my brain would cope with that.  Can you not just consider a 2D example lifted into homogeneous 3D space?" CreationDate="2018-01-24T16:34:00.687" UserId="209" />
  <row Id="9396" PostId="6157" Score="0" Text="&quot;*However, if i render this with a custom shader that implements blinn lighting, the sphere appears black, because the &quot;viewDir&quot; calculation is negative, since the eye is centered inside the sphere.*&quot; If you're able to see the sphere from the outside, then I submit that your &quot;eye is centered inside the sphere&quot; notion is incorrect. Either that, or you have a different understanding of what the word &quot;eye&quot; means." CreationDate="2018-01-24T17:54:16.477" UserId="2654" />
  <row Id="9397" PostId="6157" Score="0" Text="I mean that the model view is centered inside the sphere." CreationDate="2018-01-24T18:25:31.790" UserId="3332" />
  <row Id="9398" PostId="6157" Score="0" Text="From the glOrtho docs: &quot;Typically, the matrix mode is GL_PROJECTION, and left bottom - nearVal and right top - nearVal specify the points on the near clipping plane that are mapped to the lower left and upper right corners of the window, respectively, assuming that the eye is located at (0, 0, 0). - farVal specifies the location of the far clipping plane. Both nearVal and farVal can be either positive or negative.&quot;  So they even seem to confirm that the eye is at 0,0 but the near clip can be negative so you see stuff &quot;behind&quot; you." CreationDate="2018-01-24T18:28:21.763" UserId="3332" />
  <row Id="9399" PostId="6154" Score="0" Text="Thanks for the answer. What happens to the weight function if p is one of the polygon vertices. The Ai triangle becomes a line with zero area? Also what do you mean by signed area?" CreationDate="2018-01-24T20:30:34.333" UserId="6041" />
  <row Id="9400" PostId="6154" Score="0" Text="I added a paragraph explaining what to do at the boundaries. For the signed triangle area consult [this.](http://demonstrations.wolfram.com/Signed2DTriangleAreaFromTheCrossProductOfEdgeVectors/)" CreationDate="2018-01-24T20:59:56.930" UserId="7724" />
  <row Id="9401" PostId="6156" Score="1" Text="Yes, the standard way to understand this is to visualize the lower-dimensional analogue of this operation, i.e. a 2D translation represented as a 3D shear." CreationDate="2018-01-25T03:23:53.210" UserId="106" />
  <row Id="9402" PostId="6163" Score="0" Text="Would it be reasonable to say that the cosine distribution integrates to $\pi$ because $cos$ is a one-dimensional function? So integrating it over a hemispherical domain gives you the solid angle consumed by the disk passing through the equator?" CreationDate="2018-01-25T07:15:10.197" UserId="7868" />
  <row Id="9403" PostId="6163" Score="1" Text="Yeah that's an interesting way of looking at it. Intuitively, any infinitesimal patch of the hemisphere's surface will have its projected area onto the equatorial disk scaled by N.L, so the sum of the projected areas is just the area of the disk. Spherical calculus is not really my strong suit though ;)" CreationDate="2018-01-25T10:02:18.347" UserId="1937" />
  <row Id="9404" PostId="6151" Score="0" Text="That's not exactly accurate, though. Commands that actually send data to the API are *still* synchronous, e.g. you can immediately free the memory that you called `glBufferData` with right after calling it, since it's copied into the API then. What *is* possibly unsynchronized is the transfer of that data to the actual GPU memory. However, the same asynchronous transfer approaches can be applied for data download, too. It's just that in this case you usually have to wait for all previous operations to finish." CreationDate="2018-01-25T12:52:34.560" UserId="6" />
  <row Id="9405" PostId="6151" Score="0" Text="@ChristianRau but most of the time the data you send through glBufferData is first copied to another part of memory from which the actual transfer to the GPU will take place asynchronously." CreationDate="2018-01-25T13:02:30.370" UserId="137" />
  <row Id="9406" PostId="6151" Score="0" Text="Yeah, but you could as well do that with `glReadPixels`." CreationDate="2018-01-25T13:03:44.503" UserId="6" />
  <row Id="9407" PostId="6151" Score="0" Text="@ChristianRau no because the driver cannot know you want that data until you call glReadPixels which is the trigger to make the gpu start sending data to RAM. The glReadPixels call cannot return until the GPU is done and the data is ready to be used." CreationDate="2018-01-25T13:07:39.497" UserId="137" />
  <row Id="9408" PostId="6151" Score="0" Text="No, `glReadPixels` *will* return immediately if you tell it to copy that data into a buffer object. Then you can do other things and *then* you can retrieve the data from the buffer whenever you want." CreationDate="2018-01-25T13:16:57.980" UserId="6" />
  <row Id="9409" PostId="6151" Score="0" Text="@ChristianRau oh you were using `GL_PIXEL_PACK_BUFFER`, yeah that will let you do transfers async (though you have to assume the driver isn't dumb enough to leave that in vram)" CreationDate="2018-01-25T13:24:04.890" UserId="137" />
  <row Id="9410" PostId="6166" Score="0" Text="Awesome, Your comment about GL_LIGHT_MODEL_LOCAL_VIEWER seems right on. I checked our code and we don't set that, and it appears the default is 0 which means that our viewer is always infinitely far away, even in perspective mode (without shaders).  So I assume that even though the specular does move around when i rotate, its because it is rotating the direction of the z-axis.  However, when i translate, the specular doesn't move around, even in perspective.  Thanks!" CreationDate="2018-01-25T15:49:27.037" UserId="3332" />
  <row Id="9411" PostId="6139" Score="2" Text="Are you assuming a GPU-accelerated MSAA implementation? Because in that case the GPU absorbs most of the cost.  If your runtime is limited by fragment shader execution time, then the slowdown will be in proportion to the number of pixels where an edge is visible, because this is where you'll execute your shader multiple times." CreationDate="2018-01-25T16:04:06.747" UserId="3386" />
  <row Id="9418" PostId="6168" Score="0" Text="Thanks, and I guess it's reasonable advice. It just seemed too elaborate and I don't like to have entirely different approaches for 3 and 4 hardware when it's just a seemingly minor hurdle." CreationDate="2018-01-25T18:18:00.657" UserId="6" />
  <row Id="9419" PostId="6163" Score="0" Text="I guess that's understandable :P. Thanks for the answers." CreationDate="2018-01-25T19:39:15.643" UserId="7868" />
  <row Id="9420" PostId="6169" Score="0" Text="on a phong brdf, samples are generated in a space where the reflected direction is the z axis. thats where the dependence comes from" CreationDate="2018-01-26T05:41:23.580" UserId="5703" />
  <row Id="9421" PostId="6169" Score="0" Text="Still, the phi angle is sampled over a circle uniformly instead of favoring the specular direction! This is the part I didn't get" CreationDate="2018-01-26T11:41:14.783" UserId="7871" />
  <row Id="9422" PostId="6173" Score="2" Text="&quot;The RGB values are most likely gamma compressed.&quot; That's common for 8-bit images, but you need to actually *know*, not just guess. Monitor calibration is a bit of a red herring nowadays, because applications should be drawing sRGB and the monitor should be interpreting its input as sRGB anyway." CreationDate="2018-01-26T14:50:06.340" UserId="2041" />
  <row Id="9423" PostId="6173" Score="3" Text="A typical display is calibrated to sRGB, not gamma 2.2.  So you should use the linear to sRGB or sRGB to linear conversions.  And a 50% sRGB value should be 188, not 186.  (see Wikipedia article for sRGB which says that a normalized 50% intensity should get an sRGB value of (1.055*0.5^(1/2.4))-0.055 = 0.735358, which is about 187.516 in 8-bit sRGB, hence the logic of encoding it as 188." CreationDate="2018-01-26T20:32:44.903" UserId="8009" />
  <row Id="9424" PostId="6174" Score="0" Text="Can't thank you enough, as a side question: after computing a sample according to this model, the resulting radiance should be divided by the pdf specified in the first formula I've posted?" CreationDate="2018-01-26T22:56:02.827" UserId="7871" />
  <row Id="9425" PostId="6174" Score="0" Text="it should be multiplied by the brdf and divided by the pdf. those usually cancel each other out so it is often not necesary" CreationDate="2018-01-26T23:11:15.717" UserId="5703" />
  <row Id="9426" PostId="6175" Score="2" Text="Any time you have instancing in your scene, i.e. multiple copies of the same base geometry, you can represent that as a single leaf node with multiple transformation nodes as parents." CreationDate="2018-01-27T08:15:41.513" UserId="106" />
  <row Id="9427" PostId="6175" Score="0" Text="I completely missed that common use case, that's a very practical and clear example. Thanks." CreationDate="2018-01-27T09:24:26.123" UserId="8101" />
  <row Id="9428" PostId="6176" Score="0" Text="Sorry, my question was indeed a little vague. I had in mind scene graphs for 3D rendering. The collision examples are a bit too much into the app-specific side of things from my point view. At the app level, it's easy to imagine almost anything as a graph of relationships between different objects. However, as pointed out by @Rahul, one example is when multiple nodes with different transformations share the same geometry, in that case it's a non-tree graph." CreationDate="2018-01-27T09:30:57.180" UserId="8101" />
  <row Id="9429" PostId="6175" Score="0" Text="@Rahul You should post that as an answer, even though it's short. It's exactly the example the questioner was looking for." CreationDate="2018-01-27T13:50:07.987" UserId="2041" />
  <row Id="9431" PostId="6173" Score="0" Text="The monitor calibration image is indeed kind of misleading for my argument. It does show that I should perform gamma expansion, but not which one.&#xA;Am I correct to conclude that most images will likely be encoded with sRGB and I should use the formula provided by @Wyck? ($1.055L^{1/2.4}-0.055$)" CreationDate="2018-01-27T16:01:51.310" UserId="8097" />
  <row Id="9432" PostId="6177" Score="1" Text="Yes, you can do that. What issue are you having with doing that? What have you tried that hasn't worked? You need to give us more information." CreationDate="2018-01-27T22:48:00.177" UserId="3003" />
  <row Id="9433" PostId="6177" Score="0" Text="I added more information above, does it help? Thanks." CreationDate="2018-01-27T23:21:01.233" UserId="5183" />
  <row Id="9434" PostId="6177" Score="0" Text="Yes, it does. Thank you!" CreationDate="2018-01-27T23:29:27.447" UserId="3003" />
  <row Id="9435" PostId="6179" Score="0" Text="Thanks for the solution. It makes sense! Although I'm testing things out, and I'm suspecting that framebuffer[j] isn't a value between 0 and 1. I'm trying to figure out if it's a value between 0 and 255 instead and hoping there isn't some other subtle transformation in the values." CreationDate="2018-01-28T03:01:36.930" UserId="5183" />
  <row Id="9436" PostId="6180" Score="0" Text="that is the correct way. maybe darkness comes from not doing calculations in linear space?" CreationDate="2018-01-28T20:42:45.743" UserId="5703" />
  <row Id="9437" PostId="6180" Score="0" Text="Have you tried increasing the light amounts?" CreationDate="2018-01-28T21:24:31.973" UserId="3003" />
  <row Id="9438" PostId="6180" Score="0" Text="@SebastiánMestre What do you mean by linear space? I'm sorry, I'm new to this." CreationDate="2018-01-28T23:42:09.827" UserId="7925" />
  <row Id="9439" PostId="6180" Score="0" Text="@user1118321 No, I've been using only one light, a directional light. In my mind it should be enough and it shouldn't be as dark as it is." CreationDate="2018-01-28T23:43:13.327" UserId="7925" />
  <row Id="9440" PostId="6180" Score="0" Text="i mean linear colour space. images are usually gamma compressed and can give wrong results when using them in shading. they should first be gamma corrected. you can search what that means online." CreationDate="2018-01-29T00:18:30.707" UserId="5703" />
  <row Id="9442" PostId="6159" Score="0" Text="As a designer of a GPU lossless image compression system, most of these seem to make sense, with the possible exception of using 32-bit float over 16-bit. Surely the LSBs of floating point data become increasingly less coherent and thus harder to compress." CreationDate="2018-01-29T09:06:14.237" UserId="209" />
  <row Id="9443" PostId="6183" Score="2" Text="This is a great example of a homework question where you've &quot;done your homework&quot;: you've shown how far you got and what you have, and where you're stuck. Good work!" CreationDate="2018-01-29T11:14:23.163" UserId="2041" />
  <row Id="9444" PostId="6183" Score="1" Text="Thank you, now if only someone could help me figure it out,haha :D" CreationDate="2018-01-29T11:22:46.313" UserId="8108" />
  <row Id="9445" PostId="6184" Score="0" Text="Oh, you were probably typing out your answer when I replied above! Thank you very much for the detailed answer, Dan! Everything is much clearer now." CreationDate="2018-01-29T12:22:44.820" UserId="8108" />
  <row Id="9446" PostId="6184" Score="0" Text="One question though, about the tea-pot. If the bounding box does not change, am I correct to assume that the enclosed object does not matter? The tea pot fits within the bounding box but the bounding box remains the same (cfr. the question), so it does not affect the surface area, which is still calculated using the 6 surfaces of the cubic bounding box." CreationDate="2018-01-29T12:27:11.987" UserId="8108" />
  <row Id="9447" PostId="6184" Score="0" Text="Oh of course, this version of the SAH just uses the box volume. Ignore that bit then! I was thinking of a different version that actually uses the triangle areas in the leaf nodes. In that case, ask yourself the same question as for the last part: will changing the objects change any number in the heuristic? Will they change the actual hit-ratio of the box? What does this tell you about how the heuristic relates to the measured performance?" CreationDate="2018-01-29T13:07:05.770" UserId="2041" />
  <row Id="9448" PostId="6184" Score="0" Text="I would say that changing the objects or the camera position doesn't change the heuristic, seen as though the surface area is based on Crofton's theorem (independent of the camera). So I'd assume that (in line with what you said) the position of the camera does not influence the score. But, it's only a heuristic so the position of the camera WILL influence the hit ratio. E.g: the 1st setup when we view a camera from above (y axis): if a ray hits the bounding box there, we have a much higher probability of actually hitting an object which is not the case if we have a camera along the z-axis." CreationDate="2018-01-29T14:25:22.897" UserId="8108" />
  <row Id="9449" PostId="6181" Score="0" Text="And these terms aren't adressed anywhere in the paper?" CreationDate="2018-01-29T23:01:46.227" UserId="6" />
  <row Id="9450" PostId="6181" Score="0" Text="Not in a meaningful way. The paper deals with a vertex based model (the equations and the logic behind it describe the deformation of the mesh in terms of the displacement of the vertices) but it doesn't discuss triangle-based models. I can only guess what they're about. Moreover I don't know why triangle-based models would be more &quot;expressive&quot; than vertex-based ones." CreationDate="2018-01-30T14:15:30.450" UserId="8107" />
  <row Id="9451" PostId="6187" Score="0" Text="How much manual effort do you want to put in?  For example, would it be okay if you had to specify the (x,y) coordinates of the start of the color scale and the (x,y) coordinates of the end of the color scale, and manually input the numeric values?  Or are you hoping for an Artificial Intelligence solution that could potentially 1) Find the legend, 2) OCR the labelling and hash marks, 3) determine an appropriate function (potentially non-linear) to map the colors to values, 4) deal with non-data in the image, such as labels." CreationDate="2018-01-30T17:52:34.567" UserId="8009" />
  <row Id="9452" PostId="6181" Score="0" Text="They are cited from this paper:  http://people.csail.mit.edu/sumner/research/deftransfer/Sumner2004DTF.pdf" CreationDate="2018-01-30T18:05:50.243" UserId="8009" />
  <row Id="9453" PostId="6190" Score="0" Text="Yeah I should have pointed out I've tried doing edge1.cross(egde2) without any noticeable difference. The dot product of the incident ray and the normal also does not seem to have a noticeable effect when I change the comparison sign or just remove it completely." CreationDate="2018-01-30T21:34:17.907" UserId="6760" />
  <row Id="9454" PostId="6189" Score="0" Text="The look of the gradients in your first image looks like swapped normals to me. Have you tried rendering your normals to check that they are correct? My suspicion would be directed toward vertex indices." CreationDate="2018-01-31T02:21:01.513" UserId="182" />
  <row Id="9456" PostId="6188" Score="1" Text="you can get into trouble if compression artifacts crop up though" CreationDate="2018-01-31T09:11:01.963" UserId="137" />
  <row Id="9457" PostId="6189" Score="0" Text="I have updated my answer to include an image of the normals, and it seems to be what I would expect. Or rather, the normals are consistent on the triangles that make up a plane." CreationDate="2018-01-31T09:39:40.003" UserId="6760" />
  <row Id="9458" PostId="6188" Score="0" Text="Indeed, this technique would not be very robust in presence of artefacts. Although, the colour look up could be relaxed to find the nearest match instead of an exact match. In any case it remains an approximation." CreationDate="2018-01-31T09:54:04.650" UserId="7724" />
  <row Id="9459" PostId="6187" Score="0" Text="Wyck, thanks for your answer. I'm willing to put lots of efforts to prepare my classes. And yes it would be totally OK if I had to specify the coordinates of the start and end of the color scale and manually input the numeric values. Keep in mind I'm not searching for a software which takes hundreds of maps and automatically converts to data, I only need to get numbers from a few maps per year (eg 30). So no, I am not looking for the AI approach you mention." CreationDate="2018-01-31T13:41:04.353" UserId="8116" />
  <row Id="9460" PostId="6187" Score="0" Text="All I want is to be able to do one, or maybe two things: 1) click at any point in the image and get the value corresponding to that particular color and maybe 2) measure the area covered by any particular color (by user input)." CreationDate="2018-01-31T13:43:41.207" UserId="8116" />
  <row Id="9461" PostId="6188" Score="0" Text="Thanks, I'm aware of the artifacts introduced by lossy formats like JPEG. However for my classes I am not interested in retrieving &quot;pure&quot;, real, accurate data but rather I take my map (jpg) as the &quot;truth&quot; and work from there. For my purposes this is enough. Otherwhise I would not even bother. All I care is that a given RGB value equals some quantity (eg elevation) within a given image. For example, I won't be comparing two JPG. I only work with one image at the time, and only when the scale color bar is part of the image." CreationDate="2018-01-31T13:48:31.220" UserId="8116" />
  <row Id="9462" PostId="6187" Score="0" Text="I know there are programs (and have used them) to digitize plots (eg a XY plot from a JPG): you click at the beginning and at the end of each axis and input the corresponding values. Then you click on each data point and the program automatically gives you the X,Y coordinate of each data point. I was hoping to find something like this, does it make sense?" CreationDate="2018-01-31T13:53:32.297" UserId="8116" />
  <row Id="9463" PostId="6187" Score="0" Text="I don't know of a software package that provides this kind of lookup functionality out-of-the-box.  But as a developer of graphical software, I'm inclined to write it from scratch.  Or if you need help implementing a specific operation, then ask for specific help.  Or create an open source project on GitHub and I'll contribute.  :)  Reynold's answer is on the right track for the technique." CreationDate="2018-01-31T15:13:07.300" UserId="8009" />
  <row Id="9464" PostId="6191" Score="1" Text="You need to search for &quot;Homography&quot;.  Estimate a homography that places the image in the scene based on the normals and transform your image by the homography." CreationDate="2018-01-31T15:35:36.560" UserId="8009" />
  <row Id="9465" PostId="6189" Score="0" Text="I don't think the problem is in the code you've shown.  It looks more like the light isn't being transformed into the coordinate system of the object correctly.  Where's the code that shades the pixel based on the normal?  Does it perform the N·L calculation in world space or object space? (Transform light to object space or transform object to light space, or transform both to world space?)" CreationDate="2018-01-31T15:47:17.417" UserId="8009" />
  <row Id="9466" PostId="6179" Score="1" Text="@Stackmm, It *is* a value between 0 and 255.  They are 8-bit normalized, meaning the values in the interval 0.0 through 1.0 are mapped to the 8-bit codes 0x00 through 0xFF.  The 8-bit value you are looking for that equals 0.1 is: `(unsigned char)(0.1 * 255.0 + 0.5)` or about 26." CreationDate="2018-01-31T15:57:17.780" UserId="8009" />
  <row Id="9467" PostId="6189" Score="0" Text="The lighting is done by sampling a unit sphere or hemisphere. I just use the normal as calculated by the cross product of the edges or the normals from blender. I'll link the code that samples the hemisphere." CreationDate="2018-01-31T16:10:28.110" UserId="6760" />
  <row Id="9468" PostId="6192" Score="1" Text="The problem is common to all ray based rendering in general for what that's worth!" CreationDate="2018-01-31T18:45:43.530" UserId="56" />
  <row Id="9469" PostId="6187" Score="0" Text="example 2 is not easy example 1 is indeed very easy. should be doable in on any sane GUI toolkit quickly." CreationDate="2018-01-31T19:16:52.503" UserId="38" />
  <row Id="9470" PostId="6179" Score="0" Text="Thank you very much! That solved everything. I'm glad I learned about the mapping." CreationDate="2018-01-31T22:55:37.343" UserId="5183" />
  <row Id="9471" PostId="6179" Score="0" Text="Ah, I failed to notice that you were using `GL_UNSIGNED_BYTE` as the type for the call to `glReadPixels`. You could use `GL_FLOAT` to get floating point values. (Just make sure to allocate space for them. Your current allocation specifies `new unsigned char []`.)" CreationDate="2018-02-01T01:30:05.267" UserId="3003" />
  <row Id="9472" PostId="6191" Score="0" Text="So it is possible to estimate a homography using normals, right? I have found homography estimation using two images or through finding vanishing points in a single image so far, but not using normals. Is there a specific term for this that I should search for?" CreationDate="2018-02-01T05:07:23.783" UserId="8112" />
  <row Id="9473" PostId="6196" Score="0" Text="Sorry for all the layout errors... Was typing this on my phone without any preview. I'll correct that knce I'm at my pc." CreationDate="2018-02-01T06:21:05.257" UserId="7008" />
  <row Id="9475" PostId="6195" Score="1" Text="Really interesting question. I long wondered about what the practical relevance of this function actually is nowadays, especially since there are no ranged calls for any of the more advanced drawcalls that arose later on, like instanced rendering for example. It *seems* like a relic of the past (maybe from the times before VBOs)." CreationDate="2018-02-01T10:53:23.370" UserId="6" />
  <row Id="9476" PostId="6181" Score="0" Text="@Wyck so the expressiveness of triangle-based models is just a practical fact, there is no mathematical or computational reason" CreationDate="2018-02-01T14:19:37.097" UserId="8107" />
  <row Id="9477" PostId="6201" Score="0" Text="I find it hard to believe that there's no explicit way to have double-buffering. Uh, bad news then. I liked when my arrays had a fixed size." CreationDate="2018-02-01T17:01:01.330" UserId="8118" />
  <row Id="9478" PostId="6201" Score="0" Text="@demanze: Vulkan exists to tell you how the hardware works and for you to interface with it. What you do with that information is up to you. If a hard FIFO implementation has a `maxImageCount` of 1, then you can create your own image to serve as a second buffer for a double-buffering system. Or you can just fail to have your application work, as hard FIFO with only 1 presentable image makes it pretty much impossible to present images on every vblank, no matter how fast you render." CreationDate="2018-02-01T17:09:17.603" UserId="2654" />
  <row Id="9479" PostId="6201" Score="0" Text="@demanze: If you want 2 presentable images, and the implementation supports 2 presentable images, then you can ask for 2 presentable images. Now it might give you more than that, but it will never give you *less*." CreationDate="2018-02-01T17:10:50.243" UserId="2654" />
  <row Id="9480" PostId="6201" Score="0" Text="No, I didn't mean that I was afraid there would be only 1. I'm actually scared one implementation could give me 3 or more. In my current prototype, I have an array of size 2 (for pointers to images/imageViews/framebuffers apart of the Swapchain). Now, I could put a maximum of 16 or 64 and keep a fixed sized array but I would rather have a way to have always 2 with the spec backing me up somewhere. :)" CreationDate="2018-02-01T17:16:29.903" UserId="8118" />
  <row Id="9481" PostId="6199" Score="0" Text="&quot;You copy out the values you know you need, issue a rendering command with that, and then return to the caller&quot; - here is specifying the the indices in an EBO what you mean when you &quot;copy out the values&quot;, and what do you mean by &quot;return to the caller&quot; - the caller being your application here?" CreationDate="2018-02-01T17:40:37.500" UserId="7000" />
  <row Id="9482" PostId="6199" Score="1" Text="&quot;*what you mean when you &quot;copy out the values&quot;*&quot; Vertex arrays contain the vertex data that you're going to render with. Copying out the values would be copying out the data that will be rendered. &quot;*what do you mean by &quot;return to the caller&quot;*&quot; The caller of `glDrawRangeElements`." CreationDate="2018-02-01T17:43:06.120" UserId="2654" />
  <row Id="9485" PostId="6191" Score="0" Text="I assume the normals help you estimate depth so that you get 3d points.  You don't need that if the surface is planar though.  Just identify the four corners of the rectangle that you want to map in the target image and use the homography." CreationDate="2018-02-02T02:41:47.837" UserId="8009" />
  <row Id="9487" PostId="6203" Score="1" Text="&quot;Multisample Anti-Aliasing method does is ..... &quot; we compute 2x, 4x, 8x or 16x times a similar value&quot;&#xA;I think you are describing SSAA, Super Sample AA, not MSAA.  MSAA will only run the pixel shader *up to* the minimum of the sample rate AND the number of objects crossing the pixel." CreationDate="2018-02-02T09:09:53.810" UserId="209" />
  <row Id="9489" PostId="6203" Score="0" Text="You are correct, I updated the answer." CreationDate="2018-02-02T09:42:24.417" UserId="110" />
  <row Id="9491" PostId="6205" Score="0" Text="What is the *valence* of a vertex, edge or face?" CreationDate="2018-02-02T20:44:30.863" UserId="1981" />
  <row Id="9492" PostId="6205" Score="0" Text="The valence is the number of neighbors of a vertex (= number of edges joining in that vertex) or face (= number of edges around it)." CreationDate="2018-02-02T22:22:00.857" UserId="8141" />
  <row Id="9493" PostId="6208" Score="0" Text="Looks good! However, I am wondering if we can use the available value of valence 4 (vertices = 'g' and faces = '274'), to encode equation 3 &amp; 4. Also, for [x3] we have only 1 face." CreationDate="2018-02-03T07:53:48.710" UserId="8141" />
  <row Id="9494" PostId="6208" Score="0" Text="Ah right I forgot that g is given in the end. Assumin g is known we have two equations in three variables.  We already included that the valence of g is 4 in equation (2) and the single face with valece x3 in equation (1). I still think one more piece of information is missing." CreationDate="2018-02-03T09:30:44.800" UserId="1981" />
  <row Id="9495" PostId="6208" Score="0" Text="Ahh, I think No, we have number of edges given in question, let's suppose &quot;q&quot; (see 3rd sentence of question). Still I am unsure how to get final value...&#xA;By the way, If 2*number of edges = Faces, then for valence 4 we have 137 (274/2) edges." CreationDate="2018-02-03T09:59:19.910" UserId="8141" />
  <row Id="9496" PostId="6208" Score="0" Text="Also, I think given that object is sphere and x3 valence has 1 face only, then x3 should be equal to 2, because valence is nothing but number of neighbors (and for 1 face it can have only 2)." CreationDate="2018-02-03T10:13:27.993" UserId="8141" />
  <row Id="9497" PostId="6208" Score="0" Text="A face with only two neighbours isn't really a face, a face needs at least 3 neighbours. And the information that we have a sphere tells us nothing but the genus." CreationDate="2018-02-03T11:20:20.060" UserId="1981" />
  <row Id="9498" PostId="6212" Score="0" Text="I know what you mean. But my question is how to modify the data. You said &quot; You should avoid making lots of small uploads of data like that.&quot; Of course, we should avoid, but it's just a simple example to describe my question in detail." CreationDate="2018-02-03T20:22:11.120" UserId="6806" />
  <row Id="9499" PostId="6212" Score="0" Text="@shashack: &quot;*But my question is how to modify the data.*&quot; But you already know how to modify the data. That's how you put it there in the first place." CreationDate="2018-02-03T20:23:46.333" UserId="2654" />
  <row Id="9500" PostId="6212" Score="0" Text="Ok what if the number of vertex size is 100000000 and I just need to modify just 1 vertex in the buffer data. Do I need to bind the buffer and upload data again? It would take so much time." CreationDate="2018-02-03T20:26:19.953" UserId="6806" />
  <row Id="9501" PostId="6212" Score="0" Text="@shashack: Then only upload what you need. Just like your two `glBufferSubData` calls each only uploaded part of the total buffer size. There's no limit on how much or ***how little*** data you can upload. I don't know how to make &quot;If you want to upload 12 bytes worth of data, you can.&quot; more clear without literally writing the code for you." CreationDate="2018-02-03T20:27:46.983" UserId="2654" />
  <row Id="9502" PostId="6212" Score="0" Text="Oh so you mean I can call glBufferSubData super many time? For example, If I have vertex 100000000 size, then  100000000 time?" CreationDate="2018-02-03T20:33:52.820" UserId="6806" />
  <row Id="9503" PostId="6212" Score="0" Text="@shashack: &quot;*I just need to modify just 1 vertex in the buffer data*&quot; &quot;*I have vertex 100000000 size, then 100000000 time?*&quot; Pick a scenario. Either you need to update one vertex, or you need to update *all of them*." CreationDate="2018-02-03T21:08:18.123" UserId="2654" />
  <row Id="9504" PostId="6212" Score="0" Text="I got it! Thank you for answering. I couldn't think to use glBuuferSubData so many. Good day!" CreationDate="2018-02-03T21:26:31.587" UserId="6806" />
  <row Id="9507" PostId="6215" Score="0" Text="Ok, that makes sense, I guess that if you know the amount of time its going to be idling you could also do some work there :)" CreationDate="2018-02-05T10:26:23.547" UserId="7107" />
  <row Id="9508" PostId="6215" Score="0" Text="@Nacho you can also put the fenceEvent in a list so you can use WaitForMulitpleObjects and also wait on (for example) Async File IO to complete on the same thread." CreationDate="2018-02-05T10:30:54.590" UserId="137" />
  <row Id="9509" PostId="6191" Score="0" Text="This would work only if there is a rectangle and it can be found, right? I need to find areas with no semantically significant content, such as empty walls, and insert a logo there. I couldn't succeed using corner detection-based methods or homography estimation using parallel lines. Therefore I though surface normals can be useful since there is a normal for each pixel." CreationDate="2018-02-05T13:03:01.603" UserId="8112" />
  <row Id="9510" PostId="6191" Score="0" Text="Normals are as useful as derivatives in Calculus.  Much like how you can integrate derivatives to get the original function (+/- a constant), you can do something similar to your normals to get the original surface (+/- a global translation). But knowing the geometry of the surface doesn't describe WHERE you want to put your image on that surface.  I feel like I still don't understand how you want to describe where the image goes.  But I also feel like you can bypass the problem if you use the homography." CreationDate="2018-02-05T14:18:40.407" UserId="8009" />
  <row Id="9511" PostId="6215" Score="0" Text="@Nacho The idea is that you shouldn't block on the fence until you have to, because the next thing you're going to do on the CPU depends on prior GPU commands having finished. For example, you might have a thread that composites frames generated from another thread, and the producer thread sends a fence along with other per-frame data, to keep the data and the frames in sync." CreationDate="2018-02-05T19:02:56.293" UserId="2041" />
  <row Id="9512" PostId="6224" Score="0" Text="I still struggle to understand, the incoming light is $L_i$,. Isn't $f_r$ the one that has low frequency information?" CreationDate="2018-02-06T15:47:15.377" UserId="228" />
  <row Id="9513" PostId="6224" Score="0" Text="Moreover... according to what you're saying, low frequency $f_r$ means that $f_r$ is a narrow band signal, let's assume it is very narrow... that means that $f_r$ is almost a constant, therefore it can be factorized. However in the approximation what happens is that you factor $L_i$, and moreover an extra factor $\frac{1}{N}$ comes out, which I still struggle to understand where it comes from." CreationDate="2018-02-06T15:53:01.127" UserId="228" />
  <row Id="9514" PostId="6100" Score="0" Text="Hi, what do you mean by &quot;distributed&quot;? could you elaborate that?" CreationDate="2018-02-06T15:57:15.587" UserId="228" />
  <row Id="9515" PostId="6226" Score="0" Text="I know I'm being annoying but I still have few unclear points... First $f_r$ is actually $f_r(l,v)$, the integral is computed over the hemisphere parametrized wrt $l$ (here $v$ is fixed). If $f_r(l,v)$ is low frequency (in signal processing terms) that means, I guess, that $$f_r(l,v) = \sum_{k=0}^{N-1} \alpha_k(v) \psi_k(l)$$, I'm assuming that $psi_k$ are spherical harmonics on the unit sphere. Is this interpretation correct? I know you said you don't know the derivation, but I'm trying to work out a bit of rigour for that formula." CreationDate="2018-02-06T17:18:38.590" UserId="228" />
  <row Id="9516" PostId="6226" Score="0" Text="Also you mention &quot;the two functions are not correlated&quot;, which functions? And why are you saying they're not correlated?" CreationDate="2018-02-06T17:28:28.660" UserId="228" />
  <row Id="9517" PostId="6226" Score="0" Text="The two functions are $f_r$ and $L_i$. I say they are not correlated because the brdf shouldn't depend on the environment" CreationDate="2018-02-06T17:34:10.573" UserId="5703" />
  <row Id="9518" PostId="6226" Score="0" Text="I'd go as far as to say that this formula cannot be found if not by eye or by some sort of probabilistic analysis. And i don't have a strong enough grasp on probability to produce a proper derivation. Keep in mind that it is often said by CG professionals that &quot;if it looks right, it is right&quot; so rigorous proofs are not always needed. That is, unless you are doing scientific research" CreationDate="2018-02-06T17:38:46.553" UserId="5703" />
  <row Id="9519" PostId="6221" Score="0" Text="Just a note: &quot;low-frequency&quot; here refers to angular frequency, ie how rapidly the function varies with input direction—nothing to do with the wavelength of light." CreationDate="2018-02-06T19:04:02.467" UserId="48" />
  <row Id="9520" PostId="6220" Score="0" Text="Circles don't pass through grid points nicely - with the exception of some [pythagorean triples](https://en.wikipedia.org/wiki/Pythagorean_triple).  So you're going to have to relax your requirements.  You'll have to adjust the topology of your surface to tessellate the quads of your grid with triangles that transition from the front to the side." CreationDate="2018-02-06T20:10:18.520" UserId="8009" />
  <row Id="9521" PostId="6229" Score="1" Text="The answer to this is a large list of things, but SSAO algorithms fall into this group. PCF shadow sampling as well (making the sampled shadow maps look nicer).  Purely ray based shadows can give nice soft edges if you take enough samples per pixel.  Another technique is to render a shadow map from a random spot on the light source every frame and let something like temporal anti aliasing integrate the correct result over time." CreationDate="2018-02-07T00:45:03.020" UserId="56" />
  <row Id="9522" PostId="6219" Score="1" Text="[You already posted this question](https://android.stackexchange.com/q/190621/12442) on [android.se]. Don't post the exact same question on multiple Stack Exchange sites. It leads to information being split up and it wastes the time of people who are trying to help." CreationDate="2018-02-07T09:57:39.677" UserId="2041" />
  <row Id="9523" PostId="6226" Score="0" Text="Hi Again, just a last question. From the examples you gave me I have the feeling we are actually trying to split the integral and approximate those using montecarlo technique. The importance sampling is applied to the brdf one, while standard montecarlo integration seems to be applied for the $L_i$ term. Is this correct?" CreationDate="2018-02-07T12:12:54.760" UserId="228" />
  <row Id="9524" PostId="6231" Score="0" Text="can you specify, what the individual symbols ($\Theta, \Psi, ...$) represent? the symbols in BRDFs differ from book to book and I for one have learned with RealTimeRendering, Physically Based Rendering and Digital Modeling of Material Appearance, not your source. And even now, I can't recall how they did it individually from the top of my head, so I have a hard time grasping what is done in your equations." CreationDate="2018-02-07T13:33:42.107" UserId="7008" />
  <row Id="9525" PostId="6226" Score="0" Text="Any numerical integration techniques can be applied to either. The point is that they should approximate the integral. You can even do importance sampling with different importance distributions for each integral" CreationDate="2018-02-07T15:11:18.943" UserId="5703" />
  <row Id="9526" PostId="6226" Score="0" Text="But since L_i will be precomputed from an image, it would be reasonable to use all data without importance sampling" CreationDate="2018-02-07T15:12:54.513" UserId="5703" />
  <row Id="9527" PostId="6231" Score="0" Text="@Tare I hope is clearer now" CreationDate="2018-02-07T16:06:45.253" UserId="228" />
  <row Id="9530" PostId="6234" Score="0" Text="what about `glBlitFramebuffer`? Can't I copy default framebuffer's images or depth to another framebuffer with it?" CreationDate="2018-02-07T18:03:14.550" UserId="6929" />
  <row Id="9531" PostId="6234" Score="1" Text="@recp: Yes, you could. Or you could just render to your own image and *not* have to do a copy. Why pick the slower option?" CreationDate="2018-02-07T20:15:28.880" UserId="2654" />
  <row Id="9532" PostId="6234" Score="0" Text="&quot;Why pick the slower option?&quot; maybe this is what I was looking for. I don't know which one is faster so I tried to save texture space. As a solution I will render opaque objects to another framebuff and share its depth buffer with transparency framebuff, this seems better way and fits with your answer I guess" CreationDate="2018-02-07T20:18:20.410" UserId="6929" />
  <row Id="9533" PostId="6235" Score="0" Text="This is very interesting indeed, but it seems like it would suffer a lot for varying geometry scenes" CreationDate="2018-02-07T21:35:18.593" UserId="7462" />
  <row Id="9534" PostId="6235" Score="0" Text="It depends—as long as the transformations you make on individual objects don’t involve deformation (i.e. moving a thing around as opposed to bending it), you can keep using the original volume texture for each object and just apply an appropriate inverse transformation when you’re sampling it. But yes, this approach isn’t as helpful for deformable geometry like animated characters." CreationDate="2018-02-07T21:39:34.217" UserId="506" />
  <row Id="9535" PostId="6235" Score="0" Text="That's not the only thing I am thinking of, imagine objects appearing and disapearing from your scene (projectiles for example), this seems like it would have a problematic scaling time for something like that" CreationDate="2018-02-07T21:47:34.503" UserId="7462" />
  <row Id="9536" PostId="6235" Score="1" Text="You could replace the SDF with a cube map. Nvidia's GPU Gems has a chapter about this. https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch17.html This would result in a lower quality but it is easy to generate and a lot cheaper than SDF to generate, especially when things are animated." CreationDate="2018-02-08T07:23:51.670" UserId="4908" />
  <row Id="9537" PostId="6229" Score="1" Text="Some minor comments: *Shadow maps*: The main disadvantage is not so much numerical limitations but sampling rate. It's hard to get this above the required Nyquist rate for the final image (without excessively sampling other areas), leading to frequent aliasing artefacts.&#xA;*Shadow Volumes*: I don't understand the &quot;finding the last object&quot; problem.  One problem that sometimes crops up is keeping the volume closed when clipping against the front clip plane, but there are several ways to avoid this problem." CreationDate="2018-02-08T16:01:45.160" UserId="209" />
  <row Id="9539" PostId="6241" Score="1" Text="I don't quite understand. What's the problem, or what are you trying to achieve?" CreationDate="2018-02-09T08:29:49.473" UserId="2041" />
  <row Id="9540" PostId="6243" Score="0" Text="What's the actual question? If your aim is to have a discussion, SE sites don't really work for that. (See [help/dont-ask] for more.)" CreationDate="2018-02-09T13:38:09.740" UserId="2041" />
  <row Id="9541" PostId="6239" Score="0" Text="You forgot the foreshortening term in the equation." CreationDate="2018-02-09T13:38:35.383" UserId="228" />
  <row Id="9542" PostId="6243" Score="0" Text="But http://codereview.stackexchange.com is a part of SE." CreationDate="2018-02-09T13:43:41.223" UserId="5430" />
  <row Id="9543" PostId="6243" Score="0" Text="It is sad, that there is no ideareview SE." CreationDate="2018-02-09T13:45:56.237" UserId="5430" />
  <row Id="9544" PostId="6239" Score="0" Text="Yes but it seems like you managed to understand it so it's not a big deal" CreationDate="2018-02-09T14:11:37.297" UserId="5703" />
  <row Id="9545" PostId="6239" Score="0" Text="I like your reply XD. I see what you mean anyway." CreationDate="2018-02-09T14:26:48.360" UserId="228" />
  <row Id="9546" PostId="6242" Score="0" Text="It would be much preferable you'd ask for an actual *algorithm* to do that, as this is more what this site is for, rather than advice for what software to use. The general problem is interesting, but we don't really do software recommendation here." CreationDate="2018-02-09T15:41:19.810" UserId="6" />
  <row Id="9547" PostId="6241" Score="0" Text="when I am using phone to watch a video it plays in full screen (1920x1080), but when I tilt it(rotate) the video will be played only in part of the screen, the rest of the  areas of screen is black. I wanted to know whether the resolution of image(video) is changed to achieve this! If so how does this happen. Think in terms of aspect ratio or some thing like that" CreationDate="2018-02-09T16:08:25.027" UserId="8153" />
  <row Id="9548" PostId="6233" Score="0" Text="@joojaa - I saw that question. But the answers still don't answer how or why exactly the axes coincide when they are supposed to remain perpendicular. Or if is it related to non-ortho axes mentioned in the OP. Why are we using a non-ortho frame." CreationDate="2018-02-09T16:50:39.503" UserId="6046" />
  <row Id="9550" PostId="6233" Score="0" Text="@joojaa - that's what I want to see with the coordinate frame. Show me a situation where I end up with a rotation repeating another rotation? For this to happen in a physical gimbal the rings must coincide, hence for this to happen with a coordinate frame the axes must coincide. If the axes aren't coinciding as you say, then the gimbal lock never happens as all 3 axes point in different directions maintaining the 3 degrees of freedom." CreationDate="2018-02-09T17:20:07.667" UserId="6046" />
  <row Id="9553" PostId="6242" Score="0" Text="@ChristianRau thanks for your note! I've edited the question." CreationDate="2018-02-09T18:17:01.890" UserId="2750" />
  <row Id="9555" PostId="6245" Score="0" Text="Thanks for this. I was actually working with some images converted to numpy matrices which are uint8 and it automatically uses modulo when you exceed the unsigned byte length so that's why I thought maybe modulo was used in graphics somewhere. Can I ask for clarification on the clamping? So is it common to add/multiply images together? If so, in my samples working with grayscale images it would seem like the final image would just instantly oversaturate and &quot;white-out&quot; so to speak. Is this normal and do we still just clamp the values?" CreationDate="2018-02-10T04:36:49.960" UserId="8180" />
  <row Id="9556" PostId="6233" Score="0" Text="@joojaa - Every time some one asks this question people start giving examples and pictures of &quot;physical gimbal&quot;. I'm asking you to restrict yourself to a coordinate frame even though I know that the concept originated from the physical gimbal. I can understand what's happening in the physical gimbal but not in the coordinate frame. In the picture it says rotation direction is lost but &quot;HOW&quot; is it happening when the 3 coordinate axes don't coincide according to you?" CreationDate="2018-02-10T10:42:34.173" UserId="6046" />
  <row Id="9557" PostId="6233" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/72953/discussion-between-joojaa-and-wandering-warrior)." CreationDate="2018-02-10T10:52:00.943" UserId="38" />
  <row Id="9559" PostId="6247" Score="0" Text="Ok wait. your post was helpful but I think I finally understand. Will post my own answer shortly." CreationDate="2018-02-10T14:01:25.413" UserId="6046" />
  <row Id="9560" PostId="6246" Score="0" Text="Thanks! Normalization into 0-1 floating point values makes sense to me because then multiplying won't cause the threshold to be exceeded. What would you do in the case of adding/subtracting images together? Would you clamp the values between 0-1 after the arithmetic?" CreationDate="2018-02-10T15:41:30.773" UserId="8180" />
  <row Id="9561" PostId="6246" Score="0" Text="Yes, once you've completed all the operations you need to on the image, only then do you clamp it. So if you're adding 2 images, then blurring the result, or scaling it, or whatever, you do all of those things at the higher bit depth and then clamp the final output (if needed)." CreationDate="2018-02-10T15:43:40.317" UserId="3003" />
  <row Id="9562" PostId="6242" Score="0" Text="Also note that there is a [Software Recommendation Stack Exchange](https://softwarerecs.stackexchange.com). Perhaps your original question would be appropriate there? (I'd check the FAQ first to be sure.)" CreationDate="2018-02-10T15:46:04.730" UserId="3003" />
  <row Id="9563" PostId="6246" Score="0" Text="I guess this doesn't work for grayscale, right? Let's say I have a standard 8-bit RGB image, 8-bits for each channel.&#xA;&#xA;Image 1 is a solid red color image where every pixel is [200, 0, 0].&#xA;Image 2 is a solid dark red color image where every pixel is [100, 0, 0].&#xA;&#xA;When I up-convert to a higher bit depth, do I literally just use 16-bits for each channel, so my final image would be an image where every pixel is now [300, 0, 0]? Now my choices are (A) Leave it at this 16-bit per channel depth or (B) Convert back to 8-bit in which case the image will clamp back to [255, 0, 0]. Is that right?" CreationDate="2018-02-10T15:53:01.867" UserId="8180" />
  <row Id="9564" PostId="6246" Score="0" Text="This absolutely works for grayscale. It also works for other color spaces like HSV, YCbCr, La*b*, etc. And yes, you have the right idea about what I'm suggesting." CreationDate="2018-02-10T15:59:25.987" UserId="3003" />
  <row Id="9565" PostId="6219" Score="0" Text="I'm voting to close this question as off-topic because it is about the inner working of a phone unrelated to the topic of computer graphics." CreationDate="2018-02-10T17:11:45.653" UserId="6" />
  <row Id="9566" PostId="6245" Score="0" Text="Yes, it's pretty common, though we usually think of images as representing fractional values in [0, 1] rather than integers 0 to 255. Alpha blending, for instance, involves adding and multiplying. Depending on the images and the operations you're doing, saturating to white may indeed happen, but in other cases (like alpha blending layers of images on top of each other) it's not really an issue. Also, if you work with HDR images, you can leave them unclamped while you do a bunch of operations on them, then apply a [tone mapping](https://en.wikipedia.org/wiki/Tone_mapping) function at the end." CreationDate="2018-02-10T18:56:30.607" UserId="48" />
  <row Id="9574" PostId="6257" Score="0" Text="Super great!! Thanks a lot. I didn't know there are different pointer setting function such as glVertexAttribLPointer." CreationDate="2018-02-12T02:09:22.343" UserId="6806" />
  <row Id="9575" PostId="6192" Score="0" Text="Would backface culling be a solution to this?" CreationDate="2018-02-12T06:02:51.487" UserId="3073" />
  <row Id="9579" PostId="6263" Score="3" Text="Don't post images instead of formulae or text. They're not accessible to people who use screen-readers, and it means nobody can find your question in search." CreationDate="2018-02-12T23:00:53.263" UserId="2041" />
  <row Id="9580" PostId="6265" Score="0" Text="I don't think I've misunderstood the meaning of $D$. According to the original paper written by torrance, the microfacets distribution $D$ quantify the amount of microfacets that would reflect the the light coming from a direction into another. Isn't that what it is?" CreationDate="2018-02-13T09:34:26.557" UserId="228" />
  <row Id="9581" PostId="6266" Score="0" Text="So literally the roughness map is the distribution parameters per point. Is that correct?" CreationDate="2018-02-13T09:35:15.517" UserId="228" />
  <row Id="9582" PostId="6265" Score="0" Text="Not as in total numbers, but essentially, yes. Your wording/question structure made me assume you got it wrong." CreationDate="2018-02-13T09:43:02.467" UserId="7008" />
  <row Id="9583" PostId="6265" Score="1" Text="Not as in total, but in percentage yes (it's a distribution function, in sense of probability but built upon the number of microfacets facing the halfway vector between incident light and reflection direction." CreationDate="2018-02-13T09:48:21.610" UserId="228" />
  <row Id="9584" PostId="6263" Score="0" Text="thanks @DanHulme . i Did not knew that. But tell me my answer if you can see it." CreationDate="2018-02-13T13:27:17.837" UserId="8202" />
  <row Id="9585" PostId="6269" Score="0" Text="i understand,but my idea is: 110011 + hash code = 00ii00 = 0110, i is complex number and converted to 1,1 is converted to 0 and 00 is 0." CreationDate="2018-02-13T16:03:53.910" UserId="8204" />
  <row Id="9586" PostId="6269" Score="0" Text="so 01101110 will become 101001,let's decode: original zero is always invert to 1,1 is always converted to zero,two zero after converted is converted to only 0." CreationDate="2018-02-13T16:16:26.610" UserId="8204" />
  <row Id="9587" PostId="6266" Score="0" Text="That is correct." CreationDate="2018-02-13T16:20:47.560" UserId="5703" />
  <row Id="9588" PostId="6269" Score="2" Text="I have no idea what you're talking about. I don't think you're describing a hash code." CreationDate="2018-02-13T18:58:18.497" UserId="2041" />
  <row Id="9589" PostId="6267" Score="0" Text="I'm not sure how can I derive this projection matrix. Can you explain it more clearly?" CreationDate="2018-02-13T22:17:12.740" UserId="8191" />
  <row Id="9590" PostId="6273" Score="0" Text="I don't quite understand how the last object information is helpful for standard shadow mapping. Isn't any element behind &quot;first one&quot; along direction from light source occluded? Can you explain what are you trying to achieve? Are you up to some kind of  transparency? If so then &quot;depth peeling&quot; might be the keyword." CreationDate="2018-02-13T23:33:54.620" UserId="4958" />
  <row Id="9591" PostId="6273" Score="0" Text="Can you clarify what's meant by &quot;first&quot; and &quot;closest&quot; here? Are you talking about closest distance to the light source, closest distance to the camera, or something else? A diagram might help. I'm having a hard time understanding the question." CreationDate="2018-02-13T23:34:48.683" UserId="48" />
  <row Id="9592" PostId="6269" Score="0" Text="then i quantized the code with my own algorithm,i have test this several times with different code length and it can recreate 99% of code. 101001 = 001.after recreated = 01101110,this is normalized and it works for all different binary code." CreationDate="2018-02-14T00:10:12.183" UserId="8204" />
  <row Id="9593" PostId="6269" Score="0" Text="i even still can use hash code,but i use real time hash with only hash the high logic value in a binary code." CreationDate="2018-02-14T00:13:01.460" UserId="8204" />
  <row Id="9594" PostId="6273" Score="0" Text="@narthex go outside on a sunny day and look at something casting a shadow, a tree, a car, a sign. Put your hand on the shadow and lift it up a couple centimeters, you will notice that there are 2 shadows now, one is your hand one is the bigger shadow of the object." CreationDate="2018-02-14T00:32:42.787" UserId="7462" />
  <row Id="9595" PostId="6273" Score="2" Text="The latter shadow is caused by your hand blocking light from the rest of the sky, not from it extra-blocking light that the first object already blocked. The usual way to approximate that is with ambient occlusion, but I’d be interested to hear your approach." CreationDate="2018-02-14T00:44:09.030" UserId="506" />
  <row Id="9596" PostId="6273" Score="0" Text="It's not light from the rest of the sky, as you have noticed when you go outside, shadows are not balck, this is because air refracts light so at all points in space there is residual  light rays bouncing on the air molecules, what we commonly know as &quot;ambient ligth&quot;. When your hand is close to the ground it blocks a little more of these residual light rays, thus generating a second, darker, shadow. Taking this into consideration, if you know how far away you are from the object that last occluded the light from you, you can calculated a shadow intensity modifier." CreationDate="2018-02-14T00:54:11.550" UserId="7462" />
  <row Id="9597" PostId="6273" Score="0" Text="If what you're after is simply the inversion of order, why not simply invert the depth test?" CreationDate="2018-02-14T08:38:01.440" UserId="2817" />
  <row Id="9598" PostId="6217" Score="0" Text="Perhaps if you posted an image from a different game where you feel there's a discontinuity it might make things clearer. Also, it's a bit difficult to tell from a single frame." CreationDate="2018-02-14T09:08:25.027" UserId="209" />
  <row Id="9599" PostId="6268" Score="0" Text="Are you implying that everyone keeps a database of images on their computer that's indexed by the &quot;universal hash&quot;?" CreationDate="2018-02-14T09:09:21.453" UserId="209" />
  <row Id="9600" PostId="6269" Score="1" Text="@Lan... it sounds like you have no idea how compression works. Do some research to understand the pigeon hole issue." CreationDate="2018-02-14T09:41:24.667" UserId="137" />
  <row Id="9601" PostId="6273" Score="0" Text="Very interesting. I actually have been observing shadows like you said but to spot a phenomenon know as variable penumbra size - the longer distance from occluder to receiver, the bigger penumbra. You probably mean some Ambient Occlusion or Sky Occlusion." CreationDate="2018-02-14T09:58:42.230" UserId="4958" />
  <row Id="9602" PostId="6273" Score="0" Text="At some point, when distance is big, the shadow dissapears into blurriness. But it depends on light angular size." CreationDate="2018-02-14T10:07:24.417" UserId="4958" />
  <row Id="9603" PostId="6267" Score="0" Text="I have added the complete derivation of a simpler version, plus the explanation on the differences to the matrix you wanted to achieve. hope that helps." CreationDate="2018-02-14T10:24:33.613" UserId="7008" />
  <row Id="9605" PostId="6274" Score="0" Text="By crush z, I mean making them 0 to make points become 2d" CreationDate="2018-02-14T13:45:55.453" UserId="8191" />
  <row Id="9606" PostId="6267" Score="0" Text="Thank you very much, I understand more thoroughly now. 1 question though, so the third row, mapping [-1, 1], aren't there any matrices multiplication that can get to that row? What I mean is a combination of scaling, translating, shearing, etc" CreationDate="2018-02-14T15:32:27.753" UserId="8191" />
  <row Id="9607" PostId="6226" Score="0" Text="Related question, just asked, https://dsp.stackexchange.com/questions/47180/how-to-model-a-generic-narrow-band-signal" CreationDate="2018-02-14T17:11:43.767" UserId="228" />
  <row Id="9608" PostId="6278" Score="0" Text="What does it mean to &quot;remesh&quot; a mesh?" CreationDate="2018-02-15T01:40:17.160" UserId="2654" />
  <row Id="9609" PostId="6274" Score="0" Text="@ManhNguyenHuu: But the points *never* &quot;become 2d&quot;. That's the point. They're *always* three-dimensional positions, even in window-space." CreationDate="2018-02-15T01:49:01.053" UserId="2654" />
  <row Id="9610" PostId="6271" Score="0" Text="&quot;*So why do we need homogenous divide?*&quot; Perhaps the questions you need to ask yourself are 1) what are the alternatives? 2) are they more efficient or more amenable to hardware implementation?" CreationDate="2018-02-15T01:54:52.107" UserId="2654" />
  <row Id="9611" PostId="6278" Score="0" Text="Typically those remeshers voxelize a mesh and then use algorithms like dual contouring to recreate a polygonal mesh. @NicolBolas It's usually a way to clean up the topology in modeling. For example, if you use boolean/CSG operations to create a mesh, the resulting topology might be nasty and not work so well for subdivision surfaces and might not be so suitable for sculpting (the control points might be unevenly distributed). &quot;Remeshing&quot; would keep the overall shape but give you a very uniform topology that can be easier to work with for some cases." CreationDate="2018-02-15T04:43:53.073" UserId="2247" />
  <row Id="9612" PostId="6239" Score="0" Text="If this answer helped you, it would be great if you upvoted it or selected it as correct." CreationDate="2018-02-15T05:26:21.993" UserId="5703" />
  <row Id="9613" PostId="6267" Score="0" Text="Although I guess it's possible, there is no matrix multiplication that I know of that does this. I also don't see the reason why you would want to add another multiplication, that achieves nothing else than the single matrix but adds an overhead of calculations?" CreationDate="2018-02-15T08:04:16.830" UserId="7008" />
  <row Id="9615" PostId="6283" Score="0" Text="I already thought about that solution but I wanted to do stick everything inside the shader. I'll keep this as a backup solution, I guess :)" CreationDate="2018-02-15T17:30:37.923" UserId="2372" />
  <row Id="9616" PostId="6283" Score="2" Text="@MaT: If you want everything inside the shader, then presumably the frequency is also computed within the shader as some function `float frequency = f(_Time)`. Then you should do some calculus by hand to figure out $g(t) = \int_0^t f(\tau)\,\mathrm d\tau$ and implement that in the shader as well. Then you can do `float s = sin(g(_Time))`." CreationDate="2018-02-16T03:15:16.187" UserId="106" />
  <row Id="9617" PostId="6278" Score="0" Text="Thanks @TeamUpvote. In this case I am not looking for uniform topology (necessarily), as I just want the meshes for machine learning purposes." CreationDate="2018-02-16T20:43:52.527" UserId="7622" />
  <row Id="9618" PostId="6268" Score="0" Text="I also went through a stage of thinking up lots of brilliant ideas to use hashes to get around limits on compression. It took a long time to realise why they definitely cannot work. I recommend reading about [data compression](https://en.wikipedia.org/wiki/Data_compression)." CreationDate="2018-02-16T21:41:34.287" UserId="231" />
  <row Id="9619" PostId="6278" Score="0" Text="It's a bit tricky -- mainly the fact that the input mesh is quad-based shouldn't matter so much, at least I can't think of remeshing algorithms that would care, so to speak -- only thing important typically is the output requirements. Voxelization followed by an algorithm to turn those voxels back into a polygonal mesh tends to be one of the most straightforward ways to do it with lots of control over the resolution of the results. Is Blender's remesh inadequate in some way? I think it's using the method I proposed: https://docs.blender.org/manual/en/dev/modeling/modifiers/generate/remesh.html" CreationDate="2018-02-17T05:21:19.370" UserId="2247" />
  <row Id="9620" PostId="6273" Score="2" Text="&quot;Ambient light&quot; *is* light from the rest of the sky, or the rest of the environment in general if you're indoors. Anyway, you should definitely look into depth peeling (from the perspective of the light source), as narthex suggested in the very first comment." CreationDate="2018-02-17T05:32:57.970" UserId="106" />
  <row Id="9621" PostId="6278" Score="0" Text="When I used that tool, either it would lose the quality of the mesh (shape was distorted). To avoid that it lets you increase the poly count, but to keep the shape the same you need to blow up the poly count by 10x or more." CreationDate="2018-02-17T09:05:36.267" UserId="7622" />
  <row Id="9622" PostId="6287" Score="0" Text="Even if not using classic rasterization on top of the raytraced image, there's still some nice convenience in having the resulting imagine in an FBO or a texture, provided you don't *just* want to write it out into an image file you have to display it at *some* point. Besides that, there's also nice convenience things an inherently graphical task might still profit from, like builtin matrix types or builtin access to the hardware's advanced texture filtering functionality." CreationDate="2018-02-17T16:26:14.133" UserId="6" />
  <row Id="9623" PostId="6287" Score="0" Text="I don't have an Nvidia card but AMD one. Thanks for the links helps a bit. But I think compute shader would be better, since I'd have to use OpenGL anyways to output the result to a window. Keeping this question open a little more in case anybody has some more interesting details or first hand experience." CreationDate="2018-02-18T11:04:05.743" UserId="6046" />
  <row Id="9624" PostId="6290" Score="0" Text="I dont think that's the case. From what I read in 3D Math Primer and here I quote it: &quot;If we interpret the rows of a matrix as the basis vectors of a coordinate space, then multiplication by the matrix performs a coordinate space transformation. If aM=b, we say that M transformed a to b.&quot;, then the way I was building the matrix was correct. 3D Math Primer use row matrices." CreationDate="2018-02-18T18:24:39.197" UserId="8191" />
  <row Id="9625" PostId="6291" Score="0" Text="Can you post an image showing what you mean?" CreationDate="2018-02-18T18:52:39.997" UserId="3003" />
  <row Id="9626" PostId="6291" Score="0" Text="@user1118321 sure, I updated question and added images" CreationDate="2018-02-18T19:18:35.753" UserId="6929" />
  <row Id="9627" PostId="6291" Score="0" Text="@recp: Additive blending is not transparency. The math simply doesn't work out that way." CreationDate="2018-02-18T20:42:37.693" UserId="2654" />
  <row Id="9628" PostId="6291" Score="0" Text="@NicolBolas thanks for your comments; yep additive blending itself is not transparency but weighted, blended OIT algorithm requires additive blending ONE, ONE. I'm using GL_ONE_MINUS_SRC_ALPHA/GL_SRC_ALPHA in compositing pass" CreationDate="2018-02-18T20:47:36.217" UserId="6929" />
  <row Id="9629" PostId="6291" Score="0" Text="I want to disable additive blending ( or additive blending LOOK ) in shader if alpha is 1. I want to do this for specific fragments/pixels, this is why I want to do it in shader ( or with helper framebuffs / buffs ... )" CreationDate="2018-02-18T20:50:31.607" UserId="6929" />
  <row Id="9630" PostId="6291" Score="0" Text="@recp: My overall point is that you're doing the wrong thing. It's as wrong as taking 2 seconds and multiplying it by 3 seconds and saying that you have a velocity of 6 m/s. You're not getting the right look because you're using the math incorrectly. Additive blending has a meaning, and if it's not doing what you want, then you're not using it correctly. Simply having an alpha of 1 mean &quot;opaque&quot; won't help you. After all, an alpha of 0.99 won't be &quot;nearly opaque&quot;, so it's going to look wrong if it suddenly jumps to &quot;opaque&quot;." CreationDate="2018-02-18T23:07:44.283" UserId="2654" />
  <row Id="9631" PostId="6292" Score="0" Text="The OP said they are using [weighted, blended OIT](http://jcgt.org/published/0002/02/09/), which does use `glBlendFunci(0, GL_ONE,  GL_ONE)` in a transparency accumulation pass." CreationDate="2018-02-19T05:29:17.703" UserId="106" />
  <row Id="9632" PostId="6290" Score="1" Text="@ManhNguyenHuu - Nathan is correct. Your quote doesn't imply anything about from which space does &quot;a' transforms to. IF you construct a matrix with the rows or columns as the basis vectors and multiply it with the vector of that space it will give you the world space representation of that vector" CreationDate="2018-02-19T08:36:08.823" UserId="6046" />
  <row Id="9633" PostId="6292" Score="0" Text="That is correct, plus the additive blending blends (RGB * A, A) * Weight. I don't know how dual source blending works I'll investigate it, it seems interesting! But on the other hand I need to two different render targets: accumulation (vec4: RGBA16F) and revelage (float)" CreationDate="2018-02-19T11:34:11.413" UserId="6929" />
  <row Id="9634" PostId="6292" Score="0" Text="Maybe I can use three render targets (the first two for dual-source blending and last one for revelage), in the first pass dual-source is disabled and in other pass it is enabled (full screen quad), then compositing all passes? This would add extra pass only for dual-source blending but if it will fix the artifact I will implement it" CreationDate="2018-02-19T11:41:48.593" UserId="6929" />
  <row Id="9635" PostId="6293" Score="0" Text="Thank you, I understand much more thoroughly now." CreationDate="2018-02-19T15:47:46.880" UserId="8191" />
  <row Id="9636" PostId="6298" Score="0" Text="Thanks. I asked a purer version of this question at [math stackexchange](https://math.stackexchange.com/questions/2657669/radius-of-circle-tangent-to-sphere-which-obscures-that-sphere/2657673#2657673) and got an equivalent answer which gives another hint at how to solve this." CreationDate="2018-02-19T22:40:40.187" UserId="8233" />
  <row Id="9637" PostId="6300" Score="1" Text="I have it on good authority that for OpenGL on macOS, the limit is 32 virtual screens, fwiw. Not sure if the limit holds for Metal." CreationDate="2018-02-20T03:53:29.420" UserId="3003" />
  <row Id="9638" PostId="6302" Score="0" Text="lol this is what happens when you stop programming for a couple of years :-) thanks for idiot-checking me" CreationDate="2018-02-20T08:16:54.957" UserId="8233" />
  <row Id="9640" PostId="6303" Score="2" Text="If you need the reflections off wavy water to be that realistic, you should use ray-tracing. Any rasterization technique is going to involve *some* trade-offs." CreationDate="2018-02-20T16:12:51.840" UserId="2041" />
  <row Id="9641" PostId="6303" Score="0" Text="Check new [McGuire's paper](http://research.nvidia.com/publication/real-time-global-illumination-using-precomputed-light-field-probes) which introduces real time raytracing using grid of cubemaps. For dynamic objects, voxel cone tracing might be good because voxelization in real time is quite gpu friendly." CreationDate="2018-02-20T16:39:10.993" UserId="4958" />
  <row Id="9642" PostId="6303" Score="0" Text="You can also combine both SSR and VCT when SSR fails. The downside of voxels is finite precision measured in voxel size." CreationDate="2018-02-20T16:54:51.763" UserId="4958" />
  <row Id="9643" PostId="6300" Score="0" Text="Yeah, I couldn't find anything about documented limits either, I'm more concerned about what the practical limitations are, like if the driver or OS get buggy beyond a certain number of cards. The miners seem to be saying that windows used to hit a wall at 8 GPUs but the creators update fixed that, now it sounds like some drivers might have that same limit so they're saying you can use 8 AMD cards + 8 Nvidia cards. No word on if you can run monitors off all those cards though. The PCIe bandwidth definitely seems like it would become an issue, but I suspect that would be application dependent." CreationDate="2018-02-20T17:01:29.560" UserId="7385" />
  <row Id="9644" PostId="6300" Score="0" Text="The miners also seem to be saying that Linux has fewer issues working with a large number of GPUs, so that might be an option as well, still I haven't seen proof of any systems outputting from more than 4 GPUs simultaneously." CreationDate="2018-02-20T17:04:27.373" UserId="7385" />
  <row Id="9645" PostId="6305" Score="0" Text="Just to make sure you don't do work you don't need to do: you are aware that OpenGL and DirectX do clipping on their own, right?&#xA;&#xA;One thing you also may consider is at which axis you are looking. Some rendering APIs look along negative z when in camera space, so if you clip depending on positive z values but look along the negative z axis, then there would be an error in your case.&#xA;&#xA;For the original question: there are algorithms for clipping, that calculate which parts of triangles you need to draw. It has been some time though, so I'd need to look up even the names." CreationDate="2018-02-21T07:07:52.393" UserId="7008" />
  <row Id="9646" PostId="6305" Score="0" Text="Upon a quick search, the two algorithms we looked at in university where [Cohen-Sutherland][1] and [Cyrus-Beck][2]. They are only for line clipping thought - Polygons need their own algorithms. I suggest looking into the [Wikipedia algorithm list][3] to get a feeling for what you have to consider when trying to do clipping yourself.&#xA;&#xA;  [1]: https://en.wikipedia.org/wiki/Cohen%E2%80%93Sutherland_algorithm&#xA;  [2]: https://en.wikipedia.org/wiki/Cyrus%E2%80%93Beck_algorithm&#xA;  [3]: https://en.wikipedia.org/wiki/Clipping_(computer_graphics)#Algorithms" CreationDate="2018-02-21T07:13:40.700" UserId="7008" />
  <row Id="9649" PostId="6303" Score="0" Text="Thank you for the comments. UE4's planar Reflections looks very realistic. Is there any article about how Planar Reflection works? https://docs.unrealengine.com/latest/INT/Engine/Rendering/LightingAndShadows/PlanarReflections/index.html" CreationDate="2018-02-21T14:56:02.000" UserId="8235" />
  <row Id="9650" PostId="6275" Score="0" Text="+1 Great question (most students have these same questions and doubts, but just accept the homogeneous coordinates solution without a deeper reflection about it)... and great answer! Indeed, I think that this answer would be a good roadmap for anyone trying to explain/teach the need for homogeneous coordinates in a CG course." CreationDate="2018-02-21T15:03:02.447" UserId="5681" />
  <row Id="9651" PostId="6305" Score="0" Text="@Tare Im not using openGL or directX actually. Im writing a simple ray tracer and there is preciously little info to start" CreationDate="2018-02-21T16:38:05.843" UserId="8191" />
  <row Id="9652" PostId="6312" Score="0" Text="To clarify: When I mentioned “cross-program”, I was referring to for example an executable calling into a third-party library from the same thread, rather than anything like cross-process interop. But it may fit the same ballpark regardless." CreationDate="2018-02-21T17:10:22.483" UserId="7215" />
  <row Id="9653" PostId="6312" Score="0" Text="@haasn: In that case, the client third-party library and the calling layer should both agree on such things. That is, as part of the client library's documentation, it says &quot;you must have enabled extensions/features X, Y, Z&quot; or it has a way to inform you of what extensions/features are enabled." CreationDate="2018-02-21T17:38:49.297" UserId="2654" />
  <row Id="9654" PostId="6311" Score="0" Text="Thank you for your lenghty and detailed response. However the sun still looks unrealistic ( I attached an image in the description above). Is this a tone mapping issue? I want my sun to look more like a glow." CreationDate="2018-02-21T17:49:20.640" UserId="8242" />
  <row Id="9655" PostId="6311" Score="0" Text="It sounds like you want to be implementing a bloom filter?" CreationDate="2018-02-21T17:50:10.393" UserId="7215" />
  <row Id="9659" PostId="6307" Score="1" Text="You probably also want to look into a bloom effect to simulate the glare of the sun." CreationDate="2018-02-21T18:28:56.290" UserId="48" />
  <row Id="9660" PostId="6307" Score="0" Text="@NathanReed She probably mean radial blur light shafts." CreationDate="2018-02-21T18:40:58.213" UserId="4958" />
  <row Id="9663" PostId="4513" Score="0" Text="Related: https://stackoverflow.com/questions/726379/multiple-viewports-in-opengl/36048277#36048277" CreationDate="2018-02-21T22:50:19.510" UserId="5388" />
  <row Id="9664" PostId="6305" Score="0" Text="Just define 6 3D-planes for the frustum. Near clipping and far clipping planes should have normal aligned with z-axis in camera space. The other four planes should be defined according to the field-of-view angle that you defined. All plane normals should point to the inside of the clipping volume. Then you have to calculate polygon intersection with each plane and clip accordingly." CreationDate="2018-02-21T23:40:37.523" UserId="7957" />
  <row Id="9665" PostId="6303" Score="1" Text="Have you considered using the so-called planar reflections using an extra camera from the other side of the reflective plane? Usually warping the uv maps of the projected image acording to normals is enough to fake the bumpiness convincingly" CreationDate="2018-02-22T01:35:42.217" UserId="5703" />
  <row Id="9670" PostId="6316" Score="3" Text="This doesn't account for multiple layers of transparent geometry. What happens if i'm looking through a window at another transparent object?" CreationDate="2018-02-22T07:57:48.010" UserId="3073" />
  <row Id="9673" PostId="6313" Score="4" Text="*&quot; in early 3d (2000 - 2004)&quot;*  Hah! You youngsters." CreationDate="2018-02-22T09:01:18.807" UserId="209" />
  <row Id="9674" PostId="6275" Score="1" Text="Good answer. I'd add to the last part, even in the case of a simple projection transform with no other matrix multiplies the Z component doesn't carry over untouched - it ends up in the form A*Z + B, used for near and far clipping and perspective-correct interpolation. So even in the simplest case you still need to store the original Z value somewhere." CreationDate="2018-02-22T09:43:23.457" UserId="1937" />
  <row Id="9676" PostId="6314" Score="0" Text="What steps have you taken so far to try to narrow down the cause of the problem?" CreationDate="2018-02-22T12:20:57.837" UserId="182" />
  <row Id="9680" PostId="6314" Score="0" Text="I've reviewed over the way that you calculate the t value for a cube, over and over again, that seems to be correct. The way that the shadows are calculated seems to be correct because the circles clearly work. And I'm assigning the ray values and the outrecord values fairly identical to the sphere. I'm beginning to think its not in this class, but I've searched my other classes and they all seem to be correct." CreationDate="2018-02-22T15:55:54.573" UserId="8246" />
  <row Id="9681" PostId="6100" Score="0" Text="Hi, I'm sorry to bring this up again. I'm noticing that your explanation, for which I've managed to use as inspiration to work out some math, is missing the $\cos$ factor. Would your splitting explanation still be valid?" CreationDate="2018-02-22T18:41:56.520" UserId="228" />
  <row Id="9682" PostId="6319" Score="0" Text="You can use multiple GBuffers instead (Deep GBuffers) to capture multiple layers of transparency; which is still &quot;deferred&quot; shading." CreationDate="2018-02-22T18:44:07.040" UserId="2287" />
  <row Id="9683" PostId="6100" Score="0" Text="@user8469759 oh i missed that in my explanation. I might be wrong but: Similarly to the $1/N$, i think it should be kept on both sides. This is because the $\cos$ is sort of a conversion from one domain of light direction to another. You can think of it like when you do substitution while solving an integral; you should keep the &quot;$du/dx$&quot;" CreationDate="2018-02-22T19:25:35.480" UserId="5703" />
  <row Id="9684" PostId="6275" Score="0" Text="@russ - true that, but thinking from his perspective, even tho the final Z value is A*Z + B, we still have the original vector IF we aren't overwriting/ storing the modified vector in the original variable. So he could argue we still have the original Z value stored. That's why tried to give an explanation where the division factor isn't just &quot;-z&quot; it's something entirely else." CreationDate="2018-02-22T19:32:53.403" UserId="6046" />
  <row Id="9685" PostId="6100" Score="0" Text="Not sure I'm following" CreationDate="2018-02-22T19:54:23.800" UserId="228" />
  <row Id="9686" PostId="6275" Score="0" Text="&quot;why not just use the space for a single float more and get rid of this stupid check&quot; Why not let the type system keep track of which things are points and which vectors at compile time, and get rid of this stupid extra float that needs to be multiplied and added at runtime " CreationDate="2018-02-22T21:30:48.550" UserId="48" />
  <row Id="9687" PostId="6275" Score="0" Text="Also FYI, to the point about SIMD, storing an xyzw vector as a SIMD register actually isn't best practice. Better is to go SOA and have one register of 4 vectors' X values, another of the 4 Y values, etc. This generalizes a lot better to larger SIMD widths. Note that a lot of newer CPUs have 8-wide SIMD, and GPUs have 32- or 64-wide SIMD." CreationDate="2018-02-22T21:37:52.833" UserId="48" />
  <row Id="9688" PostId="6100" Score="0" Text="The $\cos\theta$ is a conversion factor from the light incoming from direction $\theta$ to the intensity of light projected on the surface." CreationDate="2018-02-22T23:01:17.830" UserId="5703" />
  <row Id="9689" PostId="6196" Score="0" Text="Sorry for the wait...ended up getting caught up with bdpt and forgetting about materials for a while :). It sounds like GGX/Smith is a strictly specular NDF and Oren-Nayar is a strictly diffuse NDF (i.e. it's meaningless to expect diffuse illumination from GGX/Smith or specular illumination from Oren-Nayar, even if you swap out the underlying macrosurfaces), is that right?" CreationDate="2018-02-22T23:53:32.453" UserId="7868" />
  <row Id="9691" PostId="6314" Score="0" Text="Can we see the code which generates the ray towards the light source ? Maybe a quick debug check you can do is to output per-pixel object normals to your image buffer to make sure nothing weird is going on there" CreationDate="2018-02-23T02:50:01.423" UserId="3073" />
  <row Id="9692" PostId="6329" Score="0" Text="www.scratchapixel.com has a great introduction to graphics." CreationDate="2018-02-23T03:16:32.843" UserId="3073" />
  <row Id="9693" PostId="6196" Score="0" Text="Yes, that is (mostly) correct. One thing about Oren-Nayar is that it incorporates some aspect of specular lights, because it has some view dependency. However, there are no real highlights, there is just a shadowing effect where $n \cdot v \rightarrow 0$ (with $n$ is surface normale and $v$ is view vector), so I'd not go as far and say that that effect is real specular lighting effect." CreationDate="2018-02-23T06:35:48.447" UserId="7008" />
  <row Id="9694" PostId="6330" Score="0" Text="If the eye is at the origin then the view vector is the negative position (usually also normalized to have length 1)" CreationDate="2018-02-23T06:47:34.667" UserId="5703" />
  <row Id="9695" PostId="6332" Score="0" Text="It is very common (specially in more advanced brdfs) to use the vector that points from the shaded fragment to the eye as the view vector instead. It would be good if you mentioned this in your answer." CreationDate="2018-02-23T07:01:53.580" UserId="5703" />
  <row Id="9696" PostId="6332" Score="0" Text="you are right, i'll add it" CreationDate="2018-02-23T07:11:07.477" UserId="7008" />
  <row Id="9697" PostId="6331" Score="0" Text="It would be helpful to see the code for getBarycentricCoordinate." CreationDate="2018-02-23T08:41:46.640" UserId="7724" />
  <row Id="9698" PostId="6275" Score="0" Text="@Nathan Reed - don't understand what you meant by &quot;type system&quot; in your first comment? If you meant something like separate classes for points and vector each having a `*` operator defined for itself and a matrix and the actual `1` or `0` hardcoded inside that function, then it isn't applicable in for example shaders." CreationDate="2018-02-23T09:10:12.350" UserId="6046" />
  <row Id="9699" PostId="6329" Score="0" Text="Perhaps you could also try Morgan McGuire's Graphics Codex: http://graphicscodex.com/ &#xA;&#xA;It's both an e-text book and, AFAICS, a set of programming projects. (Note, I've not tried it myself)" CreationDate="2018-02-23T09:10:56.930" UserId="209" />
  <row Id="9700" PostId="6100" Score="0" Text="I can understand the meaning but my point is that Karis formula doesn't take into account that when he splits the formula. Also, if $f_r$ is low frequency, which means essentialy the lobe looks more and more a diffuse one, the cosine factor would change that shape hence the splitting can't be applied anymore. You could though apply the splitting if the light is low frequency, in the sense that the intensity doesn't change much with the direction." CreationDate="2018-02-23T10:32:13.990" UserId="228" />
  <row Id="9701" PostId="6325" Score="0" Text="I feel like this is pretty close, but I'd love a slightly more in-depth answer - specifically how this texture lookup works and *why* specifically this method results in artifacts of the shadow on walls and ceilings." CreationDate="2018-02-23T11:02:52.883" UserId="8245" />
  <row Id="9703" PostId="6196" Score="0" Text="Great, thank you! My mostly-unfinished materials system defines a &quot;material&quot; as a vector of BRDF probabilities; would I get reasonable results if I had two randomly-selected + separate specular/diffuse microfacet BRDFs (given distinct specular/diffuse roughnesses + PDFs for the individual BRDFs)?" CreationDate="2018-02-23T13:08:25.943" UserId="7868" />
  <row Id="9704" PostId="6196" Score="0" Text="I am not entirely sure, how you are doing this. If you want to know, if you can reasonable results by combining an arbitrary diffuse BRDF with an arbitrary specular BRDF, then yes, it is possible. In fact, if you look through somewhat modern games and papers, that's what people have been doing for quite some time - they choose one BRDF for diffuse and one for specular lighting and then manually tweak the properties until the result looks good. The combinations are up to the teams: A lot of people used GGX+Lambert, but also Beckmann+Lamber or either specular BRDF with Oren-Nayar has been used." CreationDate="2018-02-23T13:37:22.687" UserId="7008" />
  <row Id="9705" PostId="6325" Score="0" Text="@SteffanDonal fair point. I will add some details of what I had in mind." CreationDate="2018-02-23T14:19:02.863" UserId="5717" />
  <row Id="9706" PostId="6331" Score="0" Text="@Reynolds I just edit and add the code for getBarycentricCoordinate" CreationDate="2018-02-23T15:41:40.333" UserId="8191" />
  <row Id="9707" PostId="6196" Score="0" Text="Suppose I only want to deal in combinations of pure BXDFs (mostly for code simplicity). Each axis of the material probability vector stores the chance of selecting one of these pure BXDFs (e.g. 40% of the rays incident with $(0.4, 0.6)$ will be processed with a pure difffuse BRDF), and I render the material by accumulating as many samples as possible and using the probabilities of each BRDF to weigh their value appropriately." CreationDate="2018-02-23T19:40:33.560" UserId="7868" />
  <row Id="9708" PostId="6196" Score="0" Text="So my question is, will this work if the pure diffuse/pure specular contributors are microfacet BRDFs? From the sounds of your comment (+ the note about blending being possible so long as the result is normalized) it sounds like it will, but I just want to make sure anyways" CreationDate="2018-02-23T19:49:04.033" UserId="7868" />
  <row Id="9709" PostId="6334" Score="2" Text="What actually is your question?" CreationDate="2018-02-23T23:35:14.650" UserId="2041" />
  <row Id="9710" PostId="6334" Score="0" Text="Hi @DanHulme, sorry for not being clear. I'm looking for exchange of idea/insights and FOSS recommendation I shall explore in." CreationDate="2018-02-24T04:45:27.417" UserId="8260" />
  <row Id="9711" PostId="6334" Score="1" Text="Open-ended questions are not a good fit for this forum and recommendations are strictly off-topic as described in the [What topics can I ask here](https://computergraphics.stackexchange.com/help/on-topic) help page." CreationDate="2018-02-24T06:23:25.957" UserId="3003" />
  <row Id="9712" PostId="6334" Score="0" Text="@user1118321 oh, i apologize. As i see some users asked question type like this and peoples answered, guess i should check properly next time. Btw, do you have any advice on where is the appropriate place to ask?" CreationDate="2018-02-24T06:58:08.537" UserId="8260" />
  <row Id="9714" PostId="6331" Score="0" Text="What language are you using? Having negative barycentric coordinates means you're sampling *outside* of the triangle." CreationDate="2018-02-24T22:57:21.890" UserId="1981" />
  <row Id="9715" PostId="6339" Score="0" Text="Ah! I was thinking the inverse preserved original multiplication order so it effectively created a mirror transformation, but it doesn't. Does this explain why it works to use it as the view matrix - because any subsequent rotations of the camera will end up being done first, thus keeping an object in view?" CreationDate="2018-02-25T21:14:37.390" UserId="8268" />
  <row Id="9716" PostId="6339" Score="0" Text="No, the inverse undoes the original transform, so multiplying the two together will give you an identity matrix. Not quite sure what you're asking about the view matrix, but a view matrix in general is the inverse of the camera's transform - it moves the camera to the origin with no rotation. Applying this matrix to the other objects in your scene means that as your camera moves through the world, after applying your view matrix the camera is a fixed point, and the world is moving around it with the inverse motion." CreationDate="2018-02-26T06:24:10.137" UserId="1937" />
  <row Id="9720" PostId="6325" Score="0" Text="Great update, thank you!" CreationDate="2018-02-26T16:59:25.340" UserId="8245" />
  <row Id="9721" PostId="6334" Score="0" Text="You could try [softwarerecs.se] but the requirements will still need to be very clear. See their [on topic page](https://softwarerecs.stackexchange.com/help/on-topic) for further information." CreationDate="2018-02-26T17:51:54.223" UserId="231" />
  <row Id="9722" PostId="6295" Score="0" Text="I wonder if this site should even exit. The answer rate here is too low." CreationDate="2018-02-26T18:32:51.880" UserId="213" />
  <row Id="9723" PostId="6328" Score="0" Text="Not necessarily. The main lights that cover the whole scene need to be done for each pixel, right enough. But the less-important lights that only affect a small local area can be rasterized as geometry, and a pixel shader run additively over just their area of effect. Good illustration of this technique used in GTA5 at http://www.adriancourreges.com/blog/2015/11/02/gta-v-graphics-study-part-2/  (last 2 slides)" CreationDate="2018-02-27T05:25:25.383" UserId="1937" />
  <row Id="9724" PostId="6328" Score="0" Text="@russ that is not really related to my answer or to the nature of deferred shading. What you propose is just a technique for LODing distant light sources" CreationDate="2018-02-27T05:30:40.267" UserId="5703" />
  <row Id="9725" PostId="6328" Score="0" Text="Did you look at the last couple of slides where they're rendering the streetlamps and car lights at night? I'm not talking about the billboarding of distant light sources, rather the technique of rasterizing a sphere, cone, or whatever that covers a light's area of effect, and running an additive pixel shader only for the pixels it covers." CreationDate="2018-02-27T07:08:59.060" UserId="1937" />
  <row Id="9726" PostId="6328" Score="0" Text="Seems the term I'm looking for is 'Light Volumes'. Better illustration here - http://www.spellcasterstudios.com/?p=339" CreationDate="2018-02-27T07:25:38.953" UserId="1937" />
  <row Id="9727" PostId="6328" Score="0" Text="@russ ah i see what you mean. I will edit my answer to touch on that subject later." CreationDate="2018-02-27T07:29:20.873" UserId="5703" />
  <row Id="9728" PostId="6173" Score="1" Text="Given that you are only doing half-toning, you could probably get away with a *much* cheaper approximation of gamma/sRGB such as using a square root." CreationDate="2018-02-27T08:46:41.673" UserId="209" />
  <row Id="9729" PostId="6313" Score="0" Text="There were more earlier games from 90's :) Those were incredible in their times." CreationDate="2018-02-27T11:00:16.990" UserId="8249" />
  <row Id="9730" PostId="6343" Score="0" Text="Yes this is the technique the person is looking for.  It is perfectly crisp because the shadows are actual rendered geometry.  It would be nice if you could provide a simple description of the technique!&#xA;Wikipedia has some decent info too: https://en.wikipedia.org/wiki/Shadow_volume" CreationDate="2018-02-27T14:49:51.247" UserId="56" />
  <row Id="9731" PostId="6313" Score="1" Text="https://en.wikipedia.org/wiki/Shadow_volume &lt;-- that" CreationDate="2018-02-27T14:50:03.383" UserId="56" />
  <row Id="9732" PostId="6278" Score="0" Text="A 2d mesh or a surface mesh?" CreationDate="2018-02-27T17:32:05.100" UserId="192" />
  <row Id="9733" PostId="6343" Score="0" Text="@AlanWolfe Shadow volumes would explain the crispness, however does not fit the OP's statement that the shadows extend above the character (closer to the light than the shadow caster). The classic shadow volume algorithm doesn't have that artifact." CreationDate="2018-02-27T21:41:03.067" UserId="48" />
  <row Id="9734" PostId="4563" Score="0" Text="I read this comment so I joined computergraphics and upvoted this answer." CreationDate="2018-02-27T22:48:06.533" UserId="8289" />
  <row Id="9739" PostId="6173" Score="0" Text="@Wyck a typical display is non calibrated." CreationDate="2018-02-28T11:08:03.270" UserId="38" />
  <row Id="9740" PostId="6343" Score="0" Text="Exactly @NathanReed - Shadow volumes were one of my first research points but it just didn't seem right." CreationDate="2018-02-28T12:49:34.270" UserId="8245" />
  <row Id="9741" PostId="6278" Score="0" Text="Edited to say 3D mesh." CreationDate="2018-02-28T18:57:19.157" UserId="7622" />
  <row Id="9742" PostId="6351" Score="1" Text="did you profile and see which steps take a long time?" CreationDate="2018-03-01T15:30:38.810" UserId="137" />
  <row Id="9743" PostId="6351" Score="1" Text="&quot; (Admittedly using python). Are there alternative solutions ?&quot; C++ is a good alternative. How fast do you need it to go? It may simply be unachievable in Python." CreationDate="2018-03-01T15:32:09.767" UserId="2041" />
  <row Id="9744" PostId="6351" Score="1" Text="@ratchet freak: Will do and post resuts." CreationDate="2018-03-01T16:31:12.697" UserId="8303" />
  <row Id="9745" PostId="6351" Score="0" Text="You can 1) try C++   2) try to convert your code to make it work on the gpu  3) using spatial data structures" CreationDate="2018-03-01T17:42:42.857" UserId="6046" />
  <row Id="9746" PostId="6350" Score="0" Text="Perhaps I'm misunderstanding what you're doing, but why does the light increase from 1 to 4 at a distance of 0.5? Shouldn't it decrease everywhere that's not the center of the light?" CreationDate="2018-03-02T03:13:31.550" UserId="3003" />
  <row Id="9748" PostId="6351" Score="0" Text="Are you just firing primary rays or are you also  doing secondaries, e.g. reflections?" CreationDate="2018-03-02T09:49:13.113" UserId="209" />
  <row Id="9749" PostId="6353" Score="0" Text="At the first boundary, wouldn't D^2 + 1 be 1.25?" CreationDate="2018-03-03T05:21:11.770" UserId="3003" />
  <row Id="9750" PostId="6357" Score="0" Text="Thanks. I don't yet know how to obtain the time. I am using Windows 10." CreationDate="2018-03-03T05:47:17.710" UserId="5183" />
  <row Id="9751" PostId="6357" Score="0" Text="I haven't dealt with Windows in a few years, but [this question](https://stackoverflow.com/questions/3729169/how-can-i-get-the-windows-system-time-with-millisecond-resolution) on Stack Overflow seems like it might have some good ideas. At least there are lots of different answers so one of them will probably do the trick." CreationDate="2018-03-03T05:50:50.153" UserId="3003" />
  <row Id="9752" PostId="6353" Score="0" Text="Yeah, so you'd have an intensity of 0.8" CreationDate="2018-03-03T09:26:29.957" UserId="1937" />
  <row Id="9753" PostId="6357" Score="0" Text="If you want high precision time in the win32 API you can use QueryPerformanceCounter and QueryPerformanceFrequency" CreationDate="2018-03-03T11:09:18.030" UserId="3073" />
  <row Id="9754" PostId="6350" Score="0" Text="It increases because I am using 1/(distance*distance) as falloff, so 1/(0.5*0.5) = 4" CreationDate="2018-03-03T13:43:15.103" UserId="8305" />
  <row Id="9756" PostId="6353" Score="0" Text="&quot;to get the right attenuation you need to track the total distance back to the light source and square that&quot;  &#xA;&#xA;Why is that? What is the difference between a light source with intensity 1 at distance 2 and a light source with intensity 0.25 at distance 1?  &#xA;&#xA;The +1 constant does get more reasonable results, but it still doesn't seem physically correct.  &#xA;&#xA;I'll look at tracking individual rays/distances, but I was hoping to aggregate light at each point in the grid, so I don't &quot;know&quot; the original distance." CreationDate="2018-03-03T14:06:30.700" UserId="8305" />
  <row Id="9757" PostId="6353" Score="0" Text="Because the distance to the original light is what matters for intensity. The ratios of the squares change as you move away from the light. Let's say you have a grid size of 2 - when moving from distance 2 to distance 4, the light should get 4x dimmer, but when moving from distance 10 to 12 it will only get 1.44x dimmer." CreationDate="2018-03-03T14:28:53.433" UserId="1937" />
  <row Id="9759" PostId="6357" Score="0" Text="Is it possible to do this using frame rate instead of time?" CreationDate="2018-03-04T01:23:39.743" UserId="5183" />
  <row Id="9760" PostId="6357" Score="0" Text="If you know the frame rate and know how many frames you've already processed, then you can calculate the time by multiplying the frame rate by the number of frames." CreationDate="2018-03-04T01:58:05.843" UserId="3003" />
  <row Id="9761" PostId="5681" Score="1" Text="You can make your pool allocate groups of objects, say 64 objects at a time when requesting an object, by linking the first object in that block to your 'used list' and the other 63 to your 'free list' to avoid hammering the memory allocator." CreationDate="2018-03-04T06:16:54.823" UserId="3073" />
  <row Id="9762" PostId="6357" Score="0" Text="1/FPS = time taken per frame, so yes you can use that. However to calculate FPS you would have needed to take a snapshot of some timer anyway ?" CreationDate="2018-03-04T06:19:32.640" UserId="3073" />
  <row Id="9763" PostId="319" Score="0" Text="link don't work" CreationDate="2018-03-04T08:25:21.580" UserId="7220" />
  <row Id="9764" PostId="362" Score="0" Text="link don't work" CreationDate="2018-03-04T08:26:58.750" UserId="7220" />
  <row Id="9765" PostId="6350" Score="0" Text="Are you like... propagating your light one grid cell at a time, without knowing where it originally came from? That's a bit what it looks like but I'm uncertain I understand what you're trying to do." CreationDate="2018-03-04T15:13:23.077" UserId="5717" />
  <row Id="9767" PostId="6350" Score="0" Text="Yep. Each cell stores its light output in various directions. A shader runs for each grid cell, and is responsible for taking light from neighboring cells, casting it through the current cell (reflecting it if it hits something, in this case it doesn't), and writing the result to the current cell. So I always know the position and approx direction of the light, but I don't know where it came from (because all light in a particular direction is combined)" CreationDate="2018-03-04T16:21:09.353" UserId="8305" />
  <row Id="9769" PostId="1746" Score="0" Text="xyY is indeed linear, normalized." CreationDate="2018-03-04T18:38:52.920" UserId="5556" />
  <row Id="9770" PostId="6363" Score="0" Text="&quot;Pls give teh codez&quot; questions are not suitable for this site. Please try on your own and then tell us where you got stuck so we can help you." CreationDate="2018-03-04T19:54:01.397" UserId="2041" />
  <row Id="9772" PostId="1746" Score="0" Text="@troy_s Its energetically linear, but its not linear in percieved color distance. Its just really hard to make a space that is uniform in perceptual distance between 2 points." CreationDate="2018-03-04T20:51:57.023" UserId="38" />
  <row Id="9773" PostId="5520" Score="0" Text="I would be suspicious of this line&gt; Vector samplePositionLight = samplePosition + sunDirection * (tCurrentLight + segmentLengthLight * rand.NextDouble());     &gt;&gt; This should walk the sample-to-light in even steps, why is it doing a random length walk ?" CreationDate="2018-03-05T08:06:14.887" UserId="3073" />
  <row Id="9774" PostId="5520" Score="0" Text="I've implemented this algorithm before and also found it unnecessary to generate multiple rays for each 'pixel'. A single ray, which raymarches from the view direction, and for each of those steps marches towards the light was enough to get high quality results. Have you tried just one call to ComputeIncidentLight per pixel? (Also see my previous post, you have modified the sample-&gt;light ray march in a way that may have broken it)" CreationDate="2018-03-05T08:55:51.550" UserId="3073" />
  <row Id="9775" PostId="5520" Score="0" Text="@PaulHK, well even with one sample ppx you still need 16 x 8 samples for marching algorithm. You may think this isn't costly but if you do, then you may use Monte Carlo method instead of marching the along the ray in steps." CreationDate="2018-03-05T12:16:17.097" UserId="6041" />
  <row Id="9776" PostId="6348" Score="0" Text="I haven't seen any library which provides this sort of functionality and is GPU-accelerated as it'd be quite cumbersome. The only things I've seen close are CUDA's libraries for math, linear algebra etc.&#xA;&#xA;You can either code your data structure and routines manually which again would be pita OR instead of going for the GPU why not go towards multi-threading on the cpu. Check out OpenMP and for ray-intersection with polyhedra you could construct a spatial data structure if there are many of them." CreationDate="2018-03-05T16:48:42.700" UserId="6046" />
  <row Id="9777" PostId="6314" Score="0" Text="what Paul said, I don't see the code anywhere for shadows, its just for intersections. How do you compute your shadows?" CreationDate="2018-03-05T16:58:05.927" UserId="6046" />
  <row Id="9778" PostId="6357" Score="0" Text="Thanks. I think the question I am really asking is the following: What I want to do is make sure the speed of my motion is the same regardless of whether I oscillate between 0 and 1 or between 0 and 10. In the response, user1118321 says &quot;You can make it go from 0 to 10 by multiplying the result by 10:..&quot;, but this seems to increase the speed of my motion. How do I maintain speed?" CreationDate="2018-03-05T20:10:04.313" UserId="5183" />
  <row Id="9779" PostId="1746" Score="0" Text="Perceptually uniform is a far better term than &quot;linear&quot;. There is already enough stupidity around that term." CreationDate="2018-03-05T21:51:07.897" UserId="5556" />
  <row Id="9780" PostId="6370" Score="1" Text="you should check wikipedia for converting from Axis-angle to euler angle rotations." CreationDate="2018-03-05T22:26:45.147" UserId="6046" />
  <row Id="9781" PostId="6360" Score="0" Text="Never heard of anything like that. And it doesn't make sense, `glTex[ture]Storage` allocates, `glTex[ture]SubImageXD` copies. Those are two entirely orthogonal concepts which modern GL makes a big deal about explicitly separating. I'm pretty sure they just mean the latter there. However, with the SuperBible I also don't really know if they might not have added their own custom functions (even if it would be odd to name them like genuine GL calls)." CreationDate="2018-03-06T01:53:42.637" UserId="6" />
  <row Id="9782" PostId="5520" Score="0" Text="But stepping by rand.NextDouble() in numLightSamples steps means you don't end up at isectSun.Dist as the original code does, this i think is the cause of your problem." CreationDate="2018-03-06T02:08:50.527" UserId="3073" />
  <row Id="9783" PostId="6357" Score="1" Text="When you multiply the rotation by 10 you're changing the amplitude of the rotation, but not the frequency. So it will rotate 10 degrees per second instead of 1 degree per second. If you want to also change the frequency to match, you should divide time by 10. Then you'll get 10 degrees per 10 seconds, which is still 1 degree per second." CreationDate="2018-03-06T02:28:53.340" UserId="3003" />
  <row Id="9784" PostId="5520" Score="0" Text="Yes 16x8 samples/pix is heavy, you can reduce this a little and get away with good results (usually lower samples makes the atmosphere shadow area more coarse). You could also look into rendering the atmosphere at a lower resolution and upscale" CreationDate="2018-03-06T03:56:59.813" UserId="3073" />
  <row Id="9785" PostId="1746" Score="0" Text="@troy_s Right, good name for it, changed. I was actually sitting here after answering and thinking asking a question on mathematics ehat would be the minimum requirement for linear. So as to check would linear ever eeven qualify for color." CreationDate="2018-03-06T05:38:31.507" UserId="38" />
  <row Id="9786" PostId="5139" Score="0" Text="What texture wrapping mode for the depth texture do you have set ? I have a feeling it should be CLAMP" CreationDate="2018-03-06T06:58:20.157" UserId="3073" />
  <row Id="9787" PostId="6373" Score="0" Text="Stereographic projection implies projecting the surface of a sphere onto a plane. So unless your model is also spherical you're gonna get self-intersections on the plane. Is this what you want?" CreationDate="2018-03-06T10:53:43.707" UserId="1937" />
  <row Id="9789" PostId="6374" Score="1" Text="SNORMs and UNORMs are signed/unsigned *integers* divided by (2^M-1)" CreationDate="2018-03-06T13:48:32.523" UserId="209" />
  <row Id="9790" PostId="6374" Score="0" Text="oh, I see. So all bits go directly into encoding the variable, rather than splitting it into exponent and mantissa, thus the (slightly) better precision?" CreationDate="2018-03-06T13:49:53.247" UserId="7008" />
  <row Id="9791" PostId="6374" Score="2" Text="Well, all the bits are used in floats too :-)   Perhaps a better description is that range is more limited and that the precision throughout the range is uniform, rather than, as with floats, variable.  FWIW colour channel data, e.g,. RGB888, uses 8-bit unorms." CreationDate="2018-03-06T13:54:06.593" UserId="209" />
  <row Id="9792" PostId="6135" Score="0" Text="Please, do not add another question to this post. If there is anything else you need help with, then post another question." CreationDate="2018-03-06T15:49:42.347" UserId="5703" />
  <row Id="9793" PostId="6373" Score="0" Text="@russ It is nearly spherical (a polyhedron)." CreationDate="2018-03-06T16:53:36.207" UserId="8325" />
  <row Id="9794" PostId="6375" Score="1" Text="Nice answer but the part about 3 vs 5 digits of precision is misleading. It makes it sound as if a normalized integer is always better. Floating point has uniform relative precision while a normalized integer has uniform absolute precision. It just happens that the latter is what you want for model vertices." CreationDate="2018-03-06T18:48:54.933" UserId="5717" />
  <row Id="9795" PostId="6375" Score="0" Text="I don't need really large position values, this was probably badly stated on my part. What I meant was models at &quot;really large [i.e. far away from origin] positions in the world&quot;. Imagine a car simulator that spans more than 500km * 500km levels. For some reason I missinterpreted one of the articles sentence to say something about this. In any case, thanks for the answer, I'll edit the &quot;disservice&quot; question to mark it for future readers." CreationDate="2018-03-07T06:47:02.287" UserId="7008" />
  <row Id="9797" PostId="6348" Score="0" Text="FWIW GPUs excel at computing results in image space. Do you need the actual geometric data of, say, union/intersection/difference or just *images* of the results of these operations?" CreationDate="2018-03-07T11:53:37.530" UserId="209" />
  <row Id="9798" PostId="6373" Score="0" Text="Do you need an actual stereographic projection? Or just any conformal one-to-one projection to the plane would also work. There are better approaches to map surfaces (or any topological disk) to a plane. You acn even map a topological sphere (closed surface) to the plane without seams decomposing it in two charts. Closed surface of high genus can be also mapped but would require more charts." CreationDate="2018-03-08T01:17:29.530" UserId="7957" />
  <row Id="9799" PostId="6373" Score="0" Text="@MauricioCeleLopezBelon I wanted sterographic, but that works too." CreationDate="2018-03-08T01:18:27.273" UserId="8325" />
  <row Id="9800" PostId="6380" Score="0" Text="Are you sure it's using Metal? Perhaps those files are from the iOS version but are unused on Android and they just forgot to remove them." CreationDate="2018-03-08T02:29:18.487" UserId="3003" />
  <row Id="9801" PostId="6380" Score="0" Text="Yeah it says in the config json files metal api true which means it is enabled, im trying to understand why they would have it in there, as the android development team is seperate from ios team, it realy should be vulcan api for best android performance, the game looks like crap after this recent update." CreationDate="2018-03-08T03:26:42.110" UserId="8334" />
  <row Id="9802" PostId="6383" Score="0" Text="Are you sure scene.objects[k].vertices.size() is the same size as the number of indices?" CreationDate="2018-03-08T11:09:24.993" UserId="7724" />
  <row Id="9803" PostId="6348" Score="1" Text="@SimonF Indeed I would need the results of the operation. Image data (i.e. the 2D-Projection) is usually not needed." CreationDate="2018-03-08T12:02:51.987" UserId="8297" />
  <row Id="9804" PostId="6383" Score="0" Text="@Reynolds No, it doesn't. But I thought the whole point of having indices is that you dont need to duplicate vertices? Because one vertices could belong to many triangles" CreationDate="2018-03-08T13:51:14.133" UserId="8191" />
  <row Id="9805" PostId="6383" Score="0" Text="The second arguments is the number of indices you want to read from your array indices. So it should be indices.size()." CreationDate="2018-03-08T14:28:44.317" UserId="7724" />
  <row Id="9806" PostId="6383" Score="0" Text="@Reynolds Oh my, thank you very much. I find openGL documentation very confusing." CreationDate="2018-03-08T14:33:57.473" UserId="8191" />
  <row Id="9807" PostId="6386" Score="0" Text="What are you planning to use the tangent plane for? Texturing?" CreationDate="2018-03-08T20:53:07.643" UserId="506" />
  <row Id="9808" PostId="6386" Score="0" Text="Does [this Mathematics Stack Exchange answer](https://math.stackexchange.com/questions/752252/find-plane-by-normal-and-instance-point-distance-between-origin-and-plane) provide enough information?" CreationDate="2018-03-09T03:12:08.377" UserId="3003" />
  <row Id="9809" PostId="6387" Score="0" Text="It's speculation but [this reddit thread](https://www.reddit.com/r/pcgaming/comments/3xgmz9/vulkan_api_specification_is_complete_and/) suggests that it's perhaps &quot;graphics rendering&quot;." CreationDate="2018-03-09T03:29:04.543" UserId="3003" />
  <row Id="9810" PostId="6386" Score="0" Text="@Noah Witherspoon, i will embed an image into this tangent plane. (So it will look like so natural)" CreationDate="2018-03-09T04:39:59.687" UserId="8130" />
  <row Id="9811" PostId="6386" Score="0" Text="@user1118321, i will look and test that question immediately. I hope it will work. I will feedback about it. Thanks" CreationDate="2018-03-09T04:41:59.217" UserId="8130" />
  <row Id="9812" PostId="6377" Score="0" Text="@Dan Hulme  Can you help me?" CreationDate="2018-03-09T07:36:47.067" UserId="8048" />
  <row Id="9813" PostId="6387" Score="0" Text="This is purely speculative, but another low-level, explicit, IHV-specific used the &quot;gr&quot; prefix for its function names: [GLIDE](http://www.gamers.org/dEngine/xf3D/glide/glidepgm.htm)." CreationDate="2018-03-09T07:55:29.940" UserId="2654" />
  <row Id="9814" PostId="6377" Score="0" Text="I can't. Don't forget that you're the only person who can run this code, so it's going to be harder for anyone else to find the bug. You just need to debug it the same way as any other program. You shouldn't expect to just find the bug by looking at the source and guessing: software development doesn't work that way." CreationDate="2018-03-09T08:23:44.273" UserId="2041" />
  <row Id="9815" PostId="6380" Score="0" Text="Is an entry in a JSON file all you have to go on? Maybe it's the same entry in both platforms and it's just not used on Android." CreationDate="2018-03-09T10:19:02.160" UserId="2041" />
  <row Id="9816" PostId="6389" Score="0" Text="are you calling this method in every frame? i don't see it affecting anything, still interesting to know. also, my guess is a problem in the shader rather than your c++ code, can you share that?" CreationDate="2018-03-09T15:47:19.950" UserId="7008" />
  <row Id="9817" PostId="6380" Score="0" Text="Yes dan all i found was this in the Engine config json, it says ios metal driver enabled : true." CreationDate="2018-03-09T17:23:55.663" UserId="8334" />
  <row Id="9818" PostId="6380" Score="0" Text="I admit i know a small amount about graphics engines and drivers but not a lot, hence me coming here to ask you guys." CreationDate="2018-03-09T17:25:43.170" UserId="8334" />
  <row Id="9819" PostId="6377" Score="0" Text="Your reflect method might not be well implemented. Add that to the post" CreationDate="2018-03-09T18:19:58.810" UserId="5703" />
  <row Id="9820" PostId="6390" Score="1" Text="don't understand what is the use of spherical coordinates? and why is the ray originating at image plane isn't there a common point (eye) for example?" CreationDate="2018-03-09T18:47:06.217" UserId="6046" />
  <row Id="9821" PostId="6378" Score="0" Text="What units are you using for earthRadius? AFAIK this algorithm assumes metres." CreationDate="2018-03-10T06:12:00.463" UserId="3073" />
  <row Id="9822" PostId="6348" Score="0" Text="I think you can accelerate your algorithms more easily using heterogeneous computing, meaning using a mix of CPUs and GPUs. For that you have several technologies available: C++ AMP, AMD HCC, C++ SYCL, OpenCL" CreationDate="2018-03-10T11:37:19.470" UserId="7957" />
  <row Id="9823" PostId="6390" Score="0" Text="Can't you just put the point ($r$,$\theta$,$\phi$) in camera coordinates and form a line with the eye point (0,0,0) and find the point of intersection of that line and the projection plane with normal (0,0,1)?" CreationDate="2018-03-10T13:21:30.023" UserId="7957" />
  <row Id="9824" PostId="6392" Score="2" Text="I was able to remove it by going to &lt;https://vectortoons.com/product/sad-men-collection/&gt; and paying the $10 for it." CreationDate="2018-03-10T17:02:09.650" UserId="3003" />
  <row Id="9825" PostId="6392" Score="0" Text="This isn't a site for helping you pirate stock images. If you have an actual question that relates to graphics algorithms and research, please [edit] your question so that it can be reopened." CreationDate="2018-03-10T18:19:39.453" UserId="2041" />
  <row Id="9826" PostId="6388" Score="0" Text="I think the OP wants arbitrary regular polygons though, like in the Desmos they linked, not just circles." CreationDate="2018-03-11T02:14:35.797" UserId="48" />
</comments>